samples  5000  graph  10 30 ER mlp  minibatch size  100  noise  0.8  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.1229782946562032
iteration 1 in outer loop, alpha = 1.1229782946562032, rho = 1.0, h = 1.1229782946562032
cuda
iteration 1 in inner loop,alpha 1.1229782946562032 rho 1.0 h 0.7586153562268656
iteration 2 in inner loop,alpha 1.1229782946562032 rho 10.0 h 0.35052850700977345
iteration 3 in inner loop,alpha 1.1229782946562032 rho 100.0 h 0.11468940960946838
iteration 2 in outer loop, alpha = 12.591919255603042, rho = 100.0, h = 0.11468940960946838
cuda
iteration 1 in inner loop,alpha 12.591919255603042 rho 100.0 h 0.05177271318199317
iteration 2 in inner loop,alpha 12.591919255603042 rho 1000.0 h 0.015651355822457802
iteration 3 in outer loop, alpha = 28.243275078060844, rho = 1000.0, h = 0.015651355822457802
cuda
iteration 1 in inner loop,alpha 28.243275078060844 rho 1000.0 h 0.0076855114245848455
iteration 2 in inner loop,alpha 28.243275078060844 rho 10000.0 h 0.0013998791853246928
iteration 4 in outer loop, alpha = 42.242066931307775, rho = 10000.0, h = 0.0013998791853246928
cuda
iteration 1 in inner loop,alpha 42.242066931307775 rho 10000.0 h 0.0007027753513231261
iteration 2 in inner loop,alpha 42.242066931307775 rho 100000.0 h 0.00030411348816983264
iteration 5 in outer loop, alpha = 72.65341574829104, rho = 100000.0, h = 0.00030411348816983264
cuda
iteration 1 in inner loop,alpha 72.65341574829104 rho 100000.0 h 0.0001791983137202635
iteration 6 in outer loop, alpha = 251.85172946855454, rho = 1000000.0, h = 0.0001791983137202635
Threshold 0.3
[[0.004 0.    0.381 0.    1.153 0.    1.81  0.    0.001 0.007]
 [2.236 0.002 1.567 0.    0.873 0.044 0.118 0.1   0.024 2.624]
 [0.002 0.    0.007 0.    2.014 0.    1.93  0.    0.001 0.   ]
 [1.276 0.411 1.491 0.    0.837 0.427 1.188 0.255 2.731 2.82 ]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.   ]
 [0.183 0.008 0.191 0.    0.252 0.001 0.316 0.004 0.005 2.98 ]
 [0.    0.    0.001 0.    1.63  0.    0.004 0.    0.    0.   ]
 [1.804 0.007 1.384 0.    1.752 0.109 0.273 0.002 0.001 0.099]
 [2.334 0.04  2.341 0.    0.947 0.077 0.593 0.226 0.004 0.327]
 [0.176 0.    2.517 0.    1.745 0.    2.592 0.002 0.003 0.006]]
[[0.    0.    0.381 0.    1.153 0.    1.81  0.    0.    0.   ]
 [2.236 0.    1.567 0.    0.873 0.    0.    0.    0.    2.624]
 [0.    0.    0.    0.    2.014 0.    1.93  0.    0.    0.   ]
 [1.276 0.411 1.491 0.    0.837 0.427 1.188 0.    2.731 2.82 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.316 0.    0.    2.98 ]
 [0.    0.    0.    0.    1.63  0.    0.    0.    0.    0.   ]
 [1.804 0.    1.384 0.    1.752 0.    0.    0.    0.    0.   ]
 [2.334 0.    2.341 0.    0.947 0.    0.593 0.    0.    0.327]
 [0.    0.    2.517 0.    1.745 0.    2.592 0.    0.    0.   ]]
{'fdr': 0.12903225806451613, 'tpr': 0.9, 'fpr': 0.26666666666666666, 'f1': 0.8852459016393444, 'shd': 7, 'npred': 31, 'ntrue': 30}
[1.724e-04 3.808e-01 3.379e-05 1.153e+00 4.081e-04 1.810e+00 3.070e-04
 6.309e-04 6.753e-03 2.236e+00 1.567e+00 2.266e-04 8.734e-01 4.403e-02
 1.178e-01 1.004e-01 2.385e-02 2.624e+00 2.358e-03 4.826e-05 5.173e-06
 2.014e+00 1.673e-05 1.930e+00 4.792e-04 9.228e-04 3.199e-04 1.276e+00
 4.115e-01 1.491e+00 8.369e-01 4.273e-01 1.188e+00 2.547e-01 2.731e+00
 2.820e+00 1.154e-04 4.169e-06 8.245e-05 1.259e-05 4.576e-07 4.648e-04
 5.936e-05 7.089e-05 1.730e-05 1.828e-01 8.228e-03 1.911e-01 2.287e-04
 2.521e-01 3.163e-01 4.434e-03 5.116e-03 2.980e+00 2.219e-04 3.902e-05
 6.819e-04 9.136e-06 1.630e+00 2.799e-05 7.727e-05 1.733e-05 3.261e-04
 1.804e+00 7.169e-03 1.384e+00 2.494e-04 1.752e+00 1.091e-01 2.734e-01
 9.024e-04 9.853e-02 2.334e+00 3.958e-02 2.341e+00 1.096e-04 9.470e-01
 7.738e-02 5.933e-01 2.256e-01 3.273e-01 1.756e-01 2.198e-04 2.517e+00
 6.991e-05 1.745e+00 1.212e-04 2.592e+00 2.423e-03 2.913e-03]
[[0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 1. 0. 1. 1. 0. 0. 0. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]]
[0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
aucroc, aucpr (0.98, 0.9708577841911175)
cuda
1210
cuda
Objective function 461.65 = squared loss an data 284.59 + 0.5*rho*h**2 176.710652 + alpha*h 0.000000 + L2reg 0.19 + L1reg 0.16 ; SHD = 55 ; DAG False
||w||^2 0.7574263362956288
exp ma of ||w||^2 519.3994696319241
||w|| 0.8703024395551404
exp ma of ||w|| 0.7258734782257277
||w||^2 0.11970794413192909
exp ma of ||w||^2 0.3677224010757581
||w|| 0.34598835837630304
exp ma of ||w|| 0.5864186839312269
||w||^2 0.21879152310006775
exp ma of ||w||^2 0.2537504925643501
||w|| 0.46775156130158213
exp ma of ||w|| 0.4842604096975595
cuda
Objective function 64.22 = squared loss an data 63.24 + 0.5*rho*h**2 0.690866 + alpha*h 0.000000 + L2reg 0.20 + L1reg 0.08 ; SHD = 32 ; DAG False
Proportion of microbatches that were clipped  0.7335134276179882
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.1754709660006117
iteration 1 in outer loop, alpha = 1.1754709660006117, rho = 1.0, h = 1.1754709660006117
cuda
1210
cuda
Objective function 65.60 = squared loss an data 63.24 + 0.5*rho*h**2 0.690866 + alpha*h 1.381732 + L2reg 0.20 + L1reg 0.08 ; SHD = 32 ; DAG False
||w||^2 0.5371086461346187
exp ma of ||w||^2 3.4135982126272317
||w|| 0.7328769652094537
exp ma of ||w|| 0.7117893369088464
||w||^2 0.9447687921425268
exp ma of ||w||^2 0.3370630779304229
||w|| 0.9719921769965676
exp ma of ||w|| 0.5449525555354008
||w||^2 0.7735694764238621
exp ma of ||w||^2 0.5836258886982667
||w|| 0.8795279850146112
exp ma of ||w|| 0.722597385924615
||w||^2 0.5790240661119774
exp ma of ||w||^2 0.6078602262886246
||w|| 0.7609363088406135
exp ma of ||w|| 0.7436171700867863
||w||^2 0.2728086486951682
exp ma of ||w||^2 0.6219017636427653
||w|| 0.522310873613759
exp ma of ||w|| 0.7512296688925852
cuda
Objective function 17.42 = squared loss an data 15.52 + 0.5*rho*h**2 0.395256 + alpha*h 1.045121 + L2reg 0.38 + L1reg 0.08 ; SHD = 32 ; DAG False
Proportion of microbatches that were clipped  0.750364726860107
iteration 1 in inner loop, alpha 1.1754709660006117 rho 1.0 h 0.8891078967640951
1210
cuda
Objective function 20.97 = squared loss an data 15.52 + 0.5*rho*h**2 3.952564 + alpha*h 1.045121 + L2reg 0.38 + L1reg 0.08 ; SHD = 32 ; DAG False
||w||^2 719307659.8374759
exp ma of ||w||^2 2443841760.1730742
||w|| 26819.91162993413
exp ma of ||w|| 42577.60893766922
||w||^2 145886935.2021807
exp ma of ||w||^2 320721425.25390476
||w|| 12078.366412813477
exp ma of ||w|| 14954.056602008173
||w||^2 264.7738683958725
exp ma of ||w||^2 34370.173977206054
||w|| 16.271873536746543
exp ma of ||w|| 56.10849583178939
||w||^2 0.6320682830273948
exp ma of ||w||^2 10.254340321761276
||w|| 0.7950272215637618
exp ma of ||w|| 1.0766567778288714
||w||^2 1.0079033639329165
exp ma of ||w||^2 0.8966184839088686
||w|| 1.0039439047740248
exp ma of ||w|| 0.9056756614764929
||w||^2 1.1424832412653143
exp ma of ||w||^2 0.8677682998717587
||w|| 1.0688700768874178
exp ma of ||w|| 0.8784953632065212
||w||^2 1.4020236698759025
exp ma of ||w||^2 0.9697026616533432
||w|| 1.1840708044183432
exp ma of ||w|| 0.922818716648906
||w||^2 0.4277679323039546
exp ma of ||w||^2 1.0273278350020543
||w|| 0.6540397023911886
exp ma of ||w|| 0.9451959835929297
||w||^2 0.16948645621035657
exp ma of ||w||^2 0.9510324027645601
||w|| 0.4116873282120262
exp ma of ||w|| 0.9152162361584056
||w||^2 1.6016327588006296
exp ma of ||w||^2 1.1709696151764584
||w|| 1.2655563040815805
exp ma of ||w|| 1.0071269781289351
cuda
Objective function 12.60 = squared loss an data 10.85 + 0.5*rho*h**2 0.724268 + alpha*h 0.447380 + L2reg 0.51 + L1reg 0.07 ; SHD = 25 ; DAG False
Proportion of microbatches that were clipped  0.7600252604988949
iteration 2 in inner loop, alpha 1.1754709660006117 rho 10.0 h 0.38059646211589815
1210
cuda
Objective function 19.12 = squared loss an data 10.85 + 0.5*rho*h**2 7.242683 + alpha*h 0.447380 + L2reg 0.51 + L1reg 0.07 ; SHD = 25 ; DAG False
||w||^2 16607791711.764936
exp ma of ||w||^2 73988898792.00943
||w|| 128871.22142575098
exp ma of ||w|| 242907.57482479405
||w||^2 398326198.8766204
exp ma of ||w||^2 3355261489.6324363
||w|| 19958.11110492725
exp ma of ||w|| 49041.41170738629
||w||^2 43973.86067945025
exp ma of ||w||^2 2198597.9653435173
||w|| 209.69945321686046
exp ma of ||w|| 761.4890200742955
||w||^2 2.5564331860273737
exp ma of ||w||^2 9.91835372642188
||w|| 1.598884982113277
exp ma of ||w|| 1.522582377464032
||w||^2 4.297532073281927
exp ma of ||w||^2 1.4329985657767785
||w|| 2.073048979952458
exp ma of ||w|| 1.1360539460608052
||w||^2 1.1993225188665537
exp ma of ||w||^2 1.5694751335327675
||w|| 1.095135844937309
exp ma of ||w|| 1.192082772854796
||w||^2 3.363724558844598
exp ma of ||w||^2 1.5422414331342862
||w|| 1.8340459533077675
exp ma of ||w|| 1.1709904412514052
cuda
Objective function 12.86 = squared loss an data 11.24 + 0.5*rho*h**2 0.827282 + alpha*h 0.151201 + L2reg 0.58 + L1reg 0.06 ; SHD = 18 ; DAG True
Proportion of microbatches that were clipped  0.7706065318818041
iteration 3 in inner loop, alpha 1.1754709660006117 rho 100.0 h 0.12862988857418678
iteration 2 in outer loop, alpha = 14.03845982341929, rho = 100.0, h = 0.12862988857418678
cuda
1210
cuda
Objective function 14.51 = squared loss an data 11.24 + 0.5*rho*h**2 0.827282 + alpha*h 1.805766 + L2reg 0.58 + L1reg 0.06 ; SHD = 18 ; DAG True
||w||^2 22557145812.271595
exp ma of ||w||^2 30948529872.875576
||w|| 150190.3652444843
exp ma of ||w|| 154745.27532514365
||w||^2 724287900.3602144
exp ma of ||w||^2 4628633473.588305
||w|| 26912.59742871755
exp ma of ||w|| 57539.64375200705
||w||^2 911405745.8967055
exp ma of ||w||^2 3304242507.4910107
||w|| 30189.49727797244
exp ma of ||w|| 48514.09559786127
||w||^2 109624474.55821393
exp ma of ||w||^2 430019505.730772
||w|| 10470.170703394188
exp ma of ||w|| 15823.498113912763
||w||^2 4.458783205620708
exp ma of ||w||^2 21.754945524758195
||w|| 2.1115831041237065
exp ma of ||w|| 1.7382559036948266
v before min max tensor([[ 4.793e+01, -1.843e+01, -1.941e+01,  1.100e+01,  4.411e+01,  3.635e+00,
         -3.347e+01,  5.733e+00, -1.891e+01, -1.861e+01],
        [-2.894e+01,  5.854e+01, -4.705e-01, -1.114e+01, -2.803e+01, -1.365e+01,
         -2.239e+01,  1.454e+01,  1.141e+01,  1.135e+01],
        [ 2.631e+00, -2.411e+01, -2.593e+01, -1.766e+01, -7.050e+00, -2.338e+01,
         -2.167e+01, -2.376e+01, -7.964e+00, -2.902e+00],
        [ 1.771e+02, -2.806e+01, -2.590e+01, -2.779e+00,  1.023e+01, -1.509e+01,
         -3.133e+01,  9.144e+01, -2.444e+01,  4.166e+01],
        [ 1.003e+02,  1.803e+02, -1.335e+01, -2.117e+01,  1.442e+01,  6.080e+01,
         -2.272e+01, -1.875e+01, -1.062e+01, -1.929e+01],
        [-1.083e+01,  7.370e+01, -1.568e+01, -2.852e+01, -1.741e+01, -4.665e+00,
         -2.521e+01, -2.638e+01, -5.254e+00,  7.526e+01],
        [ 4.157e+02, -3.212e+01,  8.586e+00, -1.426e+01,  7.077e+01, -2.287e+01,
          5.840e+01, -2.807e+01, -1.746e+01,  2.744e+00],
        [-1.925e+01, -2.901e+01,  6.538e+00, -5.329e+00,  3.356e+01,  4.302e+01,
          7.432e+00, -2.478e+01, -2.202e+01, -2.332e+01],
        [ 4.879e+01,  1.432e+01, -1.787e+01, -3.220e+01,  7.253e+01, -1.105e+01,
         -7.500e-01, -2.562e+01, -2.178e+01,  5.662e+00],
        [ 4.308e+01,  1.710e+01, -3.018e+01, -2.027e+01, -2.780e+01, -3.018e+01,
         -2.934e+01, -1.791e+01,  5.704e+01, -2.928e+01],
        [ 2.670e+01, -2.280e+01, -3.438e+01, -2.327e+01,  4.166e+01, -2.143e+01,
         -2.876e+01, -3.329e+01, -1.573e+01, -1.941e+01],
        [-2.111e+01, -2.051e+01, -2.203e+01,  4.610e+01,  7.925e+01, -2.466e+01,
         -2.483e+01,  3.218e+01, -5.319e+00, -1.840e+01],
        [ 2.113e+01, -8.414e+00, -2.187e+01, -7.766e-01, -2.101e+01, -2.843e+01,
         -2.097e+01, -1.810e+01, -2.811e+01, -1.501e+01],
        [-2.728e+01, -2.818e+01,  1.190e+01, -2.251e+01,  1.227e+02, -1.581e+01,
          1.769e+01,  3.867e+00, -1.995e+01, -1.049e+01],
        [ 3.547e+01,  1.389e+02,  6.154e+01,  3.022e+01, -9.107e+00, -1.692e+01,
         -3.213e+01, -1.147e+01, -1.531e+01,  1.640e+01],
        [-3.846e+00,  1.352e+02, -2.886e+01,  6.581e+01,  7.991e+00, -7.568e+00,
          2.648e+00, -2.247e+01, -2.784e+01, -3.450e+01],
        [-2.618e+01,  2.587e+01,  1.754e+01, -1.670e+01,  7.575e+00,  9.240e+00,
         -3.003e-01,  9.926e+01,  8.065e+00,  5.689e+01],
        [-3.068e+01, -2.584e+01,  2.032e+01, -1.443e+01,  1.583e+01, -1.547e+01,
         -3.721e+00,  7.237e+01, -2.065e+01, -1.740e+01],
        [ 1.044e+02, -1.939e+01, -3.524e+01, -1.014e+00,  1.158e+02, -1.469e+01,
         -2.333e+01, -2.153e+01,  7.540e+01,  9.468e+00],
        [-3.681e+00, -1.770e+01, -6.550e+00,  1.317e+02,  1.373e+01,  1.480e+01,
         -2.875e+01,  1.193e+01,  1.000e+01, -2.435e+01],
        [ 8.034e+01, -1.960e+01, -2.028e+01, -5.288e+00, -1.055e+01, -2.734e+01,
          2.518e+01,  3.204e+01, -1.168e+01, -2.057e+01],
        [-3.243e+01,  3.085e+01, -1.763e+01, -2.313e+01,  1.153e+02, -1.203e+01,
         -2.183e+01, -2.145e+01, -2.117e+01,  1.967e+02],
        [-3.222e+01,  1.466e+00,  7.204e+01,  8.427e+01, -1.371e+01, -8.264e+00,
         -3.006e+01,  2.705e+01, -1.631e+01, -1.043e+01],
        [ 2.047e+01,  3.220e+01,  3.323e+02,  1.747e+01,  3.278e+01,  3.089e+01,
         -2.793e+01, -2.524e+01, -6.586e+00,  3.043e+01],
        [-4.330e+00, -2.809e+01,  8.771e+01,  8.011e+00, -9.456e-01,  3.497e+00,
         -1.157e+01, -2.143e+01, -2.198e+01, -2.596e+01],
        [ 1.750e+02, -5.700e+00, -1.032e+01,  5.499e+01, -1.207e+01,  5.892e+01,
          2.271e+01, -2.603e+01,  1.607e+00, -2.806e+01],
        [ 1.221e+02, -2.740e+01, -3.649e+01, -2.850e+01,  1.548e+02, -2.368e+01,
         -2.228e+01, -1.160e+01, -1.728e+01, -2.939e+01],
        [-3.353e+01, -2.082e+01,  3.211e+02, -1.406e+01, -1.774e+00, -2.748e+01,
         -2.410e+01, -2.585e+01, -3.514e+01, -2.148e+01],
        [-1.660e+01, -9.854e+00, -2.229e+01, -1.607e+01, -2.530e+01, -2.461e+01,
         -9.057e+00,  1.970e+01, -3.030e+00, -3.069e+01],
        [-3.038e+01, -2.180e+01,  8.164e+01,  9.930e+01, -2.550e+01, -2.173e+01,
          2.578e+01, -6.520e+00, -8.536e+00, -2.054e+01],
        [ 7.367e+00, -1.864e+01, -4.811e+00,  2.445e+02,  1.501e+01, -1.666e+01,
         -4.546e+00,  1.652e+02, -9.701e+00, -2.126e+01],
        [ 4.392e+01, -1.666e+01,  5.023e+01,  2.923e+01,  6.483e+01,  6.301e+01,
          2.128e+02,  2.772e+01,  1.060e+01, -1.812e+01],
        [-2.273e+01,  6.860e+01, -6.225e+00,  6.671e+01, -1.005e+01, -2.767e+01,
         -2.648e+01, -9.191e+00,  2.602e+01,  1.297e+00],
        [-3.234e+01, -2.018e+01, -2.392e+01,  2.466e+01, -1.778e+01,  1.599e+02,
          2.441e+01, -2.077e+01, -7.666e+00, -3.234e+01],
        [-2.187e+01, -2.166e+01,  3.534e+01,  8.958e+00, -6.049e+00,  2.007e+01,
         -2.048e+01,  4.054e+01, -2.276e+01, -2.006e+01],
        [-2.048e+01,  2.579e+01,  1.522e+01, -8.846e+00,  1.965e+02, -7.060e+00,
         -1.158e+01,  8.898e+00, -1.860e+01,  3.682e+00],
        [-2.442e+01,  5.395e+01,  2.679e+02, -2.358e+01, -3.741e+01,  4.337e+01,
         -1.269e+01, -1.014e+01, -8.659e+00,  9.856e+01],
        [-3.521e+01, -1.947e+01,  6.674e+01,  1.340e+00, -3.289e+01, -7.177e+00,
         -2.081e+01, -2.163e+01,  4.826e+01,  6.468e+01],
        [-1.967e+01,  4.385e+01,  2.576e+01,  2.865e+01, -3.402e+01, -2.685e+01,
         -2.228e+01, -1.927e+01,  2.635e+01, -2.587e+01],
        [-2.198e+01,  7.231e+01,  1.555e+02, -2.262e+01, -7.235e+00, -2.074e+01,
         -1.205e+01, -3.258e+01,  2.001e+01, -2.627e+01],
        [ 1.050e+02, -1.189e+01, -3.512e+01, -1.832e+01,  2.725e+01,  1.097e+02,
         -2.808e+01, -9.212e+00, -2.149e+01,  2.195e+01],
        [ 4.278e+02, -1.173e+01,  4.807e+01,  8.064e+01,  3.274e+02, -3.160e+01,
          1.336e+02,  2.806e+01, -2.766e+01, -1.454e+01],
        [ 5.750e+01,  1.712e+01, -2.507e+01, -8.559e+00,  5.850e+01, -2.282e+01,
         -1.193e+01, -2.541e+01, -1.626e+01,  1.175e+01],
        [-2.023e+01, -3.254e+01,  1.095e+01, -2.912e+01,  1.884e+01,  4.932e+01,
          9.594e-01, -1.048e+01, -1.352e+01,  1.105e+02],
        [-3.478e+00, -1.663e+01,  1.875e+01,  3.841e+01,  1.861e+01, -1.257e+01,
         -1.961e+01, -2.251e+01, -2.817e+01, -2.046e+01],
        [-9.817e+00, -2.707e+01, -2.091e+01, -1.400e+01, -1.704e+01, -2.506e+01,
          9.883e+01,  1.201e+02, -6.597e+00, -2.199e+01],
        [ 4.588e+01, -1.710e+01, -1.947e+01, -2.382e+01,  1.190e+01, -3.352e+01,
         -5.324e+00, -7.652e+00,  2.527e+01,  5.182e+01],
        [ 9.033e+01,  2.129e+01,  1.666e+02, -9.130e+00,  1.683e+01,  5.369e+01,
         -2.334e+01,  6.883e+01, -2.194e+01,  7.018e+01],
        [ 2.386e+02,  8.379e+01,  1.289e+02, -2.689e+01,  7.831e+01,  4.316e+01,
          3.917e+01, -1.777e+01, -1.574e+01,  1.599e+02],
        [-1.712e+01,  1.819e+00, -1.977e+01, -1.408e+01,  4.240e+01, -1.578e+01,
          1.919e+01, -7.715e+00, -2.459e+01, -1.823e+01],
        [-2.645e+01, -1.140e+01, -2.148e+01,  6.046e+01, -6.330e+00,  7.096e+00,
         -1.205e+01, -2.971e+01, -1.065e+01, -2.502e+00],
        [ 3.198e+01, -9.165e+00,  1.591e+02,  4.295e+01,  1.131e+02, -2.979e+01,
         -1.376e+01, -2.383e+01, -3.530e+00,  2.829e+01],
        [-2.097e+01,  3.245e+01, -2.813e+01, -2.263e+01, -2.926e+00,  1.346e+01,
         -2.053e+01, -2.426e+01, -2.628e+01, -2.761e+01],
        [-2.466e+01, -1.083e+01,  3.132e+01, -2.528e+01,  1.087e+02,  4.506e+01,
         -3.407e+01,  4.385e+00, -2.047e+01, -1.835e+01],
        [-1.443e+01,  4.564e+01, -1.396e+01,  1.889e+01,  2.784e+01, -3.279e+01,
         -2.256e+01, -1.179e+01, -6.340e+00, -1.688e+01],
        [-2.951e+01, -2.334e+01,  7.815e+01,  1.967e+01, -1.620e+01,  1.609e+02,
         -2.328e+01, -2.394e+01, -9.253e-01,  9.669e+00],
        [-2.601e+01, -4.770e+00,  4.347e+01, -1.282e+01, -1.531e+01,  2.220e+02,
         -2.975e+01,  7.618e+01, -1.772e+01, -1.727e+01],
        [-2.587e+01, -1.566e+01,  1.736e+02,  2.532e+01, -2.472e+00,  2.289e+00,
         -7.394e+00,  7.855e-01, -1.380e+01, -2.260e+01],
        [-3.593e+01,  4.102e+01,  2.693e+01, -2.576e+00,  2.232e+01,  4.634e+01,
         -3.375e+01,  3.352e+01,  1.490e+02, -4.604e+00],
        [-2.378e+01, -8.003e+00, -6.210e+00, -8.141e+00,  1.985e+01, -3.355e+00,
         -1.162e+00,  6.277e+00, -3.336e+01, -1.945e+01],
        [ 4.122e+02, -1.383e+01, -5.746e+00, -1.316e+00, -3.137e+01, -3.513e+01,
         -3.650e+01,  1.871e+01,  1.263e+02,  5.912e+01],
        [-1.983e+01, -2.833e+01,  8.883e+01,  2.684e+01, -2.211e+01, -1.372e+01,
         -6.187e+00, -3.112e+01, -1.602e+01,  1.097e+01],
        [-3.112e+01, -2.678e+01, -2.356e+01, -1.260e+01,  3.653e+01, -2.517e+01,
          3.449e+02,  3.470e+01, -2.579e+00,  1.453e+00],
        [-3.262e+01, -1.785e+01, -1.718e+01,  2.967e+00, -2.500e+01, -2.627e+01,
          4.912e+01, -2.804e+01, -6.438e+00,  4.730e+00],
        [-3.230e+01, -4.966e-01, -2.159e+01, -1.971e+01, -1.592e+00, -1.338e+01,
          1.331e+01, -2.826e+01, -2.630e+01,  1.349e+02],
        [-3.705e+01, -4.995e+00, -3.609e+01, -1.147e+01, -3.426e+01,  9.560e-01,
          2.285e+02,  2.135e+02, -2.017e+01, -7.967e+00],
        [ 3.889e+01, -3.024e+01,  1.800e+01,  7.919e+01,  4.850e+01, -3.389e+01,
         -2.540e+01,  1.285e+01, -2.304e+01,  1.207e+01],
        [ 3.216e+00, -9.839e+00, -3.116e+01,  6.946e+01, -2.490e+01,  1.966e+01,
          2.539e+02, -1.461e+01, -3.103e+01,  8.388e+01],
        [-2.161e+00, -1.913e+01, -3.169e+01,  3.266e+01, -1.357e+01,  1.137e+02,
         -4.065e+00, -2.486e+00, -1.998e+01, -1.224e+01],
        [-2.605e+01, -3.408e+01, -3.441e+01, -2.285e+01, -2.478e+01,  5.797e+01,
          3.614e+02, -2.193e+01,  1.402e+00, -3.016e+01],
        [-2.455e+01,  3.060e+01,  4.293e+00,  2.825e+00,  2.287e-01,  1.773e+01,
         -5.650e+00,  1.203e+01, -2.714e+01, -2.624e+01],
        [-3.643e+00, -1.433e+01,  5.309e+00, -2.954e+01, -2.138e+01,  2.306e+01,
          2.033e+01, -1.036e+01,  6.627e+01, -2.467e+01],
        [-9.391e+00, -1.244e+01,  1.325e+01, -2.724e+01, -2.360e+01, -1.881e+01,
          9.193e+01,  5.733e+01,  7.951e+00, -1.807e+01],
        [-3.695e+01, -2.369e+01, -1.495e+01,  1.245e+01,  1.276e+01, -1.197e+00,
         -7.030e-01,  4.615e+01,  1.595e+01, -2.977e+01],
        [-1.093e+01, -1.338e+01, -2.411e+01, -2.087e+01, -6.925e+00, -2.087e+01,
         -1.547e+01, -3.193e+01, -3.246e+01,  3.355e+01],
        [-3.189e+01, -2.256e+01,  2.051e+01, -1.215e+01,  7.038e+00,  4.132e+00,
          1.472e+01,  4.463e+01, -3.263e+01, -1.021e+01],
        [-1.519e+01, -2.430e+01,  3.913e+01, -1.283e+01,  1.996e+01, -3.022e+01,
          9.954e+00, -2.224e+01,  5.956e+00, -2.246e+00],
        [-1.512e+01, -2.082e+01, -2.384e+01, -1.799e+01, -2.341e+01, -1.915e+01,
          5.271e+01, -2.186e+00, -2.317e+00, -1.534e+01],
        [-2.071e-01, -2.976e+01, -3.060e+01, -2.297e+01, -2.747e+01,  6.182e+01,
         -5.036e+00, -2.448e+01,  4.016e+01, -1.802e+01],
        [-1.907e+01, -1.551e+01, -1.447e+01,  1.152e+02, -1.170e+01,  1.938e+00,
         -1.692e+01,  1.175e+02,  8.082e+01, -2.846e+01],
        [-2.376e+00, -1.400e+01, -1.830e+01, -2.081e+01, -1.287e+01,  6.646e-01,
         -2.334e+01, -1.847e+01, -2.175e+01,  2.644e+01],
        [ 2.145e+02,  3.127e+02, -2.910e+01, -2.234e+01, -3.579e+01, -2.254e+01,
         -2.315e+01, -1.814e+01, -2.855e+01, -1.037e+01],
        [-2.925e+01,  2.335e+01, -1.620e+01,  5.611e+01, -2.808e+01,  8.971e+01,
          3.335e+01, -1.072e+01, -1.779e+01,  7.962e+00],
        [ 5.251e+01, -2.639e+01, -2.345e+01,  8.456e+01,  2.920e+01, -3.568e+00,
         -7.622e+00,  6.624e+00,  6.468e+00, -2.696e+01],
        [-3.134e+01,  1.709e+01, -1.824e+01,  2.477e+01, -1.651e+01, -1.409e+01,
         -2.612e+01, -2.540e+01,  1.707e+01, -2.453e+01],
        [-1.454e+01, -2.078e+01, -3.479e+01, -1.601e+01,  1.598e+01, -1.480e+01,
          2.658e+00, -1.621e+01,  6.966e+01,  9.463e+01],
        [ 7.369e+00,  4.026e+01,  1.753e+02, -1.458e+01,  1.191e+01,  4.918e+00,
         -1.301e+01, -1.849e+01, -2.253e+01,  7.646e+01],
        [ 2.768e+01,  2.594e+01, -2.160e+01,  6.182e+01, -2.416e+01,  6.732e+00,
         -1.953e+01,  3.778e+01,  7.533e+01,  2.074e+01],
        [-3.183e+01,  3.373e+01, -8.098e+00, -1.480e+01, -1.276e+00,  1.852e+02,
         -2.035e+01,  1.125e+01, -2.176e+01,  1.602e+01],
        [-2.364e+01, -2.626e+01,  1.753e+01, -1.928e+01,  2.737e+01,  6.057e-01,
          6.327e+01, -8.135e+00, -1.978e+01, -2.153e+01],
        [-2.512e+01,  2.180e+01, -2.665e+01, -2.793e+01, -3.046e+00, -2.034e+01,
         -3.128e+00, -1.678e+01, -1.959e+01,  2.417e+01],
        [-2.288e+01, -1.899e+01, -1.955e+01,  2.212e-01,  4.755e+01, -8.803e+00,
          9.989e+00,  4.573e+00, -2.315e+01,  9.834e+01],
        [ 1.465e+02, -2.595e+01,  1.193e+02, -1.845e+01,  9.438e+00, -3.439e+01,
         -7.419e+00, -2.810e+01,  7.956e+01, -2.269e+01],
        [ 4.148e+01,  5.937e+00, -2.101e+01,  1.208e+01, -2.637e+01, -1.119e+01,
          1.828e+02,  3.713e+01,  7.143e+01,  1.676e+02],
        [-1.598e+01, -3.387e+01,  2.728e+01,  4.752e+01, -2.106e+01, -9.234e+00,
          5.660e+01,  3.193e+01, -1.506e+01, -3.368e+01],
        [ 3.065e+02,  3.860e+02,  2.181e+01, -2.278e+01,  4.059e+01, -1.610e+01,
          1.842e+02,  5.935e+01,  6.162e+00, -2.285e+01],
        [-1.774e+01, -6.902e+00, -1.908e+01,  2.922e+00, -1.367e+01,  2.934e+01,
          3.831e+02,  8.614e+01,  2.163e+00, -2.382e+01],
        [ 2.194e+01,  9.440e+00, -9.856e+00, -2.164e+01,  6.953e+00,  5.366e+01,
         -6.073e+00,  3.289e+01,  3.587e+00, -9.497e+00],
        [ 8.852e+01,  3.658e+01, -3.546e+00, -1.143e+01, -1.998e+01, -1.030e+01,
          4.460e+01, -2.357e+01, -5.462e+00, -4.717e+00],
        [ 1.898e+02, -2.598e+01,  8.121e+01, -2.370e+00, -1.267e+01,  1.539e+02,
          1.075e+02,  1.385e+02, -3.321e+01,  1.462e+02]], device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 3.635e+00,
         1.000e-12, 5.733e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01],
        [2.631e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e-12, 8.586e+00, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 2.744e+00],
        [1.000e-12, 1.000e-12, 6.538e+00, 1.000e-12, 1.000e+01, 1.000e+01,
         7.432e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 5.662e+00],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 3.867e+00, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 7.991e+00, 1.000e-12,
         2.648e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 7.575e+00, 9.240e+00,
         1.000e-12, 1.000e+01, 8.065e+00, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 9.468e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.466e+00, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 8.011e+00, 1.000e-12, 3.497e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.607e+00, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [7.367e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.297e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 8.958e+00, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 8.898e+00, 1.000e-12, 3.682e+00],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.340e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         9.594e-01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01],
        [1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.819e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 7.096e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 4.385e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 9.669e+00],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 2.289e+00,
         1.000e-12, 7.855e-01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 6.277e+00, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.453e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 2.967e+00, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 4.730e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 9.560e-01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01],
        [3.216e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.402e+00, 1.000e-12],
        [1.000e-12, 1.000e+01, 4.293e+00, 2.825e+00, 2.287e-01, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 5.309e+00, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e+01, 7.951e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 7.038e+00, 4.132e+00,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         9.954e+00, 1.000e-12, 5.956e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.938e+00,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 6.646e-01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 7.962e+00],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 6.624e+00, 6.468e+00, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         2.658e+00, 1.000e-12, 1.000e+01, 1.000e+01],
        [7.369e+00, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 4.918e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 6.732e+00,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 6.057e-01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 2.212e-01, 1.000e+01, 1.000e-12,
         9.989e+00, 4.573e+00, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 9.438e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e+01, 5.937e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 6.162e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 2.922e+00, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e+01, 2.163e+00, 1.000e-12],
        [1.000e+01, 9.440e+00, 1.000e-12, 1.000e-12, 6.953e+00, 1.000e+01,
         1.000e-12, 1.000e+01, 3.587e+00, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01]], device='cuda:0')
v before min max tensor([-3.038e+01, -1.796e+01,  6.096e+01, -8.206e+00,  7.830e-02, -2.080e+01,
         2.000e+00, -1.311e+01,  2.252e+01, -1.660e+01, -2.742e+01,  6.302e+01,
        -2.476e+01, -1.844e+01, -1.693e+01, -2.431e+01, -1.071e+01, -1.329e+01,
        -6.715e+00, -9.353e+00,  1.488e+01,  6.711e+00, -5.648e+00, -3.647e+01,
        -2.962e+01,  1.729e+00, -2.150e+01, -2.775e+00, -1.895e+01,  1.932e+01,
        -2.644e+01, -2.398e+01, -1.835e+01, -2.708e+01, -8.519e+00,  5.936e-01,
        -3.989e+00,  1.586e+01,  1.136e+02, -1.785e+01, -8.260e+00, -1.380e+01,
        -1.138e+01, -1.183e+01, -3.079e+01,  3.031e+02, -1.679e+01, -7.520e+00,
        -1.213e+01, -1.229e+01,  1.515e+01, -1.131e+01, -1.780e+01, -2.138e+00,
        -2.568e+01, -2.336e+01,  1.231e+00, -2.491e+01,  1.957e+01, -1.927e+01,
        -1.988e+01, -8.913e-01, -2.754e+01,  7.182e+01, -3.696e+01,  5.036e+01,
        -7.091e+00,  1.703e+01, -1.282e+01,  1.007e+02, -1.974e+01, -2.114e+01,
        -3.306e+00, -2.973e+01,  3.001e-01, -1.789e+01, -1.879e+01,  2.129e+01,
         1.234e+02, -2.138e+01, -1.455e+01, -2.168e+01, -1.068e+01, -1.572e+01,
        -4.327e+00, -1.884e+01, -5.351e-01, -1.835e+01, -2.117e+01, -9.046e+00,
         1.526e+02, -1.527e+01, -2.609e+01,  2.869e+01, -3.163e+01, -5.925e-01,
         7.432e+01,  1.201e+01,  2.554e+01,  2.936e+01], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 7.830e-02, 1.000e-12,
        2.000e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 6.711e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.729e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 5.936e-01,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.231e+00, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 3.001e-01, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01], device='cuda:0')
v before min max tensor([[[ -1.356],
         [-23.906],
         [194.950],
         [-29.437],
         [ 24.949],
         [ 38.156],
         [ 25.153],
         [ 16.215],
         [ 29.765],
         [ 59.723]],

        [[  8.666],
         [ 21.046],
         [-29.893],
         [-17.313],
         [-16.596],
         [ 60.905],
         [ 58.309],
         [ 44.248],
         [-30.257],
         [-25.945]],

        [[-19.711],
         [ -3.778],
         [232.514],
         [-15.751],
         [-19.053],
         [195.771],
         [-17.596],
         [ 13.032],
         [-28.261],
         [-36.059]],

        [[ 25.382],
         [ -1.839],
         [-29.865],
         [-15.156],
         [-13.304],
         [-21.015],
         [  6.138],
         [-14.746],
         [ 36.233],
         [ 99.678]],

        [[-10.510],
         [-18.574],
         [ 10.348],
         [-25.952],
         [-21.571],
         [ 44.980],
         [-20.793],
         [-30.490],
         [  4.476],
         [-14.361]],

        [[-17.471],
         [ 13.747],
         [-13.140],
         [  2.167],
         [-31.990],
         [ 15.957],
         [-10.680],
         [-35.751],
         [  3.545],
         [ 58.768]],

        [[-28.828],
         [-21.117],
         [  5.968],
         [-17.776],
         [-21.713],
         [-23.386],
         [-14.239],
         [-11.467],
         [-20.872],
         [ 53.948]],

        [[ 10.239],
         [  9.050],
         [ -1.784],
         [-27.307],
         [  3.679],
         [ 15.119],
         [-35.467],
         [-11.186],
         [-18.367],
         [ 82.924]],

        [[ -5.133],
         [ -8.441],
         [ 39.497],
         [ -7.282],
         [-32.590],
         [ -2.671],
         [-11.717],
         [ -8.987],
         [ -9.623],
         [-18.538]],

        [[ 17.171],
         [-21.116],
         [-10.912],
         [-32.376],
         [  8.032],
         [-25.750],
         [ -9.333],
         [ 17.260],
         [-24.371],
         [ 23.318]]], device='cuda:0')
v tensor([[[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01]],

        [[8.666e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.138e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [4.476e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [2.167e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [3.545e+00],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [5.968e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [9.050e+00],
         [1.000e-12],
         [1.000e-12],
         [3.679e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.032e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]]], device='cuda:0')
v before min max tensor([[  1.808],
        [-11.786],
        [ -3.141],
        [-18.337],
        [-27.746],
        [-23.057],
        [ 16.893],
        [-22.682],
        [ 41.498],
        [ 68.378]], device='cuda:0')
v tensor([[1.808e+00],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01]], device='cuda:0')
a after update for 1 param tensor([[-7.066e-02, -1.588e-02, -6.387e-02, -2.529e-02, -1.745e-02, -4.875e-03,
         -5.573e-02,  4.466e-02, -2.002e-03, -8.448e-02],
        [ 2.170e-02,  3.601e-02, -4.090e-02,  9.687e-03, -1.306e-02, -3.955e-04,
          1.995e-02, -4.817e-02, -1.200e-02, -2.552e-02],
        [-1.586e-02, -1.440e-02, -1.839e-02, -2.530e-02,  2.034e-02, -9.251e-03,
         -5.586e-02,  8.095e-02,  8.878e-03,  3.338e-02],
        [ 4.418e-02,  3.954e-02,  1.473e-02, -1.554e-02,  1.395e-02,  9.673e-03,
          5.737e-02,  7.780e-03,  2.176e-02, -6.724e-03],
        [ 9.481e-03, -6.219e-02,  1.122e-02,  4.676e-02, -1.658e-02,  6.272e-03,
         -3.338e-02,  3.354e-02,  2.276e-02,  3.011e-03],
        [ 3.190e-02,  3.537e-02,  3.416e-02,  7.120e-02, -4.191e-02, -2.313e-02,
          4.023e-03, -2.572e-02,  3.404e-02, -3.399e-02],
        [-2.927e-03, -5.182e-02,  6.380e-03,  1.361e-02, -8.225e-02, -2.334e-02,
         -4.493e-02,  1.913e-02,  3.850e-03,  3.965e-03],
        [ 2.788e-02, -3.742e-02,  4.006e-03, -1.462e-02, -1.205e-02,  1.037e-02,
          1.662e-02,  6.778e-03, -1.399e-02, -3.590e-02],
        [-1.556e-02, -5.835e-04,  2.610e-02, -4.671e-02,  3.246e-02,  3.201e-02,
         -9.345e-03,  3.177e-03, -8.169e-03,  3.331e-03],
        [ 2.449e-02, -2.151e-02,  4.269e-02, -4.178e-02, -9.568e-03,  3.859e-02,
         -3.348e-02, -5.916e-02,  6.468e-03,  1.298e-02],
        [-2.187e-02, -4.532e-02,  7.729e-02,  6.230e-06, -7.462e-03, -6.322e-02,
         -9.908e-03,  7.658e-03, -2.493e-02,  2.018e-02],
        [-1.362e-02,  2.314e-02,  2.640e-02,  6.516e-02, -2.319e-02, -2.474e-02,
         -6.527e-02,  2.236e-02,  5.464e-02,  1.413e-02],
        [ 4.674e-02, -1.279e-03,  3.239e-02,  1.722e-03,  4.382e-02, -8.346e-03,
         -3.941e-03, -6.881e-05, -1.717e-02,  8.413e-03],
        [-1.015e-02, -1.299e-02,  3.564e-02, -8.397e-03, -1.487e-03, -1.493e-03,
          2.214e-02,  3.826e-02, -1.804e-02,  3.948e-02],
        [ 1.027e-02,  2.111e-03,  1.176e-02,  3.101e-02,  5.484e-03,  8.203e-04,
         -3.077e-02, -5.722e-03,  4.793e-02, -4.309e-02],
        [-1.264e-03,  1.415e-02,  6.275e-03,  2.581e-02, -3.807e-03,  3.895e-02,
          3.666e-03,  1.895e-02,  3.509e-03, -3.833e-02],
        [-9.007e-03, -8.500e-03,  8.422e-04, -2.984e-02,  1.400e-02,  2.767e-03,
          2.223e-02,  7.737e-04, -3.633e-02, -1.436e-02],
        [-8.532e-03,  1.708e-02, -3.603e-02,  2.465e-02,  8.913e-03, -1.230e-02,
          1.784e-02,  8.745e-03,  1.787e-03, -1.478e-02],
        [-3.019e-02,  2.577e-02, -1.318e-04, -2.031e-03, -7.776e-03,  1.753e-02,
         -2.570e-02, -8.344e-02,  5.169e-02,  2.579e-02],
        [-1.893e-02,  3.779e-02,  6.593e-04,  1.828e-02,  1.081e-02,  3.696e-02,
         -6.140e-02, -8.690e-03,  8.043e-03,  7.202e-03],
        [-2.039e-02,  1.329e-02,  2.783e-04, -2.788e-02,  1.975e-03, -2.580e-03,
         -7.876e-03,  7.698e-02,  7.028e-02, -2.509e-03],
        [-1.176e-01,  3.168e-02,  1.218e-03,  3.858e-02, -2.695e-02, -2.143e-02,
          2.071e-02, -7.655e-03, -1.782e-02, -4.829e-02],
        [-2.397e-02,  3.513e-02,  1.272e-02, -1.155e-02,  2.083e-02,  1.029e-02,
         -1.973e-02,  9.441e-03,  2.113e-02,  1.014e-02],
        [ 3.044e-02,  1.814e-02, -4.582e-02, -2.250e-02,  2.233e-02, -2.932e-02,
         -4.121e-03, -5.932e-02, -1.706e-02,  9.552e-03],
        [-8.236e-04, -5.141e-02, -2.670e-02,  5.214e-03,  1.613e-02,  9.765e-03,
          3.334e-03,  1.339e-02,  1.662e-02, -3.826e-02],
        [-3.560e-02,  6.215e-02,  7.848e-03,  6.363e-02,  4.214e-03, -3.058e-03,
         -1.683e-02,  4.864e-04,  8.296e-03,  2.710e-02],
        [ 2.213e-02,  2.349e-02,  3.291e-02, -6.568e-03,  3.666e-02,  3.062e-02,
         -2.824e-02,  6.525e-03,  1.791e-02,  1.479e-02],
        [-1.153e-02,  8.041e-02,  2.351e-02,  1.469e-02,  1.657e-02,  2.888e-02,
          2.928e-03, -3.713e-03,  6.020e-02,  4.039e-02],
        [ 1.237e-02,  2.702e-04, -1.448e-02, -5.692e-02,  1.281e-02,  2.581e-02,
         -3.769e-02, -2.787e-02,  9.335e-03,  4.314e-02],
        [ 9.252e-03,  2.377e-02, -7.433e-02,  2.581e-02,  5.115e-02,  1.158e-02,
         -2.574e-02, -7.714e-03, -1.724e-02, -3.906e-02],
        [ 2.760e-03, -3.622e-02, -3.238e-02, -2.923e-02,  1.208e-02,  3.222e-03,
         -2.583e-02, -1.660e-02, -2.828e-02, -1.871e-02],
        [ 6.748e-03, -6.422e-04, -1.345e-02, -3.530e-03,  6.021e-04, -3.596e-02,
         -4.073e-02, -2.704e-02, -1.333e-02, -7.940e-03],
        [-2.524e-02,  4.754e-02, -3.957e-03, -3.039e-02, -5.148e-03, -4.109e-02,
         -2.838e-02, -1.099e-02,  2.030e-02,  5.661e-03],
        [-1.369e-02,  3.198e-02, -2.546e-03,  2.780e-02, -3.758e-04,  2.149e-02,
          1.614e-02,  1.062e-02,  2.264e-02, -5.336e-02],
        [ 9.521e-03,  3.041e-02, -2.981e-02,  1.520e-02, -2.437e-02, -2.975e-02,
         -5.390e-02, -1.888e-02, -9.276e-02, -1.626e-02],
        [ 8.750e-02, -8.396e-03,  1.109e-02, -2.979e-03, -1.875e-02, -1.226e-02,
          6.109e-03, -7.201e-03, -1.145e-02, -2.631e-02],
        [ 5.256e-03, -1.453e-02,  7.656e-02,  8.225e-03,  3.344e-02, -1.136e-02,
          4.662e-02, -8.998e-03,  7.948e-02, -2.047e-02],
        [-4.605e-02, -6.125e-02,  6.073e-02,  6.305e-03,  1.057e-02, -4.675e-03,
         -4.005e-02,  5.502e-02, -1.101e-02, -1.510e-02],
        [-3.246e-02, -3.047e-02,  8.985e-03,  9.575e-03,  6.575e-02,  3.171e-03,
          1.999e-02,  6.336e-03, -4.662e-02,  1.951e-02],
        [-3.682e-03, -1.428e-02, -4.620e-03,  3.633e-02, -5.359e-03,  2.902e-02,
          3.360e-03, -5.374e-02,  4.203e-02,  2.837e-03],
        [-4.354e-02, -2.634e-03, -4.591e-02, -3.456e-03,  4.243e-02, -3.599e-02,
          1.733e-02, -1.052e-02,  3.804e-02,  7.595e-02],
        [ 2.365e-02,  2.700e-03,  2.194e-02,  4.983e-02,  2.968e-03, -4.352e-02,
          2.757e-02,  6.638e-02,  7.406e-03, -1.349e-02],
        [-7.852e-03, -3.337e-02,  1.116e-02, -7.034e-03,  6.728e-03,  9.854e-02,
         -1.816e-03,  1.802e-03, -6.649e-03, -2.572e-03],
        [ 5.310e-02, -2.592e-02,  4.344e-02, -5.961e-04,  2.267e-02, -7.977e-03,
          1.389e-01,  3.738e-03,  1.227e-02, -2.969e-02],
        [ 1.780e-02,  2.208e-04, -4.374e-02,  3.286e-02, -2.432e-02,  7.756e-03,
         -2.604e-02,  3.677e-02,  3.717e-02, -2.632e-02],
        [-3.153e-02, -3.158e-03, -4.517e-03, -6.437e-02, -1.508e-03,  1.999e-02,
         -3.895e-02,  1.503e-02,  6.915e-02, -1.728e-03],
        [-5.061e-02, -5.130e-03,  2.350e-03,  8.559e-03,  2.118e-02,  6.604e-03,
         -2.220e-02, -1.336e-02,  4.584e-02, -4.230e-03],
        [ 8.566e-02,  1.211e-02,  3.745e-02,  2.589e-02, -1.983e-02, -1.495e-02,
         -1.557e-02, -2.086e-02, -3.681e-02, -1.103e-02],
        [-4.927e-02,  6.457e-02,  4.898e-02,  1.882e-03, -1.878e-02, -2.052e-02,
         -1.067e-01,  4.302e-02, -5.893e-02,  1.580e-03],
        [-1.459e-02,  1.058e-02,  5.424e-02, -6.382e-03, -2.686e-03, -6.582e-02,
         -2.201e-02, -1.356e-03,  1.331e-02, -2.562e-02],
        [ 1.157e-03, -9.893e-03,  7.461e-03, -4.938e-02,  2.208e-02, -3.996e-02,
         -8.819e-04,  6.270e-02, -5.324e-04, -6.880e-03],
        [ 1.353e-02, -3.453e-02,  1.349e-01, -2.615e-02, -1.371e-02, -1.091e-02,
          9.533e-03, -4.524e-02,  2.104e-03,  2.389e-02],
        [-2.118e-02, -3.657e-02,  4.017e-02,  6.043e-02, -2.403e-02,  2.000e-04,
         -2.512e-02,  1.985e-02, -3.644e-02,  2.304e-03],
        [-2.031e-02, -6.008e-02,  5.544e-03,  5.059e-02, -1.944e-02,  1.009e-02,
         -2.033e-02,  3.963e-02, -1.189e-02, -2.535e-02],
        [-1.105e-02,  4.295e-02, -2.184e-02, -4.010e-04, -1.006e-02, -2.115e-02,
         -5.034e-02,  3.849e-02,  6.619e-03, -9.870e-04],
        [-1.035e-02, -2.010e-02, -3.908e-02, -7.639e-02,  1.029e-02, -3.105e-03,
         -9.752e-02, -3.812e-02,  2.435e-02,  8.972e-02],
        [ 1.475e-02,  1.216e-02, -3.765e-02, -3.874e-04,  9.590e-03,  2.118e-02,
         -3.018e-02,  7.464e-03, -3.700e-02,  3.024e-02],
        [-2.413e-03,  8.965e-03,  6.552e-02,  8.177e-03, -1.147e-02, -5.768e-02,
          1.192e-02, -1.834e-02, -1.750e-02,  4.928e-02],
        [-3.026e-02, -7.007e-02, -4.475e-02, -1.714e-02, -3.387e-02,  4.477e-03,
          7.451e-03, -4.062e-02, -4.873e-04,  3.957e-02],
        [ 1.016e-03, -9.952e-03, -8.455e-03, -2.246e-02, -3.457e-02, -1.028e-02,
         -2.484e-02,  2.374e-02,  6.603e-02, -2.749e-02],
        [-5.749e-02, -1.952e-02, -1.005e-02,  8.886e-03, -2.803e-02,  2.549e-02,
          3.153e-02, -5.138e-02, -3.782e-02, -4.600e-02],
        [-6.942e-02,  1.763e-02,  2.643e-02,  1.537e-02,  7.491e-03, -3.060e-02,
          8.092e-02, -7.088e-02, -1.974e-02,  4.479e-02],
        [-1.380e-02,  1.762e-02, -3.906e-03,  8.941e-03,  1.075e-02, -4.222e-02,
          4.853e-02, -1.149e-01, -4.437e-02, -8.203e-03],
        [ 1.963e-02,  5.602e-02,  3.447e-02, -3.385e-03,  2.049e-02, -2.610e-02,
          7.955e-02, -5.100e-03, -4.406e-02, -7.074e-04],
        [ 1.296e-02, -5.331e-03,  1.998e-02, -2.281e-02,  2.064e-02, -2.289e-02,
          2.648e-02, -2.766e-02,  3.329e-03,  4.165e-02],
        [ 8.969e-03,  2.106e-02,  3.747e-02,  1.271e-02,  3.812e-02,  1.372e-02,
          5.950e-02, -1.249e-02,  3.434e-02,  3.406e-02],
        [-7.014e-02,  2.379e-02,  1.402e-02, -1.574e-02,  2.304e-02, -3.315e-02,
          3.153e-02,  2.313e-02,  2.911e-03, -4.475e-02],
        [-3.691e-02,  3.060e-02,  1.058e-01,  7.280e-02,  3.350e-03, -5.072e-02,
          3.699e-02, -1.278e-02, -1.567e-03, -2.485e-02],
        [-1.494e-02,  2.975e-02, -1.743e-02, -3.271e-02,  4.850e-02,  3.002e-02,
          1.791e-02, -7.752e-02, -1.199e-02, -3.448e-03],
        [ 1.706e-02, -2.855e-02,  7.826e-02,  9.083e-03,  2.732e-02,  2.573e-02,
         -6.576e-03, -4.030e-02, -1.337e-02, -5.638e-03],
        [-7.139e-03,  8.944e-03, -1.745e-02, -3.496e-02, -1.752e-02,  9.947e-02,
          2.040e-03,  1.493e-02, -2.426e-02, -4.289e-02],
        [-1.134e-03, -2.535e-02, -2.059e-02, -2.773e-03,  3.668e-03, -2.203e-04,
          2.646e-02, -2.012e-02,  5.545e-02, -4.308e-02],
        [-3.127e-02, -3.788e-02,  5.239e-03, -3.360e-02, -7.615e-03,  1.291e-02,
          4.786e-02, -1.238e-04, -6.381e-02,  9.809e-04],
        [ 5.243e-03, -1.381e-03, -8.389e-02, -2.208e-02, -4.282e-04,  9.676e-03,
         -8.334e-03,  9.387e-03,  2.605e-02, -1.314e-02],
        [ 3.562e-02, -5.401e-02, -2.146e-02, -1.917e-02,  2.242e-02, -2.331e-02,
         -1.954e-02, -9.230e-03, -2.017e-02,  3.354e-03],
        [ 1.508e-02,  2.974e-02,  5.264e-02, -1.019e-02,  2.484e-03,  6.353e-02,
          3.241e-02,  6.310e-02,  1.875e-03,  1.993e-02],
        [-2.329e-03,  6.617e-02, -1.668e-02,  2.617e-02, -1.540e-02,  1.686e-03,
         -6.669e-03,  1.315e-02,  2.280e-02,  3.301e-02],
        [-1.581e-02,  6.147e-02, -3.365e-02,  2.153e-02,  2.705e-02, -5.193e-02,
         -1.535e-02,  4.417e-02, -3.097e-03,  2.635e-03],
        [ 1.350e-03,  2.507e-03,  1.433e-02,  1.034e-02, -9.910e-03,  4.010e-02,
         -5.123e-02, -3.983e-03,  1.098e-02,  6.786e-03],
        [-6.648e-02, -4.443e-03, -1.777e-02,  2.413e-02, -1.472e-02,  1.184e-02,
          2.861e-02,  2.082e-02, -1.547e-04, -3.043e-02],
        [-5.924e-02,  7.080e-03,  1.275e-02,  1.192e-02,  1.919e-02, -2.859e-03,
         -1.177e-02, -4.468e-03, -8.582e-03, -1.258e-02],
        [ 1.797e-02,  1.075e-02, -1.227e-02,  1.278e-02, -5.315e-03,  3.281e-02,
          2.407e-02, -5.273e-02,  4.539e-02,  3.034e-02],
        [ 3.484e-03, -1.866e-02,  9.538e-03,  4.847e-03, -6.801e-02, -1.512e-03,
          3.364e-02,  7.297e-03,  2.527e-02, -1.089e-02],
        [ 3.057e-02,  1.779e-02,  3.279e-02, -3.091e-02, -4.823e-03,  4.886e-02,
          1.404e-02, -4.006e-02,  8.595e-03,  5.970e-02],
        [ 5.429e-02, -5.328e-03, -1.635e-02, -1.859e-05,  1.055e-03,  1.960e-02,
         -2.884e-03, -9.857e-03, -2.444e-02,  2.590e-02],
        [-1.986e-02,  6.399e-02,  4.110e-02,  1.407e-03,  3.098e-03,  1.168e-02,
         -3.660e-04, -3.856e-04, -2.204e-02,  4.866e-02],
        [ 1.845e-02,  1.380e-02, -2.820e-02, -2.093e-02,  2.675e-02, -6.594e-03,
         -2.059e-03,  1.310e-02,  3.833e-02, -2.536e-03],
        [ 1.601e-02, -1.631e-02,  3.091e-02, -3.708e-02, -7.073e-03, -5.647e-03,
         -3.280e-02, -4.990e-02, -2.513e-02, -2.848e-02],
        [ 4.886e-02, -1.328e-02,  5.426e-02, -2.310e-02, -2.845e-03, -1.077e-02,
         -6.441e-02, -7.055e-02,  3.126e-02, -2.998e-02],
        [ 1.484e-02,  1.914e-02,  6.879e-03, -1.125e-02, -3.193e-02,  1.566e-02,
         -7.611e-03, -5.728e-02,  2.228e-02,  1.383e-02],
        [ 1.175e-02,  7.829e-03, -4.863e-03, -1.951e-02, -1.322e-03, -8.564e-02,
         -2.530e-02,  4.368e-04,  5.625e-02,  1.444e-02],
        [ 3.280e-03, -5.236e-04, -4.440e-02, -2.386e-02,  1.188e-02,  3.429e-02,
         -5.178e-03, -1.664e-02, -4.905e-02,  1.388e-02],
        [ 2.103e-03, -9.505e-03,  1.154e-02, -2.278e-02, -4.481e-02,  7.777e-02,
         -1.064e-02, -5.932e-02,  2.558e-02, -2.601e-02],
        [ 4.751e-03,  5.056e-05, -1.134e-02,  2.443e-02, -1.965e-02, -1.945e-02,
         -3.448e-02, -8.532e-03, -2.926e-02, -7.662e-05],
        [ 4.852e-02,  1.041e-02, -2.789e-02,  1.838e-02,  3.580e-02,  2.565e-02,
          9.943e-03,  1.573e-02, -3.558e-02,  7.164e-03],
        [ 3.522e-02, -3.749e-02, -1.276e-02,  4.298e-02,  1.772e-02, -3.774e-02,
          3.917e-02, -1.737e-02,  2.648e-02,  2.824e-02],
        [ 4.438e-04,  8.961e-03, -1.000e-02, -2.402e-03,  1.469e-02,  7.272e-03,
         -1.417e-02,  2.285e-02, -1.416e-02, -1.035e-02],
        [ 1.886e-02,  2.332e-02, -5.056e-02,  2.190e-02,  9.371e-03,  1.144e-02,
         -6.051e-02,  1.835e-02,  2.220e-02,  1.145e-03],
        [ 3.536e-03, -3.639e-02, -1.821e-02,  1.531e-02,  1.989e-02,  1.954e-02,
         -4.613e-02,  2.183e-02,  1.217e-02,  6.551e-03],
        [-4.339e-02,  4.939e-02, -2.044e-04, -3.666e-02, -3.241e-02, -4.148e-02,
          3.594e-02,  2.316e-02,  5.515e-02,  7.915e-03]], device='cuda:0')
s after update for 1 param tensor([[2.280, 1.049, 1.272, 1.300, 1.870, 2.126, 2.004, 2.202, 1.098, 2.190],
        [1.860, 1.946, 1.147, 1.221, 1.597, 1.400, 1.288, 1.976, 1.138, 1.804],
        [1.416, 1.377, 1.481, 1.148, 2.008, 1.424, 1.563, 1.358, 0.484, 1.567],
        [2.498, 1.597, 1.751, 1.699, 2.169, 1.763, 1.905, 1.315, 1.393, 1.674],
        [2.363, 2.017, 1.556, 1.306, 2.001, 1.716, 1.522, 1.722, 1.472, 1.166],
        [1.657, 1.497, 1.275, 1.681, 1.776, 1.536, 1.469, 1.605, 1.504, 1.760],
        [2.102, 1.857, 2.066, 0.833, 1.975, 1.622, 1.952, 1.597, 1.090, 1.779],
        [2.001, 1.668, 1.634, 1.766, 1.921, 1.374, 1.537, 1.773, 1.491, 1.368],
        [2.152, 1.509, 1.450, 1.848, 1.999, 1.440, 1.522, 1.555, 1.344, 1.039],
        [2.510, 1.760, 1.797, 1.162, 1.783, 1.894, 1.761, 1.984, 2.005, 1.684],
        [2.367, 1.301, 1.956, 1.688, 1.957, 1.220, 1.832, 2.018, 1.130, 1.409],
        [1.705, 1.643, 1.317, 2.450, 1.862, 1.461, 1.783, 2.230, 1.655, 1.096],
        [1.931, 1.329, 1.394, 0.047, 1.195, 1.629, 1.224, 1.473, 1.599, 0.866],
        [1.894, 1.849, 2.009, 1.721, 1.929, 1.546, 1.965, 1.502, 1.249, 1.108],
        [1.773, 1.613, 1.747, 1.612, 1.689, 2.144, 1.833, 1.174, 1.404, 1.567],
        [1.271, 1.448, 1.650, 1.393, 0.956, 1.083, 1.664, 1.295, 1.584, 2.040],
        [1.510, 1.393, 1.581, 1.464, 1.665, 1.547, 1.022, 1.691, 1.624, 1.273],
        [1.761, 1.469, 1.658, 1.628, 1.952, 1.799, 1.442, 1.877, 1.176, 1.366],
        [2.101, 1.959, 2.007, 0.076, 1.934, 0.870, 1.876, 1.226, 1.605, 1.847],
        [1.501, 2.009, 1.730, 1.836, 1.874, 1.891, 1.719, 1.739, 1.727, 1.503],
        [2.324, 1.303, 1.154, 1.193, 1.353, 1.708, 1.830, 1.939, 1.529, 1.496],
        [1.846, 1.723, 1.570, 1.407, 2.084, 0.712, 1.847, 1.280, 1.567, 1.640],
        [1.940, 1.578, 1.808, 1.446, 1.837, 1.470, 1.894, 1.443, 1.129, 0.593],
        [1.687, 2.033, 2.217, 2.027, 1.932, 1.968, 1.874, 1.566, 1.060, 1.482],
        [1.751, 1.955, 2.345, 1.647, 1.332, 0.970, 0.784, 1.476, 1.339, 1.883],
        [1.845, 1.866, 1.294, 2.186, 1.776, 1.950, 2.115, 1.512, 1.311, 1.596],
        [1.670, 1.842, 2.082, 1.621, 1.776, 1.649, 1.822, 1.609, 1.549, 1.674],
        [1.922, 1.581, 2.268, 0.969, 1.030, 1.565, 1.955, 1.744, 1.999, 1.619],
        [1.689, 0.572, 1.505, 1.512, 1.987, 2.028, 1.892, 2.000, 0.812, 1.993],
        [1.879, 1.291, 2.024, 1.438, 2.007, 1.235, 2.014, 1.043, 1.097, 1.857],
        [1.571, 1.442, 1.539, 2.205, 2.020, 1.025, 1.277, 1.856, 1.288, 1.399],
        [1.541, 1.108, 1.939, 1.536, 1.594, 1.583, 2.125, 1.438, 1.824, 1.106],
        [1.725, 2.070, 0.958, 2.138, 1.094, 1.574, 1.909, 1.718, 1.668, 1.139],
        [1.841, 1.470, 1.371, 2.175, 1.011, 1.571, 1.973, 1.661, 1.345, 1.979],
        [1.249, 1.348, 2.159, 1.569, 1.185, 1.837, 1.563, 1.475, 2.237, 1.332],
        [1.446, 1.796, 1.140, 1.665, 1.687, 0.626, 0.933, 1.409, 1.119, 1.815],
        [1.458, 1.196, 2.140, 1.936, 2.216, 1.754, 1.418, 1.392, 1.479, 2.058],
        [2.033, 1.778, 1.513, 1.032, 1.890, 1.386, 1.521, 1.782, 1.841, 1.865],
        [1.431, 2.019, 1.743, 2.191, 1.939, 1.619, 2.167, 1.363, 2.171, 1.471],
        [1.999, 2.126, 1.993, 1.359, 1.493, 1.206, 0.966, 1.859, 1.426, 1.749],
        [1.798, 1.496, 2.162, 1.177, 2.020, 1.472, 1.596, 1.075, 1.572, 1.952],
        [1.552, 1.730, 1.387, 1.930, 2.292, 2.000, 2.091, 1.774, 1.624, 1.251],
        [1.933, 1.991, 1.545, 1.405, 1.846, 2.151, 1.081, 1.741, 1.487, 2.285],
        [1.511, 1.856, 1.970, 1.793, 1.989, 1.599, 1.917, 1.489, 1.408, 1.698],
        [0.930, 1.071, 1.701, 1.622, 1.686, 1.204, 2.075, 1.319, 1.637, 1.210],
        [1.300, 1.767, 1.466, 1.736, 1.538, 1.446, 1.913, 2.017, 1.788, 1.297],
        [1.795, 1.004, 1.481, 1.357, 2.169, 1.998, 1.576, 0.941, 1.562, 2.100],
        [2.210, 1.424, 1.505, 1.538, 1.989, 1.530, 1.375, 2.109, 1.364, 1.791],
        [1.828, 1.895, 2.290, 1.529, 2.089, 1.376, 2.243, 1.854, 1.369, 1.793],
        [1.630, 0.979, 1.461, 0.990, 1.530, 1.520, 1.892, 1.016, 1.541, 1.731],
        [1.551, 0.854, 1.292, 2.100, 2.013, 1.872, 1.531, 1.691, 1.016, 0.777],
        [2.227, 1.557, 2.155, 1.386, 1.502, 1.724, 1.654, 1.363, 0.498, 2.335],
        [1.580, 1.862, 1.604, 1.690, 1.117, 1.499, 1.323, 1.387, 1.847, 1.630],
        [1.437, 1.792, 1.682, 1.570, 1.740, 2.111, 2.011, 1.399, 1.678, 1.617],
        [1.237, 1.621, 1.534, 1.447, 1.587, 2.117, 1.301, 1.405, 0.386, 1.156],
        [1.754, 1.558, 2.167, 1.618, 1.273, 1.413, 1.405, 1.687, 0.814, 1.741],
        [1.735, 1.407, 2.066, 0.880, 1.131, 1.756, 1.779, 1.390, 1.066, 1.424],
        [1.487, 1.186, 2.141, 1.887, 1.538, 1.835, 1.635, 1.274, 1.188, 1.473],
        [2.043, 2.148, 1.571, 1.323, 2.044, 1.617, 1.922, 1.929, 2.000, 1.285],
        [1.353, 1.463, 0.659, 1.435, 2.109, 1.689, 1.952, 1.850, 1.969, 1.626],
        [2.390, 1.817, 1.801, 1.732, 1.791, 1.998, 2.126, 2.331, 1.862, 2.070],
        [1.878, 2.114, 2.080, 1.850, 1.824, 1.715, 1.805, 2.028, 1.539, 2.081],
        [1.880, 1.975, 2.017, 0.746, 2.059, 1.696, 2.219, 2.086, 1.769, 1.706],
        [2.146, 2.235, 1.842, 0.553, 1.684, 1.607, 2.183, 1.997, 1.958, 1.949],
        [2.009, 0.961, 1.365, 1.291, 1.588, 1.382, 2.095, 1.943, 1.579, 1.836],
        [2.288, 1.574, 2.058, 0.767, 2.359, 1.760, 2.341, 2.533, 1.273, 1.577],
        [2.342, 1.957, 2.252, 1.693, 1.874, 1.942, 2.164, 2.322, 1.541, 2.201],
        [2.054, 1.849, 1.957, 1.659, 1.890, 1.928, 2.321, 1.812, 1.976, 2.171],
        [1.570, 1.186, 1.833, 1.741, 1.410, 1.798, 1.372, 1.822, 1.391, 1.726],
        [2.060, 1.975, 2.150, 1.322, 1.662, 2.361, 2.234, 1.975, 1.851, 1.890],
        [1.808, 1.894, 1.503, 1.451, 1.254, 2.205, 1.072, 2.384, 1.579, 1.792],
        [1.250, 1.381, 2.059, 1.751, 1.603, 1.024, 1.696, 1.614, 1.703, 1.568],
        [1.263, 1.292, 1.836, 1.710, 1.829, 1.423, 1.917, 1.790, 2.018, 1.032],
        [2.133, 1.598, 1.692, 1.562, 1.269, 1.958, 1.504, 2.119, 1.789, 1.704],
        [1.363, 1.911, 1.371, 1.587, 1.580, 1.385, 1.229, 1.896, 1.930, 1.774],
        [1.832, 1.300, 2.167, 1.763, 1.462, 1.806, 1.520, 2.265, 1.882, 1.312],
        [1.168, 1.446, 1.227, 1.034, 1.960, 2.027, 1.108, 1.513, 1.864, 2.110],
        [1.228, 1.777, 1.597, 1.429, 1.798, 1.159, 1.939, 1.753, 1.130, 0.972],
        [1.967, 1.764, 1.742, 1.488, 1.631, 1.784, 1.651, 1.438, 1.797, 1.163],
        [1.803, 0.893, 1.896, 1.810, 0.668, 1.567, 1.585, 2.310, 1.421, 1.747],
        [2.103, 1.313, 1.317, 1.800, 1.759, 0.455, 1.371, 1.154, 1.743, 2.005],
        [2.101, 1.756, 1.811, 1.308, 2.054, 1.466, 1.384, 1.228, 1.823, 1.770],
        [2.170, 1.904, 1.030, 1.796, 2.001, 1.520, 2.250, 1.736, 1.673, 1.379],
        [2.272, 1.652, 1.602, 1.922, 1.690, 1.344, 1.637, 1.936, 1.879, 1.897],
        [1.891, 1.611, 1.287, 1.578, 1.432, 1.222, 1.635, 1.585, 2.141, 1.774],
        [1.239, 1.729, 1.993, 1.454, 1.978, 1.240, 1.611, 1.113, 2.248, 1.753],
        [1.878, 1.868, 1.431, 1.211, 1.906, 1.696, 0.798, 1.574, 2.003, 1.936],
        [1.902, 1.728, 1.256, 1.789, 1.374, 0.999, 2.150, 1.784, 1.872, 1.556],
        [1.967, 1.899, 0.981, 1.314, 1.154, 1.723, 1.665, 1.869, 1.803, 1.734],
        [1.403, 1.618, 2.031, 1.599, 1.963, 1.305, 2.040, 1.797, 1.840, 1.227],
        [1.507, 1.776, 1.684, 1.775, 0.234, 1.726, 1.113, 1.407, 1.116, 1.743],
        [1.793, 1.137, 1.141, 0.713, 2.002, 0.757, 1.729, 1.881, 1.444, 2.168],
        [2.319, 1.775, 1.751, 1.228, 1.587, 1.971, 1.324, 1.746, 2.068, 1.372],
        [1.936, 2.085, 1.548, 1.843, 1.628, 1.095, 1.847, 1.793, 1.991, 2.063],
        [1.941, 1.930, 1.794, 1.780, 1.877, 1.692, 1.833, 1.874, 1.316, 1.964],
        [1.770, 1.781, 1.991, 1.518, 2.056, 1.394, 2.085, 2.169, 1.815, 1.303],
        [1.657, 1.329, 1.794, 1.265, 1.763, 2.053, 2.019, 1.888, 0.923, 1.911],
        [2.014, 1.899, 1.642, 1.905, 1.388, 1.883, 2.166, 1.701, 1.650, 1.551],
        [1.324, 1.925, 1.893, 1.383, 1.168, 1.790, 2.068, 1.457, 1.873, 1.372],
        [2.181, 1.610, 1.723, 1.313, 0.722, 1.806, 2.067, 2.002, 2.106, 1.925]],
       device='cuda:0')
b after update for 1 param tensor([[67.057, 45.479, 50.084, 50.625, 60.724, 64.756, 62.858, 65.893, 46.536,
         65.721],
        [60.571, 61.941, 47.558, 49.061, 56.115, 52.551, 50.400, 62.425, 47.374,
         59.652],
        [52.838, 52.105, 54.034, 47.572, 62.930, 52.994, 55.522, 51.750, 30.887,
         55.581],
        [70.180, 56.116, 58.768, 57.888, 65.397, 58.970, 61.286, 50.929, 52.410,
         57.451],
        [68.270, 63.072, 55.397, 50.746, 62.810, 58.169, 54.784, 58.273, 53.883,
         47.953],
        [57.156, 54.339, 50.150, 57.577, 59.172, 55.032, 53.822, 56.254, 54.467,
         58.909],
        [64.383, 60.519, 63.834, 40.535, 62.402, 56.563, 62.040, 56.118, 46.362,
         59.234],
        [62.817, 57.353, 56.774, 59.011, 61.554, 52.058, 55.053, 59.128, 54.216,
         51.944],
        [65.138, 54.552, 53.469, 60.362, 62.781, 53.287, 54.783, 55.379, 51.479,
         45.255],
        [70.361, 58.908, 59.535, 47.866, 59.293, 61.117, 58.932, 62.552, 62.876,
         57.626],
        [68.327, 50.661, 62.113, 57.699, 62.119, 49.052, 60.099, 63.089, 47.207,
         52.705],
        [57.978, 56.923, 50.963, 69.504, 60.602, 53.680, 59.294, 66.320, 57.132,
         46.482],
        [61.705, 51.195, 52.433,  9.642, 48.535, 56.685, 49.139, 53.891, 56.159,
         41.315],
        [61.118, 60.379, 62.943, 58.257, 61.673, 55.222, 62.248, 54.425, 49.622,
         46.736],
        [59.123, 56.407, 58.693, 56.375, 57.705, 65.021, 60.124, 48.121, 52.616,
         55.593],
        [50.067, 53.440, 57.038, 52.411, 43.417, 46.208, 57.283, 50.541, 55.898,
         63.430],
        [54.567, 52.412, 55.835, 53.725, 57.307, 55.239, 44.890, 57.753, 56.597,
         50.104],
        [58.938, 53.817, 57.176, 56.661, 62.049, 59.563, 53.325, 60.833, 48.164,
         51.899],
        [64.373, 62.149, 62.916, 12.228, 61.760, 41.412, 60.828, 49.171, 56.265,
         60.360],
        [54.403, 62.949, 58.410, 60.169, 60.787, 61.063, 58.223, 58.561, 58.358,
         54.435],
        [67.704, 50.691, 47.703, 48.506, 51.647, 58.032, 60.081, 61.836, 54.909,
         54.312],
        [60.340, 58.286, 55.638, 52.669, 64.112, 37.483, 60.347, 50.233, 55.588,
         56.875],
        [61.852, 55.776, 59.705, 53.391, 60.180, 53.837, 61.122, 53.339, 47.193,
         34.194],
        [57.672, 63.312, 66.113, 63.225, 61.733, 62.301, 60.792, 55.575, 45.712,
         54.067],
        [58.765, 62.097, 68.006, 56.999, 51.251, 43.734, 39.314, 53.951, 51.383,
         60.936],
        [60.323, 60.667, 50.509, 65.654, 59.185, 62.005, 64.575, 54.606, 50.849,
         56.095],
        [57.384, 60.277, 64.073, 56.530, 59.183, 57.030, 59.944, 56.325, 55.273,
         57.456],
        [61.559, 55.837, 66.875, 43.715, 45.076, 55.554, 62.099, 58.646, 62.787,
         56.511],
        [57.717, 33.584, 54.483, 54.609, 62.601, 63.245, 61.090, 62.804, 40.021,
         62.694],
        [60.865, 50.465, 63.185, 53.244, 62.906, 49.354, 63.024, 45.343, 46.517,
         60.509],
        [55.667, 53.324, 55.082, 65.937, 63.108, 44.953, 50.185, 60.502, 50.391,
         52.526],
        [55.127, 46.750, 61.830, 55.042, 56.062, 55.871, 64.730, 53.259, 59.982,
         46.711],
        [58.325, 63.889, 43.459, 64.925, 46.452, 55.714, 61.353, 58.200, 57.346,
         47.395],
        [60.256, 53.839, 51.994, 65.489, 44.651, 55.652, 62.378, 57.231, 51.503,
         62.475],
        [49.634, 51.551, 65.250, 55.619, 48.340, 60.182, 55.515, 53.941, 66.417,
         51.253],
        [53.396, 59.506, 47.404, 57.300, 57.672, 35.133, 42.896, 52.718, 46.966,
         59.819],
        [53.622, 48.565, 64.960, 61.781, 66.100, 58.806, 52.884, 52.387, 54.010,
         63.708],
        [63.317, 59.208, 54.619, 45.117, 61.046, 52.281, 54.765, 59.280, 60.255,
         60.645],
        [53.117, 63.102, 58.630, 65.726, 61.830, 56.496, 65.371, 51.843, 65.431,
         53.855],
        [62.793, 64.751, 62.684, 51.776, 54.263, 48.764, 43.653, 60.540, 53.026,
         58.724],
        [59.549, 54.308, 65.293, 48.175, 63.117, 53.875, 56.109, 46.033, 55.679,
         62.047],
        [55.321, 58.403, 52.294, 61.696, 67.236, 62.804, 64.208, 59.147, 56.590,
         49.677],
        [61.734, 62.667, 55.191, 52.639, 60.343, 65.130, 46.179, 58.596, 54.158,
         67.128],
        [54.595, 60.503, 62.336, 59.457, 62.624, 56.152, 61.489, 54.193, 52.692,
         57.860],
        [42.836, 45.962, 57.913, 56.564, 57.657, 48.727, 63.973, 50.995, 56.823,
         48.857],
        [50.631, 59.037, 53.764, 58.507, 55.077, 53.391, 61.417, 63.069, 59.384,
         50.578],
        [59.490, 44.487, 54.050, 51.723, 65.397, 62.772, 55.751, 43.080, 55.500,
         64.356],
        [66.016, 52.997, 54.487, 55.075, 62.624, 54.933, 52.072, 64.484, 51.868,
         59.435],
        [60.041, 61.124, 67.196, 54.904, 64.190, 52.099, 66.510, 60.459, 51.960,
         59.460],
        [56.702, 43.944, 53.678, 44.194, 54.933, 54.745, 61.090, 44.759, 55.131,
         58.421],
        [55.308, 41.043, 50.481, 64.347, 63.012, 60.763, 54.947, 57.751, 44.763,
         39.146],
        [66.276, 55.408, 65.185, 52.289, 54.424, 58.303, 57.116, 51.836, 31.345,
         67.852],
        [55.815, 60.597, 56.247, 57.726, 46.927, 54.378, 51.087, 52.301, 60.357,
         56.697],
        [53.228, 59.451, 57.590, 55.647, 58.580, 64.523, 62.970, 52.524, 57.518,
         56.472],
        [49.383, 56.540, 55.002, 53.426, 55.935, 64.609, 50.654, 52.643, 27.575,
         47.745],
        [58.818, 55.438, 65.374, 56.483, 50.113, 52.794, 52.640, 57.676, 40.059,
         58.595],
        [58.490, 52.677, 63.822, 41.656, 47.228, 58.846, 59.232, 52.362, 45.856,
         52.988],
        [54.150, 48.360, 64.971, 61.004, 55.069, 60.161, 56.775, 50.120, 48.403,
         53.897],
        [63.466, 65.085, 55.659, 51.078, 63.496, 56.465, 61.559, 61.672, 62.800,
         50.348],
        [51.648, 53.715, 36.062, 53.195, 64.494, 57.708, 62.040, 60.408, 62.307,
         56.626],
        [68.659, 59.863, 59.595, 58.440, 59.428, 62.769, 64.754, 67.806, 60.593,
         63.895],
        [60.856, 64.571, 64.043, 60.404, 59.976, 58.152, 59.658, 63.237, 55.084,
         64.059],
        [60.889, 62.401, 63.068, 38.348, 63.724, 57.827, 66.145, 64.133, 59.069,
         57.994],
        [65.061, 66.387, 60.264, 33.031, 57.626, 56.292, 65.615, 62.751, 62.139,
         62.001],
        [62.936, 43.541, 51.877, 50.452, 55.962, 52.211, 64.275, 61.901, 55.802,
         60.175],
        [67.167, 55.716, 63.705, 38.887, 68.212, 58.911, 67.945, 70.679, 50.097,
         55.774],
        [67.963, 62.124, 66.640, 57.781, 60.787, 61.887, 65.320, 67.673, 55.131,
         65.885],
        [63.638, 60.391, 62.130, 57.205, 61.045, 61.662, 67.658, 59.770, 62.421,
         65.431],
        [55.640, 48.361, 60.120, 58.603, 52.733, 59.546, 52.021, 59.936, 52.368,
         58.347],
        [63.742, 62.401, 65.110, 51.065, 57.258, 68.233, 66.374, 62.415, 60.410,
         61.049],
        [59.716, 61.108, 54.451, 53.493, 49.723, 65.941, 45.971, 68.563, 55.796,
         59.452],
        [49.652, 52.183, 63.723, 58.764, 56.228, 44.935, 57.838, 56.413, 57.950,
         55.601],
        [49.912, 50.479, 60.164, 58.075, 60.049, 52.968, 61.484, 59.411, 63.084,
         45.116],
        [64.858, 56.134, 57.760, 55.503, 50.019, 62.132, 54.466, 64.647, 59.389,
         57.969],
        [51.847, 61.388, 51.991, 55.946, 55.815, 52.255, 49.240, 61.149, 61.692,
         59.148],
        [60.100, 50.634, 65.377, 58.957, 53.697, 59.684, 54.740, 66.830, 60.915,
         50.858],
        [47.995, 53.396, 49.190, 45.154, 62.170, 63.227, 46.754, 54.623, 60.626,
         64.507],
        [49.219, 59.202, 56.111, 53.093, 59.543, 47.817, 61.842, 58.796, 47.201,
         43.790],
        [62.275, 58.986, 58.608, 54.161, 56.716, 59.316, 57.064, 53.259, 59.535,
         47.880],
        [59.625, 41.958, 61.154, 59.743, 36.308, 55.590, 55.911, 67.499, 52.945,
         58.699],
        [64.400, 50.886, 50.965, 59.584, 58.903, 29.940, 51.994, 47.707, 58.631,
         62.888],
        [64.365, 58.844, 59.761, 50.796, 63.640, 53.761, 52.238, 49.217, 59.951,
         59.081],
        [65.413, 61.270, 45.071, 59.520, 62.814, 54.754, 66.607, 58.509, 57.435,
         52.145],
        [66.934, 57.080, 56.202, 61.567, 57.729, 51.487, 56.823, 61.785, 60.877,
         61.171],
        [61.065, 56.360, 50.373, 55.787, 53.137, 49.084, 56.789, 55.910, 64.975,
         59.150],
        [49.429, 58.396, 62.694, 53.538, 62.462, 49.449, 56.359, 46.850, 66.577,
         58.802],
        [60.849, 60.692, 53.125, 48.863, 61.314, 57.829, 39.668, 55.711, 62.852,
         61.781],
        [61.247, 58.381, 49.761, 59.391, 52.047, 44.380, 65.111, 59.307, 60.761,
         55.400],
        [62.282, 61.196, 43.994, 50.895, 47.709, 58.286, 57.293, 60.705, 59.636,
         58.481],
        [52.594, 56.483, 63.281, 56.146, 62.211, 50.720, 63.421, 59.537, 60.231,
         49.194],
        [54.510, 59.174, 57.620, 59.168, 21.485, 58.341, 46.847, 52.668, 46.915,
         58.627],
        [59.470, 47.354, 47.435, 37.489, 62.833, 38.625, 58.396, 60.903, 53.371,
         65.383],
        [67.620, 59.163, 58.768, 49.215, 55.946, 62.350, 51.095, 58.671, 63.864,
         52.014],
        [61.796, 64.124, 55.246, 60.279, 56.657, 46.461, 60.344, 59.459, 62.665,
         63.780],
        [61.872, 61.695, 59.476, 59.245, 60.842, 57.765, 60.129, 60.784, 50.944,
         62.228],
        [59.087, 59.258, 62.664, 54.710, 63.677, 52.433, 64.115, 65.404, 59.819,
         50.682],
        [57.169, 51.197, 59.483, 49.942, 58.955, 63.627, 63.095, 61.017, 42.664,
         61.385],
        [63.017, 61.198, 56.905, 61.287, 52.315, 60.938, 65.350, 57.917, 57.038,
         55.306],
        [51.098, 61.619, 61.093, 52.231, 47.984, 59.421, 63.855, 53.595, 60.783,
         52.018],
        [65.584, 56.344, 58.298, 50.894, 37.732, 59.670, 63.840, 62.839, 64.440,
         61.618]], device='cuda:0')
clipping threshold 1.154907057130173
a after update for 1 param tensor([ 0.011,  0.004,  0.028,  0.016,  0.045, -0.015,  0.025, -0.019,  0.046,
        -0.007, -0.017, -0.032,  0.031, -0.072, -0.038, -0.007, -0.037,  0.018,
        -0.000, -0.014,  0.005, -0.001, -0.033,  0.068,  0.054,  0.022, -0.020,
         0.030,  0.045, -0.064, -0.054, -0.044, -0.013,  0.021, -0.033, -0.001,
         0.056, -0.015, -0.013,  0.011, -0.057, -0.001, -0.003,  0.008,  0.037,
         0.002,  0.012,  0.005, -0.019, -0.018,  0.005, -0.003,  0.006, -0.021,
        -0.030, -0.021,  0.072,  0.058, -0.042, -0.040, -0.020,  0.023, -0.002,
         0.003, -0.008, -0.043, -0.003,  0.023,  0.036, -0.039,  0.049, -0.045,
         0.004, -0.003, -0.005, -0.033, -0.043,  0.002, -0.055,  0.010,  0.013,
         0.019,  0.014,  0.021, -0.025, -0.022,  0.026,  0.044,  0.014, -0.021,
        -0.049,  0.009, -0.016,  0.008, -0.061,  0.008, -0.012,  0.005,  0.017,
        -0.025], device='cuda:0')
s after update for 1 param tensor([1.746, 1.022, 1.680, 0.519, 1.528, 1.424, 1.296, 0.867, 1.865, 0.950,
        1.612, 1.964, 1.581, 2.015, 1.996, 1.819, 1.144, 1.365, 1.426, 1.200,
        1.288, 1.386, 0.975, 2.140, 1.929, 1.268, 1.262, 1.048, 1.264, 1.584,
        1.723, 1.585, 1.491, 1.668, 2.349, 0.813, 1.813, 1.925, 1.446, 1.595,
        1.399, 0.828, 0.647, 1.613, 1.763, 2.136, 1.395, 0.465, 1.086, 1.591,
        1.618, 0.978, 1.838, 0.769, 1.513, 1.329, 1.347, 2.036, 1.714, 1.332,
        1.529, 0.931, 1.566, 1.780, 2.161, 1.874, 0.692, 1.547, 1.118, 1.464,
        1.253, 1.363, 0.197, 1.690, 1.463, 1.454, 1.069, 1.840, 1.976, 1.456,
        1.771, 1.629, 1.169, 1.769, 1.560, 1.625, 1.340, 1.351, 1.498, 2.056,
        1.987, 1.162, 2.017, 1.601, 1.817, 1.119, 1.433, 1.608, 1.725, 1.461],
       device='cuda:0')
b after update for 1 param tensor([58.672, 44.887, 57.562, 31.987, 54.885, 52.997, 50.559, 41.349, 60.653,
        43.279, 56.378, 62.231, 55.832, 63.038, 62.746, 59.885, 47.492, 51.888,
        53.032, 48.652, 50.395, 52.281, 43.849, 64.967, 61.673, 50.009, 49.880,
        45.471, 49.923, 55.892, 58.294, 55.910, 54.220, 57.355, 68.063, 40.029,
        59.801, 61.610, 53.406, 56.092, 52.520, 40.410, 35.725, 56.392, 58.957,
        64.904, 52.457, 30.266, 46.275, 56.005, 56.488, 43.907, 60.205, 38.949,
        54.625, 51.187, 51.541, 63.370, 58.143, 51.249, 54.918, 42.843, 55.576,
        59.241, 65.276, 60.789, 36.943, 55.224, 46.953, 53.735, 49.711, 51.854,
        19.685, 57.729, 53.717, 53.550, 45.908, 60.244, 62.428, 53.589, 59.097,
        56.686, 48.006, 59.058, 55.466, 56.611, 51.406, 51.625, 54.344, 63.678,
        62.605, 47.865, 63.069, 56.190, 59.860, 46.974, 53.165, 56.314, 58.317,
        53.681], device='cuda:0')
clipping threshold 1.154907057130173
a after update for 1 param tensor([[[-0.042],
         [ 0.007],
         [ 0.025],
         [ 0.011],
         [-0.048],
         [ 0.002],
         [ 0.010],
         [-0.021],
         [ 0.048],
         [ 0.029]],

        [[ 0.011],
         [ 0.006],
         [ 0.012],
         [-0.014],
         [-0.029],
         [-0.015],
         [-0.013],
         [ 0.006],
         [ 0.021],
         [-0.009]],

        [[-0.020],
         [ 0.002],
         [-0.010],
         [-0.044],
         [-0.016],
         [-0.003],
         [-0.017],
         [-0.004],
         [-0.006],
         [-0.034]],

        [[ 0.050],
         [ 0.017],
         [-0.015],
         [-0.034],
         [-0.011],
         [-0.006],
         [-0.067],
         [ 0.001],
         [ 0.009],
         [ 0.001]],

        [[ 0.046],
         [ 0.003],
         [-0.045],
         [-0.035],
         [ 0.016],
         [ 0.073],
         [ 0.007],
         [-0.025],
         [-0.043],
         [ 0.027]],

        [[-0.010],
         [-0.015],
         [ 0.035],
         [-0.003],
         [-0.010],
         [ 0.043],
         [-0.015],
         [ 0.050],
         [-0.014],
         [-0.023]],

        [[ 0.032],
         [ 0.025],
         [ 0.076],
         [ 0.048],
         [-0.003],
         [ 0.005],
         [ 0.018],
         [ 0.033],
         [-0.005],
         [-0.004]],

        [[ 0.018],
         [-0.009],
         [-0.005],
         [ 0.038],
         [-0.007],
         [ 0.002],
         [ 0.115],
         [ 0.014],
         [ 0.020],
         [ 0.018]],

        [[ 0.009],
         [ 0.001],
         [ 0.017],
         [ 0.001],
         [ 0.035],
         [ 0.049],
         [ 0.008],
         [ 0.055],
         [ 0.026],
         [-0.071]],

        [[ 0.016],
         [-0.007],
         [-0.019],
         [-0.005],
         [-0.046],
         [ 0.045],
         [-0.025],
         [ 0.024],
         [-0.022],
         [-0.016]]], device='cuda:0')
s after update for 1 param tensor([[[1.582],
         [1.790],
         [2.104],
         [1.763],
         [1.823],
         [1.466],
         [1.738],
         [1.720],
         [2.090],
         [1.854]],

        [[1.560],
         [1.889],
         [1.704],
         [1.118],
         [1.034],
         [1.895],
         [2.033],
         [1.471],
         [1.738],
         [2.039]],

        [[1.123],
         [0.290],
         [1.921],
         [1.708],
         [1.834],
         [1.881],
         [1.016],
         [1.527],
         [1.609],
         [2.056]],

        [[1.739],
         [1.651],
         [1.789],
         [1.811],
         [0.957],
         [1.235],
         [2.124],
         [1.538],
         [1.349],
         [1.487]],

        [[1.513],
         [1.656],
         [1.851],
         [1.820],
         [1.282],
         [1.682],
         [1.199],
         [1.938],
         [1.387],
         [0.958]],

        [[1.425],
         [1.511],
         [1.343],
         [1.048],
         [1.974],
         [1.862],
         [1.192],
         [2.108],
         [1.410],
         [2.304]],

        [[1.692],
         [1.643],
         [1.697],
         [1.618],
         [1.463],
         [1.670],
         [2.097],
         [0.702],
         [1.259],
         [1.522]],

        [[1.609],
         [2.089],
         [1.802],
         [1.584],
         [1.971],
         [1.622],
         [2.016],
         [1.436],
         [1.100],
         [1.692]],

        [[1.160],
         [1.012],
         [1.896],
         [1.762],
         [1.862],
         [1.394],
         [0.688],
         [1.282],
         [1.446],
         [2.273]],

        [[1.765],
         [1.544],
         [1.180],
         [1.844],
         [2.348],
         [1.726],
         [0.658],
         [1.656],
         [1.737],
         [1.334]]], device='cuda:0')
b after update for 1 param tensor([[[55.848],
         [59.413],
         [64.417],
         [58.963],
         [59.951],
         [53.760],
         [58.542],
         [58.240],
         [64.193],
         [60.458]],

        [[55.459],
         [61.032],
         [57.963],
         [46.947],
         [45.151],
         [61.133],
         [63.325],
         [53.853],
         [58.543],
         [63.413]],

        [[47.063],
         [23.923],
         [61.548],
         [58.041],
         [60.142],
         [60.898],
         [44.751],
         [54.877],
         [56.325],
         [63.680]],

        [[58.557],
         [57.057],
         [59.404],
         [59.766],
         [43.438],
         [49.351],
         [64.713],
         [55.069],
         [51.574],
         [54.144]],

        [[54.624],
         [57.151],
         [60.421],
         [59.906],
         [50.277],
         [57.592],
         [48.635],
         [61.816],
         [52.293],
         [43.470]],

        [[53.017],
         [54.582],
         [51.470],
         [45.458],
         [62.386],
         [60.596],
         [48.489],
         [64.478],
         [52.732],
         [67.405]],

        [[57.756],
         [56.919],
         [57.853],
         [56.494],
         [53.708],
         [57.384],
         [64.305],
         [37.214],
         [49.831],
         [54.794]],

        [[56.332],
         [64.185],
         [59.608],
         [55.881],
         [62.344],
         [56.557],
         [63.057],
         [53.207],
         [46.577],
         [57.772]],

        [[47.837],
         [44.673],
         [61.154],
         [58.938],
         [60.588],
         [52.437],
         [36.825],
         [50.282],
         [53.402],
         [66.955]],

        [[59.004],
         [55.174],
         [48.235],
         [60.296],
         [68.045],
         [58.336],
         [36.009],
         [57.139],
         [58.526],
         [51.289]]], device='cuda:0')
clipping threshold 1.154907057130173
a after update for 1 param tensor([[ 0.041],
        [-0.011],
        [ 0.006],
        [-0.015],
        [-0.054],
        [-0.011],
        [ 0.048],
        [ 0.041],
        [-0.013],
        [ 0.021]], device='cuda:0')
s after update for 1 param tensor([[1.742],
        [1.493],
        [0.926],
        [1.346],
        [1.841],
        [1.574],
        [2.369],
        [1.516],
        [1.729],
        [1.895]], device='cuda:0')
b after update for 1 param tensor([[58.611],
        [54.269],
        [42.725],
        [51.513],
        [60.255],
        [55.713],
        [68.352],
        [54.684],
        [58.385],
        [61.131]], device='cuda:0')
clipping threshold 1.154907057130173
||w||^2 0.7746759901329674
exp ma of ||w||^2 1.8125599476688266
||w|| 0.8801567986063434
exp ma of ||w|| 1.2614318342313073
||w||^2 3.9013670502673987
exp ma of ||w||^2 1.7507582184010912
||w|| 1.975187851893434
exp ma of ||w|| 1.247892260166948
cuda
Objective function 13.33 = squared loss an data 11.10 + 0.5*rho*h**2 0.352305 + alpha*h 1.178404 + L2reg 0.64 + L1reg 0.06 ; SHD = 19 ; DAG True
Proportion of microbatches that were clipped  0.7696190166069684
iteration 1 in inner loop, alpha 14.03845982341929 rho 100.0 h 0.08394110373698105
1210
cuda
Objective function 16.50 = squared loss an data 11.10 + 0.5*rho*h**2 3.523054 + alpha*h 1.178404 + L2reg 0.64 + L1reg 0.06 ; SHD = 19 ; DAG True
||w||^2 45478779526.898994
exp ma of ||w||^2 53350255329.210175
||w|| 213257.54271982735
exp ma of ||w|| 206087.4774296192
||w||^2 160677.78294709278
exp ma of ||w||^2 4130182.77782259
||w|| 400.84633333372625
exp ma of ||w|| 1027.6507385640361
||w||^2 1265.630165139428
exp ma of ||w||^2 191173.21702597293
||w|| 35.57569627062031
exp ma of ||w|| 146.88935068970477
||w||^2 2.3205693439615334
exp ma of ||w||^2 1.8414352983238453
||w|| 1.5233415060194262
exp ma of ||w|| 1.2827562424453423
cuda
Objective function 13.23 = squared loss an data 11.55 + 0.5*rho*h**2 0.477181 + alpha*h 0.433687 + L2reg 0.71 + L1reg 0.05 ; SHD = 21 ; DAG True
Proportion of microbatches that were clipped  0.7716836734693877
iteration 2 in inner loop, alpha 14.03845982341929 rho 1000.0 h 0.030892750266410474
iteration 3 in outer loop, alpha = 44.93121008982976, rho = 1000.0, h = 0.030892750266410474
cuda
1210
cuda
Objective function 14.18 = squared loss an data 11.55 + 0.5*rho*h**2 0.477181 + alpha*h 1.388049 + L2reg 0.71 + L1reg 0.05 ; SHD = 21 ; DAG True
||w||^2 14482936040.60869
exp ma of ||w||^2 44773336018.01522
||w|| 120345.07069510031
exp ma of ||w|| 184202.90313361495
||w||^2 1436493752.6242917
exp ma of ||w||^2 8292751623.536881
||w|| 37901.10489978216
exp ma of ||w|| 75194.55157406116
||w||^2 0.5949468955883573
exp ma of ||w||^2 4.723741794819422
||w|| 0.7713280077816165
exp ma of ||w|| 1.4966884712366533
||w||^2 1.16336507563491
exp ma of ||w||^2 2.2430741430184753
||w|| 1.078594027257202
exp ma of ||w|| 1.3930350243120273
||w||^2 1.8671268695374572
exp ma of ||w||^2 1.9297953757024944
||w|| 1.3664285087546502
exp ma of ||w|| 1.3297608126768994
cuda
Objective function 13.57 = squared loss an data 11.76 + 0.5*rho*h**2 0.172269 + alpha*h 0.834002 + L2reg 0.74 + L1reg 0.06 ; SHD = 20 ; DAG True
Proportion of microbatches that were clipped  0.7728813559322034
iteration 1 in inner loop, alpha 44.93121008982976 rho 1000.0 h 0.018561758530147188
1210
cuda
Objective function 15.12 = squared loss an data 11.76 + 0.5*rho*h**2 1.722694 + alpha*h 0.834002 + L2reg 0.74 + L1reg 0.06 ; SHD = 20 ; DAG True
||w||^2 848218216712.299
exp ma of ||w||^2 509359832775.3968
||w|| 920987.6311396906
exp ma of ||w|| 577498.1336452906
||w||^2 76945703.11938104
exp ma of ||w||^2 200378842.2805752
||w|| 8771.86998988135
exp ma of ||w|| 10382.393946835173
||w||^2 1602696.4670173845
exp ma of ||w||^2 24610897.311038446
||w|| 1265.9764875452404
exp ma of ||w|| 3178.2768990364134
||w||^2 210427.99806431192
exp ma of ||w||^2 7114611.384340123
||w|| 458.72431597236255
exp ma of ||w|| 1530.65363819599
||w||^2 111476.01482709272
exp ma of ||w||^2 1124602.2480539212
||w|| 333.88024024654817
exp ma of ||w|| 485.1170188052487
||w||^2 7275.789021908246
exp ma of ||w||^2 463100.41207439493
||w|| 85.29823574909534
exp ma of ||w|| 273.3031922185144
||w||^2 0.9798792049779123
exp ma of ||w||^2 2.236548397168584
||w|| 0.9898884810815369
exp ma of ||w|| 1.4226309161994561
||w||^2 1.2703334871414051
exp ma of ||w||^2 1.8770329890782598
||w|| 1.1270907182393994
exp ma of ||w|| 1.3079181773611086
||w||^2 0.868181176721868
exp ma of ||w||^2 1.7153124450082755
||w|| 0.9317624035782234
exp ma of ||w|| 1.2638101878869765
cuda
Objective function 13.57 = squared loss an data 12.05 + 0.5*rho*h**2 0.333472 + alpha*h 0.366938 + L2reg 0.76 + L1reg 0.06 ; SHD = 21 ; DAG True
Proportion of microbatches that were clipped  0.7750556792873051
iteration 2 in inner loop, alpha 44.93121008982976 rho 10000.0 h 0.008166658616836742
1210
cuda
Objective function 16.57 = squared loss an data 12.05 + 0.5*rho*h**2 3.334716 + alpha*h 0.366938 + L2reg 0.76 + L1reg 0.06 ; SHD = 21 ; DAG True
||w||^2 363867.476701269
exp ma of ||w||^2 17172859.240683325
||w|| 603.2142875473598
exp ma of ||w|| 1699.0398334511042
||w||^2 106551.23782365111
exp ma of ||w||^2 8669467.446409125
||w|| 326.42187093338447
exp ma of ||w|| 1120.2749285897257
||w||^2 12.953787139395725
exp ma of ||w||^2 3953.6392311855348
||w|| 3.599136999253533
exp ma of ||w|| 6.81449499065716
||w||^2 6.40259422973285
exp ma of ||w||^2 210.7174282712071
||w|| 2.53033480585729
exp ma of ||w|| 2.4106915388159824
||w||^2 1.186380951335207
exp ma of ||w||^2 2.0185772775645314
||w|| 1.0892111601224104
exp ma of ||w|| 1.3573794435265294
||w||^2 0.4921151646671427
exp ma of ||w||^2 2.0485591645580845
||w|| 0.701509204976772
exp ma of ||w|| 1.3790085193639838
||w||^2 0.9068261502017239
exp ma of ||w||^2 1.835577913007563
||w|| 0.952274199063339
exp ma of ||w|| 1.295555459888922
cuda
Objective function 13.53 = squared loss an data 12.15 + 0.5*rho*h**2 0.409514 + alpha*h 0.128587 + L2reg 0.80 + L1reg 0.05 ; SHD = 20 ; DAG True
Proportion of microbatches that were clipped  0.770745200130166
iteration 3 in inner loop, alpha 44.93121008982976 rho 100000.0 h 0.002861867848151789
iteration 4 in outer loop, alpha = 331.1179949050087, rho = 100000.0, h = 0.002861867848151789
cuda
1210
cuda
Objective function 14.35 = squared loss an data 12.15 + 0.5*rho*h**2 0.409514 + alpha*h 0.947616 + L2reg 0.80 + L1reg 0.05 ; SHD = 20 ; DAG True
||w||^2 34957984124.89119
exp ma of ||w||^2 729999238488.9025
||w|| 186970.54346845974
exp ma of ||w|| 430402.60892254964
||w||^2 11466.329269578644
exp ma of ||w||^2 4310490.231226537
||w|| 107.08094727624818
exp ma of ||w|| 362.5335347489182
||w||^2 2.0340346198400536
exp ma of ||w||^2 2.183394128278138
||w|| 1.4261958560590666
exp ma of ||w|| 1.4325967069271854
cuda
Objective function 13.55 = squared loss an data 12.00 + 0.5*rho*h**2 0.135094 + alpha*h 0.544272 + L2reg 0.81 + L1reg 0.05 ; SHD = 22 ; DAG True
Proportion of microbatches that were clipped  0.7722391408879629
iteration 1 in inner loop, alpha 331.1179949050087 rho 100000.0 h 0.001643739117152876
iteration 5 in outer loop, alpha = 1974.8571120578845, rho = 1000000.0, h = 0.001643739117152876
Threshold 0.3
[[0.005 0.007 0.027 0.012 0.062 0.016 0.277 0.004 0.005 0.025]
 [0.787 0.005 0.829 0.146 0.306 0.071 0.454 0.395 0.198 0.847]
 [0.165 0.007 0.003 0.015 0.016 0.018 0.117 0.008 0.006 0.04 ]
 [0.358 0.023 0.273 0.006 0.277 0.083 0.54  0.054 0.016 1.614]
 [0.087 0.02  0.338 0.018 0.006 0.018 0.115 0.018 0.007 0.086]
 [0.409 0.062 0.174 0.066 0.381 0.005 0.501 0.068 0.061 1.043]
 [0.02  0.005 0.089 0.003 0.065 0.008 0.007 0.031 0.004 0.008]
 [0.658 0.013 0.697 0.095 0.162 0.082 0.2   0.005 0.247 0.212]
 [1.339 0.016 1.042 0.48  0.634 0.084 0.74  0.027 0.006 0.446]
 [0.184 0.004 0.23  0.002 0.09  0.005 0.784 0.017 0.01  0.004]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.787 0.    0.829 0.    0.306 0.    0.454 0.395 0.    0.847]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.358 0.    0.    0.    0.    0.    0.54  0.    0.    1.614]
 [0.    0.    0.338 0.    0.    0.    0.    0.    0.    0.   ]
 [0.409 0.    0.    0.    0.381 0.    0.501 0.    0.    1.043]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.658 0.    0.697 0.    0.    0.    0.    0.    0.    0.   ]
 [1.339 0.    1.042 0.48  0.634 0.    0.74  0.    0.    0.446]
 [0.    0.    0.    0.    0.    0.    0.784 0.    0.    0.   ]]
{'fdr': 0.391304347826087, 'tpr': 0.4666666666666667, 'fpr': 0.6, 'f1': 0.5283018867924527, 'shd': 22, 'npred': 23, 'ntrue': 30}
[0.007 0.027 0.012 0.062 0.016 0.277 0.004 0.005 0.025 0.787 0.829 0.146
 0.306 0.071 0.454 0.395 0.198 0.847 0.165 0.007 0.015 0.016 0.018 0.117
 0.008 0.006 0.04  0.358 0.023 0.273 0.277 0.083 0.54  0.054 0.016 1.614
 0.087 0.02  0.338 0.018 0.018 0.115 0.018 0.007 0.086 0.409 0.062 0.174
 0.066 0.381 0.501 0.068 0.061 1.043 0.02  0.005 0.089 0.003 0.065 0.008
 0.031 0.004 0.008 0.658 0.013 0.697 0.095 0.162 0.082 0.2   0.247 0.212
 1.339 0.016 1.042 0.48  0.634 0.084 0.74  0.027 0.446 0.184 0.004 0.23
 0.002 0.09  0.005 0.784 0.017 0.01 ]
[[0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 1. 0. 1. 1. 0. 0. 0. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]]
[0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
aucroc, aucpr (0.7805555555555556, 0.7052953357047064)
Iterations 630
Achieves (6.682218196030888, 1e-05)-DP
