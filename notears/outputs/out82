samples  5000  graph  15 30 ER mlp  minibatch size  100  noise  0.75  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13253709978899053
iteration 2 in outer loop, alpha = 14.593841648015513, rho = 100.0, h = 0.13253709978899053
cuda
iteration 1 in inner loop,alpha 14.593841648015513 rho 100.0 h 0.0741535482580904
iteration 2 in inner loop,alpha 14.593841648015513 rho 1000.0 h 0.026277736531699958
iteration 3 in outer loop, alpha = 40.87157817971547, rho = 1000.0, h = 0.026277736531699958
cuda
iteration 1 in inner loop,alpha 40.87157817971547 rho 1000.0 h 0.01649112523327645
iteration 2 in inner loop,alpha 40.87157817971547 rho 10000.0 h 0.006577863967617503
iteration 3 in inner loop,alpha 40.87157817971547 rho 100000.0 h 0.0012276698258713026
iteration 4 in outer loop, alpha = 163.63856076684573, rho = 100000.0, h = 0.0012276698258713026
cuda
iteration 1 in inner loop,alpha 163.63856076684573 rho 100000.0 h 0.00021865143877164428
iteration 5 in outer loop, alpha = 185.50370464401016, rho = 100000.0, h = 0.00021865143877164428
cuda
iteration 1 in inner loop,alpha 185.50370464401016 rho 100000.0 h 0.000155683333863621
iteration 6 in outer loop, alpha = 341.18703850763114, rho = 1000000.0, h = 0.000155683333863621
Threshold 0.3
[[0.002 0.    0.    0.005 0.198 0.    0.    0.    0.    2.725 0.244 0.
  0.    0.    0.008]
 [2.538 0.001 0.002 0.263 0.219 0.01  0.01  0.    0.001 2.208 0.267 1.353
  0.    0.001 0.099]
 [0.041 0.07  0.001 0.145 0.16  0.036 0.14  0.001 0.068 0.139 0.173 0.164
  0.    0.003 0.006]
 [0.138 0.    0.001 0.001 0.199 0.    0.    0.    0.    0.243 0.418 0.001
  0.    0.    1.022]
 [0.    0.    0.001 0.    0.004 0.    0.    0.    0.    0.    0.001 0.
  0.    0.    0.001]
 [2.666 0.031 0.    0.988 0.374 0.001 1.396 0.    0.055 1.747 1.31  1.481
  0.02  0.    1.921]
 [0.413 0.044 0.    0.407 0.405 0.    0.001 0.    0.055 0.194 1.711 0.098
  0.    0.002 0.164]
 [2.7   0.002 0.    2.101 0.13  0.602 0.048 0.    0.004 0.349 0.164 0.06
  0.012 0.022 0.118]
 [0.074 0.381 0.002 0.659 0.398 0.002 0.001 0.001 0.003 2.29  2.157 1.037
  0.    0.001 0.37 ]
 [0.    0.    0.    0.    2.764 0.    0.    0.    0.    0.002 0.341 0.
  0.    0.    0.003]
 [0.    0.    0.    0.001 3.098 0.    0.    0.    0.001 0.001 0.009 0.
  0.    0.    0.002]
 [0.345 0.    0.    0.219 0.147 0.    0.003 0.    0.002 0.433 0.526 0.001
  0.    0.001 0.034]
 [0.04  0.074 0.001 0.099 3.058 0.01  0.    0.    1.286 0.116 1.868 0.032
  0.001 0.017 0.017]
 [0.127 1.414 0.    1.131 1.235 0.004 0.005 0.    3.442 0.37  0.503 0.157
  0.01  0.002 0.562]
 [0.074 0.001 0.001 0.    0.102 0.    0.    0.    0.    0.113 0.165 0.002
  0.    0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.725 0.    0.
  0.    0.    0.   ]
 [2.538 0.    0.    0.    0.    0.    0.    0.    0.    2.208 0.    1.353
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.418 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.666 0.    0.    0.988 0.374 0.    1.396 0.    0.    1.747 1.31  1.481
  0.    0.    1.921]
 [0.413 0.    0.    0.407 0.405 0.    0.    0.    0.    0.    1.711 0.
  0.    0.    0.   ]
 [2.7   0.    0.    2.101 0.    0.602 0.    0.    0.    0.349 0.    0.
  0.    0.    0.   ]
 [0.    0.381 0.    0.659 0.398 0.    0.    0.    0.    2.29  2.157 1.037
  0.    0.    0.37 ]
 [0.    0.    0.    0.    2.764 0.    0.    0.    0.    0.    0.341 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.098 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.345 0.    0.    0.    0.    0.    0.    0.    0.    0.433 0.526 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.058 0.    0.    0.    1.286 0.    1.868 0.
  0.    0.    0.   ]
 [0.    1.414 0.    1.131 1.235 0.    0.    0.    3.442 0.37  0.503 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[5.354e-05 1.613e-04 5.351e-03 1.984e-01 6.281e-05 1.305e-04 2.840e-05
 3.693e-04 2.725e+00 2.440e-01 3.590e-04 7.910e-05 1.094e-04 7.583e-03
 2.538e+00 2.060e-03 2.625e-01 2.194e-01 9.984e-03 9.997e-03 2.547e-04
 1.496e-03 2.208e+00 2.672e-01 1.353e+00 8.829e-05 8.645e-04 9.901e-02
 4.117e-02 6.985e-02 1.451e-01 1.600e-01 3.642e-02 1.396e-01 5.039e-04
 6.810e-02 1.389e-01 1.725e-01 1.636e-01 4.707e-04 2.600e-03 5.901e-03
 1.384e-01 1.333e-04 6.341e-04 1.991e-01 1.998e-05 3.028e-04 3.095e-05
 1.572e-04 2.433e-01 4.178e-01 7.390e-04 5.504e-05 1.374e-04 1.022e+00
 5.474e-05 4.492e-06 5.561e-04 1.696e-04 9.611e-06 6.491e-05 4.169e-06
 1.628e-04 3.352e-04 8.504e-04 5.896e-05 1.856e-05 6.345e-05 6.916e-04
 2.666e+00 3.054e-02 1.413e-04 9.885e-01 3.740e-01 1.396e+00 1.607e-04
 5.460e-02 1.747e+00 1.310e+00 1.481e+00 1.972e-02 2.279e-04 1.921e+00
 4.126e-01 4.363e-02 1.092e-04 4.072e-01 4.052e-01 2.251e-04 7.580e-05
 5.459e-02 1.940e-01 1.711e+00 9.813e-02 3.046e-04 1.713e-03 1.640e-01
 2.700e+00 2.436e-03 2.491e-04 2.101e+00 1.300e-01 6.023e-01 4.837e-02
 4.161e-03 3.494e-01 1.645e-01 6.025e-02 1.232e-02 2.193e-02 1.179e-01
 7.440e-02 3.814e-01 2.099e-03 6.593e-01 3.981e-01 2.087e-03 1.259e-03
 1.449e-03 2.290e+00 2.157e+00 1.037e+00 5.385e-05 5.209e-04 3.701e-01
 1.967e-04 4.028e-06 1.406e-04 4.286e-04 2.764e+00 5.975e-06 4.610e-04
 8.872e-06 4.161e-04 3.406e-01 2.846e-04 1.968e-05 1.051e-04 2.536e-03
 2.443e-04 5.306e-05 3.887e-04 7.031e-04 3.098e+00 7.886e-05 4.677e-04
 9.804e-06 6.282e-04 1.289e-03 1.901e-04 1.292e-04 2.153e-04 1.550e-03
 3.454e-01 3.602e-04 2.258e-04 2.195e-01 1.472e-01 2.360e-04 3.262e-03
 6.962e-05 1.502e-03 4.328e-01 5.261e-01 5.410e-05 5.551e-04 3.361e-02
 4.005e-02 7.410e-02 1.416e-03 9.892e-02 3.058e+00 1.018e-02 2.723e-04
 3.139e-04 1.286e+00 1.162e-01 1.868e+00 3.246e-02 1.747e-02 1.660e-02
 1.274e-01 1.414e+00 1.234e-04 1.131e+00 1.235e+00 3.988e-03 5.441e-03
 2.831e-04 3.442e+00 3.696e-01 5.030e-01 1.568e-01 1.016e-02 5.624e-01
 7.431e-02 7.554e-04 6.295e-04 6.314e-05 1.016e-01 3.281e-05 1.319e-04
 9.819e-06 4.708e-04 1.132e-01 1.645e-01 1.514e-03 1.179e-04 5.295e-05]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9812962962962963, 0.9721580682526954)
cuda
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 59.12587266402984 clip 2.301154480791764
total norm for a microbatch 38.614530272902094 clip 3.6335650159454
total norm for a microbatch 31.764112336104393 clip 28.691528776909347
total norm for a microbatch 39.053780911767035 clip 31.36220723103103
total norm for a microbatch 36.59978311321986 clip 32.27450251369757
total norm for a microbatch 25.70230306697766 clip 27.583567369610734
cuda
Objective function 37.32 = squared loss an data 35.15 + 0.5*rho*h**2 1.679682 + alpha*h 0.000000 + L2reg 0.33 + L1reg 0.15 ; SHD = 59 ; DAG False
Proportion of microbatches that were clipped  0.9008937120970316
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.832856690815447
iteration 1 in outer loop, alpha = 1.832856690815447, rho = 1.0, h = 1.832856690815447
cuda
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 40.68 = squared loss an data 35.15 + 0.5*rho*h**2 1.679682 + alpha*h 3.359364 + L2reg 0.33 + L1reg 0.15 ; SHD = 59 ; DAG False
total norm for a microbatch 38.252995687732195 clip 4.554978865441029
total norm for a microbatch 53.21115759224056 clip 5.98011779677846
total norm for a microbatch 134.10369325375024 clip 5.98011779677846
total norm for a microbatch 31.225251776243862 clip 7.9500404460471925
total norm for a microbatch 48.34977488745899 clip 21.125579965153147
total norm for a microbatch 22.902751868814722 clip 29.14748278929569
total norm for a microbatch 28.061766762542643 clip 29.14748278929569
cuda
Objective function 18.00 = squared loss an data 14.65 + 0.5*rho*h**2 0.582853 + alpha*h 1.978896 + L2reg 0.65 + L1reg 0.15 ; SHD = 44 ; DAG False
Proportion of microbatches that were clipped  0.9119272018199545
iteration 1 in inner loop, alpha 1.832856690815447 rho 1.0 h 1.079678735796847
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 23.25 = squared loss an data 14.65 + 0.5*rho*h**2 5.828531 + alpha*h 1.978896 + L2reg 0.65 + L1reg 0.15 ; SHD = 44 ; DAG False
total norm for a microbatch 37.593169757130305 clip 1.7127507768024548
total norm for a microbatch 23.40426229587764 clip 4.264841689888263
total norm for a microbatch 80.94280988158627 clip 17.380468830934742
total norm for a microbatch 68.98111355405284 clip 27.592943805137516
total norm for a microbatch 44.19041235061265 clip 42.3414129342922
total norm for a microbatch 56.88206912109267 clip 41.31540879046226
cuda
Objective function 16.27 = squared loss an data 13.39 + 0.5*rho*h**2 1.057417 + alpha*h 0.842882 + L2reg 0.85 + L1reg 0.13 ; SHD = 35 ; DAG True
Proportion of microbatches that were clipped  0.9241498197774644
iteration 2 in inner loop, alpha 1.832856690815447 rho 10.0 h 0.4598733156525405
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 25.78 = squared loss an data 13.39 + 0.5*rho*h**2 10.574173 + alpha*h 0.842882 + L2reg 0.85 + L1reg 0.13 ; SHD = 35 ; DAG True
total norm for a microbatch 132.0162130287551 clip 15.020579517072397
total norm for a microbatch 25.560509528498176 clip 49.77521816042617
total norm for a microbatch 119.70717103598486 clip 52.277500299990216
cuda
Objective function 17.17 = squared loss an data 14.30 + 0.5*rho*h**2 1.472085 + alpha*h 0.314492 + L2reg 0.96 + L1reg 0.12 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.9301597243971187
iteration 3 in inner loop, alpha 1.832856690815447 rho 100.0 h 0.17158585308646934
iteration 2 in outer loop, alpha = 18.99144199946238, rho = 100.0, h = 0.17158585308646934
cuda
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 20.12 = squared loss an data 14.30 + 0.5*rho*h**2 1.472085 + alpha*h 3.258663 + L2reg 0.96 + L1reg 0.12 ; SHD = 37 ; DAG True
total norm for a microbatch 66.21938317347823 clip 2.6816129656819547
total norm for a microbatch 122.8924996868028 clip 4.637624057169128
total norm for a microbatch 103.45684349041207 clip 10.360116268678821
total norm for a microbatch 83.20487984535202 clip 14.716890398629449
total norm for a microbatch 53.026411377452376 clip 20.72602495076353
total norm for a microbatch 64.05910851786952 clip 32.81102339716593
cuda
Objective function 18.77 = squared loss an data 14.72 + 0.5*rho*h**2 0.673911 + alpha*h 2.204825 + L2reg 1.05 + L1reg 0.12 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.9354471544715447
iteration 1 in inner loop, alpha 18.99144199946238 rho 100.0 h 0.11609569937361108
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 24.84 = squared loss an data 14.72 + 0.5*rho*h**2 6.739106 + alpha*h 2.204825 + L2reg 1.05 + L1reg 0.12 ; SHD = 39 ; DAG True
total norm for a microbatch 84.37183812931092 clip 1.0
total norm for a microbatch 87.38897967623318 clip 1.5660775951591195
total norm for a microbatch 92.19282856674042 clip 3.3066864048390743
total norm for a microbatch 68.39532797361927 clip 9.155735673332364
total norm for a microbatch 73.00789775154163 clip 11.00081104225364
cuda
Objective function 18.08 = squared loss an data 14.82 + 0.5*rho*h**2 1.111097 + alpha*h 0.895260 + L2reg 1.13 + L1reg 0.12 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.9337051792828686
iteration 2 in inner loop, alpha 18.99144199946238 rho 1000.0 h 0.04714016062415105
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 28.08 = squared loss an data 14.82 + 0.5*rho*h**2 11.110974 + alpha*h 0.895260 + L2reg 1.13 + L1reg 0.12 ; SHD = 36 ; DAG True
total norm for a microbatch 91.40455390455301 clip 1.8832828303503548
total norm for a microbatch 59.5056270716245 clip 3.309663241988424
total norm for a microbatch 77.92382210660922 clip 3.309663241988424
total norm for a microbatch 59.86599270739838 clip 26.519579569524577
total norm for a microbatch 140.15750741430858 clip 31.991320006842397
cuda
Objective function 18.07 = squared loss an data 15.05 + 0.5*rho*h**2 1.378851 + alpha*h 0.315378 + L2reg 1.20 + L1reg 0.12 ; SHD = 38 ; DAG True
Proportion of microbatches that were clipped  0.9394868484750686
iteration 3 in inner loop, alpha 18.99144199946238 rho 10000.0 h 0.01660632891374192
iteration 3 in outer loop, alpha = 185.05473113688157, rho = 10000.0, h = 0.01660632891374192
cuda
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 20.82 = squared loss an data 15.05 + 0.5*rho*h**2 1.378851 + alpha*h 3.073080 + L2reg 1.20 + L1reg 0.12 ; SHD = 38 ; DAG True
total norm for a microbatch 132.88691564448112 clip 1.3012637762245307
total norm for a microbatch 172.8675870581441 clip 1.428728413180174
total norm for a microbatch 62.73099188091226 clip 5.343526481691496
total norm for a microbatch 124.26369023720517 clip 10.840452323959955
total norm for a microbatch 60.38597794718275 clip 12.939628995728423
total norm for a microbatch 69.99966226157906 clip 51.55392867798302
total norm for a microbatch 87.89470987967441 clip 60.70977518883477
total norm for a microbatch 81.63775451393941 clip 67.75798341690597
total norm for a microbatch 102.90491118514464 clip 73.3084078941032
total norm for a microbatch 159.22682400473786 clip 74.32876114176588
total norm for a microbatch 81.81639508744713 clip 74.32876114176588
cuda
Objective function 18.21 = squared loss an data 14.56 + 0.5*rho*h**2 0.468134 + alpha*h 1.790607 + L2reg 1.27 + L1reg 0.13 ; SHD = 38 ; DAG True
Proportion of microbatches that were clipped  0.9445676274944568
iteration 1 in inner loop, alpha 185.05473113688157 rho 10000.0 h 0.009676092810657622
noise_multiplier  0.75  noise_multiplier_b  5.0  noise_multiplier_delta  0.7521183158458503
cuda
Objective function 22.43 = squared loss an data 14.56 + 0.5*rho*h**2 4.681339 + alpha*h 1.790607 + L2reg 1.27 + L1reg 0.13 ; SHD = 38 ; DAG True
total norm for a microbatch 129.41868456594173 clip 4.090724270103214
total norm for a microbatch 117.44931614707622 clip 10.73436651970534
total norm for a microbatch 132.09950196776808 clip 101.5834028491712
total norm for a microbatch 154.79982261893807 clip 140.36467421816423
cuda
Objective function 19.40 = squared loss an data 14.80 + 0.5*rho*h**2 1.993017 + alpha*h 1.168344 + L2reg 1.31 + L1reg 0.13 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.9709368403961682
iteration 2 in inner loop, alpha 185.05473113688157 rho 100000.0 h 0.006313504186485019
iteration 4 in outer loop, alpha = 6498.558917621901, rho = 1000000.0, h = 0.006313504186485019
Threshold 0.3
[[0.006 0.038 0.184 0.41  0.469 0.088 0.025 0.004 0.1   1.067 0.094 0.204
  0.044 0.018 0.243]
 [0.193 0.008 0.552 0.515 0.352 0.194 0.071 0.034 0.077 0.521 0.226 0.711
  0.053 0.008 0.429]
 [0.034 0.012 0.007 0.187 0.288 0.067 0.014 0.021 0.025 0.089 0.065 0.229
  0.033 0.014 0.231]
 [0.018 0.022 0.047 0.005 0.185 0.034 0.011 0.003 0.024 0.046 0.036 0.035
  0.011 0.008 0.391]
 [0.004 0.007 0.024 0.034 0.006 0.017 0.004 0.006 0.003 0.004 0.003 0.047
  0.005 0.003 0.061]
 [0.119 0.034 0.098 0.203 0.361 0.008 0.011 0.03  0.068 0.332 0.163 0.146
  0.024 0.016 0.369]
 [0.244 0.065 0.38  0.599 0.425 0.528 0.005 0.145 0.115 0.388 0.852 0.628
  0.185 0.029 0.384]
 [1.181 0.218 0.329 1.053 0.332 0.335 0.038 0.005 0.199 0.516 0.199 0.355
  0.16  0.046 0.502]
 [0.057 0.098 0.122 0.221 0.555 0.101 0.058 0.04  0.006 1.041 0.852 0.403
  0.012 0.003 0.183]
 [0.006 0.011 0.073 0.107 1.771 0.017 0.013 0.009 0.007 0.005 0.078 0.084
  0.013 0.002 0.216]
 [0.077 0.042 0.112 0.188 2.191 0.062 0.007 0.036 0.007 0.101 0.005 0.173
  0.006 0.003 0.263]
 [0.042 0.013 0.046 0.193 0.164 0.066 0.015 0.012 0.018 0.083 0.042 0.006
  0.015 0.004 0.138]
 [0.241 0.095 0.254 0.59  0.92  0.334 0.027 0.039 0.312 0.539 1.156 0.403
  0.007 0.026 0.628]
 [0.23  0.826 0.394 0.613 0.491 0.507 0.273 0.25  2.768 0.703 0.72  0.913
  0.236 0.007 0.555]
 [0.033 0.019 0.046 0.02  0.102 0.023 0.021 0.01  0.023 0.047 0.019 0.07
  0.01  0.007 0.007]]
[[0.    0.    0.    0.41  0.469 0.    0.    0.    0.    1.067 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.552 0.515 0.352 0.    0.    0.    0.    0.521 0.    0.711
  0.    0.    0.429]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.391]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.361 0.    0.    0.    0.    0.332 0.    0.
  0.    0.    0.369]
 [0.    0.    0.38  0.599 0.425 0.528 0.    0.    0.    0.388 0.852 0.628
  0.    0.    0.384]
 [1.181 0.    0.329 1.053 0.332 0.335 0.    0.    0.    0.516 0.    0.355
  0.    0.    0.502]
 [0.    0.    0.    0.    0.555 0.    0.    0.    0.    1.041 0.852 0.403
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.771 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.191 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.59  0.92  0.334 0.    0.    0.312 0.539 1.156 0.403
  0.    0.    0.628]
 [0.    0.826 0.394 0.613 0.491 0.507 0.    0.    2.768 0.703 0.72  0.913
  0.    0.    0.555]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.5660377358490566, 'tpr': 0.7666666666666667, 'fpr': 0.4, 'f1': 0.5542168674698795, 'shd': 36, 'npred': 53, 'ntrue': 30}
[3.763e-02 1.843e-01 4.103e-01 4.695e-01 8.815e-02 2.539e-02 3.707e-03
 1.005e-01 1.067e+00 9.366e-02 2.042e-01 4.440e-02 1.816e-02 2.432e-01
 1.926e-01 5.516e-01 5.150e-01 3.519e-01 1.938e-01 7.053e-02 3.363e-02
 7.662e-02 5.207e-01 2.255e-01 7.107e-01 5.251e-02 8.143e-03 4.287e-01
 3.392e-02 1.200e-02 1.868e-01 2.878e-01 6.715e-02 1.445e-02 2.103e-02
 2.490e-02 8.926e-02 6.509e-02 2.290e-01 3.286e-02 1.372e-02 2.312e-01
 1.809e-02 2.220e-02 4.667e-02 1.845e-01 3.416e-02 1.066e-02 2.954e-03
 2.437e-02 4.592e-02 3.570e-02 3.481e-02 1.134e-02 8.146e-03 3.911e-01
 4.433e-03 7.300e-03 2.368e-02 3.407e-02 1.675e-02 3.614e-03 6.123e-03
 2.815e-03 3.819e-03 2.767e-03 4.689e-02 4.538e-03 2.578e-03 6.133e-02
 1.188e-01 3.448e-02 9.774e-02 2.034e-01 3.612e-01 1.078e-02 2.983e-02
 6.751e-02 3.323e-01 1.633e-01 1.458e-01 2.404e-02 1.608e-02 3.688e-01
 2.441e-01 6.461e-02 3.803e-01 5.995e-01 4.249e-01 5.282e-01 1.452e-01
 1.148e-01 3.883e-01 8.519e-01 6.279e-01 1.853e-01 2.902e-02 3.836e-01
 1.181e+00 2.181e-01 3.292e-01 1.053e+00 3.322e-01 3.352e-01 3.803e-02
 1.994e-01 5.161e-01 1.986e-01 3.554e-01 1.603e-01 4.573e-02 5.019e-01
 5.690e-02 9.823e-02 1.224e-01 2.207e-01 5.550e-01 1.009e-01 5.786e-02
 3.959e-02 1.041e+00 8.520e-01 4.033e-01 1.212e-02 2.782e-03 1.827e-01
 5.655e-03 1.088e-02 7.272e-02 1.074e-01 1.771e+00 1.691e-02 1.290e-02
 8.702e-03 6.609e-03 7.776e-02 8.388e-02 1.295e-02 2.175e-03 2.159e-01
 7.718e-02 4.215e-02 1.125e-01 1.876e-01 2.191e+00 6.217e-02 7.298e-03
 3.551e-02 6.749e-03 1.013e-01 1.734e-01 6.290e-03 3.409e-03 2.633e-01
 4.238e-02 1.263e-02 4.602e-02 1.933e-01 1.637e-01 6.594e-02 1.456e-02
 1.155e-02 1.808e-02 8.290e-02 4.170e-02 1.454e-02 4.021e-03 1.383e-01
 2.411e-01 9.479e-02 2.536e-01 5.900e-01 9.198e-01 3.335e-01 2.676e-02
 3.904e-02 3.116e-01 5.386e-01 1.156e+00 4.025e-01 2.638e-02 6.279e-01
 2.297e-01 8.255e-01 3.936e-01 6.127e-01 4.912e-01 5.070e-01 2.730e-01
 2.503e-01 2.768e+00 7.032e-01 7.202e-01 9.132e-01 2.360e-01 5.552e-01
 3.304e-02 1.864e-02 4.642e-02 1.998e-02 1.024e-01 2.350e-02 2.068e-02
 9.803e-03 2.269e-02 4.691e-02 1.883e-02 6.984e-02 9.996e-03 6.908e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8664814814814815, 0.6624109234080773)
Iterations 567
Achieves (7.541179662743805, 1e-05)-DP
