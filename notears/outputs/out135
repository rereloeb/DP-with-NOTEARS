samples  5000  graph  30 90 ER mlp  minibatch size  100  noise  0.6  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.115034010050131
iteration 1 in outer loop, alpha = 2.115034010050131, rho = 1.0, h = 2.115034010050131
cuda
iteration 1 in inner loop,alpha 2.115034010050131 rho 1.0 h 1.3374026753201562
iteration 2 in inner loop,alpha 2.115034010050131 rho 10.0 h 0.574042422965384
iteration 3 in inner loop,alpha 2.115034010050131 rho 100.0 h 0.17847008685236432
iteration 2 in outer loop, alpha = 19.962042695286563, rho = 100.0, h = 0.17847008685236432
cuda
iteration 1 in inner loop,alpha 19.962042695286563 rho 100.0 h 0.09450110805936873
iteration 2 in inner loop,alpha 19.962042695286563 rho 1000.0 h 0.03444846883433783
iteration 3 in outer loop, alpha = 54.410511529624394, rho = 1000.0, h = 0.03444846883433783
cuda
iteration 1 in inner loop,alpha 54.410511529624394 rho 1000.0 h 0.017564110805800226
iteration 2 in inner loop,alpha 54.410511529624394 rho 10000.0 h 0.00594910519521008
iteration 4 in outer loop, alpha = 113.90156348172519, rho = 10000.0, h = 0.00594910519521008
cuda
iteration 1 in inner loop,alpha 113.90156348172519 rho 10000.0 h 0.0025859652653217324
iteration 2 in inner loop,alpha 113.90156348172519 rho 100000.0 h 0.0008041747349878392
iteration 5 in outer loop, alpha = 194.3190369805091, rho = 100000.0, h = 0.0008041747349878392
cuda
iteration 1 in inner loop,alpha 194.3190369805091 rho 100000.0 h 0.0003717061904424668
iteration 6 in outer loop, alpha = 566.0252274229758, rho = 1000000.0, h = 0.0003717061904424668
Threshold 0.3
[[0.006 0.001 0.001 2.103 0.    0.001 0.    0.001 0.001 0.    0.    0.002
  0.    0.    0.    0.183 0.    0.    1.111 0.308 0.    0.    1.713 0.
  0.    0.    0.067 0.    0.12  0.   ]
 [1.063 0.003 0.367 0.556 0.    0.126 0.001 0.423 0.001 0.    0.    1.021
  0.    0.    0.    1.6   0.    0.001 1.043 0.401 0.001 0.001 0.09  0.
  0.    0.001 0.486 0.    0.19  0.   ]
 [0.362 0.001 0.003 0.33  0.001 0.117 0.    0.253 0.    0.    0.    0.311
  0.    0.    0.    0.216 0.    0.001 0.172 0.244 0.    0.    0.14  0.
  0.    0.    0.241 0.    0.164 0.   ]
 [0.    0.    0.    0.005 0.    0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    1.719 0.    0.    1.027 1.25  0.    0.    0.253 0.
  0.    0.    0.043 0.    0.146 0.   ]
 [0.218 0.172 0.23  0.233 0.002 0.132 0.008 0.141 0.001 0.    0.    0.386
  0.    0.    0.    0.256 0.    0.    0.139 0.238 0.    0.    0.106 0.
  0.    0.    0.063 0.    0.197 0.   ]
 [0.432 0.    0.001 0.28  0.001 0.002 0.001 0.001 0.    0.    0.    0.001
  0.    0.    0.    0.158 0.    0.    0.981 1.24  0.    0.    0.088 0.
  0.    0.    0.063 0.    1.186 0.   ]
 [0.073 0.218 0.15  0.184 0.063 0.124 0.001 0.075 0.587 0.006 0.032 0.09
  0.    0.004 0.    0.148 0.    0.001 0.797 0.086 0.032 0.05  0.073 0.
  0.    0.002 0.084 0.    0.06  0.001]
 [1.027 0.001 0.003 0.429 0.001 2.012 0.003 0.002 0.    0.    0.    0.003
  0.    0.    0.    0.221 0.    0.    0.153 1.235 0.    0.001 0.09  0.
  0.    0.    0.046 0.    0.235 0.   ]
 [0.154 0.247 0.123 0.186 0.155 0.088 0.001 0.169 0.001 0.005 0.094 0.138
  0.    0.016 0.    0.261 0.    0.001 0.135 1.17  0.106 0.132 0.094 0.011
  0.    0.004 0.591 0.    0.114 0.   ]
 [0.149 0.131 0.18  1.666 0.055 0.327 0.022 2.593 0.016 0.004 0.429 0.59
  0.002 3.404 0.017 0.326 0.002 0.47  0.115 0.216 0.292 0.1   0.383 0.111
  0.    0.459 0.17  0.001 0.276 0.28 ]
 [1.058 0.106 0.129 2.078 0.158 0.171 0.031 0.214 0.004 0.001 0.001 0.249
  0.006 0.086 0.002 1.149 0.001 0.074 0.234 0.138 2.828 1.136 1.586 0.076
  0.001 0.274 0.119 0.001 0.097 0.076]
 [0.2   0.    0.002 0.307 0.    0.116 0.002 0.292 0.    0.    0.    0.003
  0.    0.    0.    0.204 0.    0.    0.199 0.346 0.    0.    1.445 0.001
  0.    0.    0.02  0.    0.159 0.   ]
 [1.137 0.898 0.069 0.193 0.005 1.818 0.644 0.104 0.171 0.028 0.059 0.252
  0.001 0.003 0.003 0.109 0.008 0.169 0.693 0.099 0.024 0.025 0.174 0.045
  0.003 3.171 0.116 0.003 1.055 0.472]
 [0.961 2.186 1.874 0.746 0.024 0.292 0.011 1.882 0.002 0.    0.005 1.893
  0.004 0.001 0.    0.34  0.001 0.203 0.359 0.835 0.123 0.013 0.251 0.004
  0.    0.023 0.153 0.001 0.244 0.071]
 [0.157 0.079 1.645 0.159 0.085 0.142 0.42  0.087 0.001 0.001 0.003 0.084
  0.008 0.038 0.    1.441 0.016 0.062 0.105 0.79  0.025 0.014 0.124 0.012
  0.005 0.079 0.014 0.027 0.054 0.059]
 [0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.003 0.    0.    0.119 0.29  0.    0.    0.004 0.
  0.    0.    0.001 0.    0.099 0.   ]
 [0.159 0.046 1.448 0.164 0.022 0.108 0.7   0.125 0.044 0.022 0.004 0.16
  0.006 0.002 0.003 0.102 0.001 0.04  0.11  0.817 0.055 0.02  0.087 0.028
  0.013 0.016 0.482 0.006 0.166 1.979]
 [0.151 0.131 0.158 1.574 1.368 0.095 0.189 0.295 0.153 0.    0.001 0.31
  0.    0.    0.    1.176 0.    0.001 0.143 0.246 0.025 0.002 0.19  0.
  0.    0.    0.106 0.    1.03  0.   ]
 [0.001 0.    0.    0.003 0.001 0.    0.001 0.    0.    0.    0.    0.001
  0.    0.    0.    0.011 0.    0.    0.007 0.011 0.    0.    0.006 0.
  0.    0.    0.011 0.    0.005 0.   ]
 [0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.043 0.003 0.    0.    0.006 0.
  0.    0.    0.002 0.    1.209 0.   ]
 [0.92  1.285 1.852 0.328 2.128 0.104 0.005 0.116 0.001 0.    0.    1.394
  0.    0.001 0.001 0.63  0.002 0.008 0.158 1.039 0.002 0.001 0.157 0.001
  0.    0.004 0.175 0.    0.146 0.001]
 [0.804 0.241 0.193 0.311 0.129 0.031 0.013 0.14  0.002 0.001 0.    0.281
  0.004 0.028 0.004 0.328 0.    0.066 0.141 0.136 0.463 0.001 0.076 0.035
  0.    0.124 0.076 0.    0.08  0.025]
 [0.    0.001 0.    0.002 0.    0.    0.001 0.    0.    0.    0.    0.001
  0.    0.    0.    0.145 0.001 0.    0.146 0.101 0.    0.    0.004 0.
  0.    0.    0.003 0.    0.34  0.   ]
 [1.207 0.074 0.085 0.231 0.101 0.034 0.071 0.289 0.005 0.002 0.001 0.134
  0.    0.037 0.001 0.089 0.005 0.194 0.097 0.107 0.082 0.002 0.093 0.001
  0.    0.001 0.012 0.    0.172 0.101]
 [0.062 0.249 0.024 0.292 0.107 0.163 0.077 0.435 0.    4.128 1.068 0.245
  0.013 0.328 0.003 0.04  0.011 0.178 0.081 0.086 0.108 0.019 0.21  0.003
  0.001 1.68  0.022 0.002 0.712 0.143]
 [0.495 0.4   0.201 0.257 0.092 1.954 0.087 0.138 0.051 0.001 0.    0.065
  0.    0.014 0.    0.161 0.001 0.392 0.197 0.221 0.122 0.003 0.064 0.254
  0.001 0.001 0.658 0.014 1.202 1.969]
 [0.007 0.004 0.001 0.017 0.006 0.008 0.002 0.002 0.    0.    0.    0.052
  0.    0.    0.    0.267 0.    0.003 0.358 0.183 0.001 0.    1.667 0.002
  0.    0.    0.005 0.    1.184 0.001]
 [0.23  0.025 0.018 0.175 2.204 0.176 0.034 1.724 0.023 0.001 0.013 2.053
  0.003 0.037 0.002 0.147 0.021 1.036 0.116 0.323 0.006 0.036 0.091 1.504
  0.006 0.002 0.043 0.001 0.089 0.079]
 [0.    0.    0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    0.001 0.    0.    0.101 0.    0.    0.    0.001 0.
  0.    0.    0.002 0.    0.003 0.   ]
 [0.23  0.08  1.817 0.222 0.106 0.097 0.059 0.092 0.054 0.    0.    0.119
  0.    0.001 0.    0.183 0.    1.126 0.143 0.201 0.036 0.001 0.11  0.002
  0.    0.    0.805 0.    0.319 0.001]]
[[0.    0.    0.    2.103 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.111 0.308 0.    0.    1.713 0.
  0.    0.    0.    0.    0.    0.   ]
 [1.063 0.    0.367 0.556 0.    0.    0.    0.423 0.    0.    0.    1.021
  0.    0.    0.    1.6   0.    0.    1.043 0.401 0.    0.    0.    0.
  0.    0.    0.486 0.    0.    0.   ]
 [0.362 0.    0.    0.33  0.    0.    0.    0.    0.    0.    0.    0.311
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.719 0.    0.    1.027 1.25  0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.386
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.432 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.981 1.24  0.    0.    0.    0.
  0.    0.    0.    0.    1.186 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.587 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.797 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [1.027 0.    0.    0.429 0.    2.012 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    1.235 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    1.17  0.    0.    0.    0.
  0.    0.    0.591 0.    0.    0.   ]
 [0.    0.    0.    1.666 0.    0.327 0.    2.593 0.    0.    0.429 0.59
  0.    3.404 0.    0.326 0.    0.47  0.    0.    0.    0.    0.383 0.
  0.    0.459 0.    0.    0.    0.   ]
 [1.058 0.    0.    2.078 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.149 0.    0.    0.    0.    2.828 1.136 1.586 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.307 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.346 0.    0.    1.445 0.
  0.    0.    0.    0.    0.    0.   ]
 [1.137 0.898 0.    0.    0.    1.818 0.644 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.693 0.    0.    0.    0.    0.
  0.    3.171 0.    0.    1.055 0.472]
 [0.961 2.186 1.874 0.746 0.    0.    0.    1.882 0.    0.    0.    1.893
  0.    0.    0.    0.34  0.    0.    0.359 0.835 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.645 0.    0.    0.    0.42  0.    0.    0.    0.    0.
  0.    0.    0.    1.441 0.    0.    0.    0.79  0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.448 0.    0.    0.    0.7   0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.817 0.    0.    0.    0.
  0.    0.    0.482 0.    0.    1.979]
 [0.    0.    0.    1.574 1.368 0.    0.    0.    0.    0.    0.    0.31
  0.    0.    0.    1.176 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.03  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.209 0.   ]
 [0.92  1.285 1.852 0.328 2.128 0.    0.    0.    0.    0.    0.    1.394
  0.    0.    0.    0.63  0.    0.    0.    1.039 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.804 0.    0.    0.311 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.328 0.    0.    0.    0.    0.463 0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.34  0.   ]
 [1.207 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.435 0.    4.128 1.068 0.
  0.    0.328 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    1.68  0.    0.    0.712 0.   ]
 [0.495 0.4   0.    0.    0.    1.954 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.392 0.    0.    0.    0.    0.    0.
  0.    0.    0.658 0.    1.202 1.969]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.358 0.    0.    0.    1.667 0.
  0.    0.    0.    0.    1.184 0.   ]
 [0.    0.    0.    0.    2.204 0.    0.    1.724 0.    0.    0.    2.053
  0.    0.    0.    0.    0.    1.036 0.    0.323 0.    0.    0.    1.504
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.817 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    1.126 0.    0.    0.    0.    0.    0.
  0.    0.    0.805 0.    0.319 0.   ]]
{'fdr': 0.3252032520325203, 'tpr': 0.9222222222222223, 'fpr': 0.11594202898550725, 'f1': 0.7793427230046949, 'shd': 46, 'npred': 123, 'ntrue': 90}
[8.443e-04 8.012e-04 2.103e+00 3.901e-04 6.228e-04 3.746e-04 6.479e-04
 5.759e-04 1.567e-05 2.505e-05 1.530e-03 5.072e-05 1.856e-05 1.178e-04
 1.827e-01 2.782e-05 3.504e-04 1.111e+00 3.081e-01 4.579e-04 1.338e-04
 1.713e+00 2.761e-05 1.879e-05 4.908e-05 6.667e-02 1.625e-05 1.196e-01
 1.225e-04 1.063e+00 3.667e-01 5.555e-01 2.197e-04 1.264e-01 1.188e-03
 4.229e-01 9.818e-04 6.706e-05 1.874e-05 1.021e+00 3.111e-04 2.116e-05
 1.586e-04 1.600e+00 8.016e-05 1.073e-03 1.043e+00 4.005e-01 7.131e-04
 6.614e-04 8.969e-02 2.193e-04 1.018e-06 8.258e-04 4.859e-01 2.621e-04
 1.902e-01 4.735e-04 3.615e-01 1.165e-03 3.301e-01 6.779e-04 1.174e-01
 2.182e-04 2.528e-01 3.059e-04 1.354e-04 1.412e-05 3.113e-01 9.077e-06
 2.095e-04 2.449e-05 2.160e-01 6.496e-05 1.204e-03 1.720e-01 2.439e-01
 2.780e-04 1.671e-04 1.399e-01 3.501e-04 5.268e-05 7.514e-06 2.406e-01
 1.554e-04 1.645e-01 9.406e-05 4.743e-05 4.793e-05 1.677e-04 2.157e-04
 4.110e-04 1.339e-03 7.425e-05 3.056e-04 4.052e-05 9.975e-05 4.756e-04
 8.895e-06 1.743e-05 4.707e-04 1.719e+00 2.770e-05 1.061e-04 1.027e+00
 1.250e+00 6.594e-05 4.982e-05 2.529e-01 3.957e-05 2.532e-06 4.017e-05
 4.347e-02 1.336e-05 1.463e-01 8.532e-05 2.182e-01 1.721e-01 2.297e-01
 2.328e-01 1.324e-01 8.319e-03 1.413e-01 7.604e-04 8.816e-05 6.665e-05
 3.859e-01 9.391e-06 3.199e-04 4.637e-04 2.561e-01 4.553e-05 2.828e-04
 1.386e-01 2.379e-01 4.761e-04 1.609e-04 1.061e-01 2.583e-04 1.564e-05
 1.304e-04 6.276e-02 2.992e-06 1.966e-01 1.577e-04 4.317e-01 4.209e-04
 9.290e-04 2.802e-01 9.068e-04 1.408e-03 6.495e-04 1.736e-04 8.269e-06
 2.715e-05 1.031e-03 1.923e-05 6.505e-05 3.058e-04 1.581e-01 8.744e-05
 1.801e-04 9.806e-01 1.240e+00 3.031e-05 4.105e-04 8.759e-02 3.838e-04
 9.648e-06 9.285e-05 6.260e-02 4.710e-05 1.186e+00 1.639e-04 7.291e-02
 2.176e-01 1.500e-01 1.839e-01 6.258e-02 1.236e-01 7.471e-02 5.867e-01
 5.925e-03 3.203e-02 8.964e-02 4.436e-04 4.269e-03 3.546e-04 1.480e-01
 4.843e-04 5.012e-04 7.974e-01 8.567e-02 3.247e-02 5.039e-02 7.324e-02
 1.932e-04 9.729e-05 1.566e-03 8.409e-02 1.561e-04 5.977e-02 5.766e-04
 1.027e+00 5.429e-04 3.328e-03 4.288e-01 8.632e-04 2.012e+00 2.645e-03
 4.129e-04 1.131e-05 2.505e-05 2.727e-03 1.199e-05 1.150e-04 1.411e-04
 2.210e-01 1.191e-04 2.669e-04 1.533e-01 1.235e+00 3.195e-05 6.716e-04
 9.031e-02 1.272e-04 8.156e-06 1.608e-04 4.591e-02 7.818e-05 2.348e-01
 4.986e-04 1.543e-01 2.467e-01 1.227e-01 1.863e-01 1.546e-01 8.843e-02
 8.630e-04 1.688e-01 4.841e-03 9.380e-02 1.378e-01 3.274e-04 1.559e-02
 2.037e-04 2.607e-01 2.531e-04 1.092e-03 1.346e-01 1.170e+00 1.057e-01
 1.316e-01 9.407e-02 1.118e-02 1.343e-04 3.849e-03 5.908e-01 3.287e-04
 1.141e-01 4.459e-04 1.490e-01 1.311e-01 1.804e-01 1.666e+00 5.520e-02
 3.273e-01 2.206e-02 2.593e+00 1.630e-02 4.287e-01 5.895e-01 1.963e-03
 3.404e+00 1.688e-02 3.262e-01 1.855e-03 4.701e-01 1.153e-01 2.158e-01
 2.917e-01 1.002e-01 3.834e-01 1.106e-01 4.244e-04 4.595e-01 1.703e-01
 1.076e-03 2.757e-01 2.799e-01 1.058e+00 1.055e-01 1.292e-01 2.078e+00
 1.581e-01 1.709e-01 3.092e-02 2.137e-01 4.361e-03 8.751e-04 2.485e-01
 5.884e-03 8.566e-02 2.411e-03 1.149e+00 1.258e-03 7.428e-02 2.341e-01
 1.377e-01 2.828e+00 1.136e+00 1.586e+00 7.556e-02 1.108e-03 2.735e-01
 1.189e-01 1.407e-03 9.670e-02 7.561e-02 2.004e-01 1.624e-04 1.743e-03
 3.068e-01 3.935e-04 1.158e-01 1.685e-03 2.922e-01 4.988e-04 3.584e-05
 3.622e-05 5.529e-05 6.066e-05 2.691e-04 2.039e-01 2.082e-04 1.689e-04
 1.993e-01 3.456e-01 2.077e-04 5.420e-05 1.445e+00 1.197e-03 2.149e-05
 2.031e-04 2.014e-02 1.581e-04 1.586e-01 8.827e-05 1.137e+00 8.978e-01
 6.873e-02 1.929e-01 5.231e-03 1.818e+00 6.437e-01 1.039e-01 1.711e-01
 2.782e-02 5.897e-02 2.519e-01 2.730e-03 2.783e-03 1.086e-01 8.192e-03
 1.685e-01 6.926e-01 9.857e-02 2.438e-02 2.459e-02 1.738e-01 4.524e-02
 2.507e-03 3.171e+00 1.160e-01 3.180e-03 1.055e+00 4.720e-01 9.614e-01
 2.186e+00 1.874e+00 7.464e-01 2.367e-02 2.920e-01 1.105e-02 1.882e+00
 1.977e-03 1.199e-04 5.303e-03 1.893e+00 4.255e-03 9.696e-05 3.400e-01
 5.277e-04 2.032e-01 3.586e-01 8.353e-01 1.232e-01 1.263e-02 2.513e-01
 4.007e-03 7.365e-05 2.334e-02 1.527e-01 5.739e-04 2.435e-01 7.130e-02
 1.573e-01 7.914e-02 1.645e+00 1.586e-01 8.546e-02 1.420e-01 4.201e-01
 8.707e-02 1.013e-03 1.399e-03 2.945e-03 8.398e-02 8.128e-03 3.770e-02
 1.441e+00 1.561e-02 6.247e-02 1.047e-01 7.898e-01 2.511e-02 1.444e-02
 1.239e-01 1.189e-02 5.454e-03 7.941e-02 1.431e-02 2.696e-02 5.418e-02
 5.888e-02 1.888e-05 5.979e-05 1.805e-04 6.540e-04 1.037e-04 3.850e-04
 2.305e-04 5.086e-05 2.537e-05 1.952e-05 6.506e-06 2.271e-04 1.113e-05
 1.081e-05 5.459e-05 2.248e-05 5.835e-05 1.191e-01 2.895e-01 3.724e-05
 4.861e-05 4.425e-03 4.610e-05 1.269e-05 2.011e-05 1.069e-03 5.531e-06
 9.872e-02 3.897e-05 1.590e-01 4.561e-02 1.448e+00 1.640e-01 2.176e-02
 1.083e-01 6.998e-01 1.253e-01 4.387e-02 2.234e-02 4.482e-03 1.596e-01
 6.224e-03 1.980e-03 2.502e-03 1.021e-01 4.025e-02 1.095e-01 8.173e-01
 5.516e-02 2.032e-02 8.745e-02 2.808e-02 1.283e-02 1.564e-02 4.825e-01
 5.607e-03 1.659e-01 1.979e+00 1.508e-01 1.312e-01 1.584e-01 1.574e+00
 1.368e+00 9.523e-02 1.895e-01 2.948e-01 1.526e-01 6.455e-05 1.330e-03
 3.102e-01 2.360e-05 2.309e-04 3.359e-04 1.176e+00 1.110e-04 1.430e-01
 2.462e-01 2.479e-02 2.385e-03 1.902e-01 1.815e-04 9.529e-06 8.494e-05
 1.065e-01 1.899e-05 1.030e+00 1.740e-04 1.173e-03 1.820e-04 1.337e-04
 3.029e-03 5.375e-04 1.323e-04 5.378e-04 2.089e-04 2.563e-04 1.380e-05
 1.031e-04 5.810e-04 2.586e-05 1.663e-05 3.761e-05 1.096e-02 6.178e-05
 9.514e-05 1.121e-02 7.365e-05 8.919e-05 6.430e-03 8.334e-05 9.455e-06
 2.422e-05 1.091e-02 4.757e-05 4.711e-03 6.892e-05 3.074e-05 1.882e-04
 1.263e-04 5.232e-04 1.206e-04 1.378e-04 5.595e-05 1.831e-05 2.036e-05
 5.177e-06 2.141e-05 1.774e-04 7.020e-05 1.351e-05 6.860e-05 3.319e-04
 7.080e-05 1.498e-05 4.270e-02 7.120e-05 1.828e-05 5.706e-03 3.336e-05
 1.363e-05 1.738e-05 1.955e-03 3.783e-06 1.209e+00 8.186e-05 9.203e-01
 1.285e+00 1.852e+00 3.276e-01 2.128e+00 1.035e-01 5.052e-03 1.163e-01
 8.992e-04 6.686e-05 1.114e-04 1.394e+00 4.815e-04 1.258e-03 9.627e-04
 6.303e-01 1.685e-03 7.863e-03 1.583e-01 1.039e+00 5.522e-04 1.571e-01
 7.647e-04 1.348e-05 3.826e-03 1.752e-01 2.001e-04 1.461e-01 1.081e-03
 8.045e-01 2.415e-01 1.932e-01 3.115e-01 1.294e-01 3.101e-02 1.312e-02
 1.402e-01 1.795e-03 5.647e-04 8.841e-05 2.812e-01 3.864e-03 2.776e-02
 3.511e-03 3.279e-01 3.752e-04 6.562e-02 1.411e-01 1.363e-01 4.626e-01
 7.640e-02 3.525e-02 3.119e-04 1.237e-01 7.643e-02 9.998e-05 7.967e-02
 2.457e-02 7.177e-05 5.880e-04 2.775e-04 1.983e-03 4.364e-04 2.782e-04
 6.822e-04 4.069e-05 3.583e-04 1.720e-05 7.500e-05 9.956e-04 5.137e-05
 8.208e-05 6.349e-05 1.448e-01 6.440e-04 2.858e-04 1.456e-01 1.010e-01
 2.534e-04 8.900e-05 2.219e-04 2.989e-05 1.287e-04 2.737e-03 6.931e-05
 3.399e-01 2.784e-04 1.207e+00 7.377e-02 8.534e-02 2.309e-01 1.013e-01
 3.353e-02 7.146e-02 2.892e-01 4.885e-03 1.795e-03 1.080e-03 1.339e-01
 1.406e-04 3.712e-02 1.372e-03 8.921e-02 5.191e-03 1.937e-01 9.706e-02
 1.073e-01 8.220e-02 1.726e-03 9.309e-02 8.998e-05 5.002e-04 1.244e-02
 1.016e-04 1.725e-01 1.007e-01 6.173e-02 2.494e-01 2.391e-02 2.921e-01
 1.075e-01 1.634e-01 7.679e-02 4.352e-01 3.474e-04 4.128e+00 1.068e+00
 2.450e-01 1.347e-02 3.281e-01 3.141e-03 3.972e-02 1.134e-02 1.778e-01
 8.056e-02 8.608e-02 1.081e-01 1.933e-02 2.099e-01 2.719e-03 1.680e+00
 2.164e-02 1.638e-03 7.115e-01 1.426e-01 4.951e-01 4.004e-01 2.006e-01
 2.566e-01 9.210e-02 1.954e+00 8.709e-02 1.383e-01 5.063e-02 8.716e-04
 1.494e-04 6.506e-02 1.056e-05 1.383e-02 2.268e-04 1.610e-01 1.479e-03
 3.920e-01 1.966e-01 2.207e-01 1.218e-01 2.779e-03 6.433e-02 2.540e-01
 5.114e-04 6.582e-01 1.364e-02 1.202e+00 1.969e+00 7.381e-03 4.393e-03
 8.531e-04 1.729e-02 6.355e-03 7.547e-03 1.612e-03 2.169e-03 4.678e-04
 3.632e-05 3.093e-05 5.220e-02 7.008e-05 7.047e-05 1.284e-04 2.671e-01
 1.577e-04 3.366e-03 3.579e-01 1.830e-01 1.244e-03 3.654e-04 1.667e+00
 1.694e-03 1.928e-05 1.783e-04 3.101e-04 1.184e+00 5.042e-04 2.296e-01
 2.501e-02 1.801e-02 1.746e-01 2.204e+00 1.762e-01 3.364e-02 1.724e+00
 2.315e-02 1.301e-03 1.262e-02 2.053e+00 2.945e-03 3.748e-02 1.608e-03
 1.467e-01 2.146e-02 1.036e+00 1.164e-01 3.235e-01 6.074e-03 3.612e-02
 9.052e-02 1.504e+00 6.021e-03 1.797e-03 4.291e-02 8.868e-02 7.878e-02
 1.262e-04 4.558e-04 2.290e-04 1.342e-04 2.457e-04 6.362e-05 5.908e-04
 1.381e-05 7.228e-05 9.100e-06 1.482e-05 1.669e-04 1.420e-05 8.967e-06
 1.707e-05 5.408e-04 8.536e-05 4.815e-05 1.009e-01 2.181e-04 1.370e-04
 3.939e-04 8.062e-04 2.455e-05 2.149e-05 9.706e-05 1.939e-03 8.439e-06
 1.408e-04 2.303e-01 7.995e-02 1.817e+00 2.224e-01 1.061e-01 9.675e-02
 5.928e-02 9.239e-02 5.448e-02 1.444e-04 4.184e-04 1.191e-01 1.567e-05
 9.236e-04 2.487e-04 1.834e-01 9.057e-05 1.126e+00 1.427e-01 2.013e-01
 3.645e-02 1.315e-03 1.098e-01 2.313e-03 4.840e-05 1.148e-04 8.046e-01
 1.503e-04 3.188e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 1. 0. 0. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.9803561253561254, 0.9430858766365515)
cuda
9630
cuda
Objective function 982.29 = squared loss an data 726.28 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 415 ; DAG False
||w||^2 0.22454776108453406
exp ma of ||w||^2 5.770460316253713
||w|| 0.4738647075743604
exp ma of ||w|| 0.4003137250853003
||w||^2 0.1665093918597134
exp ma of ||w||^2 0.12342238944779821
||w|| 0.40805562348742774
exp ma of ||w|| 0.3476895645732108
||w||^2 0.1407464390676939
exp ma of ||w||^2 0.12127763926489322
||w|| 0.3751618838150991
exp ma of ||w|| 0.344898446182079
||w||^2 0.10194618625628543
exp ma of ||w||^2 0.11627538168665233
||w|| 0.3192901286546226
exp ma of ||w|| 0.33589635379990335
||w||^2 0.1339880087180412
exp ma of ||w||^2 0.12030620283818816
||w|| 0.3660437251450176
exp ma of ||w|| 0.3423719225901865
||w||^2 0.07088376700572421
exp ma of ||w||^2 0.11713036981022117
||w|| 0.2662400552240857
exp ma of ||w|| 0.33506282266500964
||w||^2 0.05936234840175576
exp ma of ||w||^2 0.11490167664042542
||w|| 0.2436438967053264
exp ma of ||w|| 0.3315254751267679
cuda
Objective function 213.04 = squared loss an data 209.91 + 0.5*rho*h**2 2.057352 + alpha*h 0.000000 + L2reg 0.58 + L1reg 0.49 ; SHD = 130 ; DAG False
Proportion of microbatches that were clipped  0.7221340107970784
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.0284730754201235
iteration 1 in outer loop, alpha = 2.0284730754201235, rho = 1.0, h = 2.0284730754201235
cuda
9630
cuda
Objective function 217.15 = squared loss an data 209.91 + 0.5*rho*h**2 2.057352 + alpha*h 4.114703 + L2reg 0.58 + L1reg 0.49 ; SHD = 130 ; DAG False
||w||^2 42160.25936698046
exp ma of ||w||^2 2231255.311472738
||w|| 205.32963587115344
exp ma of ||w|| 817.004651582212
||w||^2 31491.73823171567
exp ma of ||w||^2 1310804.4710890546
||w|| 177.4591170712727
exp ma of ||w|| 571.481039354826
||w||^2 254.06396149708402
exp ma of ||w||^2 59468.47207853769
||w|| 15.939383974830521
exp ma of ||w|| 80.84683888595653
||w||^2 0.15857501798144094
exp ma of ||w||^2 0.18206935735824992
||w|| 0.3982147887528048
exp ma of ||w|| 0.4115452267211738
||w||^2 0.13180052649107774
exp ma of ||w||^2 0.2020960134660532
||w|| 0.36304342232173514
exp ma of ||w|| 0.4291596477521958
cuda
Objective function 85.27 = squared loss an data 78.87 + 0.5*rho*h**2 1.429031 + alpha*h 3.429297 + L2reg 1.06 + L1reg 0.48 ; SHD = 132 ; DAG False
Proportion of microbatches that were clipped  0.7317864676294012
iteration 1 in inner loop, alpha 2.0284730754201235 rho 1.0 h 1.6905802584444025
9630
cuda
Objective function 98.13 = squared loss an data 78.87 + 0.5*rho*h**2 14.290308 + alpha*h 3.429297 + L2reg 1.06 + L1reg 0.48 ; SHD = 132 ; DAG False
||w||^2 0.4362276035489889
exp ma of ||w||^2 0.5012948217148024
||w|| 0.6604752861000848
exp ma of ||w|| 0.6813432239721255
||w||^2 0.30615645953494286
exp ma of ||w||^2 0.5193263817807775
||w|| 0.5533140695255659
exp ma of ||w|| 0.6982147903735904
||w||^2 0.29595099790456036
exp ma of ||w||^2 0.5163821741824153
||w|| 0.5440137846641024
exp ma of ||w|| 0.6918948560507043
cuda
Objective function 61.42 = squared loss an data 54.32 + 0.5*rho*h**2 3.626273 + alpha*h 1.727486 + L2reg 1.32 + L1reg 0.42 ; SHD = 103 ; DAG False
Proportion of microbatches that were clipped  0.7556490571918342
iteration 2 in inner loop, alpha 2.0284730754201235 rho 10.0 h 0.8516188623047611
9630
cuda
Objective function 94.06 = squared loss an data 54.32 + 0.5*rho*h**2 36.262734 + alpha*h 1.727486 + L2reg 1.32 + L1reg 0.42 ; SHD = 103 ; DAG False
||w||^2 123045886348.30254
exp ma of ||w||^2 88698048712.3633
||w|| 350778.97078973043
exp ma of ||w|| 170595.808198833
||w||^2 361929304306.957
exp ma of ||w||^2 110421200776.53197
||w|| 601605.6052821956
exp ma of ||w|| 213069.8137536103
||w||^2 6864920201.561278
exp ma of ||w||^2 24487569822.59058
||w|| 82854.81399147112
exp ma of ||w|| 140379.2738507624
||w||^2 3579625.904630734
exp ma of ||w||^2 47967593.23474855
||w|| 1891.9899324866224
exp ma of ||w|| 4396.23185407696
||w||^2 12444.39120285342
exp ma of ||w||^2 1369284.3814712565
||w|| 111.55443156976517
exp ma of ||w|| 471.0469021333108
||w||^2 4.6615601676971865
exp ma of ||w||^2 489.606149147494
||w|| 2.15906465111566
exp ma of ||w|| 4.35149118013742
||w||^2 1.0987983326766992
exp ma of ||w||^2 78.68375134143427
||w|| 1.0482358192108774
exp ma of ||w|| 2.040913369484861
||w||^2 1.054176610406155
exp ma of ||w||^2 17.480797824848423
||w|| 1.0267310311888673
exp ma of ||w|| 1.3701760859548224
||w||^2 1.809422673505783
exp ma of ||w||^2 1.0865127991834682
||w|| 1.3451478258934157
exp ma of ||w|| 1.0047284378360168
cuda
Objective function 58.69 = squared loss an data 50.56 + 0.5*rho*h**2 5.611542 + alpha*h 0.679556 + L2reg 1.48 + L1reg 0.36 ; SHD = 94 ; DAG True
Proportion of microbatches that were clipped  0.7660974463418455
iteration 3 in inner loop, alpha 2.0284730754201235 rho 100.0 h 0.3350087297306281
iteration 2 in outer loop, alpha = 35.529346048482935, rho = 100.0, h = 0.3350087297306281
cuda
9630
cuda
Objective function 69.91 = squared loss an data 50.56 + 0.5*rho*h**2 5.611542 + alpha*h 11.902641 + L2reg 1.48 + L1reg 0.36 ; SHD = 94 ; DAG True
||w||^2 2766863572.471686
exp ma of ||w||^2 19511150467.073666
||w|| 52600.98451998485
exp ma of ||w|| 118751.76087954934
||w||^2 180759.96463016525
exp ma of ||w||^2 12118304.978201516
||w|| 425.1587522680972
exp ma of ||w|| 1670.0213329108828
||w||^2 4.061637747327029
exp ma of ||w||^2 899.5842227980722
||w|| 2.015350527160729
exp ma of ||w|| 5.417266957394534
||w||^2 5.553071114587383
exp ma of ||w||^2 673.5091948178426
||w|| 2.3564955155033465
exp ma of ||w|| 4.623779352791825
||w||^2 2.5802884158773844
exp ma of ||w||^2 234.85190332287883
||w|| 1.6063276178530284
exp ma of ||w|| 2.88896504423804
||w||^2 1.618741350678902
exp ma of ||w||^2 23.10114571576439
||w|| 1.272297665909555
exp ma of ||w|| 1.5131106049889675
||w||^2 3.332936733612426
exp ma of ||w||^2 1.9486639687327367
||w|| 1.8256332418129404
exp ma of ||w|| 1.216893524984045
||w||^2 1.989197177824756
exp ma of ||w||^2 1.4039651220925942
||w|| 1.4103890164861452
exp ma of ||w|| 1.1513782686210665
||w||^2 1.775023075546404
exp ma of ||w||^2 1.3012098116634636
||w|| 1.3322999195175251
exp ma of ||w|| 1.1097767726099275
cuda
Objective function 63.01 = squared loss an data 49.81 + 0.5*rho*h**2 2.813662 + alpha*h 8.428265 + L2reg 1.62 + L1reg 0.33 ; SHD = 97 ; DAG True
Proportion of microbatches that were clipped  0.7634618513095819
iteration 1 in inner loop, alpha 35.529346048482935 rho 100.0 h 0.23721980859621894
9630
cuda
Objective function 88.33 = squared loss an data 49.81 + 0.5*rho*h**2 28.136619 + alpha*h 8.428265 + L2reg 1.62 + L1reg 0.33 ; SHD = 97 ; DAG True
||w||^2 1064357.3151191743
exp ma of ||w||^2 41260464.70591318
||w|| 1031.676943194513
exp ma of ||w|| 3565.442715034608
||w||^2 1365.600999019888
exp ma of ||w||^2 79001.13711160993
||w|| 36.95403900820434
exp ma of ||w|| 54.84129192282172
||w||^2 4.17342787274718
exp ma of ||w||^2 9.628956394005563
||w|| 2.042896931503687
exp ma of ||w|| 1.5794009394327615
v before min max tensor([[ 477.651, -133.006,  -67.960,  ...,   76.058, -146.711,  -98.314],
        [-151.564, -133.799, -141.711,  ...,  246.049,  -78.383, 1886.065],
        [1446.490, -149.851,  -61.400,  ..., -107.135,   72.032,  472.796],
        ...,
        [ -75.481,  -83.885,  -41.540,  ...,  212.885,  132.138, 7791.937],
        [-162.206, -143.216,  367.991,  ...,   82.681, -145.796,  -81.549],
        [-170.326, -171.794,  -60.280,  ...,  326.613,  217.466, 1377.940]],
       device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        ...,
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01]], device='cuda:0')
v before min max tensor([-6.929e+01, -2.801e+01,  7.355e+02, -4.583e+01, -1.259e+02, -8.618e+01,
         1.527e+02,  4.693e+00,  3.444e+02, -8.555e+01,  8.380e+02, -1.634e+02,
        -1.331e+02,  3.711e+00,  4.023e+01,  1.152e+03, -1.065e+02, -1.490e+02,
         5.638e+01, -1.376e+02, -3.640e+01, -4.650e+01, -1.488e+02, -3.019e+01,
        -1.500e+02, -4.645e+01,  1.306e+01,  3.791e+01, -3.503e+01, -7.606e+01,
         1.755e+01,  3.335e+01, -1.416e+02, -4.412e+01, -1.513e+02,  3.500e+02,
         7.974e+02,  1.295e+02, -8.091e+01,  1.785e+02,  4.441e+01, -1.569e+02,
         3.636e+01,  1.418e+01,  4.315e+00, -1.030e+02,  4.695e+01, -1.391e+02,
         2.188e+03, -4.926e+01,  5.480e+02, -1.753e+02, -1.187e+02, -8.964e+01,
         4.917e+02, -1.816e+01,  1.534e+02, -6.042e+01, -1.124e+02, -1.538e+02,
        -1.181e+02, -8.736e+01,  1.282e+03,  2.095e+02, -8.292e+01, -5.921e+00,
         1.257e+02,  1.718e+01,  2.299e+02,  2.184e+02, -7.101e+01, -1.118e+02,
        -8.086e+01,  1.929e+02, -7.914e+01,  2.598e+02, -2.906e+00, -8.216e+01,
        -1.360e+02, -8.578e+01,  2.008e+01, -1.127e+02, -8.570e+01, -7.374e+01,
        -4.109e+01, -1.180e+02, -1.325e+02, -1.364e+02,  1.708e+02, -3.919e+01,
        -1.482e+02,  6.866e+02,  6.532e+01, -1.217e+02,  7.477e+01, -1.740e+02,
        -1.823e+02, -2.946e+01,  2.124e+02, -1.077e+02,  7.642e+02,  8.194e+02,
         2.331e+02, -2.958e+01,  6.081e+01,  6.058e+02, -5.459e+01, -1.113e+02,
         2.410e+01,  2.265e+02,  1.383e+02, -1.007e+02, -1.600e+02, -1.155e+02,
         4.927e+02, -1.478e+02,  1.086e-01,  1.240e+02,  7.223e+01,  1.926e+02,
        -7.881e+01, -1.548e+02,  3.151e+01,  6.257e+01, -1.048e+02, -1.438e+02,
         2.745e+01, -1.196e+02,  1.840e+01,  7.983e+01, -5.457e+01, -1.041e+02,
        -1.476e+02,  2.888e+02, -1.358e+02, -1.166e+02,  5.248e+02, -1.738e+02,
         4.570e+02,  8.111e+00, -8.987e+00, -1.382e+02,  1.780e+02,  1.690e+02,
        -8.670e+01, -5.912e+01, -1.724e+02, -2.807e+00,  3.832e+02, -5.171e+01,
        -1.425e+02,  1.410e+02,  1.163e+02,  1.080e+03, -1.924e+01,  1.123e+02,
        -8.074e+01, -1.174e+02, -1.382e+02,  6.151e+01,  1.000e+02,  1.056e+01,
        -1.363e+02, -1.286e+02, -4.971e+01,  1.361e+02, -4.513e+01,  4.905e+01,
        -1.031e+02,  1.693e+02, -1.325e+02,  3.990e+02, -1.523e+02, -6.486e+01,
         6.149e+02,  3.341e+02, -1.054e+02, -7.028e+01,  1.270e+02, -9.926e+01,
        -1.016e+02, -1.275e+02,  1.651e+02,  5.941e+02,  4.172e+02, -1.148e+02,
        -1.838e+02,  2.263e+02, -2.717e+01,  8.338e+01, -1.377e+02, -7.810e+01,
        -5.460e+01, -1.284e+02,  2.229e+02,  7.437e+01,  6.108e+02, -5.211e+01,
         2.122e+01, -1.711e+02, -9.667e+01, -4.599e+01, -1.023e+02, -1.483e+02,
        -7.611e+01,  3.040e+02, -1.243e+02, -1.565e+02, -5.469e+01, -9.257e+01,
         6.394e+01, -1.357e+02, -1.476e+02,  2.912e+02, -6.238e+00,  1.204e+03,
        -1.081e+02,  1.606e+02, -9.460e+01, -7.445e+01,  3.295e+01, -1.182e+02,
        -1.093e+02, -1.885e+02, -1.278e+02, -1.258e+02,  6.178e+02,  5.363e+02,
        -4.918e+01, -1.286e+02,  2.532e+02, -8.908e+01, -1.423e+00, -6.474e+01,
        -1.504e+02,  1.146e+02, -3.204e+01,  7.816e+01, -1.133e+02, -1.327e+02,
        -1.065e+01, -1.151e+02,  6.147e+02, -1.120e+02, -1.248e+02, -8.556e+01,
         2.946e+02, -7.642e+01, -7.774e+01, -1.103e+02, -3.202e+01, -6.482e+01,
         9.468e+01,  1.578e+02, -8.411e+01, -2.332e+01,  6.403e+01, -1.314e+02,
        -1.012e+02,  1.913e+02, -1.068e+02,  3.950e+01, -1.458e+02, -1.106e+02,
        -1.379e+02, -5.077e+01, -1.708e+02, -1.277e+02, -9.086e+01, -1.410e+02,
         1.795e+03, -2.273e+01, -1.726e+02,  2.025e+02, -5.770e+01, -1.012e+02,
        -1.399e+02, -3.674e+01,  5.844e+01,  2.768e+01, -9.193e+00, -1.524e+01,
        -6.800e+01, -6.334e+01, -6.679e+01,  1.852e+02,  1.408e+02,  7.547e+02,
        -1.458e+02, -6.523e+01, -1.767e+02, -1.231e+02, -1.025e+02, -1.718e+02,
        -1.079e+02,  1.500e+02,  2.237e+02, -3.761e+01,  5.922e+02, -5.108e+01],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 4.693e+00, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 3.711e+00, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e+01, 4.315e+00, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.086e-01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 8.111e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 6.527e+02],
         [-1.301e+02],
         [ 2.985e+02],
         [-9.923e+01],
         [ 1.998e+01],
         [ 2.489e+02],
         [-1.802e+02],
         [ 1.582e+02],
         [-1.325e+02],
         [ 2.993e+01]],

        [[-8.353e+01],
         [-1.748e+02],
         [-9.142e+01],
         [-1.598e+02],
         [ 1.306e+01],
         [ 8.506e+01],
         [ 1.414e+02],
         [-6.131e+01],
         [ 1.862e+02],
         [ 2.659e+02]],

        [[-9.525e+01],
         [-5.958e+01],
         [-8.736e+01],
         [-1.474e+02],
         [-1.307e+02],
         [ 9.434e+01],
         [ 8.231e+00],
         [-1.390e+02],
         [ 1.978e+02],
         [-3.961e+01]],

        [[-1.754e+02],
         [ 1.873e+02],
         [ 2.153e+02],
         [ 5.602e+02],
         [ 2.328e+01],
         [ 1.114e+03],
         [-7.288e+01],
         [-1.187e+02],
         [ 3.158e+02],
         [ 4.054e+02]],

        [[ 6.006e+01],
         [ 5.405e+00],
         [ 3.947e+00],
         [ 1.502e+02],
         [ 1.176e+02],
         [-1.881e+02],
         [-1.228e+02],
         [-1.800e+02],
         [-3.782e+00],
         [-1.627e+02]],

        [[ 1.164e+02],
         [-1.342e+02],
         [-6.989e+01],
         [-9.767e+01],
         [-1.232e+02],
         [-4.426e+01],
         [ 3.934e+01],
         [-9.233e+01],
         [-1.428e+02],
         [ 1.018e+03]],

        [[ 7.246e+02],
         [-1.021e+02],
         [-1.619e+02],
         [ 1.614e+02],
         [-9.320e+00],
         [-4.067e+01],
         [-2.750e+01],
         [-1.263e+02],
         [-1.566e+02],
         [ 1.963e+02]],

        [[-9.614e+01],
         [-1.513e+02],
         [ 4.769e+02],
         [-1.139e+02],
         [ 7.371e+01],
         [ 3.566e+02],
         [-2.797e+01],
         [-9.286e+01],
         [-4.432e+01],
         [ 1.630e+02]],

        [[-5.732e+01],
         [-1.571e+02],
         [ 3.861e+01],
         [-1.210e+02],
         [-1.053e+02],
         [-9.675e+01],
         [ 8.639e+01],
         [-3.960e+00],
         [ 1.143e+03],
         [ 4.669e+01]],

        [[-1.742e+02],
         [ 7.888e+00],
         [ 3.999e+01],
         [-1.138e+02],
         [-5.079e+01],
         [-5.718e+00],
         [-3.137e+01],
         [-1.355e+02],
         [-6.601e+01],
         [ 6.925e+02]],

        [[-1.207e+02],
         [-9.537e+01],
         [-9.785e+01],
         [-1.122e+02],
         [ 3.174e+01],
         [ 9.892e+01],
         [-2.772e+01],
         [ 1.080e+02],
         [-1.033e+02],
         [ 7.123e+01]],

        [[-8.851e+01],
         [-1.588e+02],
         [-7.647e+01],
         [ 1.433e+01],
         [ 1.306e+02],
         [-4.293e+01],
         [ 2.416e+02],
         [ 8.173e+01],
         [-1.309e+02],
         [-1.326e+02]],

        [[ 1.310e+02],
         [-6.915e+01],
         [ 7.917e+01],
         [-2.669e+01],
         [-1.044e+02],
         [ 6.226e+02],
         [-6.292e+00],
         [ 1.109e+02],
         [-1.314e+02],
         [ 1.342e+02]],

        [[-1.351e+02],
         [ 6.290e+02],
         [-8.069e+01],
         [-3.693e+01],
         [-3.019e+01],
         [-1.247e+02],
         [-1.459e+02],
         [-9.866e+01],
         [-1.518e+02],
         [-1.256e+02]],

        [[-1.375e+02],
         [-4.687e+01],
         [ 1.207e+00],
         [-2.373e+01],
         [-2.143e+01],
         [-9.547e+01],
         [-1.625e+00],
         [-8.684e+01],
         [ 7.411e+01],
         [ 5.138e+01]],

        [[-1.557e+02],
         [-1.513e+02],
         [-1.767e+02],
         [ 2.767e+02],
         [-1.107e+02],
         [-5.960e+01],
         [-3.462e+01],
         [ 1.667e+02],
         [-9.999e+01],
         [-1.300e+02]],

        [[-1.104e+02],
         [-1.603e+02],
         [-4.496e+01],
         [ 9.329e+01],
         [-1.333e+02],
         [-1.626e+02],
         [ 1.187e+02],
         [-7.771e+01],
         [ 4.658e+01],
         [-1.259e+02]],

        [[-1.385e+02],
         [-8.298e+01],
         [ 1.959e+02],
         [-9.046e+01],
         [ 7.602e+02],
         [-1.187e+02],
         [-8.324e+01],
         [-1.499e+02],
         [ 5.375e+01],
         [-1.600e+02]],

        [[ 1.968e+02],
         [-1.323e+02],
         [ 1.105e+02],
         [ 7.245e+01],
         [-9.566e+01],
         [ 1.942e+01],
         [-3.287e+01],
         [-1.364e+02],
         [ 3.291e+02],
         [ 6.545e+02]],

        [[ 3.725e+02],
         [-1.182e+02],
         [-1.312e+02],
         [-1.033e+02],
         [ 4.391e+02],
         [-1.923e+02],
         [ 9.102e+01],
         [-8.909e+01],
         [ 5.861e+01],
         [-7.536e+01]],

        [[-1.611e+02],
         [-1.196e+02],
         [ 1.364e+02],
         [-4.563e+01],
         [ 5.110e+01],
         [-1.297e+02],
         [ 2.117e+02],
         [-1.721e+02],
         [-1.189e+02],
         [ 5.478e+01]],

        [[-1.312e+01],
         [-3.427e+01],
         [ 1.117e+02],
         [ 2.464e+02],
         [-1.427e+02],
         [-1.562e+02],
         [-9.420e+01],
         [ 1.614e+01],
         [-1.537e+02],
         [-1.079e+02]],

        [[-1.145e+02],
         [-1.036e+02],
         [ 5.466e+01],
         [-1.594e+02],
         [-7.763e+01],
         [-8.797e+01],
         [-7.938e+00],
         [-9.016e+01],
         [ 1.599e+01],
         [-1.699e+02]],

        [[-9.761e+01],
         [-1.359e+02],
         [-1.155e+02],
         [-4.572e+01],
         [-4.736e+01],
         [-1.273e+02],
         [-1.400e+02],
         [-1.256e+02],
         [-1.313e+02],
         [ 5.117e+02]],

        [[-9.029e+01],
         [-1.131e+02],
         [-6.165e+00],
         [ 3.036e+02],
         [-1.721e+02],
         [-1.787e+01],
         [-1.610e+02],
         [-1.254e+02],
         [ 2.393e+02],
         [ 1.613e+02]],

        [[-1.391e+02],
         [-1.258e+02],
         [ 1.345e+03],
         [-1.845e+02],
         [-1.176e+02],
         [ 4.276e+01],
         [-1.730e+02],
         [-1.871e+02],
         [ 1.844e+02],
         [-9.971e+01]],

        [[ 2.947e+02],
         [-5.332e+00],
         [ 3.554e+02],
         [-1.374e+02],
         [ 1.711e+02],
         [-1.411e+02],
         [ 1.638e+02],
         [-6.904e+01],
         [-1.159e+02],
         [-2.408e+00]],

        [[-7.800e+01],
         [ 1.107e+02],
         [-9.046e+01],
         [-5.698e+00],
         [ 6.555e+02],
         [-7.810e+01],
         [-5.701e+01],
         [-4.609e+01],
         [-1.083e+02],
         [ 3.646e+02]],

        [[-1.424e+01],
         [ 2.936e+01],
         [-9.397e+01],
         [ 4.006e+01],
         [-1.334e+02],
         [ 5.315e+01],
         [-5.214e+01],
         [-3.417e+01],
         [-1.145e+02],
         [-4.018e+00]],

        [[ 1.240e+02],
         [-9.147e+01],
         [ 7.123e+01],
         [-1.309e+02],
         [-1.391e+02],
         [-1.350e+02],
         [-8.623e+01],
         [-7.352e+01],
         [-9.292e+01],
         [-1.594e+02]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [8.231e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [5.405e+00],
         [3.947e+00],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [7.888e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.207e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[ -31.444],
        [-138.155],
        [-153.942],
        [-153.600],
        [-153.079],
        [-171.574],
        [  -4.858],
        [ -96.079],
        [ -47.454],
        [  81.284],
        [ -27.403],
        [ 159.292],
        [ -95.184],
        [ -84.460],
        [-113.533],
        [  15.127],
        [ 135.235],
        [-121.806],
        [  29.647],
        [ -61.514],
        [ 117.925],
        [-127.304],
        [-155.487],
        [-139.208],
        [ 280.362],
        [-154.618],
        [ 548.825],
        [ -65.648],
        [ -98.965],
        [-129.362]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[ 0.015, -0.087, -0.041,  ...,  0.066,  0.000,  0.035],
        [-0.032, -0.077, -0.070,  ...,  0.128, -0.063,  0.114],
        [ 0.043, -0.040,  0.020,  ..., -0.034, -0.119,  0.006],
        ...,
        [ 0.121, -0.039, -0.001,  ..., -0.163, -0.026,  0.150],
        [-0.018, -0.108, -0.172,  ...,  0.002,  0.036, -0.053],
        [-0.032, -0.126, -0.072,  ..., -0.029, -0.015, -0.040]],
       device='cuda:0')
s after update for 1 param tensor([[2.349, 1.523, 1.584,  ..., 2.314, 1.813, 1.864],
        [1.733, 1.678, 1.618,  ..., 2.010, 1.443, 2.119],
        [1.776, 1.760, 1.991,  ..., 1.183, 2.133, 1.661],
        ...,
        [2.013, 1.024, 1.744,  ..., 1.849, 1.898, 2.495],
        [1.794, 1.778, 2.051,  ..., 2.084, 1.574, 2.188],
        [1.842, 1.934, 2.094,  ..., 2.136, 1.504, 1.781]], device='cuda:0')
b after update for 1 param tensor([[194.131, 156.315, 159.436,  ..., 192.670, 170.557, 172.937],
        [166.745, 164.075, 161.095,  ..., 179.590, 152.169, 184.399],
        [168.782, 168.038, 178.708,  ..., 137.785, 184.998, 163.235],
        ...,
        [179.698, 128.202, 167.256,  ..., 172.253, 174.480, 200.082],
        [169.656, 168.882, 181.406,  ..., 182.858, 158.896, 187.346],
        [171.926, 176.156, 183.275,  ..., 185.100, 155.338, 169.029]],
       device='cuda:0')
clipping threshold 1.2503991328394621
a after update for 1 param tensor([ 1.247e-02, -1.275e-01, -3.919e-02,  9.463e-02, -8.551e-02,  5.071e-02,
         9.271e-02,  6.141e-02,  3.549e-02, -2.907e-02, -5.559e-02, -5.904e-02,
         1.244e-02,  5.871e-02, -6.569e-02,  2.629e-02,  1.117e-02, -6.593e-02,
         1.238e-01,  1.181e-01,  8.350e-02,  1.889e-03,  3.795e-02, -8.228e-02,
        -4.714e-02, -2.449e-03,  2.934e-02, -1.152e-01, -1.243e-02, -3.551e-02,
        -7.528e-02, -7.206e-02, -1.017e-01, -9.665e-02,  2.085e-02,  1.214e-02,
         1.085e-01,  2.585e-03,  1.880e-02, -1.177e-03, -4.574e-02,  3.053e-02,
        -2.937e-02, -7.916e-02,  4.070e-03,  6.301e-02, -9.661e-02,  1.447e-01,
         8.368e-03, -9.047e-03, -1.408e-03, -2.397e-02, -1.072e-02, -5.478e-02,
        -4.928e-03, -2.699e-02, -1.407e-02,  3.441e-02,  1.221e-01,  2.240e-02,
        -2.437e-02, -8.887e-02, -1.864e-01,  3.007e-02,  4.345e-02, -4.156e-02,
         2.243e-01,  1.647e-01,  1.345e-02, -9.699e-02, -2.253e-03,  6.496e-03,
         1.112e-02, -1.552e-01, -1.167e-01, -6.993e-02, -7.335e-02, -2.735e-02,
        -1.913e-01, -2.751e-02, -7.037e-03, -2.122e-02,  4.114e-03, -1.025e-03,
         3.906e-03,  7.907e-02, -1.878e-01, -4.776e-02,  1.550e-01,  4.867e-02,
         1.748e-03,  1.120e-01, -3.281e-02, -4.415e-02,  2.540e-02, -1.586e-02,
         6.941e-02, -3.267e-02, -1.720e-01,  2.910e-02,  5.370e-02, -2.427e-01,
         4.679e-03,  7.668e-03, -2.230e-02,  1.006e-01,  3.667e-03, -3.922e-02,
        -3.232e-02, -2.649e-02,  5.582e-02, -1.240e-02,  1.368e-01, -6.691e-03,
        -1.166e-01,  1.416e-02,  1.568e-03, -2.564e-02, -1.954e-01,  3.999e-02,
        -5.013e-02, -1.632e-01,  6.740e-02, -3.455e-02, -4.869e-02,  1.105e-01,
        -9.313e-03,  1.964e-01,  1.857e-01,  3.264e-02,  2.323e-02, -7.011e-03,
         7.095e-02, -1.409e-01,  1.263e-01,  2.917e-02, -5.964e-02, -1.773e-02,
         1.380e-02,  2.114e-01,  2.067e-02, -4.457e-02, -4.248e-02,  1.087e-01,
        -1.597e-01, -2.278e-04,  9.863e-03,  7.216e-03,  2.463e-03,  7.011e-02,
        -4.659e-02,  4.372e-02, -6.467e-02, -1.194e-02,  2.321e-02,  5.822e-02,
         8.509e-02, -3.258e-02, -1.056e-01, -1.069e-03,  2.600e-02, -1.688e-03,
         1.489e-01,  2.093e-01,  2.930e-02, -2.732e-02,  4.356e-02, -2.288e-02,
        -1.750e-02,  1.035e-01, -7.566e-03,  5.101e-02, -3.747e-02,  1.590e-01,
        -1.390e-01,  2.394e-02, -6.774e-02,  1.498e-01, -7.229e-02,  3.158e-02,
        -1.410e-01,  6.443e-02,  2.646e-02,  2.219e-02,  3.218e-02, -7.292e-02,
        -1.659e-01, -1.101e-01,  2.670e-02,  3.390e-02, -5.191e-02, -6.098e-02,
        -4.377e-02,  3.833e-02,  6.964e-02, -3.720e-02, -8.841e-02, -1.357e-02,
        -5.165e-03,  4.985e-02,  1.266e-02,  6.600e-02, -2.088e-03, -9.021e-02,
        -5.513e-02, -3.454e-02, -1.067e-01, -1.207e-02,  1.035e-01,  2.216e-03,
        -1.402e-02, -4.880e-02,  6.267e-02, -4.596e-02,  6.659e-03,  8.189e-03,
         6.223e-02, -7.675e-02,  5.437e-03,  4.970e-02,  7.015e-02,  2.103e-02,
        -1.228e-02, -3.561e-02,  9.285e-03,  1.186e-01, -8.271e-02, -2.784e-01,
         2.124e-02,  4.916e-03, -3.601e-02,  1.066e-02,  2.937e-02, -3.147e-02,
         4.847e-02,  1.131e-03, -1.579e-02,  1.648e-01, -4.898e-02, -2.413e-02,
         8.455e-03, -1.334e-02,  5.868e-02,  1.080e-02,  8.168e-02, -1.531e-02,
        -8.010e-02, -2.099e-01, -6.007e-02,  1.441e-02, -2.196e-04, -1.408e-01,
        -8.739e-03,  1.074e-01, -5.363e-02, -1.267e-03, -9.061e-02, -3.437e-02,
         1.392e-03, -8.118e-02, -3.242e-02, -2.004e-01,  4.916e-02, -3.700e-02,
        -1.116e-01, -2.441e-02, -4.428e-03, -1.801e-02,  7.603e-02, -1.159e-01,
         5.676e-02, -3.070e-02, -5.019e-02,  1.677e-01,  5.272e-02,  5.608e-03,
         6.977e-02, -7.024e-02,  7.467e-03,  1.291e-01,  4.960e-02, -2.408e-01,
         2.411e-02, -7.496e-02, -6.767e-02, -1.269e-02, -6.685e-02,  2.772e-02,
         1.521e-02, -5.784e-02,  2.390e-01,  9.284e-02, -1.433e-01, -2.772e-01,
        -4.958e-02, -7.756e-02, -9.215e-02, -6.472e-02, -1.813e-01, -3.886e-02],
       device='cuda:0')
s after update for 1 param tensor([1.636, 1.926, 2.284, 1.578, 1.483, 1.507, 1.663, 1.786, 2.361, 1.561,
        1.825, 1.765, 1.736, 1.336, 1.967, 1.819, 1.334, 1.668, 1.682, 1.924,
        1.592, 0.604, 1.608, 1.566, 1.685, 0.499, 2.105, 2.260, 1.671, 1.407,
        2.098, 1.701, 1.596, 1.872, 1.625, 1.979, 1.901, 1.178, 1.058, 2.025,
        1.539, 1.926, 1.940, 2.025, 1.244, 1.181, 1.896, 1.613, 2.017, 1.099,
        2.249, 1.884, 1.619, 1.227, 2.203, 1.720, 1.961, 1.727, 1.903, 1.724,
        1.665, 1.744, 2.252, 1.805, 1.500, 1.561, 2.089, 1.883, 2.014, 1.386,
        0.823, 1.279, 1.634, 1.887, 1.504, 2.194, 1.951, 1.247, 1.555, 1.077,
        1.591, 1.218, 1.015, 0.795, 1.624, 1.316, 1.736, 1.676, 2.052, 1.794,
        1.603, 1.867, 2.317, 1.445, 1.802, 1.907, 2.026, 1.865, 2.225, 1.705,
        1.698, 2.122, 1.753, 0.321, 1.634, 1.857, 1.314, 1.204, 1.418, 1.503,
        1.178, 1.325, 1.768, 1.276, 2.146, 1.637, 0.104, 1.613, 2.199, 1.546,
        2.082, 1.826, 1.953, 1.497, 1.202, 1.893, 1.344, 1.996, 1.777, 2.063,
        0.990, 1.122, 1.662, 1.643, 1.473, 1.287, 1.780, 1.886, 1.368, 1.973,
        0.125, 1.487, 2.172, 1.787, 1.790, 1.722, 1.874, 0.101, 1.629, 1.313,
        1.537, 1.979, 1.885, 2.109, 1.253, 1.907, 1.164, 1.263, 1.739, 1.595,
        1.498, 1.911, 1.970, 1.381, 1.757, 1.526, 1.568, 1.251, 1.665, 1.991,
        1.487, 1.814, 1.755, 1.168, 1.546, 1.994, 1.350, 1.329, 1.970, 1.068,
        1.218, 1.822, 1.774, 1.676, 1.213, 1.506, 2.002, 1.697, 1.005, 2.005,
        1.761, 1.713, 1.448, 1.585, 1.848, 1.635, 1.486, 1.581, 1.321, 1.890,
        1.388, 1.401, 1.298, 1.705, 1.106, 1.782, 1.362, 1.689, 1.825, 1.761,
        1.217, 1.624, 1.765, 1.595, 0.098, 1.846, 1.162, 2.095, 1.227, 1.631,
        1.815, 2.046, 1.191, 2.067, 1.513, 1.558, 1.835, 2.109, 1.917, 1.388,
        1.765, 1.111, 1.313, 1.122, 1.617, 1.722, 1.293, 1.943, 1.271, 1.616,
        1.242, 1.272, 2.048, 1.213, 1.365, 0.953, 1.748, 1.705, 1.723, 1.253,
        0.506, 1.853, 2.314, 1.667, 1.627, 1.700, 1.589, 1.858, 1.188, 1.692,
        1.187, 2.196, 1.886, 1.509, 1.524, 0.986, 1.857, 1.380, 1.807, 1.577,
        1.638, 1.702, 1.925, 2.051, 1.592, 1.134, 1.508, 1.057, 1.489, 1.990,
        1.561, 2.253, 1.256, 1.590, 1.492, 1.801, 1.669, 1.401, 1.919, 1.442,
        1.899, 1.538, 1.169, 1.899, 1.160, 2.061, 1.859, 1.680, 1.909, 1.631],
       device='cuda:0')
b after update for 1 param tensor([161.998, 175.802, 191.426, 159.108, 154.241, 155.473, 163.354, 169.254,
        194.639, 158.247, 171.098, 168.252, 166.888, 146.415, 177.641, 170.847,
        146.278, 163.584, 164.266, 175.706, 159.834,  98.399, 160.638, 158.528,
        164.437,  89.478, 183.782, 190.405, 163.730, 150.249, 183.448, 165.176,
        160.032, 173.298, 161.479, 178.181, 174.652, 137.471, 130.279, 180.234,
        157.121, 175.792, 176.427, 180.245, 141.284, 137.656, 174.424, 160.861,
        179.882, 132.780, 189.947, 173.858, 161.149, 140.280, 187.994, 166.136,
        177.368, 166.444, 174.741, 166.288, 163.426, 167.266, 190.094, 170.181,
        155.147, 158.270, 183.046, 173.811, 179.766, 149.099, 114.878, 143.257,
        161.897, 173.970, 155.340, 187.593, 176.905, 141.449, 157.965, 131.420,
        159.759, 139.763, 127.621, 112.920, 161.433, 145.309, 166.878, 163.970,
        181.424, 169.662, 160.353, 173.067, 192.819, 152.251, 170.050, 174.934,
        180.269, 172.984, 188.943, 165.404, 165.032, 184.496, 167.696,  71.763,
        161.885, 172.597, 145.209, 139.001, 150.804, 155.307, 137.477, 145.825,
        168.406, 143.096, 185.537, 162.033,  40.892, 160.876, 187.805, 157.509,
        182.759, 171.166, 177.001, 154.984, 138.847, 174.278, 146.862, 178.936,
        168.853, 181.919, 126.057, 134.138, 163.312, 162.343, 153.700, 143.676,
        169.000, 173.953, 148.129, 177.929,  44.827, 154.447, 186.666, 169.299,
        169.449, 166.218, 173.398,  40.272, 161.666, 145.132, 157.045, 178.201,
        173.896, 183.936, 141.808, 174.926, 136.635, 142.361, 167.032, 159.986,
        155.004, 175.073, 177.775, 148.871, 167.868, 156.473, 158.622, 141.689,
        163.424, 178.722, 154.440, 170.608, 167.794, 136.908, 157.479, 178.864,
        147.184, 146.004, 177.762, 130.875, 139.781, 170.948, 168.697, 163.955,
        139.513, 155.453, 179.235, 165.000, 126.973, 179.368, 168.090, 165.766,
        152.427, 159.473, 172.196, 161.969, 154.389, 159.282, 145.569, 174.150,
        149.225, 149.941, 144.316, 165.368, 133.186, 169.078, 147.797, 164.622,
        171.128, 168.068, 139.714, 161.433, 168.289, 159.969,  39.627, 172.113,
        136.513, 183.337, 140.290, 161.778, 170.636, 181.175, 138.225, 182.109,
        155.781, 158.109, 171.577, 183.938, 175.346, 149.248, 168.255, 133.510,
        145.156, 134.193, 161.042, 166.223, 144.004, 176.568, 142.771, 161.023,
        141.153, 142.831, 181.260, 139.474, 147.981, 123.651, 167.437, 165.376,
        166.240, 141.772,  90.136, 172.419, 192.685, 163.528, 161.565, 165.162,
        159.670, 172.627, 138.044, 164.778, 138.009, 187.684, 173.925, 155.596,
        156.364, 125.798, 172.608, 148.805, 170.271, 159.079, 162.111, 165.266,
        175.752, 181.408, 159.824, 134.904, 155.523, 130.219, 154.554, 178.674,
        158.232, 190.131, 141.960, 159.691, 154.733, 169.992, 163.633, 149.916,
        175.442, 152.100, 174.566, 157.104, 136.933, 174.545, 136.390, 181.830,
        172.685, 164.162, 174.988, 161.765], device='cuda:0')
clipping threshold 1.2503991328394621
a after update for 1 param tensor([[[-2.254e-02],
         [ 9.704e-02],
         [-1.588e-01],
         [ 1.135e-01],
         [-1.733e-02],
         [ 2.776e-02],
         [-4.376e-02],
         [ 1.213e-01],
         [ 8.361e-03],
         [-1.384e-01]],

        [[ 1.044e-02],
         [ 1.695e-01],
         [ 5.311e-02],
         [-2.763e-02],
         [ 2.005e-01],
         [-1.210e-01],
         [ 3.298e-02],
         [ 2.673e-03],
         [ 3.334e-02],
         [ 3.014e-02]],

        [[ 9.947e-02],
         [-3.678e-02],
         [ 5.519e-02],
         [-7.310e-02],
         [ 7.918e-02],
         [-1.462e-01],
         [-6.785e-02],
         [ 1.398e-02],
         [ 4.305e-02],
         [ 1.720e-02]],

        [[ 3.561e-02],
         [ 6.849e-02],
         [ 9.355e-02],
         [ 7.036e-02],
         [-7.532e-02],
         [-2.140e-02],
         [-1.901e-02],
         [-2.775e-02],
         [ 1.482e-02],
         [-8.354e-04]],

        [[ 1.028e-01],
         [-1.627e-03],
         [ 3.033e-02],
         [-1.089e-02],
         [-3.208e-02],
         [-3.536e-02],
         [ 3.859e-03],
         [-1.601e-02],
         [-1.534e-01],
         [-5.207e-02]],

        [[-5.794e-02],
         [ 1.833e-02],
         [-9.179e-02],
         [-7.076e-02],
         [ 6.432e-02],
         [ 8.796e-03],
         [ 9.285e-02],
         [-4.966e-02],
         [-2.266e-01],
         [ 1.042e-01]],

        [[-9.138e-02],
         [ 1.185e-01],
         [ 1.919e-01],
         [ 1.327e-02],
         [ 2.824e-03],
         [-1.398e-01],
         [ 3.565e-02],
         [ 1.550e-02],
         [-1.511e-02],
         [ 1.445e-02]],

        [[ 6.063e-02],
         [-2.690e-03],
         [ 6.419e-02],
         [-6.556e-02],
         [ 9.461e-02],
         [-6.249e-02],
         [ 1.137e-02],
         [-2.095e-01],
         [-7.869e-02],
         [-2.848e-02]],

        [[-1.316e-02],
         [-2.939e-02],
         [ 1.354e-01],
         [-2.466e-03],
         [-9.723e-02],
         [ 8.908e-02],
         [-8.479e-03],
         [-3.684e-03],
         [-2.057e-02],
         [ 4.958e-02]],

        [[ 1.901e-01],
         [-4.987e-02],
         [ 4.272e-02],
         [-5.853e-02],
         [-3.860e-02],
         [ 1.129e-01],
         [ 3.614e-02],
         [-1.117e-01],
         [ 3.035e-03],
         [-4.474e-02]],

        [[ 1.840e-01],
         [-1.339e-01],
         [ 1.055e-01],
         [ 3.929e-02],
         [-5.909e-02],
         [ 1.006e-01],
         [-1.929e-01],
         [-4.278e-02],
         [-6.707e-02],
         [ 1.291e-02]],

        [[-3.174e-02],
         [-9.500e-02],
         [-4.971e-02],
         [-5.378e-02],
         [ 2.718e-02],
         [ 3.631e-02],
         [ 4.213e-02],
         [-4.672e-02],
         [-5.289e-02],
         [-1.854e-01]],

        [[-7.346e-02],
         [ 7.541e-02],
         [ 1.429e-01],
         [ 3.674e-02],
         [ 7.544e-02],
         [ 1.043e-01],
         [-7.882e-03],
         [ 1.552e-02],
         [ 1.087e-02],
         [-2.244e-01]],

        [[-3.217e-02],
         [-1.151e-01],
         [-1.610e-02],
         [ 8.950e-02],
         [ 1.839e-02],
         [-5.417e-02],
         [ 1.971e-02],
         [ 8.615e-04],
         [-7.998e-02],
         [-3.819e-02]],

        [[ 3.623e-02],
         [ 2.304e-02],
         [-8.772e-03],
         [ 8.164e-03],
         [-3.448e-02],
         [-2.745e-02],
         [-4.019e-03],
         [ 4.812e-03],
         [ 5.259e-02],
         [-6.438e-02]],

        [[-1.859e-01],
         [-1.688e-02],
         [ 4.076e-03],
         [ 1.549e-02],
         [ 3.343e-02],
         [-1.111e-01],
         [-2.331e-02],
         [-2.848e-02],
         [-1.302e-01],
         [ 1.254e-02]],

        [[-7.632e-03],
         [ 3.290e-03],
         [-7.815e-03],
         [-2.237e-02],
         [ 3.806e-02],
         [ 4.973e-02],
         [-1.121e-01],
         [ 1.733e-02],
         [ 1.413e-01],
         [ 5.951e-02]],

        [[ 5.150e-02],
         [ 7.347e-02],
         [ 4.287e-03],
         [ 6.271e-02],
         [ 6.805e-03],
         [ 8.951e-02],
         [-1.596e-02],
         [ 3.789e-02],
         [-1.447e-01],
         [ 8.041e-03]],

        [[ 1.495e-02],
         [ 1.248e-01],
         [ 3.493e-02],
         [ 1.155e-01],
         [-5.488e-02],
         [-2.946e-02],
         [ 6.505e-03],
         [-7.598e-02],
         [ 4.259e-02],
         [ 4.172e-02]],

        [[ 1.967e-01],
         [ 2.369e-02],
         [ 1.542e-01],
         [-7.189e-02],
         [ 9.926e-03],
         [-1.338e-01],
         [ 1.464e-01],
         [-8.748e-02],
         [-1.484e-02],
         [-2.000e-02]],

        [[ 4.500e-02],
         [ 1.167e-01],
         [ 3.567e-02],
         [ 4.324e-02],
         [-3.652e-02],
         [-1.086e-02],
         [ 7.167e-02],
         [ 5.215e-02],
         [ 7.863e-02],
         [ 1.242e-01]],

        [[ 4.234e-03],
         [-2.326e-02],
         [ 3.011e-02],
         [ 1.596e-01],
         [ 1.111e-02],
         [-1.557e-03],
         [-7.597e-02],
         [-1.437e-05],
         [ 8.170e-02],
         [-2.027e-01]],

        [[-1.124e-01],
         [ 6.182e-02],
         [ 1.815e-03],
         [ 1.953e-01],
         [-5.837e-02],
         [ 1.479e-01],
         [-8.314e-02],
         [ 3.264e-02],
         [ 2.985e-02],
         [ 4.494e-02]],

        [[ 1.220e-01],
         [-6.292e-02],
         [-4.131e-02],
         [-3.001e-02],
         [-8.612e-03],
         [ 7.091e-02],
         [ 9.253e-02],
         [-8.942e-03],
         [-1.076e-01],
         [-1.065e-03]],

        [[-7.870e-02],
         [-1.504e-02],
         [-1.586e-02],
         [ 1.319e-02],
         [ 2.179e-02],
         [-4.920e-02],
         [-1.195e-01],
         [-1.293e-01],
         [ 4.188e-02],
         [ 4.669e-02]],

        [[-1.253e-02],
         [ 1.564e-02],
         [-1.936e-01],
         [-1.979e-02],
         [ 8.182e-03],
         [-3.375e-02],
         [ 1.374e-02],
         [-5.170e-02],
         [-6.333e-02],
         [ 4.864e-03]],

        [[ 7.493e-02],
         [ 4.889e-02],
         [-3.759e-02],
         [-1.475e-02],
         [ 6.552e-03],
         [ 1.101e-02],
         [ 8.006e-02],
         [-3.368e-02],
         [ 1.314e-01],
         [ 4.683e-02]],

        [[-1.570e-03],
         [-2.768e-02],
         [ 3.844e-02],
         [ 4.869e-02],
         [-5.791e-02],
         [-3.855e-03],
         [ 7.849e-02],
         [-2.391e-02],
         [ 1.094e-01],
         [ 1.415e-02]],

        [[ 5.705e-02],
         [ 9.472e-02],
         [-6.997e-02],
         [-4.729e-02],
         [-6.074e-02],
         [-9.433e-02],
         [ 2.855e-02],
         [ 4.380e-02],
         [ 1.484e-01],
         [ 3.390e-03]],

        [[ 4.185e-02],
         [-1.869e-01],
         [ 1.006e-01],
         [ 4.406e-02],
         [-1.431e-01],
         [ 1.378e-02],
         [ 1.042e-02],
         [ 2.823e-02],
         [-1.346e-01],
         [ 1.074e-01]]], device='cuda:0')
s after update for 1 param tensor([[[1.943],
         [1.567],
         [2.057],
         [1.675],
         [1.452],
         [1.439],
         [1.972],
         [2.494],
         [1.514],
         [1.887]],

        [[1.218],
         [1.927],
         [1.369],
         [1.768],
         [1.541],
         [1.942],
         [1.672],
         [1.158],
         [1.692],
         [1.418]],

        [[1.591],
         [1.971],
         [1.364],
         [1.919],
         [1.512],
         [2.307],
         [1.464],
         [1.984],
         [1.918],
         [0.733]],

        [[1.885],
         [1.926],
         [1.881],
         [1.597],
         [1.640],
         [1.903],
         [0.831],
         [1.424],
         [1.798],
         [2.125]],

        [[1.797],
         [1.897],
         [1.540],
         [1.981],
         [1.683],
         [2.062],
         [1.337],
         [1.962],
         [2.180],
         [1.835]],

        [[2.055],
         [1.442],
         [1.250],
         [1.190],
         [1.575],
         [0.994],
         [2.065],
         [1.160],
         [2.006],
         [1.916]],

        [[2.009],
         [1.203],
         [1.993],
         [1.689],
         [0.511],
         [1.710],
         [1.148],
         [1.414],
         [1.719],
         [1.570]],

        [[1.165],
         [1.635],
         [1.964],
         [1.568],
         [1.780],
         [2.114],
         [1.135],
         [2.110],
         [1.358],
         [1.869]],

        [[0.961],
         [1.708],
         [1.547],
         [1.465],
         [1.542],
         [1.670],
         [1.653],
         [1.421],
         [2.183],
         [1.793]],

        [[1.978],
         [1.249],
         [1.795],
         [1.847],
         [1.512],
         [1.637],
         [0.661],
         [1.726],
         [1.324],
         [1.359]],

        [[1.502],
         [1.112],
         [1.730],
         [1.779],
         [2.078],
         [1.620],
         [1.784],
         [2.064],
         [1.724],
         [1.815]],

        [[1.467],
         [1.837],
         [1.225],
         [1.656],
         [1.583],
         [1.023],
         [2.027],
         [1.599],
         [1.470],
         [1.565]],

        [[2.151],
         [1.497],
         [1.906],
         [1.184],
         [1.196],
         [1.928],
         [1.163],
         [1.588],
         [1.434],
         [2.240]],

        [[1.740],
         [1.804],
         [1.445],
         [1.807],
         [1.248],
         [1.384],
         [1.975],
         [1.075],
         [1.936],
         [1.903]],

        [[1.550],
         [1.704],
         [0.347],
         [1.802],
         [0.300],
         [1.627],
         [0.657],
         [1.197],
         [1.899],
         [1.772]],

        [[1.743],
         [1.763],
         [1.912],
         [1.749],
         [1.490],
         [1.971],
         [0.815],
         [2.106],
         [1.470],
         [1.510]],

        [[1.338],
         [1.932],
         [1.097],
         [1.442],
         [1.547],
         [1.862],
         [2.171],
         [1.065],
         [2.388],
         [1.552]],

        [[1.656],
         [1.149],
         [1.982],
         [1.814],
         [1.971],
         [1.284],
         [1.097],
         [1.679],
         [2.114],
         [1.758]],

        [[1.609],
         [1.786],
         [1.692],
         [1.717],
         [1.028],
         [2.102],
         [1.679],
         [1.503],
         [1.719],
         [1.923]],

        [[1.992],
         [1.663],
         [1.425],
         [1.568],
         [2.095],
         [2.066],
         [1.754],
         [0.958],
         [1.504],
         [1.076]],

        [[1.992],
         [1.786],
         [1.871],
         [1.439],
         [1.723],
         [1.481],
         [1.381],
         [2.104],
         [1.579],
         [1.801]],

        [[0.276],
         [1.763],
         [1.162],
         [1.898],
         [1.638],
         [1.783],
         [1.985],
         [1.651],
         [1.814],
         [1.574]],

        [[1.232],
         [1.391],
         [1.995],
         [1.712],
         [1.356],
         [1.442],
         [1.924],
         [1.038],
         [1.374],
         [1.889]],

        [[1.956],
         [1.467],
         [1.331],
         [1.459],
         [0.512],
         [1.781],
         [1.795],
         [1.712],
         [1.413],
         [1.792]],

        [[1.107],
         [1.724],
         [0.111],
         [1.864],
         [1.863],
         [1.270],
         [2.123],
         [1.435],
         [1.791],
         [1.628]],

        [[1.499],
         [1.461],
         [1.483],
         [2.038],
         [1.432],
         [1.841],
         [2.109],
         [2.040],
         [2.054],
         [1.072]],

        [[2.075],
         [0.770],
         [1.781],
         [1.490],
         [1.695],
         [1.577],
         [1.841],
         [0.895],
         [1.252],
         [1.370]],

        [[1.587],
         [1.347],
         [1.848],
         [1.400],
         [1.596],
         [1.194],
         [1.650],
         [0.897],
         [1.824],
         [1.387]],

        [[1.574],
         [2.267],
         [1.489],
         [2.049],
         [1.493],
         [1.793],
         [1.544],
         [1.096],
         [1.237],
         [1.427]],

        [[1.341],
         [1.408],
         [1.637],
         [1.515],
         [1.790],
         [1.503],
         [1.301],
         [1.330],
         [1.604],
         [1.756]]], device='cuda:0')
b after update for 1 param tensor([[[176.548],
         [158.543],
         [181.668],
         [163.908],
         [152.605],
         [151.920],
         [177.863],
         [200.018],
         [155.864],
         [173.987]],

        [[139.789],
         [175.837],
         [148.185],
         [168.431],
         [157.223],
         [176.523],
         [163.800],
         [136.317],
         [164.753],
         [150.852]],

        [[159.780],
         [177.833],
         [147.947],
         [175.455],
         [155.724],
         [192.368],
         [153.245],
         [178.420],
         [175.437],
         [108.418]],

        [[173.911],
         [175.775],
         [173.700],
         [160.070],
         [162.225],
         [174.730],
         [115.450],
         [151.163],
         [169.854],
         [184.628]],

        [[169.779],
         [174.441],
         [157.203],
         [178.250],
         [164.294],
         [181.871],
         [146.468],
         [177.425],
         [186.998],
         [171.557]],

        [[181.570],
         [152.103],
         [141.634],
         [138.164],
         [158.941],
         [126.280],
         [182.003],
         [136.446],
         [179.391],
         [175.312]],

        [[179.531],
         [138.895],
         [178.794],
         [164.623],
         [ 90.574],
         [165.611],
         [135.698],
         [150.600],
         [166.089],
         [158.724]],

        [[136.701],
         [161.949],
         [177.502],
         [158.619],
         [168.979],
         [184.167],
         [134.929],
         [183.989],
         [147.588],
         [173.181]],

        [[124.140],
         [165.544],
         [157.521],
         [153.333],
         [157.270],
         [163.692],
         [162.869],
         [150.985],
         [187.133],
         [169.599]],

        [[178.160],
         [141.550],
         [169.701],
         [172.120],
         [155.750],
         [162.069],
         [103.013],
         [166.420],
         [145.734],
         [147.668]],

        [[155.238],
         [133.541],
         [166.620],
         [168.919],
         [182.580],
         [161.219],
         [169.167],
         [181.963],
         [166.327],
         [170.641]],

        [[153.431],
         [171.687],
         [140.168],
         [162.996],
         [159.361],
         [128.105],
         [180.314],
         [160.154],
         [153.560],
         [158.439]],

        [[185.748],
         [154.948],
         [174.870],
         [137.801],
         [138.516],
         [175.892],
         [136.588],
         [159.623],
         [151.663],
         [189.562]],

        [[167.090],
         [170.132],
         [152.258],
         [170.249],
         [141.515],
         [148.982],
         [178.025],
         [131.347],
         [176.229],
         [174.737]],

        [[157.685],
         [165.330],
         [ 74.658],
         [170.035],
         [ 69.402],
         [161.560],
         [102.639],
         [138.588],
         [174.565],
         [168.607]],

        [[167.234],
         [168.188],
         [175.126],
         [167.491],
         [154.605],
         [177.803],
         [114.345],
         [183.792],
         [153.578],
         [155.629]],

        [[146.536],
         [176.060],
         [132.659],
         [152.077],
         [157.564],
         [172.814],
         [186.616],
         [130.702],
         [195.737],
         [157.782]],

        [[163.009],
         [135.776],
         [178.322],
         [170.590],
         [177.825],
         [143.534],
         [132.674],
         [164.143],
         [184.143],
         [167.930]],

        [[160.664],
         [169.291],
         [164.735],
         [165.950],
         [128.446],
         [183.621],
         [164.141],
         [155.278],
         [166.076],
         [175.644]],

        [[178.749],
         [163.334],
         [151.198],
         [158.591],
         [183.331],
         [182.042],
         [167.739],
         [123.992],
         [155.309],
         [131.361]],

        [[178.753],
         [169.262],
         [173.252],
         [151.947],
         [166.251],
         [154.134],
         [148.867],
         [183.723],
         [159.174],
         [169.977]],

        [[ 66.513],
         [168.160],
         [136.533],
         [174.485],
         [162.101],
         [169.132],
         [178.451],
         [162.741],
         [170.598],
         [158.928]],

        [[140.561],
         [149.389],
         [178.881],
         [165.751],
         [147.486],
         [152.114],
         [175.692],
         [129.070],
         [148.460],
         [174.085]],

        [[177.122],
         [153.407],
         [146.134],
         [152.980],
         [ 90.618],
         [169.040],
         [169.689],
         [165.745],
         [150.537],
         [169.567]],

        [[133.288],
         [166.309],
         [ 42.275],
         [172.946],
         [172.902],
         [142.744],
         [184.565],
         [151.711],
         [169.522],
         [161.586]],

        [[155.086],
         [153.082],
         [154.226],
         [180.835],
         [151.587],
         [171.847],
         [183.958],
         [180.909],
         [181.541],
         [131.143]],

        [[182.466],
         [111.110],
         [169.037],
         [154.611],
         [164.888],
         [159.079],
         [171.872],
         [119.813],
         [141.729],
         [148.250]],

        [[159.539],
         [147.004],
         [172.194],
         [149.865],
         [160.006],
         [138.403],
         [162.678],
         [119.954],
         [171.080],
         [149.177]],

        [[158.923],
         [190.708],
         [154.546],
         [181.298],
         [154.776],
         [169.626],
         [157.369],
         [132.577],
         [140.879],
         [151.279]],

        [[146.653],
         [150.312],
         [162.063],
         [155.889],
         [169.460],
         [155.296],
         [144.483],
         [146.081],
         [160.420],
         [167.850]]], device='cuda:0')
clipping threshold 1.2503991328394621
a after update for 1 param tensor([[-0.026],
        [ 0.036],
        [ 0.061],
        [ 0.057],
        [ 0.019],
        [ 0.154],
        [ 0.070],
        [ 0.064],
        [-0.092],
        [-0.030],
        [-0.103],
        [ 0.044],
        [-0.020],
        [ 0.036],
        [ 0.021],
        [ 0.067],
        [-0.027],
        [ 0.114],
        [ 0.080],
        [-0.001],
        [-0.186],
        [ 0.093],
        [ 0.054],
        [ 0.056],
        [ 0.080],
        [ 0.119],
        [-0.078],
        [ 0.023],
        [ 0.084],
        [ 0.069]], device='cuda:0')
s after update for 1 param tensor([[1.036],
        [1.514],
        [1.665],
        [1.713],
        [1.743],
        [1.859],
        [1.891],
        [1.583],
        [1.687],
        [1.700],
        [1.669],
        [2.168],
        [1.136],
        [1.334],
        [1.669],
        [2.008],
        [1.737],
        [1.548],
        [1.200],
        [0.807],
        [2.261],
        [1.367],
        [1.804],
        [1.585],
        [2.068],
        [1.677],
        [2.138],
        [1.342],
        [1.380],
        [1.680]], device='cuda:0')
b after update for 1 param tensor([[128.890],
        [155.849],
        [163.455],
        [165.797],
        [167.204],
        [172.701],
        [174.195],
        [159.373],
        [164.530],
        [165.123],
        [163.636],
        [186.498],
        [135.014],
        [146.281],
        [163.653],
        [179.468],
        [166.939],
        [157.579],
        [138.722],
        [113.750],
        [190.448],
        [148.117],
        [170.126],
        [159.471],
        [182.132],
        [164.029],
        [185.198],
        [146.711],
        [148.820],
        [164.149]], device='cuda:0')
clipping threshold 1.2503991328394621
||w||^2 1.575055521082322
exp ma of ||w||^2 3.2766419947070893
||w|| 1.2550121597348458
exp ma of ||w|| 1.4657809483696234
||w||^2 0.9996653897937815
exp ma of ||w||^2 1.6533296886616349
||w|| 0.99983268089905
exp ma of ||w|| 1.2517720848679168
cuda
Objective function 61.21 = squared loss an data 49.61 + 0.5*rho*h**2 5.749885 + alpha*h 3.810060 + L2reg 1.74 + L1reg 0.30 ; SHD = 93 ; DAG True
Proportion of microbatches that were clipped  0.7740585774058577
iteration 2 in inner loop, alpha 35.529346048482935 rho 1000.0 h 0.1072369848760033
9630
cuda
Objective function 112.96 = squared loss an data 49.61 + 0.5*rho*h**2 57.498855 + alpha*h 3.810060 + L2reg 1.74 + L1reg 0.30 ; SHD = 93 ; DAG True
||w||^2 124.34412876967524
exp ma of ||w||^2 22779.03299058845
||w|| 11.15096985780498
exp ma of ||w|| 26.9370317076278
||w||^2 1.6061778660014365
exp ma of ||w||^2 2.355010376037392
||w|| 1.2673507273053646
exp ma of ||w|| 1.5002731641097629
||w||^2 3.074437283527193
exp ma of ||w||^2 2.0076040133588826
||w|| 1.7534073353123605
exp ma of ||w|| 1.3789011203975612
||w||^2 1.5849622896547955
exp ma of ||w||^2 1.8756264336352104
||w|| 1.2589528544210047
exp ma of ||w|| 1.3365852695862321
||w||^2 1.8758851521406081
exp ma of ||w||^2 1.8829280001129078
||w|| 1.3696295674891836
exp ma of ||w|| 1.33920075398686
cuda
Objective function 62.52 = squared loss an data 51.36 + 0.5*rho*h**2 7.682894 + alpha*h 1.392723 + L2reg 1.83 + L1reg 0.25 ; SHD = 92 ; DAG True
Proportion of microbatches that were clipped  0.7762158668605591
iteration 3 in inner loop, alpha 35.529346048482935 rho 10000.0 h 0.039199218400082
iteration 3 in outer loop, alpha = 427.52153004930295, rho = 10000.0, h = 0.039199218400082
cuda
9630
cuda
Objective function 77.88 = squared loss an data 51.36 + 0.5*rho*h**2 7.682894 + alpha*h 16.758510 + L2reg 1.83 + L1reg 0.25 ; SHD = 92 ; DAG True
||w||^2 22.844578345194808
exp ma of ||w||^2 4757.165308169585
||w|| 4.779600228595987
exp ma of ||w|| 10.180351638609944
||w||^2 12.926590566176097
exp ma of ||w||^2 1152.2343999190182
||w|| 3.5953568065181094
exp ma of ||w|| 5.192421325671367
||w||^2 3.1632860828798233
exp ma of ||w||^2 110.4262259951148
||w|| 1.7785629263199612
exp ma of ||w|| 2.5648015112591445
cuda
Objective function 67.27 = squared loss an data 50.62 + 0.5*rho*h**2 3.377354 + alpha*h 11.111209 + L2reg 1.93 + L1reg 0.24 ; SHD = 97 ; DAG True
Proportion of microbatches that were clipped  0.7817371937639198
iteration 1 in inner loop, alpha 427.52153004930295 rho 10000.0 h 0.025989822225991333
9630
cuda
Objective function 97.67 = squared loss an data 50.62 + 0.5*rho*h**2 33.773543 + alpha*h 11.111209 + L2reg 1.93 + L1reg 0.24 ; SHD = 97 ; DAG True
||w||^2 5940217432098.45
exp ma of ||w||^2 3499309099644.093
||w|| 2437256.1277178996
exp ma of ||w|| 1597259.3457999984
||w||^2 2650.5805897676732
exp ma of ||w||^2 984622.4245409425
||w|| 51.48378958242753
exp ma of ||w|| 133.12614064708785
||w||^2 2.6037231438048
exp ma of ||w||^2 3.5780567851947938
||w|| 1.6136056345355267
exp ma of ||w|| 1.8424361477898636
cuda
Objective function 64.88 = squared loss an data 51.33 + 0.5*rho*h**2 6.484391 + alpha*h 4.868639 + L2reg 1.99 + L1reg 0.21 ; SHD = 91 ; DAG True
Proportion of microbatches that were clipped  0.7794498381877023
iteration 2 in inner loop, alpha 427.52153004930295 rho 100000.0 h 0.011388055862745716
iteration 4 in outer loop, alpha = 11815.57739279502, rho = 1000000.0, h = 0.011388055862745716
Threshold 0.3
[[0.003 0.258 0.126 0.106 0.18  0.047 0.097 0.012 0.064 0.163 0.026 0.035
  0.064 0.256 0.126 0.025 0.148 0.099 0.698 0.036 0.134 0.103 0.121 0.089
  0.081 0.035 0.064 0.033 0.017 0.053]
 [0.023 0.003 0.06  0.062 0.082 0.051 0.087 0.015 0.042 0.041 0.031 0.065
  0.045 0.098 0.072 0.05  0.063 0.031 0.462 0.065 0.197 0.086 0.108 0.056
  0.051 0.061 0.015 0.096 0.05  0.039]
 [0.053 0.045 0.004 0.027 0.079 0.017 0.059 0.012 0.025 0.009 0.008 0.046
  0.035 0.464 0.04  0.041 0.05  0.036 0.144 0.042 0.072 0.063 0.067 0.028
  0.014 0.044 0.025 0.034 0.016 0.017]
 [0.075 0.073 0.187 0.005 0.123 0.093 0.067 0.033 0.141 0.1   0.014 0.175
  0.047 0.087 0.075 0.023 0.096 0.09  0.823 0.044 0.126 0.133 0.202 0.128
  0.117 0.056 0.05  0.108 0.038 0.039]
 [0.025 0.046 0.051 0.029 0.004 0.069 0.03  0.026 0.023 0.075 0.027 0.038
  0.062 0.062 0.024 0.044 0.031 0.027 0.119 0.034 0.377 0.033 0.076 0.071
  0.072 0.037 0.043 0.085 0.037 0.025]
 [0.109 0.085 0.275 0.053 0.063 0.005 0.057 0.005 0.093 0.078 0.031 0.062
  0.091 0.057 0.068 0.04  0.058 0.047 0.118 0.039 0.087 0.067 0.057 0.032
  0.037 0.05  0.039 0.041 0.055 0.037]
 [0.056 0.061 0.098 0.059 0.133 0.097 0.004 0.04  0.147 0.167 0.035 0.129
  0.05  0.305 0.032 0.181 0.045 0.07  0.276 0.115 0.183 0.1   0.151 0.079
  0.147 0.017 0.041 0.063 0.057 0.035]
 [0.269 0.268 0.315 0.127 0.217 0.829 0.111 0.003 0.146 0.107 0.084 0.098
  0.096 0.258 0.074 0.071 0.222 0.088 0.411 0.171 0.159 0.206 0.193 0.075
  0.103 0.059 0.121 0.067 0.206 0.062]
 [0.093 0.096 0.146 0.031 0.212 0.042 0.032 0.026 0.003 0.107 0.03  0.069
  0.047 0.072 0.053 0.139 0.32  0.089 0.424 0.083 0.197 0.179 0.093 0.055
  0.07  0.051 0.09  0.083 0.046 0.178]
 [0.037 0.11  0.44  0.04  0.048 0.056 0.025 0.035 0.025 0.005 0.003 0.136
  0.032 0.27  0.032 0.167 0.045 0.05  0.126 0.152 0.156 0.028 0.132 0.033
  0.003 0.008 0.046 0.028 0.169 0.034]
 [0.261 0.179 0.471 0.342 0.19  0.17  0.133 0.061 0.123 1.044 0.002 0.444
  0.198 0.233 0.061 0.311 0.116 0.094 0.491 0.274 0.302 0.138 0.546 0.075
  0.073 0.271 0.091 0.083 0.137 0.224]
 [0.158 0.05  0.177 0.036 0.088 0.07  0.025 0.041 0.056 0.061 0.01  0.003
  0.042 0.343 0.026 0.167 0.068 0.045 0.148 0.038 0.113 0.034 0.581 0.021
  0.048 0.056 0.047 0.015 0.038 0.011]
 [0.089 0.121 0.121 0.096 0.063 0.042 0.096 0.053 0.08  0.126 0.025 0.097
  0.002 0.111 0.136 0.075 0.059 0.083 0.226 0.134 0.135 0.127 0.136 0.066
  0.056 0.08  0.073 0.049 0.041 0.117]
 [0.017 0.043 0.015 0.077 0.07  0.068 0.01  0.017 0.04  0.02  0.02  0.018
  0.057 0.003 0.026 0.11  0.046 0.031 0.105 0.038 0.076 0.047 0.09  0.032
  0.023 0.05  0.036 0.027 0.039 0.032]
 [0.036 0.076 0.109 0.125 0.115 0.054 0.078 0.044 0.083 0.167 0.068 0.148
  0.042 0.153 0.003 0.07  0.135 0.076 0.317 0.149 0.16  0.094 0.24  0.056
  0.036 0.041 0.055 0.167 0.044 0.047]
 [0.117 0.079 0.159 0.202 0.096 0.101 0.024 0.054 0.025 0.039 0.015 0.03
  0.046 0.061 0.071 0.003 0.055 0.056 0.249 0.02  0.063 0.038 0.138 0.081
  0.032 0.014 0.045 0.052 0.112 0.06 ]
 [0.043 0.058 0.089 0.081 0.159 0.055 0.071 0.017 0.016 0.112 0.037 0.074
  0.044 0.084 0.024 0.075 0.003 0.067 0.175 0.086 0.119 0.045 0.093 0.034
  0.059 0.035 0.034 0.056 0.036 0.036]
 [0.044 0.193 0.084 0.038 0.165 0.074 0.092 0.055 0.058 0.085 0.05  0.097
  0.06  0.108 0.071 0.086 0.073 0.003 0.186 0.092 0.096 0.064 0.159 0.143
  0.036 0.039 0.069 0.083 0.03  0.061]
 [0.008 0.01  0.038 0.007 0.037 0.035 0.015 0.014 0.009 0.026 0.007 0.02
  0.024 0.041 0.013 0.017 0.025 0.027 0.003 0.025 0.071 0.031 0.026 0.01
  0.032 0.032 0.013 0.016 0.026 0.011]
 [0.104 0.071 0.17  0.093 0.156 0.104 0.058 0.025 0.071 0.029 0.016 0.193
  0.032 0.158 0.048 0.192 0.054 0.057 0.16  0.003 0.292 0.103 0.128 0.067
  0.046 0.049 0.053 0.066 0.04  0.036]
 [0.031 0.032 0.048 0.035 0.011 0.045 0.017 0.029 0.027 0.041 0.009 0.037
  0.03  0.054 0.032 0.059 0.042 0.055 0.067 0.015 0.003 0.053 0.086 0.018
  0.023 0.016 0.029 0.032 0.024 0.033]
 [0.045 0.034 0.079 0.052 0.139 0.057 0.044 0.03  0.038 0.075 0.028 0.07
  0.047 0.099 0.054 0.104 0.104 0.078 0.166 0.035 0.086 0.003 0.133 0.05
  0.031 0.047 0.034 0.042 0.032 0.046]
 [0.062 0.03  0.093 0.022 0.066 0.059 0.029 0.017 0.041 0.043 0.009 0.008
  0.033 0.051 0.014 0.041 0.03  0.036 0.169 0.041 0.074 0.048 0.004 0.019
  0.023 0.03  0.001 0.017 0.037 0.018]
 [0.041 0.079 0.152 0.056 0.046 0.104 0.041 0.066 0.087 0.134 0.048 0.134
  0.073 0.139 0.064 0.053 0.094 0.029 0.41  0.1   0.273 0.093 0.249 0.003
  0.12  0.065 0.06  0.066 0.038 0.073]
 [0.082 0.091 0.209 0.047 0.075 0.088 0.047 0.05  0.073 0.668 0.081 0.067
  0.074 0.25  0.124 0.112 0.066 0.093 0.104 0.087 0.168 0.106 0.107 0.045
  0.003 0.105 0.062 0.114 0.123 0.051]
 [0.133 0.066 0.106 0.092 0.082 0.084 0.177 0.07  0.095 0.515 0.02  0.073
  0.051 0.121 0.119 0.206 0.117 0.108 0.095 0.117 0.228 0.128 0.122 0.086
  0.074 0.003 0.055 0.148 0.13  0.025]
 [0.069 0.305 0.117 0.102 0.079 0.126 0.111 0.038 0.06  0.108 0.066 0.098
  0.048 0.157 0.104 0.082 0.152 0.083 0.292 0.059 0.171 0.131 1.181 0.089
  0.056 0.053 0.004 0.103 0.077 0.048]
 [0.085 0.056 0.124 0.054 0.03  0.105 0.075 0.045 0.083 0.169 0.041 0.32
  0.068 0.174 0.024 0.067 0.082 0.055 0.217 0.07  0.153 0.16  0.202 0.091
  0.053 0.031 0.037 0.003 0.042 0.041]
 [0.129 0.109 0.188 0.099 0.163 0.111 0.058 0.028 0.046 0.043 0.038 0.141
  0.084 0.128 0.125 0.033 0.079 0.131 0.16  0.117 0.141 0.167 0.103 0.134
  0.039 0.044 0.068 0.122 0.004 0.082]
 [0.063 0.096 0.26  0.097 0.184 0.071 0.115 0.065 0.027 0.098 0.025 0.196
  0.026 0.087 0.12  0.069 0.101 0.08  0.216 0.087 0.171 0.134 0.159 0.05
  0.074 0.224 0.084 0.116 0.062 0.004]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.698 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.462 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.464 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.823 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.377 0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.305 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.315 0.    0.    0.829 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.411 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.32  0.    0.424 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.44  0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.471 0.342 0.    0.    0.    0.    0.    1.044 0.    0.444
  0.    0.    0.    0.311 0.    0.    0.491 0.    0.302 0.    0.546 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.343 0.    0.    0.    0.    0.    0.    0.    0.    0.581 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.317 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.41  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.668 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.515 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.305 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.181 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.32
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5862068965517241, 'tpr': 0.13333333333333333, 'fpr': 0.04927536231884058, 'f1': 0.20168067226890757, 'shd': 91, 'npred': 29, 'ntrue': 90}
[0.258 0.126 0.106 0.18  0.047 0.097 0.012 0.064 0.163 0.026 0.035 0.064
 0.256 0.126 0.025 0.148 0.099 0.698 0.036 0.134 0.103 0.121 0.089 0.081
 0.035 0.064 0.033 0.017 0.053 0.023 0.06  0.062 0.082 0.051 0.087 0.015
 0.042 0.041 0.031 0.065 0.045 0.098 0.072 0.05  0.063 0.031 0.462 0.065
 0.197 0.086 0.108 0.056 0.051 0.061 0.015 0.096 0.05  0.039 0.053 0.045
 0.027 0.079 0.017 0.059 0.012 0.025 0.009 0.008 0.046 0.035 0.464 0.04
 0.041 0.05  0.036 0.144 0.042 0.072 0.063 0.067 0.028 0.014 0.044 0.025
 0.034 0.016 0.017 0.075 0.073 0.187 0.123 0.093 0.067 0.033 0.141 0.1
 0.014 0.175 0.047 0.087 0.075 0.023 0.096 0.09  0.823 0.044 0.126 0.133
 0.202 0.128 0.117 0.056 0.05  0.108 0.038 0.039 0.025 0.046 0.051 0.029
 0.069 0.03  0.026 0.023 0.075 0.027 0.038 0.062 0.062 0.024 0.044 0.031
 0.027 0.119 0.034 0.377 0.033 0.076 0.071 0.072 0.037 0.043 0.085 0.037
 0.025 0.109 0.085 0.275 0.053 0.063 0.057 0.005 0.093 0.078 0.031 0.062
 0.091 0.057 0.068 0.04  0.058 0.047 0.118 0.039 0.087 0.067 0.057 0.032
 0.037 0.05  0.039 0.041 0.055 0.037 0.056 0.061 0.098 0.059 0.133 0.097
 0.04  0.147 0.167 0.035 0.129 0.05  0.305 0.032 0.181 0.045 0.07  0.276
 0.115 0.183 0.1   0.151 0.079 0.147 0.017 0.041 0.063 0.057 0.035 0.269
 0.268 0.315 0.127 0.217 0.829 0.111 0.146 0.107 0.084 0.098 0.096 0.258
 0.074 0.071 0.222 0.088 0.411 0.171 0.159 0.206 0.193 0.075 0.103 0.059
 0.121 0.067 0.206 0.062 0.093 0.096 0.146 0.031 0.212 0.042 0.032 0.026
 0.107 0.03  0.069 0.047 0.072 0.053 0.139 0.32  0.089 0.424 0.083 0.197
 0.179 0.093 0.055 0.07  0.051 0.09  0.083 0.046 0.178 0.037 0.11  0.44
 0.04  0.048 0.056 0.025 0.035 0.025 0.003 0.136 0.032 0.27  0.032 0.167
 0.045 0.05  0.126 0.152 0.156 0.028 0.132 0.033 0.003 0.008 0.046 0.028
 0.169 0.034 0.261 0.179 0.471 0.342 0.19  0.17  0.133 0.061 0.123 1.044
 0.444 0.198 0.233 0.061 0.311 0.116 0.094 0.491 0.274 0.302 0.138 0.546
 0.075 0.073 0.271 0.091 0.083 0.137 0.224 0.158 0.05  0.177 0.036 0.088
 0.07  0.025 0.041 0.056 0.061 0.01  0.042 0.343 0.026 0.167 0.068 0.045
 0.148 0.038 0.113 0.034 0.581 0.021 0.048 0.056 0.047 0.015 0.038 0.011
 0.089 0.121 0.121 0.096 0.063 0.042 0.096 0.053 0.08  0.126 0.025 0.097
 0.111 0.136 0.075 0.059 0.083 0.226 0.134 0.135 0.127 0.136 0.066 0.056
 0.08  0.073 0.049 0.041 0.117 0.017 0.043 0.015 0.077 0.07  0.068 0.01
 0.017 0.04  0.02  0.02  0.018 0.057 0.026 0.11  0.046 0.031 0.105 0.038
 0.076 0.047 0.09  0.032 0.023 0.05  0.036 0.027 0.039 0.032 0.036 0.076
 0.109 0.125 0.115 0.054 0.078 0.044 0.083 0.167 0.068 0.148 0.042 0.153
 0.07  0.135 0.076 0.317 0.149 0.16  0.094 0.24  0.056 0.036 0.041 0.055
 0.167 0.044 0.047 0.117 0.079 0.159 0.202 0.096 0.101 0.024 0.054 0.025
 0.039 0.015 0.03  0.046 0.061 0.071 0.055 0.056 0.249 0.02  0.063 0.038
 0.138 0.081 0.032 0.014 0.045 0.052 0.112 0.06  0.043 0.058 0.089 0.081
 0.159 0.055 0.071 0.017 0.016 0.112 0.037 0.074 0.044 0.084 0.024 0.075
 0.067 0.175 0.086 0.119 0.045 0.093 0.034 0.059 0.035 0.034 0.056 0.036
 0.036 0.044 0.193 0.084 0.038 0.165 0.074 0.092 0.055 0.058 0.085 0.05
 0.097 0.06  0.108 0.071 0.086 0.073 0.186 0.092 0.096 0.064 0.159 0.143
 0.036 0.039 0.069 0.083 0.03  0.061 0.008 0.01  0.038 0.007 0.037 0.035
 0.015 0.014 0.009 0.026 0.007 0.02  0.024 0.041 0.013 0.017 0.025 0.027
 0.025 0.071 0.031 0.026 0.01  0.032 0.032 0.013 0.016 0.026 0.011 0.104
 0.071 0.17  0.093 0.156 0.104 0.058 0.025 0.071 0.029 0.016 0.193 0.032
 0.158 0.048 0.192 0.054 0.057 0.16  0.292 0.103 0.128 0.067 0.046 0.049
 0.053 0.066 0.04  0.036 0.031 0.032 0.048 0.035 0.011 0.045 0.017 0.029
 0.027 0.041 0.009 0.037 0.03  0.054 0.032 0.059 0.042 0.055 0.067 0.015
 0.053 0.086 0.018 0.023 0.016 0.029 0.032 0.024 0.033 0.045 0.034 0.079
 0.052 0.139 0.057 0.044 0.03  0.038 0.075 0.028 0.07  0.047 0.099 0.054
 0.104 0.104 0.078 0.166 0.035 0.086 0.133 0.05  0.031 0.047 0.034 0.042
 0.032 0.046 0.062 0.03  0.093 0.022 0.066 0.059 0.029 0.017 0.041 0.043
 0.009 0.008 0.033 0.051 0.014 0.041 0.03  0.036 0.169 0.041 0.074 0.048
 0.019 0.023 0.03  0.001 0.017 0.037 0.018 0.041 0.079 0.152 0.056 0.046
 0.104 0.041 0.066 0.087 0.134 0.048 0.134 0.073 0.139 0.064 0.053 0.094
 0.029 0.41  0.1   0.273 0.093 0.249 0.12  0.065 0.06  0.066 0.038 0.073
 0.082 0.091 0.209 0.047 0.075 0.088 0.047 0.05  0.073 0.668 0.081 0.067
 0.074 0.25  0.124 0.112 0.066 0.093 0.104 0.087 0.168 0.106 0.107 0.045
 0.105 0.062 0.114 0.123 0.051 0.133 0.066 0.106 0.092 0.082 0.084 0.177
 0.07  0.095 0.515 0.02  0.073 0.051 0.121 0.119 0.206 0.117 0.108 0.095
 0.117 0.228 0.128 0.122 0.086 0.074 0.055 0.148 0.13  0.025 0.069 0.305
 0.117 0.102 0.079 0.126 0.111 0.038 0.06  0.108 0.066 0.098 0.048 0.157
 0.104 0.082 0.152 0.083 0.292 0.059 0.171 0.131 1.181 0.089 0.056 0.053
 0.103 0.077 0.048 0.085 0.056 0.124 0.054 0.03  0.105 0.075 0.045 0.083
 0.169 0.041 0.32  0.068 0.174 0.024 0.067 0.082 0.055 0.217 0.07  0.153
 0.16  0.202 0.091 0.053 0.031 0.037 0.042 0.041 0.129 0.109 0.188 0.099
 0.163 0.111 0.058 0.028 0.046 0.043 0.038 0.141 0.084 0.128 0.125 0.033
 0.079 0.131 0.16  0.117 0.141 0.167 0.103 0.134 0.039 0.044 0.068 0.122
 0.082 0.063 0.096 0.26  0.097 0.184 0.071 0.115 0.065 0.027 0.098 0.025
 0.196 0.026 0.087 0.12  0.069 0.101 0.08  0.216 0.087 0.171 0.134 0.159
 0.05  0.074 0.224 0.084 0.116 0.062]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 1. 0. 0. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.5518233618233619, 0.21648604140606853)
Iterations 567
Achieves (13.598573656938171, 1e-05)-DP
