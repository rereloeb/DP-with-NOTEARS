samples  5000  graph  15 30 ER mlp  minibatch size  50  noise  0.75  minibatches per NN training  250 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006583550827196305
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0011537970975332712
iteration 4 in outer loop, alpha = 156.00431709949223, rho = 100000.0, h = 0.0011537970975332712
cuda
iteration 1 in inner loop,alpha 156.00431709949223 rho 100000.0 h 0.00033575511004180214
iteration 5 in outer loop, alpha = 491.7594271412944, rho = 1000000.0, h = 0.00033575511004180214
Threshold 0.3
[[0.002 0.    0.    0.005 0.194 0.    0.    0.    0.    2.734 0.179 0.001
  0.001 0.    0.006]
 [2.52  0.001 0.002 0.262 0.215 0.01  0.01  0.001 0.002 2.189 0.199 1.342
  0.001 0.001 0.098]
 [0.027 0.068 0.001 0.147 0.166 0.035 0.137 0.001 0.067 0.125 0.135 0.162
  0.001 0.001 0.006]
 [0.153 0.    0.001 0.001 0.204 0.    0.    0.    0.    0.214 0.335 0.001
  0.    0.    1.022]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    0.001]
 [2.646 0.03  0.001 0.968 0.376 0.001 1.377 0.    0.055 1.746 0.956 1.472
  0.012 0.002 1.923]
 [0.414 0.044 0.    0.336 0.408 0.    0.001 0.    0.059 0.139 1.162 0.096
  0.001 0.    0.163]
 [2.69  0.002 0.    2.092 0.137 0.612 0.046 0.    0.003 0.313 0.139 0.061
  0.014 0.022 0.118]
 [0.103 0.383 0.002 0.638 0.415 0.002 0.001 0.001 0.003 2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.001 0.    0.    0.002 0.402 0.001
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.057 0.    0.001 0.    0.001 0.001 0.015 0.
  0.    0.    0.005]
 [0.37  0.    0.    0.18  0.153 0.    0.004 0.    0.002 0.427 0.58  0.001
  0.    0.001 0.032]
 [0.041 0.078 0.    0.112 3.047 0.017 0.008 0.    1.212 0.107 1.585 0.031
  0.001 0.016 0.021]
 [0.126 1.398 0.    1.127 1.231 0.003 0.006 0.    3.368 0.386 0.434 0.158
  0.011 0.002 0.562]
 [0.089 0.    0.    0.    0.111 0.    0.    0.    0.    0.106 0.081 0.002
  0.004 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.734 0.    0.
  0.    0.    0.   ]
 [2.52  0.    0.    0.    0.    0.    0.    0.    0.    2.189 0.    1.342
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.335 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.968 0.376 0.    1.377 0.    0.    1.746 0.956 1.472
  0.    0.    1.923]
 [0.414 0.    0.    0.336 0.408 0.    0.    0.    0.    0.    1.162 0.
  0.    0.    0.   ]
 [2.69  0.    0.    2.092 0.    0.612 0.    0.    0.    0.313 0.    0.
  0.    0.    0.   ]
 [0.    0.383 0.    0.638 0.415 0.    0.    0.    0.    2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.    0.    0.    0.    0.402 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.057 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.427 0.58  0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.047 0.    0.    0.    1.212 0.    1.585 0.
  0.    0.    0.   ]
 [0.    1.398 0.    1.127 1.231 0.    0.    0.    3.368 0.386 0.434 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[7.166e-05 3.031e-04 4.737e-03 1.942e-01 6.655e-05 2.207e-04 4.993e-05
 2.937e-04 2.734e+00 1.793e-01 5.105e-04 1.050e-03 1.242e-04 5.740e-03
 2.520e+00 1.954e-03 2.620e-01 2.147e-01 9.724e-03 9.801e-03 9.328e-04
 1.620e-03 2.189e+00 1.994e-01 1.342e+00 5.488e-04 9.886e-04 9.781e-02
 2.679e-02 6.849e-02 1.472e-01 1.660e-01 3.508e-02 1.368e-01 5.716e-04
 6.687e-02 1.254e-01 1.352e-01 1.619e-01 8.477e-04 1.272e-03 6.092e-03
 1.532e-01 1.509e-04 9.882e-04 2.036e-01 4.044e-05 3.897e-04 2.489e-04
 6.858e-05 2.135e-01 3.353e-01 1.155e-03 4.106e-04 1.293e-04 1.022e+00
 6.064e-05 4.791e-06 5.172e-05 2.471e-04 1.136e-05 1.928e-04 4.191e-06
 2.163e-04 3.383e-04 1.833e-03 1.007e-04 2.631e-05 7.614e-05 1.086e-03
 2.646e+00 3.009e-02 1.478e-03 9.684e-01 3.765e-01 1.377e+00 1.229e-04
 5.513e-02 1.746e+00 9.560e-01 1.472e+00 1.180e-02 1.838e-03 1.923e+00
 4.141e-01 4.437e-02 3.959e-04 3.360e-01 4.079e-01 4.254e-04 5.935e-05
 5.872e-02 1.392e-01 1.162e+00 9.588e-02 6.731e-04 4.063e-04 1.632e-01
 2.690e+00 2.126e-03 4.852e-04 2.092e+00 1.369e-01 6.121e-01 4.646e-02
 3.330e-03 3.125e-01 1.385e-01 6.062e-02 1.448e-02 2.181e-02 1.178e-01
 1.034e-01 3.829e-01 2.017e-03 6.383e-01 4.147e-01 2.074e-03 1.394e-03
 1.328e-03 2.308e+00 1.722e+00 1.030e+00 5.492e-05 3.972e-04 3.674e-01
 1.834e-04 4.607e-06 8.949e-05 4.394e-04 2.722e+00 6.113e-06 8.992e-04
 1.147e-05 4.538e-04 4.020e-01 1.020e-03 1.265e-04 1.194e-04 2.179e-03
 6.634e-04 5.288e-05 3.430e-04 1.176e-03 3.057e+00 1.252e-04 7.885e-04
 1.415e-04 9.900e-04 8.589e-04 1.728e-04 2.153e-04 3.103e-04 5.194e-03
 3.696e-01 4.210e-04 8.175e-05 1.805e-01 1.529e-01 2.371e-04 3.593e-03
 6.136e-05 1.729e-03 4.267e-01 5.799e-01 2.072e-04 6.430e-04 3.169e-02
 4.077e-02 7.764e-02 2.020e-04 1.125e-01 3.047e+00 1.651e-02 8.033e-03
 3.454e-04 1.212e+00 1.069e-01 1.585e+00 3.144e-02 1.628e-02 2.119e-02
 1.264e-01 1.398e+00 2.136e-04 1.127e+00 1.231e+00 3.196e-03 6.293e-03
 3.254e-04 3.368e+00 3.864e-01 4.339e-01 1.576e-01 1.130e-02 5.624e-01
 8.923e-02 3.157e-04 3.306e-04 7.760e-05 1.109e-01 7.500e-05 1.536e-04
 2.248e-04 2.530e-04 1.060e-01 8.126e-02 2.152e-03 4.216e-03 1.418e-04]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9805555555555556, 0.9709424613206593)
cuda
2565
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
||w||^2 5252827290.462184
exp ma of ||w||^2 582565418383.6375
||w|| 72476.39126268763
exp ma of ||w|| 415702.7030102634
||w||^2 0.2155840798204057
exp ma of ||w||^2 17217750.26834828
||w|| 0.46431032706629055
exp ma of ||w|| 15.744770041821216
||w||^2 0.10389881891575276
exp ma of ||w||^2 5171.189262606898
||w|| 0.32233339714611137
exp ma of ||w|| 0.37577154344919017
||w||^2 0.18622624362407642
exp ma of ||w||^2 146.03844255968963
||w|| 0.43153938826493743
exp ma of ||w|| 0.35645466984992824
||w||^2 0.10434590542764471
exp ma of ||w||^2 73.79939423265985
||w|| 0.3230261683326054
exp ma of ||w|| 0.3582365978433475
||w||^2 0.12178817249954839
exp ma of ||w||^2 23.801343858432176
||w|| 0.3489816220083063
exp ma of ||w|| 0.36492275471926217
||w||^2 0.10780437650658192
exp ma of ||w||^2 1.9551563138963506
||w|| 0.3283357679366991
exp ma of ||w|| 0.37373779441715
||w||^2 0.15178572072683919
exp ma of ||w||^2 0.15638122074788482
||w|| 0.389596869503387
exp ma of ||w|| 0.37779956474924203
||w||^2 0.1281471303005038
exp ma of ||w||^2 0.15306496760896493
||w|| 0.3579764381918226
exp ma of ||w|| 0.37543836032636424
||w||^2 0.07239337193073275
exp ma of ||w||^2 0.1618186324318239
||w|| 0.26906016414685535
exp ma of ||w|| 0.3832364650021318
||w||^2 0.25655080256427454
exp ma of ||w||^2 0.22053428348956816
||w|| 0.5065084427374084
exp ma of ||w|| 0.43041909263421757
||w||^2 0.08438641597506902
exp ma of ||w||^2 0.24239171803297763
||w|| 0.2904934009148384
exp ma of ||w|| 0.44659401775877705
||w||^2 0.6017818621625632
exp ma of ||w||^2 0.3924779270576987
||w|| 0.7757460036394408
exp ma of ||w|| 0.5635418828841074
||w||^2 0.1652846535413033
exp ma of ||w||^2 0.38330484842028406
||w|| 0.4065521535317496
exp ma of ||w|| 0.5496471746484252
||w||^2 0.10708709447253101
exp ma of ||w||^2 0.3193493172089612
||w|| 0.3272416453823245
exp ma of ||w|| 0.5205822982379291
||w||^2 0.09604348727371567
exp ma of ||w||^2 0.33939308721073896
||w|| 0.30990883703714495
exp ma of ||w|| 0.5353024771582777
||w||^2 0.2985006144423162
exp ma of ||w||^2 0.3611328765643232
||w|| 0.5463520974996949
exp ma of ||w|| 0.5502855593967336
cuda
Objective function 15.13 = squared loss an data 12.37 + 0.5*rho*h**2 1.870760 + alpha*h 0.000000 + L2reg 0.72 + L1reg 0.17 ; SHD = 67 ; DAG False
Proportion of microbatches that were clipped  0.7638093702120796
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.9343010315473208
iteration 1 in outer loop, alpha = 1.9343010315473208, rho = 1.0, h = 1.9343010315473208
cuda
2565
cuda
Objective function 18.88 = squared loss an data 12.37 + 0.5*rho*h**2 1.870760 + alpha*h 3.741520 + L2reg 0.72 + L1reg 0.17 ; SHD = 67 ; DAG False
||w||^2 0.5995566816802195
exp ma of ||w||^2 788542.5714060029
||w|| 0.7743104556185584
exp ma of ||w|| 22.728864811396996
||w||^2 0.11484199426871958
exp ma of ||w||^2 154.17936041110195
||w|| 0.3388834523382922
exp ma of ||w|| 0.6233205469579758
||w||^2 0.10047171593614121
exp ma of ||w||^2 1.9351984455037197
||w|| 0.3169727368972625
exp ma of ||w|| 0.629150384963226
||w||^2 0.5783673195661674
exp ma of ||w||^2 0.5263066393885317
||w|| 0.7605046479582931
exp ma of ||w|| 0.6642558399709799
||w||^2 0.32161633356704644
exp ma of ||w||^2 0.4557001841879628
||w|| 0.5671122759798508
exp ma of ||w|| 0.6173791625112729
||w||^2 0.3904395724270104
exp ma of ||w||^2 0.4672163952114368
||w|| 0.6248516403331357
exp ma of ||w|| 0.6413193236050672
||w||^2 0.20228758917966339
exp ma of ||w||^2 0.4672252238690266
||w|| 0.44976392605417276
exp ma of ||w|| 0.6406546173432646
||w||^2 0.29647245288855706
exp ma of ||w||^2 0.5022978749875139
||w|| 0.5444928400709756
exp ma of ||w|| 0.6634661306519138
||w||^2 0.6273939786685944
exp ma of ||w||^2 0.5001061959396057
||w|| 0.7920820529898366
exp ma of ||w|| 0.6638391144362734
||w||^2 0.8263869628766686
exp ma of ||w||^2 0.46074400908361635
||w|| 0.9090582835421878
exp ma of ||w|| 0.6448570586263126
||w||^2 0.18873887781314588
exp ma of ||w||^2 0.5250295688537643
||w|| 0.43444087953730354
exp ma of ||w|| 0.6828823075745658
||w||^2 1.9629596146934813
exp ma of ||w||^2 0.4814156495404635
||w|| 1.4010566065271886
exp ma of ||w|| 0.6527426260714285
||w||^2 0.3513326652930612
exp ma of ||w||^2 0.47178047898006786
||w|| 0.5927332159522201
exp ma of ||w|| 0.6496205285815092
||w||^2 0.4668884110127473
exp ma of ||w||^2 0.5674463792774652
||w|| 0.6832923320312817
exp ma of ||w|| 0.7044940976654117
||w||^2 0.4171818431093274
exp ma of ||w||^2 0.5735659921652907
||w|| 0.6458961550507383
exp ma of ||w|| 0.7052187625790105
cuda
Objective function 14.74 = squared loss an data 10.93 + 0.5*rho*h**2 0.599943 + alpha*h 2.118821 + L2reg 0.94 + L1reg 0.15 ; SHD = 53 ; DAG False
Proportion of microbatches that were clipped  0.7631557543716958
iteration 1 in inner loop, alpha 1.9343010315473208 rho 1.0 h 1.0953933647033551
2565
cuda
Objective function 20.14 = squared loss an data 10.93 + 0.5*rho*h**2 5.999433 + alpha*h 2.118821 + L2reg 0.94 + L1reg 0.15 ; SHD = 53 ; DAG False
||w||^2 0.5381138339583712
exp ma of ||w||^2 190471.59076611267
||w|| 0.7335624267629656
exp ma of ||w|| 5.6171959507768765
||w||^2 0.408432903719361
exp ma of ||w||^2 2.440138057585383
||w|| 0.6390875555973227
exp ma of ||w|| 0.8138510685440871
||w||^2 0.8769566082575936
exp ma of ||w||^2 0.7232226167626495
||w|| 0.9364596137888668
exp ma of ||w|| 0.798461194865933
||w||^2 0.43595141314432134
exp ma of ||w||^2 0.706703099041271
||w|| 0.6602661684081059
exp ma of ||w|| 0.785889097908078
||w||^2 0.764553422220554
exp ma of ||w||^2 0.7661731831522975
||w|| 0.8743874554341193
exp ma of ||w|| 0.8144819280823106
||w||^2 0.7041224226614841
exp ma of ||w||^2 0.6855129886137922
||w|| 0.8391200287571999
exp ma of ||w|| 0.780979796058603
||w||^2 0.20021648139696777
exp ma of ||w||^2 0.8401567352851174
||w|| 0.44745556360041805
exp ma of ||w|| 0.8463981964086024
||w||^2 0.3380051901514651
exp ma of ||w||^2 0.7759298874629641
||w|| 0.5813821377987675
exp ma of ||w|| 0.8236305633009958
||w||^2 0.18749839085357856
exp ma of ||w||^2 0.7513952366983039
||w|| 0.4330108438059936
exp ma of ||w|| 0.8070290622816674
||w||^2 0.21737094386399486
exp ma of ||w||^2 0.8138986844942433
||w|| 0.4662305694224638
exp ma of ||w|| 0.8503740077808378
||w||^2 0.4795947341600774
exp ma of ||w||^2 0.7568759821514859
||w|| 0.6925277858397288
exp ma of ||w|| 0.8151197488919247
cuda
Objective function 15.48 = squared loss an data 11.73 + 0.5*rho*h**2 1.459637 + alpha*h 1.045109 + L2reg 1.11 + L1reg 0.14 ; SHD = 52 ; DAG True
Proportion of microbatches that were clipped  0.7696082374708391
iteration 2 in inner loop, alpha 1.9343010315473208 rho 10.0 h 0.5403030198137042
2565
cuda
Objective function 28.62 = squared loss an data 11.73 + 0.5*rho*h**2 14.596368 + alpha*h 1.045109 + L2reg 1.11 + L1reg 0.14 ; SHD = 52 ; DAG True
||w||^2 5.383681581264759
exp ma of ||w||^2 1.187490592544004
||w|| 2.320276186419358
exp ma of ||w|| 1.017861942865889
||w||^2 0.8283391283289443
exp ma of ||w||^2 1.230926953629947
||w|| 0.9101313797078663
exp ma of ||w|| 1.0068166019420899
||w||^2 0.866024993111038
exp ma of ||w||^2 0.99820985547862
||w|| 0.9306046384534293
exp ma of ||w|| 0.9262095864721874
||w||^2 0.7909508662178494
exp ma of ||w||^2 1.045473065412044
||w|| 0.8893541849105167
exp ma of ||w|| 0.9597976104407288
||w||^2 1.2733335013503668
exp ma of ||w||^2 1.2155540629931798
||w|| 1.1284207997685822
exp ma of ||w|| 1.0148769642150206
||w||^2 2.650412970626643
exp ma of ||w||^2 1.3086208402861779
||w|| 1.6280088975882911
exp ma of ||w|| 1.044192977572654
cuda
Objective function 17.94 = squared loss an data 14.40 + 0.5*rho*h**2 1.819939 + alpha*h 0.369035 + L2reg 1.22 + L1reg 0.13 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.7682699326790494
iteration 3 in inner loop, alpha 1.9343010315473208 rho 100.0 h 0.1907846568754774
iteration 2 in outer loop, alpha = 21.01276671909506, rho = 100.0, h = 0.1907846568754774
cuda
2565
cuda
Objective function 21.58 = squared loss an data 14.40 + 0.5*rho*h**2 1.819939 + alpha*h 4.008913 + L2reg 1.22 + L1reg 0.13 ; SHD = 43 ; DAG True
||w||^2 1551.629226215394
exp ma of ||w||^2 26529195.270491384
||w|| 39.390725129342236
exp ma of ||w|| 370.15465933038536
v before min max tensor([[ -7.349,  35.274, 185.964,  ..., -30.998,  -9.569, -15.219],
        [-32.472,  24.881, -37.805,  ..., -18.216,  -8.839, -37.549],
        [147.055,  -5.505, -43.460,  ..., -44.678, 118.060, -56.493],
        ...,
        [ 37.424,  -6.234, -41.031,  ..., -17.254, 144.066,  86.439],
        [-54.694, -41.494,  75.113,  ...,  43.925, -56.808, -18.802],
        [ -5.339,  59.429, -66.520,  ...,  -3.313, -22.170, -18.875]],
       device='cuda:0')
v tensor([[1.000e-12, 1.000e+01, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e-12],
        ...,
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12]], device='cuda:0')
v before min max tensor([-1.889e+01, -3.646e+01, -5.341e+01,  1.292e+02, -2.570e+01, -4.872e+01,
         1.357e+02, -3.403e+01, -5.211e+01,  5.357e+01, -1.992e+01, -4.812e+01,
         1.822e+02,  3.202e+01,  1.291e+02, -3.799e+01, -3.072e+01, -4.670e+01,
        -5.512e+01,  1.369e+02,  3.823e+01,  1.583e+02,  9.177e+01, -3.743e+01,
        -2.166e+01, -5.924e+01, -5.303e+01, -1.784e+01,  1.556e+01,  1.027e+02,
        -1.474e+01,  7.301e+00, -4.432e+01, -6.819e+00, -4.566e+01, -4.564e+01,
        -5.055e+00,  1.342e+02, -2.394e+01, -2.117e+01, -3.670e+01, -1.266e+01,
        -4.087e+01,  1.172e+02,  1.287e+02, -4.894e+01, -3.244e+01, -5.365e+01,
         1.837e+02,  3.860e+00, -5.685e+01,  3.949e+01, -1.949e+01, -4.235e+01,
         7.646e+01,  1.354e+02, -3.941e+01,  3.576e+01,  4.944e+02, -4.022e+01,
        -2.260e+01,  1.037e+01,  1.743e+02,  1.117e+02, -3.780e+01, -2.931e+01,
        -1.078e+01, -4.677e+01,  3.383e+01,  5.924e+00, -3.690e+01, -2.695e+01,
        -4.452e+01, -4.005e+01, -2.579e+01, -3.991e+01,  8.650e+01,  1.037e+02,
         1.237e+01, -3.924e+00, -3.539e+01,  1.231e+01, -2.633e+01, -3.218e-01,
        -1.629e+01, -3.636e+01, -4.167e+01, -4.681e+01, -1.537e+01,  4.825e+01,
        -4.269e+01,  9.732e+01, -4.445e+01, -3.576e+01, -2.422e+01, -1.112e+01,
         3.844e+01, -2.561e+01,  1.224e+02,  1.036e+01, -2.687e+01, -4.937e+01,
         1.274e+01, -4.916e+01,  3.675e+01, -6.125e+01, -5.243e+01, -3.586e+01,
        -1.388e+01, -4.870e+01, -5.655e+01, -1.095e+01, -2.841e+01, -4.942e+01,
         3.005e+01, -3.081e+01,  3.127e+00, -4.214e+01, -2.456e+01,  1.317e+02,
         1.167e-01, -4.532e+01, -4.393e+01, -3.522e+01, -1.608e+01, -3.893e+01,
        -3.096e+01,  6.284e+01, -2.508e+01, -4.332e+01, -6.349e+01, -5.303e+01,
        -5.222e+01,  9.530e+01,  1.800e+02,  6.130e+00, -2.319e+01, -3.009e+01,
        -3.670e+01, -2.392e+01, -4.002e+01, -2.780e+01,  6.552e+01, -3.698e+01,
        -2.697e+01, -2.669e+01,  2.489e+01,  1.599e+02, -1.457e+01,  2.873e+01],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 7.301e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 3.860e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 5.924e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 3.127e+00, 1.000e-12, 1.000e-12, 1.000e+01,
        1.167e-01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 6.130e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
       device='cuda:0')
v before min max tensor([[[-40.503],
         [-57.471],
         [ 18.410],
         [ 13.880],
         [-32.890],
         [-36.086],
         [352.874],
         [-35.098],
         [ 83.642],
         [-14.704]],

        [[-14.579],
         [-19.330],
         [ -6.865],
         [-43.388],
         [-17.700],
         [-46.944],
         [ 45.847],
         [-11.952],
         [ 28.042],
         [-44.151]],

        [[-34.026],
         [-27.335],
         [-11.746],
         [ 70.882],
         [-53.233],
         [ 38.566],
         [ 24.137],
         [  7.108],
         [-22.547],
         [ -2.842]],

        [[ 61.032],
         [-20.219],
         [ 32.501],
         [-34.110],
         [-48.485],
         [ 62.989],
         [-21.287],
         [-21.969],
         [ 58.828],
         [-23.445]],

        [[-43.116],
         [-46.393],
         [291.088],
         [-15.822],
         [-27.708],
         [-51.065],
         [-21.899],
         [-44.901],
         [ 25.751],
         [ 27.619]],

        [[ -7.335],
         [ 33.005],
         [117.321],
         [-34.222],
         [-40.514],
         [-28.112],
         [  2.401],
         [-29.381],
         [ 29.118],
         [-19.256]],

        [[-57.656],
         [-18.271],
         [-36.821],
         [123.186],
         [-35.316],
         [ 25.302],
         [-57.096],
         [251.434],
         [-27.312],
         [-19.559]],

        [[ 37.734],
         [ 39.288],
         [ 16.539],
         [-37.714],
         [-12.597],
         [-46.263],
         [ 16.253],
         [-24.750],
         [ -3.947],
         [-46.483]],

        [[ 69.769],
         [-43.909],
         [-54.686],
         [-13.813],
         [-53.967],
         [-61.101],
         [-25.917],
         [  4.200],
         [ 70.412],
         [-38.643]],

        [[-42.488],
         [-39.655],
         [-25.542],
         [ 55.476],
         [-26.045],
         [ 43.352],
         [ 57.380],
         [181.337],
         [-48.793],
         [-41.135]],

        [[-46.442],
         [-49.968],
         [-45.187],
         [-39.709],
         [-40.762],
         [ 82.998],
         [-51.791],
         [-33.850],
         [ 82.363],
         [ 43.106]],

        [[-40.757],
         [ -1.151],
         [ 52.954],
         [  5.732],
         [-44.918],
         [-31.202],
         [-17.218],
         [ 68.272],
         [-39.405],
         [-39.158]],

        [[-13.190],
         [-32.614],
         [-65.317],
         [-45.967],
         [ 12.211],
         [-36.829],
         [-39.192],
         [-34.106],
         [132.679],
         [-30.753]],

        [[-43.921],
         [-37.968],
         [ -6.902],
         [-53.361],
         [-33.284],
         [-13.899],
         [-53.842],
         [ 28.528],
         [-32.440],
         [ 75.648]],

        [[-12.185],
         [ 28.997],
         [-19.949],
         [-58.769],
         [ 96.462],
         [-34.595],
         [  3.757],
         [  1.447],
         [ -3.297],
         [-12.647]]], device='cuda:0')
v tensor([[[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [7.108e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.401e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.200e+00],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [5.732e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [3.757e+00],
         [1.447e+00],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[ 7.471e+01],
        [-2.046e+01],
        [ 2.046e+01],
        [ 9.579e+01],
        [ 1.851e+01],
        [-4.287e+01],
        [-4.929e+01],
        [ 3.464e+01],
        [ 2.003e-01],
        [-4.129e+01],
        [ 4.007e+02],
        [ 6.504e+00],
        [ 3.507e+01],
        [-1.177e+01],
        [-3.395e+01]], device='cuda:0')
v tensor([[1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [2.003e-01],
        [1.000e-12],
        [1.000e+01],
        [6.504e+00],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[ 0.061, -0.024, -0.070,  ..., -0.161,  0.174,  0.007],
        [ 0.016,  0.245, -0.067,  ..., -0.020, -0.096, -0.055],
        [ 0.023, -0.004,  0.051,  ..., -0.042,  0.055, -0.005],
        ...,
        [-0.001, -0.050,  0.041,  ...,  0.160, -0.100,  0.011],
        [ 0.011,  0.092,  0.022,  ..., -0.026, -0.058, -0.023],
        [ 0.141, -0.002,  0.044,  ..., -0.013, -0.058, -0.023]],
       device='cuda:0')
s after update for 1 param tensor([[1.912, 1.781, 1.851,  ..., 1.179, 1.554, 1.555],
        [2.110, 2.025, 1.874,  ..., 1.873, 1.377, 1.704],
        [2.117, 1.398, 1.611,  ..., 1.514, 2.039, 1.929],
        ...,
        [2.004, 1.755, 1.465,  ..., 1.088, 1.591, 2.588],
        [1.984, 1.630, 1.952,  ..., 1.330, 1.949, 1.744],
        [1.661, 1.599, 2.253,  ..., 1.775, 1.651, 1.323]], device='cuda:0')
b after update for 1 param tensor([[ 92.534,  89.300,  91.036,  ...,  72.670,  83.428,  83.450],
        [ 97.205,  95.240,  91.599,  ...,  91.573,  78.532,  87.348],
        [ 97.358,  79.111,  84.937,  ...,  82.334,  95.553,  92.946],
        ...,
        [ 94.735,  88.647,  80.990,  ...,  69.796,  84.419, 107.651],
        [ 94.264,  85.430,  93.486,  ...,  77.177,  93.413,  88.373],
        [ 86.234,  84.616, 100.440,  ...,  89.159,  85.975,  76.962]],
       device='cuda:0')
clipping threshold 0.7367598717064202
a after update for 1 param tensor([-1.521e-02, -1.881e-01, -4.059e-02, -6.387e-02,  9.611e-02,  4.571e-03,
        -9.708e-02, -3.424e-02,  2.829e-02,  1.510e-01, -1.189e-02,  1.776e-02,
        -8.934e-02,  4.730e-02,  8.241e-02,  5.521e-02,  7.081e-02,  6.091e-02,
        -8.374e-02, -1.181e-04,  1.296e-01,  4.925e-02, -1.706e-01, -6.674e-02,
        -1.531e-02, -1.275e-01,  4.512e-02, -6.461e-02,  9.404e-02,  6.075e-02,
         2.296e-02,  4.392e-02,  4.117e-02,  1.757e-02, -9.300e-02, -1.679e-01,
         3.535e-02,  1.246e-01,  2.456e-02, -2.841e-02,  2.821e-03, -8.017e-02,
         1.189e-01,  7.330e-02,  6.999e-03, -1.042e-01, -3.009e-02,  6.972e-02,
         4.271e-02,  2.102e-01, -2.326e-01,  1.409e-01,  4.731e-02, -7.194e-02,
        -7.584e-02,  6.673e-02, -1.047e-01, -1.127e-02,  1.254e-03, -4.019e-02,
        -1.555e-01, -3.075e-01, -7.853e-02, -2.454e-02,  5.449e-02, -2.239e-02,
         2.119e-01,  5.569e-03, -4.356e-02, -1.814e-02,  8.669e-02,  2.492e-04,
        -3.885e-02,  3.532e-02,  5.950e-02,  1.688e-02,  2.777e-03,  1.124e-01,
        -2.079e-01,  4.253e-02, -2.260e-01, -6.514e-02, -4.648e-03, -4.756e-02,
         4.162e-02,  1.725e-02,  5.020e-02,  1.624e-04,  1.303e-01, -8.924e-02,
        -1.903e-02,  6.933e-04, -1.930e-01,  8.167e-02,  5.407e-02,  1.401e-01,
         1.090e-02, -1.028e-01,  6.273e-02, -1.086e-02,  6.554e-03,  1.625e-02,
        -8.050e-02, -6.632e-02,  5.851e-02, -1.262e-01, -1.426e-01,  1.189e-02,
         1.310e-01,  6.552e-02, -2.855e-02,  7.435e-02,  6.379e-02,  6.305e-02,
        -2.063e-02,  6.144e-02,  9.903e-02,  2.925e-02,  1.444e-01, -7.377e-02,
         9.986e-03,  8.048e-02,  3.451e-02,  7.553e-02, -1.039e-02,  2.945e-01,
         5.519e-02, -3.262e-02, -9.035e-02, -3.604e-02, -1.948e-03,  1.276e-03,
        -7.708e-02,  9.429e-03, -8.490e-02, -5.948e-02,  4.280e-02, -3.285e-02,
         1.121e-01,  6.416e-02, -4.148e-02,  4.960e-02, -1.233e-01, -8.733e-02,
         2.009e-02, -1.881e-02, -5.010e-02,  9.629e-02, -5.421e-02, -1.189e-02],
       device='cuda:0')
s after update for 1 param tensor([0.640, 2.018, 1.849, 1.762, 0.880, 1.651, 2.246, 1.265, 1.975, 2.463,
        1.073, 1.660, 1.368, 2.016, 2.017, 1.740, 1.560, 1.693, 1.919, 1.967,
        2.033, 1.410, 1.907, 1.286, 2.143, 2.007, 1.832, 1.333, 2.097, 2.087,
        1.369, 2.284, 1.567, 0.878, 1.625, 2.001, 1.595, 1.994, 1.965, 1.247,
        1.261, 1.228, 1.730, 2.094, 1.659, 1.754, 1.413, 1.846, 2.079, 1.908,
        1.928, 2.126, 1.751, 1.472, 2.475, 1.684, 1.768, 2.016, 2.469, 1.372,
        1.864, 1.521, 1.912, 1.980, 1.577, 1.101, 1.623, 1.822, 1.647, 1.664,
        1.606, 1.166, 1.651, 1.455, 2.054, 1.699, 1.456, 2.216, 1.961, 1.539,
        1.785, 1.653, 1.815, 1.519, 1.311, 1.237, 1.420, 1.949, 1.568, 2.142,
        1.570, 1.607, 1.517, 1.273, 1.673, 1.783, 2.007, 1.472, 1.870, 1.986,
        1.042, 1.732, 2.146, 1.679, 2.228, 2.099, 1.804, 1.268, 1.380, 1.650,
        1.973, 1.608, 1.641, 1.696, 2.002, 1.435, 1.388, 1.684, 1.732, 2.348,
        1.621, 1.666, 1.769, 1.336, 1.613, 1.347, 1.048, 2.090, 1.736, 1.803,
        2.203, 2.011, 1.768, 2.105, 2.070, 1.816, 1.382, 1.698, 1.939, 1.064,
        1.357, 1.128, 2.369, 1.395, 1.849, 1.816, 2.069, 1.856, 1.680, 1.695],
       device='cuda:0')
b after update for 1 param tensor([ 53.526,  95.054,  90.993,  88.819,  62.788,  85.988, 100.298,  75.256,
         94.038, 105.033,  69.321,  86.228,  78.268,  95.025,  95.052,  88.282,
         83.588,  87.061,  92.698,  93.854,  95.411,  79.453,  92.402,  75.883,
         97.963,  94.801,  90.589,  77.261,  96.918,  96.684,  78.292, 101.126,
         83.775,  62.705,  85.303,  94.652,  84.519,  94.507,  93.814,  74.737,
         75.134,  74.164,  88.010,  96.843,  86.189,  88.638,  79.536,  90.917,
         96.490,  92.445,  92.928,  97.571,  88.563,  81.189, 105.286,  86.849,
         88.989,  95.014, 105.159,  78.396,  91.367,  82.527,  92.535,  94.167,
         84.027,  70.205,  85.251,  90.320,  85.881,  86.327,  84.793,  72.263,
         85.980,  80.724,  95.905,  87.232,  80.762,  99.608,  93.723,  83.032,
         89.399,  86.031,  90.146,  82.484,  76.623,  74.438,  79.733,  93.429,
         83.802,  97.943,  83.858,  84.834,  82.436,  75.493,  86.565,  89.362,
         94.811,  81.183,  91.501,  94.297,  68.297,  88.065,  98.030,  86.700,
         99.883,  96.949,  89.881,  75.349,  78.611,  85.951,  93.994,  84.868,
         85.725,  87.145,  94.692,  80.160,  78.842,  86.846,  88.059, 102.546,
         85.192,  86.381,  89.008,  77.343,  84.986,  77.655,  68.523,  96.747,
         88.165,  89.848,  99.333,  94.903,  88.991,  97.089,  96.270,  90.171,
         78.676,  87.197,  93.177,  69.017,  77.952,  71.069, 103.000,  79.043,
         90.993,  90.185,  96.267,  91.177,  86.743,  87.135], device='cuda:0')
clipping threshold 0.7367598717064202
a after update for 1 param tensor([[[ 0.053],
         [-0.156],
         [-0.029],
         [-0.070],
         [-0.049],
         [ 0.062],
         [-0.023],
         [ 0.040],
         [-0.008],
         [ 0.035]],

        [[-0.137],
         [ 0.115],
         [-0.099],
         [ 0.016],
         [ 0.082],
         [ 0.104],
         [ 0.132],
         [-0.005],
         [-0.030],
         [ 0.003]],

        [[-0.127],
         [-0.075],
         [ 0.058],
         [-0.251],
         [ 0.120],
         [ 0.013],
         [-0.110],
         [-0.127],
         [ 0.123],
         [-0.094]],

        [[ 0.114],
         [-0.025],
         [-0.028],
         [ 0.041],
         [ 0.187],
         [ 0.056],
         [-0.016],
         [ 0.006],
         [ 0.058],
         [-0.038]],

        [[ 0.069],
         [ 0.069],
         [ 0.080],
         [-0.043],
         [ 0.051],
         [-0.110],
         [ 0.012],
         [-0.017],
         [-0.028],
         [ 0.106]],

        [[ 0.095],
         [-0.014],
         [-0.037],
         [-0.013],
         [-0.117],
         [-0.127],
         [-0.006],
         [ 0.239],
         [-0.037],
         [ 0.186]],

        [[ 0.124],
         [ 0.021],
         [-0.112],
         [ 0.011],
         [-0.255],
         [ 0.102],
         [ 0.028],
         [ 0.143],
         [-0.154],
         [-0.004]],

        [[-0.055],
         [ 0.094],
         [ 0.118],
         [ 0.064],
         [-0.019],
         [ 0.071],
         [-0.119],
         [-0.077],
         [-0.084],
         [ 0.013]],

        [[-0.056],
         [ 0.009],
         [-0.163],
         [-0.010],
         [ 0.018],
         [ 0.046],
         [-0.044],
         [ 0.093],
         [ 0.065],
         [ 0.185]],

        [[-0.091],
         [-0.051],
         [-0.045],
         [ 0.059],
         [-0.022],
         [-0.012],
         [ 0.031],
         [ 0.044],
         [ 0.045],
         [ 0.004]],

        [[ 0.090],
         [-0.022],
         [-0.007],
         [ 0.069],
         [ 0.008],
         [-0.053],
         [ 0.051],
         [-0.068],
         [-0.070],
         [-0.013]],

        [[ 0.262],
         [ 0.074],
         [ 0.111],
         [ 0.100],
         [-0.023],
         [ 0.052],
         [ 0.044],
         [ 0.096],
         [ 0.069],
         [ 0.011]],

        [[-0.002],
         [ 0.004],
         [-0.052],
         [ 0.003],
         [ 0.006],
         [ 0.002],
         [-0.089],
         [ 0.004],
         [-0.068],
         [-0.010]],

        [[-0.049],
         [ 0.104],
         [ 0.015],
         [ 0.075],
         [-0.077],
         [-0.154],
         [ 0.195],
         [ 0.027],
         [-0.025],
         [ 0.002]],

        [[ 0.197],
         [-0.000],
         [ 0.052],
         [-0.018],
         [ 0.056],
         [ 0.135],
         [ 0.007],
         [ 0.001],
         [ 0.034],
         [ 0.004]]], device='cuda:0')
s after update for 1 param tensor([[[1.422],
         [1.967],
         [1.913],
         [1.946],
         [1.349],
         [1.440],
         [2.067],
         [1.189],
         [2.154],
         [1.209]],

        [[2.183],
         [1.175],
         [1.355],
         [1.496],
         [2.027],
         [1.631],
         [2.305],
         [1.871],
         [1.611],
         [1.933]],

        [[1.647],
         [1.389],
         [1.441],
         [1.532],
         [1.820],
         [1.668],
         [1.677],
         [1.584],
         [1.151],
         [1.997]],

        [[1.644],
         [1.066],
         [2.221],
         [1.260],
         [1.706],
         [1.450],
         [1.151],
         [1.650],
         [1.725],
         [1.302]],

        [[1.884],
         [1.591],
         [1.897],
         [1.414],
         [1.438],
         [1.729],
         [1.541],
         [1.679],
         [1.750],
         [2.090]],

        [[1.783],
         [2.557],
         [1.510],
         [1.483],
         [1.435],
         [1.411],
         [2.178],
         [1.660],
         [1.877],
         [1.839]],

        [[1.991],
         [1.434],
         [1.340],
         [1.686],
         [1.212],
         [1.750],
         [1.977],
         [2.041],
         [0.928],
         [1.524]],

        [[1.897],
         [1.708],
         [1.486],
         [1.527],
         [1.465],
         [1.618],
         [1.939],
         [1.834],
         [1.721],
         [1.705]],

        [[1.774],
         [1.487],
         [1.894],
         [1.512],
         [1.930],
         [2.085],
         [0.919],
         [1.239],
         [1.734],
         [1.617]],

        [[1.593],
         [1.614],
         [1.761],
         [1.652],
         [1.510],
         [2.242],
         [1.979],
         [2.356],
         [1.685],
         [1.550]],

        [[1.936],
         [1.693],
         [1.567],
         [1.382],
         [1.431],
         [1.917],
         [1.755],
         [1.406],
         [2.386],
         [2.220]],

        [[2.091],
         [1.518],
         [2.128],
         [1.508],
         [1.920],
         [1.654],
         [1.094],
         [2.061],
         [1.374],
         [1.630]],

        [[1.267],
         [1.899],
         [2.219],
         [1.576],
         [1.660],
         [1.285],
         [1.330],
         [1.355],
         [2.357],
         [1.511]],

        [[1.603],
         [1.545],
         [1.129],
         [1.809],
         [1.667],
         [1.346],
         [1.880],
         [1.998],
         [1.480],
         [1.622]],

        [[1.346],
         [2.483],
         [1.530],
         [2.068],
         [1.836],
         [1.865],
         [1.832],
         [1.278],
         [1.427],
         [1.432]]], device='cuda:0')
b after update for 1 param tensor([[[ 79.789],
         [ 93.863],
         [ 92.564],
         [ 93.342],
         [ 77.739],
         [ 80.292],
         [ 96.207],
         [ 72.985],
         [ 98.217],
         [ 73.568]],

        [[ 98.876],
         [ 72.543],
         [ 77.910],
         [ 81.853],
         [ 95.271],
         [ 85.453],
         [101.596],
         [ 91.524],
         [ 84.944],
         [ 93.047]],

        [[ 85.874],
         [ 78.878],
         [ 80.330],
         [ 82.821],
         [ 90.273],
         [ 86.434],
         [ 86.670],
         [ 84.218],
         [ 71.783],
         [ 94.565]],

        [[ 85.805],
         [ 69.108],
         [ 99.725],
         [ 75.104],
         [ 87.397],
         [ 80.573],
         [ 71.798],
         [ 85.950],
         [ 87.895],
         [ 76.350]],

        [[ 91.865],
         [ 84.410],
         [ 92.176],
         [ 79.567],
         [ 80.235],
         [ 88.003],
         [ 83.063],
         [ 86.709],
         [ 88.534],
         [ 96.740]],

        [[ 89.369],
         [107.001],
         [ 82.246],
         [ 81.483],
         [ 80.170],
         [ 79.482],
         [ 98.762],
         [ 86.226],
         [ 91.681],
         [ 90.747]],

        [[ 94.429],
         [ 80.140],
         [ 77.460],
         [ 86.886],
         [ 73.662],
         [ 88.534],
         [ 94.099],
         [ 95.605],
         [ 64.474],
         [ 82.621]],

        [[ 92.170],
         [ 87.454],
         [ 81.586],
         [ 82.702],
         [ 80.995],
         [ 85.128],
         [ 93.184],
         [ 90.625],
         [ 87.793],
         [ 87.378]],

        [[ 89.143],
         [ 81.605],
         [ 92.095],
         [ 82.275],
         [ 92.957],
         [ 96.621],
         [ 64.153],
         [ 74.489],
         [ 88.133],
         [ 85.088]],

        [[ 84.453],
         [ 85.011],
         [ 88.797],
         [ 86.003],
         [ 82.245],
         [100.210],
         [ 94.141],
         [102.719],
         [ 86.869],
         [ 83.310]],

        [[ 93.111],
         [ 87.071],
         [ 83.762],
         [ 78.667],
         [ 80.059],
         [ 92.665],
         [ 88.656],
         [ 79.355],
         [103.361],
         [ 99.698]],

        [[ 96.780],
         [ 82.444],
         [ 97.619],
         [ 82.186],
         [ 92.719],
         [ 86.060],
         [ 69.988],
         [ 96.080],
         [ 78.449],
         [ 85.438]],

        [[ 75.324],
         [ 92.214],
         [ 99.690],
         [ 84.021],
         [ 86.225],
         [ 75.869],
         [ 77.164],
         [ 77.896],
         [102.742],
         [ 82.266]],

        [[ 84.725],
         [ 83.185],
         [ 71.093],
         [ 90.016],
         [ 86.413],
         [ 77.628],
         [ 91.746],
         [ 94.581],
         [ 81.411],
         [ 85.218]],

        [[ 77.651],
         [105.439],
         [ 82.780],
         [ 96.245],
         [ 90.666],
         [ 91.397],
         [ 90.576],
         [ 75.658],
         [ 79.938],
         [ 80.090]]], device='cuda:0')
clipping threshold 0.7367598717064202
a after update for 1 param tensor([[ 1.123e-01],
        [-1.662e-01],
        [ 9.809e-03],
        [ 1.501e-01],
        [ 3.068e-02],
        [-3.745e-02],
        [ 2.780e-03],
        [ 5.071e-03],
        [ 2.405e-02],
        [-2.375e-01],
        [-9.430e-02],
        [ 4.289e-02],
        [-1.696e-04],
        [ 4.027e-02],
        [-4.603e-02]], device='cuda:0')
s after update for 1 param tensor([[2.172],
        [1.456],
        [2.143],
        [1.522],
        [1.693],
        [1.731],
        [1.774],
        [1.636],
        [1.672],
        [1.491],
        [2.153],
        [1.791],
        [1.421],
        [1.474],
        [1.195]], device='cuda:0')
b after update for 1 param tensor([[98.635],
        [80.736],
        [97.964],
        [82.558],
        [87.067],
        [88.038],
        [89.121],
        [85.595],
        [86.519],
        [81.714],
        [98.197],
        [89.556],
        [79.758],
        [81.247],
        [73.139]], device='cuda:0')
clipping threshold 0.7367598717064202
||w||^2 0.947838220478519
exp ma of ||w||^2 1.2988326593911523
||w|| 0.9735698333856277
exp ma of ||w|| 1.029784927989697
||w||^2 2.2977849743750687
exp ma of ||w||^2 1.247796508633392
||w|| 1.5158446405799866
exp ma of ||w|| 1.0376460812883508
||w||^2 0.3404504822132682
exp ma of ||w||^2 1.31231825274576
||w|| 0.5834813469283043
exp ma of ||w|| 1.057806317049438
||w||^2 1.2845521168206526
exp ma of ||w||^2 1.2770329651981902
||w|| 1.1333808348567804
exp ma of ||w|| 1.0531348062048804
cuda
Objective function 18.92 = squared loss an data 14.17 + 0.5*rho*h**2 0.731173 + alpha*h 2.541021 + L2reg 1.35 + L1reg 0.13 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.7690564816313319
iteration 1 in inner loop, alpha 21.01276671909506 rho 100.0 h 0.12092746423728151
2565
cuda
Objective function 25.50 = squared loss an data 14.17 + 0.5*rho*h**2 7.311726 + alpha*h 2.541021 + L2reg 1.35 + L1reg 0.13 ; SHD = 43 ; DAG True
||w||^2 2816744571.1201777
exp ma of ||w||^2 27277339587.91964
||w|| 53073.01170199575
exp ma of ||w|| 118542.85657418438
||w||^2 29621085.714466196
exp ma of ||w||^2 4292270318.7182603
||w|| 5442.525674212865
exp ma of ||w|| 31609.40833174357
||w||^2 73803.93060681799
exp ma of ||w||^2 247275209.46437803
||w|| 271.66878842962063
exp ma of ||w|| 2687.3669406060453
||w||^2 181.4881026080618
exp ma of ||w||^2 44344426.3798421
||w|| 13.471752024442173
exp ma of ||w|| 522.5934040146295
||w||^2 186.8891543005344
exp ma of ||w||^2 28496321.719263792
||w|| 13.670740810231697
exp ma of ||w|| 340.85322299748435
||w||^2 1.0078905716391886
exp ma of ||w||^2 1.6860756907073702
||w|| 1.0039375337336425
exp ma of ||w|| 1.1828085615404056
||w||^2 0.6118609121693176
exp ma of ||w||^2 1.5008742947451088
||w|| 0.7822153873258424
exp ma of ||w|| 1.1355073173115968
||w||^2 2.1535190081649622
exp ma of ||w||^2 1.5424981676343614
||w|| 1.4674873110746007
exp ma of ||w|| 1.1588584454595605
||w||^2 1.1065152096188877
exp ma of ||w||^2 1.383822849675097
||w|| 1.0519102669044007
exp ma of ||w|| 1.083829725575962
||w||^2 0.9339177579953303
exp ma of ||w||^2 1.4842299267321035
||w|| 0.9663942042434497
exp ma of ||w|| 1.142305492566795
||w||^2 1.7418106182967676
exp ma of ||w||^2 1.59385583973671
||w|| 1.3197767304725325
exp ma of ||w|| 1.170524516311938
||w||^2 2.594549176167924
exp ma of ||w||^2 1.6590067484907902
||w|| 1.6107604341328738
exp ma of ||w|| 1.1537571873609296
cuda
Objective function 18.77 = squared loss an data 14.81 + 0.5*rho*h**2 1.305830 + alpha*h 1.073845 + L2reg 1.44 + L1reg 0.13 ; SHD = 47 ; DAG True
Proportion of microbatches that were clipped  0.7748191717669501
iteration 2 in inner loop, alpha 21.01276671909506 rho 1000.0 h 0.051104404767599476
2565
cuda
Objective function 30.52 = squared loss an data 14.81 + 0.5*rho*h**2 13.058301 + alpha*h 1.073845 + L2reg 1.44 + L1reg 0.13 ; SHD = 47 ; DAG True
||w||^2 85090808262.25343
exp ma of ||w||^2 739302217554.3962
||w|| 291703.2880552659
exp ma of ||w|| 663872.0295761508
||w||^2 1.8274152059180777
exp ma of ||w||^2 279.68841757635823
||w|| 1.351819220871666
exp ma of ||w|| 1.2492082228181813
||w||^2 2.6601137876390517
exp ma of ||w||^2 1.4370017245432798
||w|| 1.6309855264958828
exp ma of ||w|| 1.1207377209091065
||w||^2 0.5909397432755437
exp ma of ||w||^2 1.611653442534056
||w|| 0.7687260521639315
exp ma of ||w|| 1.1565540533718253
||w||^2 0.6610706637174744
exp ma of ||w||^2 1.6292825750403128
||w|| 0.813062521407471
exp ma of ||w|| 1.1914356919148257
||w||^2 1.783597692786623
exp ma of ||w||^2 1.4263248893339207
||w|| 1.3355140181917309
exp ma of ||w|| 1.1066437462951064
||w||^2 1.144221498887448
exp ma of ||w||^2 1.3271424317114568
||w|| 1.0696828964171803
exp ma of ||w|| 1.0839945998165688
||w||^2 1.5602292104354083
exp ma of ||w||^2 1.5738273472475017
||w|| 1.249091353919083
exp ma of ||w|| 1.1324791726347139
||w||^2 1.080414592960448
exp ma of ||w||^2 1.5167872054508955
||w|| 1.0394299365327362
exp ma of ||w|| 1.144387154876862
||w||^2 0.6269603815371304
exp ma of ||w||^2 1.2777948476714576
||w|| 0.7918082984770559
exp ma of ||w|| 1.070860189887117
||w||^2 1.1893486731952727
exp ma of ||w||^2 1.3062490805780211
||w|| 1.090572635451336
exp ma of ||w|| 1.0800783247538142
||w||^2 0.3895634346192932
exp ma of ||w||^2 1.2577241822666902
||w|| 0.6241501699265115
exp ma of ||w|| 1.068011347841149
||w||^2 2.0796890555950096
exp ma of ||w||^2 1.3746902319458392
||w|| 1.4421127055799106
exp ma of ||w|| 1.1105062582486813
||w||^2 0.5862768664657271
exp ma of ||w||^2 1.3041054009868187
||w|| 0.7656871857787141
exp ma of ||w|| 1.0825738513519303
cuda
Objective function 18.32 = squared loss an data 14.51 + 0.5*rho*h**2 1.725327 + alpha*h 0.390332 + L2reg 1.56 + L1reg 0.14 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.7731983418367347
iteration 3 in inner loop, alpha 21.01276671909506 rho 10000.0 h 0.01857593389961565
iteration 3 in outer loop, alpha = 206.77210571525154, rho = 10000.0, h = 0.01857593389961565
cuda
2565
cuda
Objective function 21.77 = squared loss an data 14.51 + 0.5*rho*h**2 1.725327 + alpha*h 3.840985 + L2reg 1.56 + L1reg 0.14 ; SHD = 46 ; DAG True
||w||^2 470956354600.90204
exp ma of ||w||^2 271177923813.2927
||w|| 686262.5988649694
exp ma of ||w|| 304519.6016529797
||w||^2 13109.154564270582
exp ma of ||w||^2 1937131024.6688533
||w|| 114.49521633793519
exp ma of ||w|| 3280.5756433201127
||w||^2 3.0803548572753554
exp ma of ||w||^2 2399947.806102804
||w|| 1.7550939739157432
exp ma of ||w|| 6.420696276813128
||w||^2 1.0491469838153105
exp ma of ||w||^2 1.5795781087171037
||w|| 1.0242787627473833
exp ma of ||w|| 1.1627009199209701
||w||^2 1.4504373119012444
exp ma of ||w||^2 1.2944718330656901
||w|| 1.2043410280735454
exp ma of ||w|| 1.075364691211615
||w||^2 0.497404751917357
exp ma of ||w||^2 1.5144023131132323
||w|| 0.7052692761756725
exp ma of ||w|| 1.1347545719856265
||w||^2 1.5644903764309923
exp ma of ||w||^2 1.3991694877819745
||w|| 1.2507958971914612
exp ma of ||w|| 1.1239317951890282
||w||^2 1.5958825422200573
exp ma of ||w||^2 1.4105345668797307
||w|| 1.263282447523141
exp ma of ||w|| 1.111072140415926
||w||^2 1.222647373213247
exp ma of ||w||^2 1.3483936619419752
||w|| 1.105733861837127
exp ma of ||w|| 1.090524972373748
||w||^2 1.0085580183946716
exp ma of ||w||^2 1.3998288864944854
||w|| 1.0042698932033518
exp ma of ||w|| 1.1094841318028767
cuda
Objective function 18.88 = squared loss an data 14.32 + 0.5*rho*h**2 0.567643 + alpha*h 2.203153 + L2reg 1.65 + L1reg 0.14 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.7747733418164466
iteration 1 in inner loop, alpha 206.77210571525154 rho 10000.0 h 0.010654982130834156
2565
cuda
Objective function 23.99 = squared loss an data 14.32 + 0.5*rho*h**2 5.676432 + alpha*h 2.203153 + L2reg 1.65 + L1reg 0.14 ; SHD = 46 ; DAG True
||w||^2 342270739.71805954
exp ma of ||w||^2 17214395116106.4
||w|| 18500.56052442897
exp ma of ||w|| 1009736.0446273618
||w||^2 10141554.702877838
exp ma of ||w||^2 4522517942377.455
||w|| 3184.5807734893206
exp ma of ||w|| 271701.8208122827
||w||^2 59547.15257897775
exp ma of ||w||^2 593868782834.2727
||w|| 244.0228525752819
exp ma of ||w|| 36324.48555121687
||w||^2 2.5683742187602228
exp ma of ||w||^2 916.7554090163536
||w|| 1.602614806733116
exp ma of ||w|| 1.4877892140773519
||w||^2 1.456843797480353
exp ma of ||w||^2 6.6493751425826
||w|| 1.2069978448532346
exp ma of ||w|| 1.1985102838679171
||w||^2 0.6262660326492301
exp ma of ||w||^2 1.40223215906959
||w|| 0.7913697193153337
exp ma of ||w|| 1.1195241483918528
||w||^2 0.6346043356638601
exp ma of ||w||^2 1.4006148002513688
||w|| 0.7966205719562232
exp ma of ||w|| 1.1245418653754566
||w||^2 3.8945320552916116
exp ma of ||w||^2 1.4929772129999073
||w|| 1.9734568795115872
exp ma of ||w|| 1.1738137282719536
||w||^2 1.2938962622800234
exp ma of ||w||^2 1.6138060273484394
||w|| 1.137495609784945
exp ma of ||w|| 1.2171393043558751
||w||^2 0.5884420502924538
exp ma of ||w||^2 1.5870791887155822
||w|| 0.7670997655406067
exp ma of ||w|| 1.1606791820024558
cuda
Objective function 17.96 = squared loss an data 14.06 + 0.5*rho*h**2 1.078201 + alpha*h 0.960189 + L2reg 1.72 + L1reg 0.15 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.7754169339320077
iteration 2 in inner loop, alpha 206.77210571525154 rho 100000.0 h 0.004643706924152724
iteration 4 in outer loop, alpha = 671.1427981305239, rho = 100000.0, h = 0.004643706924152724
cuda
2565
cuda
Objective function 20.12 = squared loss an data 14.06 + 0.5*rho*h**2 1.078201 + alpha*h 3.116590 + L2reg 1.72 + L1reg 0.15 ; SHD = 46 ; DAG True
||w||^2 105.24896111034086
exp ma of ||w||^2 53062037254.002106
||w|| 10.259091631832755
exp ma of ||w|| 1975.8230367883382
||w||^2 46.21713366990458
exp ma of ||w||^2 22356852230.07042
||w|| 6.798318444284923
exp ma of ||w|| 837.0880483819675
||w||^2 17.06036034826561
exp ma of ||w||^2 127614710.84911102
||w|| 4.130418907116518
exp ma of ||w|| 8.98314763723506
||w||^2 6.897097646669942
exp ma of ||w||^2 721157.1971495239
||w|| 2.62623259569101
exp ma of ||w|| 2.5972203306557478
||w||^2 1.4117973027618405
exp ma of ||w||^2 1.3667646967889933
||w|| 1.1881907686738862
exp ma of ||w|| 1.1315692159572677
||w||^2 1.9880676016657568
exp ma of ||w||^2 1.5310939624249666
||w|| 1.4099885111821857
exp ma of ||w|| 1.170198176818551
||w||^2 1.1337078161255918
exp ma of ||w||^2 1.6740178953876728
||w|| 1.064757162983932
exp ma of ||w|| 1.2128435062301879
||w||^2 0.47338410134605446
exp ma of ||w||^2 1.6143278485500752
||w|| 0.6880291428028718
exp ma of ||w|| 1.1945166147363013
||w||^2 0.8850182908871036
exp ma of ||w||^2 1.2644400500926682
||w|| 0.9407541075579227
exp ma of ||w|| 1.0935015887134532
||w||^2 1.0150680390814582
exp ma of ||w||^2 1.280567635144099
||w|| 1.0075058506437857
exp ma of ||w|| 1.0996563553037972
||w||^2 1.4814292101560873
exp ma of ||w||^2 1.5977394468642174
||w|| 1.2171397660729384
exp ma of ||w|| 1.2233623707601977
||w||^2 1.3360696511265755
exp ma of ||w||^2 1.7114134151905374
||w|| 1.1558847914591555
exp ma of ||w|| 1.2510215206103612
||w||^2 0.7503577293790972
exp ma of ||w||^2 1.50199600436465
||w|| 0.8662319143157318
exp ma of ||w|| 1.2026430423544914
||w||^2 1.1614102081836113
exp ma of ||w||^2 1.7972665393733906
||w|| 1.0776874352907764
exp ma of ||w|| 1.2818000323695726
cuda
Objective function 18.71 = squared loss an data 14.12 + 0.5*rho*h**2 0.523046 + alpha*h 2.170700 + L2reg 1.75 + L1reg 0.14 ; SHD = 41 ; DAG True
Proportion of microbatches that were clipped  0.7760942760942761
iteration 1 in inner loop, alpha 671.1427981305239 rho 100000.0 h 0.0032343347044729853
iteration 5 in outer loop, alpha = 3905.4775026035095, rho = 1000000.0, h = 0.0032343347044729853
Threshold 0.3
[[0.002 0.047 0.132 0.272 0.431 0.082 0.03  0.003 0.028 1.03  0.228 0.445
  0.008 0.008 0.213]
 [0.098 0.004 0.523 0.129 0.405 0.268 0.049 0.046 0.116 0.647 0.194 0.765
  0.04  0.004 0.344]
 [0.044 0.012 0.002 0.048 0.361 0.024 0.005 0.013 0.028 0.064 0.037 0.111
  0.006 0.006 0.059]
 [0.018 0.033 0.127 0.004 0.296 0.059 0.016 0.012 0.028 0.188 0.233 0.101
  0.016 0.005 0.323]
 [0.005 0.006 0.016 0.015 0.004 0.008 0.004 0.003 0.002 0.003 0.002 0.031
  0.001 0.001 0.012]
 [0.066 0.029 0.258 0.09  0.259 0.005 0.006 0.006 0.025 0.273 0.058 0.475
  0.029 0.004 0.159]
 [0.149 0.119 0.846 0.321 0.501 0.691 0.003 0.032 0.237 0.319 1.136 0.713
  0.044 0.02  0.555]
 [1.127 0.161 0.347 0.602 1.335 0.837 0.2   0.004 0.145 0.659 0.627 0.786
  0.177 0.025 0.658]
 [0.228 0.063 0.142 0.199 1.052 0.247 0.032 0.039 0.003 1.16  0.961 0.703
  0.02  0.002 0.203]
 [0.003 0.01  0.094 0.039 1.754 0.025 0.012 0.005 0.003 0.004 0.164 0.056
  0.004 0.001 0.061]
 [0.017 0.028 0.11  0.015 2.352 0.069 0.003 0.007 0.005 0.038 0.005 0.091
  0.003 0.002 0.142]
 [0.012 0.007 0.051 0.069 0.17  0.015 0.004 0.004 0.005 0.154 0.048 0.004
  0.006 0.002 0.071]
 [0.662 0.123 0.607 0.383 1.395 0.261 0.107 0.04  0.28  0.46  1.46  0.821
  0.004 0.027 0.797]
 [0.605 1.083 0.668 0.65  0.823 0.701 0.194 0.256 2.627 0.861 0.766 0.955
  0.182 0.004 0.803]
 [0.032 0.015 0.094 0.013 0.534 0.037 0.008 0.008 0.016 0.114 0.034 0.072
  0.006 0.005 0.004]]
[[0.    0.    0.    0.    0.431 0.    0.    0.    0.    1.03  0.    0.445
  0.    0.    0.   ]
 [0.    0.    0.523 0.    0.405 0.    0.    0.    0.    0.647 0.    0.765
  0.    0.    0.344]
 [0.    0.    0.    0.    0.361 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.323]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.475
  0.    0.    0.   ]
 [0.    0.    0.846 0.321 0.501 0.691 0.    0.    0.    0.319 1.136 0.713
  0.    0.    0.555]
 [1.127 0.    0.347 0.602 1.335 0.837 0.    0.    0.    0.659 0.627 0.786
  0.    0.    0.658]
 [0.    0.    0.    0.    1.052 0.    0.    0.    0.    1.16  0.961 0.703
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.754 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.352 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.662 0.    0.607 0.383 1.395 0.    0.    0.    0.    0.46  1.46  0.821
  0.    0.    0.797]
 [0.605 1.083 0.668 0.65  0.823 0.701 0.    0.    2.627 0.861 0.766 0.955
  0.    0.    0.803]
 [0.    0.    0.    0.    0.534 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.6111111111111112, 'tpr': 0.7, 'fpr': 0.44, 'f1': 0.4999999999999999, 'shd': 41, 'npred': 54, 'ntrue': 30}
[4.719e-02 1.325e-01 2.721e-01 4.309e-01 8.165e-02 3.044e-02 3.053e-03
 2.798e-02 1.030e+00 2.283e-01 4.447e-01 8.335e-03 8.319e-03 2.126e-01
 9.841e-02 5.232e-01 1.289e-01 4.051e-01 2.678e-01 4.935e-02 4.568e-02
 1.161e-01 6.474e-01 1.943e-01 7.650e-01 3.979e-02 4.415e-03 3.443e-01
 4.437e-02 1.229e-02 4.770e-02 3.613e-01 2.395e-02 5.270e-03 1.258e-02
 2.794e-02 6.370e-02 3.697e-02 1.115e-01 5.848e-03 6.147e-03 5.949e-02
 1.808e-02 3.305e-02 1.267e-01 2.957e-01 5.924e-02 1.636e-02 1.247e-02
 2.801e-02 1.881e-01 2.331e-01 1.009e-01 1.644e-02 5.319e-03 3.229e-01
 4.730e-03 5.994e-03 1.646e-02 1.486e-02 8.300e-03 3.997e-03 2.544e-03
 1.663e-03 2.658e-03 1.741e-03 3.062e-02 9.080e-04 1.348e-03 1.224e-02
 6.608e-02 2.899e-02 2.582e-01 9.013e-02 2.590e-01 6.045e-03 6.070e-03
 2.538e-02 2.727e-01 5.797e-02 4.749e-01 2.854e-02 4.319e-03 1.591e-01
 1.493e-01 1.187e-01 8.464e-01 3.207e-01 5.009e-01 6.905e-01 3.230e-02
 2.375e-01 3.188e-01 1.136e+00 7.128e-01 4.363e-02 1.997e-02 5.553e-01
 1.127e+00 1.614e-01 3.465e-01 6.025e-01 1.335e+00 8.369e-01 1.996e-01
 1.448e-01 6.595e-01 6.273e-01 7.864e-01 1.774e-01 2.463e-02 6.578e-01
 2.278e-01 6.275e-02 1.422e-01 1.995e-01 1.052e+00 2.469e-01 3.245e-02
 3.869e-02 1.160e+00 9.610e-01 7.035e-01 2.030e-02 1.554e-03 2.025e-01
 2.995e-03 9.744e-03 9.433e-02 3.905e-02 1.754e+00 2.510e-02 1.156e-02
 4.721e-03 3.228e-03 1.635e-01 5.553e-02 3.997e-03 1.427e-03 6.123e-02
 1.676e-02 2.807e-02 1.097e-01 1.515e-02 2.352e+00 6.948e-02 3.080e-03
 7.098e-03 4.907e-03 3.813e-02 9.083e-02 3.438e-03 1.501e-03 1.421e-01
 1.201e-02 6.695e-03 5.116e-02 6.941e-02 1.699e-01 1.536e-02 3.860e-03
 4.399e-03 4.886e-03 1.540e-01 4.794e-02 5.550e-03 1.542e-03 7.079e-02
 6.616e-01 1.229e-01 6.072e-01 3.830e-01 1.395e+00 2.612e-01 1.067e-01
 4.016e-02 2.803e-01 4.604e-01 1.460e+00 8.211e-01 2.738e-02 7.971e-01
 6.053e-01 1.083e+00 6.677e-01 6.503e-01 8.235e-01 7.008e-01 1.944e-01
 2.558e-01 2.627e+00 8.611e-01 7.664e-01 9.548e-01 1.819e-01 8.029e-01
 3.240e-02 1.480e-02 9.395e-02 1.315e-02 5.343e-01 3.737e-02 8.049e-03
 7.999e-03 1.561e-02 1.143e-01 3.362e-02 7.203e-02 6.261e-03 4.687e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8522222222222222, 0.6189116636888201)
Iterations 2500
Achieves (7.189593007809515, 1e-05)-DP
