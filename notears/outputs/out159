samples  5000  graph  20 40 ER mlp  minibatch size  100  noise  0.6  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617290166446
iteration 1 in outer loop, alpha = 1.5634617290166446, rho = 1.0, h = 1.5634617290166446
cuda
iteration 1 in inner loop,alpha 1.5634617290166446 rho 1.0 h 1.0381693524821998
iteration 2 in inner loop,alpha 1.5634617290166446 rho 10.0 h 0.44892876550036576
iteration 3 in inner loop,alpha 1.5634617290166446 rho 100.0 h 0.13769561187593382
iteration 2 in outer loop, alpha = 15.333022916610027, rho = 100.0, h = 0.13769561187593382
cuda
iteration 1 in inner loop,alpha 15.333022916610027 rho 100.0 h 0.073779210322094
iteration 2 in inner loop,alpha 15.333022916610027 rho 1000.0 h 0.02383542503082481
iteration 3 in outer loop, alpha = 39.168447947434835, rho = 1000.0, h = 0.02383542503082481
cuda
iteration 1 in inner loop,alpha 39.168447947434835 rho 1000.0 h 0.009489322240060716
iteration 2 in inner loop,alpha 39.168447947434835 rho 10000.0 h 0.0033832793282044804
iteration 4 in outer loop, alpha = 73.00124122947963, rho = 10000.0, h = 0.0033832793282044804
cuda
iteration 1 in inner loop,alpha 73.00124122947963 rho 10000.0 h 0.0014958770173265634
iteration 2 in inner loop,alpha 73.00124122947963 rho 100000.0 h 0.0005532628834927777
iteration 5 in outer loop, alpha = 128.3275295787574, rho = 100000.0, h = 0.0005532628834927777
cuda
iteration 1 in inner loop,alpha 128.3275295787574 rho 100000.0 h 0.00023391632983305044
iteration 6 in outer loop, alpha = 362.2438594118079, rho = 1000000.0, h = 0.00023391632983305044
Threshold 0.3
[[0.    0.011 0.041 2.309 0.049 0.033 0.043 0.004 1.997 0.017 1.838 0.002
  0.103 0.019 0.229 0.946 0.017 0.001 0.036 0.209]
 [0.032 0.005 0.233 0.058 0.409 0.121 0.032 0.003 3.057 0.058 0.017 0.01
  0.602 0.004 1.878 1.58  0.28  0.    0.231 0.338]
 [0.001 0.001 0.003 0.072 0.134 1.088 0.    0.    2.294 0.075 0.058 0.
  2.286 0.001 1.379 0.881 0.    0.    3.752 2.31 ]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.116 1.503 0.    0.    0.    0.001]
 [0.    0.    0.    1.68  0.001 0.189 0.    0.    0.001 0.042 0.021 0.
  0.375 0.003 0.185 0.344 0.001 0.    0.034 0.174]
 [0.    0.001 0.    0.116 0.003 0.001 0.    0.    0.005 0.051 0.066 0.
  0.241 0.    1.907 0.452 0.    0.    0.001 0.158]
 [0.002 0.016 0.615 0.057 0.229 0.115 0.005 0.    0.038 0.009 0.018 0.
  0.035 0.002 0.055 1.637 0.01  0.002 0.065 0.028]
 [0.023 0.014 0.659 0.052 0.191 0.101 4.449 0.001 0.156 0.011 0.024 0.05
  0.017 0.02  0.133 0.231 0.067 0.007 0.064 0.058]
 [0.    0.    0.    0.062 0.994 0.128 0.    0.    0.002 0.056 0.045 0.
  0.276 0.001 0.358 0.544 0.    0.    0.018 0.221]
 [0.    0.    0.    0.094 0.    0.    0.    0.    0.    0.001 0.841 0.
  0.    0.    0.208 0.247 0.    0.    0.    0.181]
 [0.    0.    0.    0.097 0.    0.    0.    0.    0.    0.    0.001 0.
  0.    0.    1.235 0.287 0.    0.    0.    2.341]
 [0.022 0.003 2.761 0.071 0.025 0.201 3.582 0.004 0.219 0.014 0.025 0.001
  0.229 0.016 0.572 0.362 0.011 0.028 0.21  0.183]
 [0.002 0.    0.    0.111 0.    0.001 0.    0.    0.001 3.185 0.124 0.
  0.002 0.    0.276 0.543 0.    0.    0.    0.425]
 [0.014 0.017 0.021 0.051 0.037 0.119 0.019 0.008 0.095 0.025 0.08  0.013
  0.273 0.    0.207 0.185 0.016 0.01  2.629 0.225]
 [0.    0.    0.    0.008 0.    0.    0.    0.    0.001 0.    0.    0.
  0.    0.    0.004 0.296 0.    0.    0.    0.003]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.004 0.008 0.    0.    0.    0.   ]
 [0.008 0.007 0.357 0.064 0.359 0.208 0.03  0.002 0.264 0.056 0.058 0.009
  0.398 0.005 1.926 1.518 0.003 0.    0.329 0.504]
 [0.026 4.257 0.016 0.055 0.212 0.016 0.021 0.014 0.59  0.024 0.021 0.004
  1.38  0.015 0.242 0.246 3.023 0.001 1.417 1.032]
 [0.006 0.001 0.    0.063 0.016 1.201 0.    0.    0.086 0.061 0.018 0.
  1.034 0.    0.414 1.344 0.    0.    0.003 0.471]
 [0.    0.    0.    2.093 0.    0.001 0.    0.    0.001 0.    0.    0.
  0.    0.    0.291 0.347 0.    0.    0.    0.003]]
[[0.    0.    0.    2.309 0.    0.    0.    0.    1.997 0.    1.838 0.
  0.    0.    0.    0.946 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.409 0.    0.    0.    3.057 0.    0.    0.
  0.602 0.    1.878 1.58  0.    0.    0.    0.338]
 [0.    0.    0.    0.    0.    1.088 0.    0.    2.294 0.    0.    0.
  2.286 0.    1.379 0.881 0.    0.    3.752 2.31 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.503 0.    0.    0.    0.   ]
 [0.    0.    0.    1.68  0.    0.    0.    0.    0.    0.    0.    0.
  0.375 0.    0.    0.344 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.907 0.452 0.    0.    0.    0.   ]
 [0.    0.    0.615 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.637 0.    0.    0.    0.   ]
 [0.    0.    0.659 0.    0.    0.    4.449 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.994 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.358 0.544 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.841 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.235 0.    0.    0.    0.    2.341]
 [0.    0.    2.761 0.    0.    0.    3.582 0.    0.    0.    0.    0.
  0.    0.    0.572 0.362 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.185 0.    0.
  0.    0.    0.    0.543 0.    0.    0.    0.425]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.629 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.357 0.    0.359 0.    0.    0.    0.    0.    0.    0.
  0.398 0.    1.926 1.518 0.    0.    0.329 0.504]
 [0.    4.257 0.    0.    0.    0.    0.    0.    0.59  0.    0.    0.
  1.38  0.    0.    0.    3.023 0.    1.417 1.032]
 [0.    0.    0.    0.    0.    1.201 0.    0.    0.    0.    0.    0.
  1.034 0.    0.414 1.344 0.    0.    0.    0.471]
 [0.    0.    0.    2.093 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.347 0.    0.    0.    0.   ]]
{'fdr': 0.3442622950819672, 'tpr': 1.0, 'fpr': 0.14, 'f1': 0.7920792079207921, 'shd': 21, 'npred': 61, 'ntrue': 40}
[1.083e-02 4.113e-02 2.309e+00 4.854e-02 3.310e-02 4.267e-02 4.093e-03
 1.997e+00 1.668e-02 1.838e+00 1.696e-03 1.026e-01 1.854e-02 2.292e-01
 9.462e-01 1.702e-02 1.418e-03 3.569e-02 2.089e-01 3.153e-02 2.332e-01
 5.776e-02 4.093e-01 1.213e-01 3.235e-02 3.126e-03 3.057e+00 5.796e-02
 1.710e-02 9.784e-03 6.016e-01 3.964e-03 1.878e+00 1.580e+00 2.800e-01
 3.837e-04 2.313e-01 3.382e-01 1.064e-03 5.875e-04 7.196e-02 1.340e-01
 1.088e+00 2.585e-04 7.017e-05 2.294e+00 7.453e-02 5.834e-02 2.484e-04
 2.286e+00 9.420e-04 1.379e+00 8.806e-01 3.115e-04 5.956e-05 3.752e+00
 2.310e+00 6.774e-05 1.068e-04 1.767e-04 5.568e-04 1.158e-03 1.635e-04
 4.241e-05 4.577e-04 2.515e-05 5.612e-05 8.438e-05 6.315e-05 4.336e-04
 1.164e-01 1.503e+00 2.305e-04 8.719e-05 2.519e-04 9.346e-04 5.757e-05
 8.142e-05 1.361e-04 1.680e+00 1.894e-01 4.821e-04 1.553e-05 6.437e-04
 4.198e-02 2.066e-02 6.139e-05 3.753e-01 2.544e-03 1.847e-01 3.441e-01
 5.454e-04 3.163e-05 3.367e-02 1.740e-01 2.530e-04 5.061e-04 9.626e-05
 1.162e-01 2.833e-03 2.013e-04 1.489e-05 5.317e-03 5.062e-02 6.561e-02
 3.344e-05 2.412e-01 2.155e-05 1.907e+00 4.519e-01 1.640e-04 1.580e-05
 6.975e-04 1.578e-01 1.950e-03 1.622e-02 6.154e-01 5.730e-02 2.292e-01
 1.146e-01 1.852e-04 3.808e-02 8.909e-03 1.824e-02 2.614e-05 3.544e-02
 2.297e-03 5.469e-02 1.637e+00 9.615e-03 1.847e-03 6.498e-02 2.779e-02
 2.321e-02 1.404e-02 6.594e-01 5.202e-02 1.907e-01 1.011e-01 4.449e+00
 1.555e-01 1.080e-02 2.358e-02 4.975e-02 1.745e-02 1.969e-02 1.325e-01
 2.309e-01 6.694e-02 7.062e-03 6.393e-02 5.750e-02 5.504e-05 2.563e-04
 3.570e-04 6.185e-02 9.940e-01 1.282e-01 4.752e-04 1.786e-05 5.605e-02
 4.509e-02 8.174e-05 2.759e-01 7.339e-04 3.580e-01 5.445e-01 3.774e-04
 3.864e-05 1.793e-02 2.212e-01 1.231e-04 1.953e-04 4.488e-05 9.403e-02
 1.584e-04 3.098e-04 2.647e-05 8.857e-06 6.916e-05 8.410e-01 1.944e-05
 1.526e-04 5.486e-06 2.083e-01 2.471e-01 6.506e-05 1.239e-05 3.442e-05
 1.811e-01 7.412e-05 2.319e-04 2.989e-05 9.662e-02 3.782e-04 1.971e-04
 2.575e-04 6.005e-06 9.804e-05 1.500e-04 6.471e-06 2.791e-05 1.706e-05
 1.235e+00 2.866e-01 2.588e-04 3.714e-05 5.780e-05 2.341e+00 2.174e-02
 3.007e-03 2.761e+00 7.092e-02 2.516e-02 2.008e-01 3.582e+00 4.404e-03
 2.194e-01 1.403e-02 2.511e-02 2.292e-01 1.551e-02 5.719e-01 3.623e-01
 1.085e-02 2.840e-02 2.099e-01 1.828e-01 1.586e-03 2.627e-04 1.454e-04
 1.114e-01 3.707e-04 1.341e-03 4.154e-05 1.323e-05 5.727e-04 3.185e+00
 1.241e-01 4.530e-05 1.144e-04 2.764e-01 5.434e-01 2.609e-04 3.908e-05
 2.087e-04 4.247e-01 1.389e-02 1.727e-02 2.055e-02 5.119e-02 3.743e-02
 1.187e-01 1.922e-02 8.201e-03 9.545e-02 2.544e-02 8.038e-02 1.323e-02
 2.733e-01 2.069e-01 1.850e-01 1.572e-02 9.519e-03 2.629e+00 2.248e-01
 1.263e-05 3.485e-04 9.140e-05 7.834e-03 4.365e-04 1.272e-04 1.049e-04
 1.888e-05 1.242e-03 1.275e-04 1.801e-04 3.163e-05 8.661e-05 1.002e-05
 2.957e-01 3.325e-04 7.170e-05 4.706e-05 2.502e-03 2.572e-05 1.612e-04
 4.349e-05 1.080e-04 2.575e-05 1.911e-04 3.544e-04 3.533e-05 4.932e-05
 1.038e-04 4.966e-05 1.274e-05 4.273e-05 1.730e-05 4.373e-03 5.220e-05
 2.942e-05 4.810e-05 1.099e-04 8.293e-03 6.514e-03 3.573e-01 6.350e-02
 3.589e-01 2.084e-01 3.007e-02 1.956e-03 2.639e-01 5.575e-02 5.816e-02
 8.985e-03 3.985e-01 4.860e-03 1.926e+00 1.518e+00 4.302e-04 3.292e-01
 5.038e-01 2.586e-02 4.257e+00 1.595e-02 5.541e-02 2.116e-01 1.585e-02
 2.141e-02 1.423e-02 5.896e-01 2.427e-02 2.075e-02 3.851e-03 1.380e+00
 1.465e-02 2.423e-01 2.458e-01 3.023e+00 1.417e+00 1.032e+00 5.642e-03
 1.145e-03 3.959e-04 6.315e-02 1.569e-02 1.201e+00 1.168e-04 1.258e-05
 8.594e-02 6.072e-02 1.824e-02 8.952e-05 1.034e+00 1.482e-05 4.138e-01
 1.344e+00 1.563e-04 1.052e-04 4.712e-01 5.036e-05 3.880e-05 1.608e-04
 2.093e+00 2.231e-04 1.159e-03 1.185e-04 2.814e-05 8.346e-04 1.626e-05
 4.600e-05 6.238e-05 2.608e-05 4.142e-05 2.907e-01 3.470e-01 9.654e-05
 8.828e-05 1.127e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9991176470588236, 0.9923514793378535)
cuda
4420
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
||w||^2 0.22569760462459063
exp ma of ||w||^2 57.248556157236685
||w|| 0.47507641977327253
exp ma of ||w|| 0.5425008037108358
||w||^2 0.2404266168824832
exp ma of ||w||^2 0.2490499686798974
||w|| 0.49033316926604426
exp ma of ||w|| 0.4897401656683734
||w||^2 0.1262385484814748
exp ma of ||w||^2 0.24048748099106163
||w|| 0.355300645202728
exp ma of ||w|| 0.48161085282533245
||w||^2 0.18689508609709257
exp ma of ||w||^2 0.19989248824304767
||w|| 0.43231364320027255
exp ma of ||w|| 0.43811195546319975
||w||^2 0.06311849458150441
exp ma of ||w||^2 0.20112642193300903
||w|| 0.25123394392777504
exp ma of ||w|| 0.4379530292347752
cuda
Objective function 162.94 = squared loss an data 161.02 + 0.5*rho*h**2 1.305659 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.25 ; SHD = 95 ; DAG False
Proportion of microbatches that were clipped  0.727243793761935
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.6159573161808538
iteration 1 in outer loop, alpha = 1.6159573161808538, rho = 1.0, h = 1.6159573161808538
cuda
4420
cuda
Objective function 165.56 = squared loss an data 161.02 + 0.5*rho*h**2 1.305659 + alpha*h 2.611318 + L2reg 0.37 + L1reg 0.25 ; SHD = 95 ; DAG False
||w||^2 12.751879632837326
exp ma of ||w||^2 5111.389249336729
||w|| 3.5709774058144537
exp ma of ||w|| 19.85183993298684
||w||^2 7.330000040146195
exp ma of ||w||^2 807.7320065212846
||w|| 2.707397281550344
exp ma of ||w|| 5.802464357122251
||w||^2 0.3462704900898182
exp ma of ||w||^2 0.36825927683284887
||w|| 0.5884475253493876
exp ma of ||w|| 0.5710474526126645
||w||^2 0.7645328170249168
exp ma of ||w||^2 0.5033741381393368
||w|| 0.8743756727087716
exp ma of ||w|| 0.6645893964199509
||w||^2 0.7443994989283925
exp ma of ||w||^2 0.6747703382020211
||w|| 0.862785894024927
exp ma of ||w|| 0.7745740794171736
cuda
Objective function 69.05 = squared loss an data 62.85 + 0.5*rho*h**2 1.986540 + alpha*h 3.221021 + L2reg 0.75 + L1reg 0.24 ; SHD = 73 ; DAG False
Proportion of microbatches that were clipped  0.7450535192993837
iteration 1 in inner loop, alpha 1.6159573161808538 rho 1.0 h 1.9932584239985118
4420
cuda
Objective function 86.93 = squared loss an data 62.85 + 0.5*rho*h**2 19.865396 + alpha*h 3.221021 + L2reg 0.75 + L1reg 0.24 ; SHD = 73 ; DAG False
||w||^2 3744414.4441958675
exp ma of ||w||^2 43793925.3647788
||w|| 1935.0489513694138
exp ma of ||w|| 4369.1718337025295
||w||^2 0.5760459344840428
exp ma of ||w||^2 1.6071064313289607
||w|| 0.7589768998355898
exp ma of ||w|| 1.0597123650082694
||w||^2 0.4989943932860182
exp ma of ||w||^2 1.2398627221127188
||w|| 0.706395351970848
exp ma of ||w|| 1.0777909808272368
||w||^2 0.7244847977858293
exp ma of ||w||^2 1.382443979547168
||w|| 0.8511667273723929
exp ma of ||w|| 1.1344464026715895
||w||^2 1.3308990071058968
exp ma of ||w||^2 1.3404619633879997
||w|| 1.1536459626358067
exp ma of ||w|| 1.1009798262488566
cuda
Objective function 43.85 = squared loss an data 36.63 + 0.5*rho*h**2 4.481958 + alpha*h 1.529955 + L2reg 0.99 + L1reg 0.21 ; SHD = 51 ; DAG False
Proportion of microbatches that were clipped  0.7689649756557249
iteration 2 in inner loop, alpha 1.6159573161808538 rho 10.0 h 0.9467795452374261
4420
cuda
Objective function 84.18 = squared loss an data 36.63 + 0.5*rho*h**2 44.819575 + alpha*h 1.529955 + L2reg 0.99 + L1reg 0.21 ; SHD = 51 ; DAG False
||w||^2 11738844580.345518
exp ma of ||w||^2 29946862694.398277
||w|| 108345.94861066803
exp ma of ||w|| 154140.3188451872
||w||^2 1.8533670859282425
exp ma of ||w||^2 2.4974153785724407
||w|| 1.3613842535919984
exp ma of ||w|| 1.5173119305702967
||w||^2 1.752504238818847
exp ma of ||w||^2 2.698640223954876
||w|| 1.3238218304661873
exp ma of ||w|| 1.5719342297472279
||w||^2 1.383917465698322
exp ma of ||w||^2 2.111538863431922
||w|| 1.1764002149346633
exp ma of ||w|| 1.4010746822209175
cuda
Objective function 41.49 = squared loss an data 35.33 + 0.5*rho*h**2 4.321875 + alpha*h 0.475096 + L2reg 1.18 + L1reg 0.18 ; SHD = 43 ; DAG False
Proportion of microbatches that were clipped  0.778628339321981
iteration 3 in inner loop, alpha 1.6159573161808538 rho 100.0 h 0.2940025375922062
iteration 2 in outer loop, alpha = 31.016211075401472, rho = 100.0, h = 0.2940025375922062
cuda
4420
cuda
Objective function 50.13 = squared loss an data 35.33 + 0.5*rho*h**2 4.321875 + alpha*h 9.118845 + L2reg 1.18 + L1reg 0.18 ; SHD = 43 ; DAG False
||w||^2 20764805888.731674
exp ma of ||w||^2 41079158761.77911
||w|| 144099.98573466853
exp ma of ||w|| 175151.8564083907
||w||^2 38328579199.166695
exp ma of ||w||^2 39449191173.30608
||w|| 195776.8607347832
exp ma of ||w|| 172079.95486693425
||w||^2 223932353.69797477
exp ma of ||w||^2 3221705624.2164054
||w|| 14964.369472115248
exp ma of ||w|| 46137.916679957176
||w||^2 2596.215918305188
exp ma of ||w||^2 308769.6008006004
||w|| 50.95307565108497
exp ma of ||w|| 173.15511331618745
||w||^2 147.66976918913204
exp ma of ||w||^2 24728.739295200274
||w|| 12.151945078428064
exp ma of ||w|| 30.379107463150007
||w||^2 3.2354959237069503
exp ma of ||w||^2 44.55052529066963
||w|| 1.7987484325794283
exp ma of ||w|| 2.168795271024958
||w||^2 3.9679099916124376
exp ma of ||w||^2 9.388553136579116
||w|| 1.9919613429011211
exp ma of ||w|| 1.8141763730980482
||w||^2 3.631436266941001
exp ma of ||w||^2 2.5336689784435764
||w|| 1.9056327733697804
exp ma of ||w|| 1.5300996793090673
||w||^2 0.9648491277397745
exp ma of ||w||^2 2.4428512258191164
||w|| 0.9822673402591448
exp ma of ||w|| 1.5090959675810722
||w||^2 1.971228934036225
exp ma of ||w||^2 2.320140170899894
||w|| 1.404004606130701
exp ma of ||w|| 1.4782050352517881
cuda
Objective function 42.01 = squared loss an data 32.98 + 0.5*rho*h**2 1.725860 + alpha*h 5.762441 + L2reg 1.36 + L1reg 0.18 ; SHD = 36 ; DAG False
Proportion of microbatches that were clipped  0.7750938469071323
iteration 1 in inner loop, alpha 31.016211075401472 rho 100.0 h 0.18578805316286306
4420
cuda
Objective function 57.54 = squared loss an data 32.98 + 0.5*rho*h**2 17.258600 + alpha*h 5.762441 + L2reg 1.36 + L1reg 0.18 ; SHD = 36 ; DAG False
||w||^2 3083634670.2081156
exp ma of ||w||^2 9609026013.93615
||w|| 55530.484152473546
exp ma of ||w|| 81349.05071239884
||w||^2 1182.601450830471
exp ma of ||w||^2 431948.01705162646
||w|| 34.388972808597686
exp ma of ||w|| 184.3140522295483
||w||^2 5.903636547357661
exp ma of ||w||^2 53.62819734690614
||w|| 2.4297400164127976
exp ma of ||w|| 2.461232152850432
cuda
Objective function 39.33 = squared loss an data 32.72 + 0.5*rho*h**2 2.678969 + alpha*h 2.270322 + L2reg 1.49 + L1reg 0.17 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.7827195919018014
iteration 2 in inner loop, alpha 31.016211075401472 rho 1000.0 h 0.0731979304571766
iteration 3 in outer loop, alpha = 104.21414153257807, rho = 1000.0, h = 0.0731979304571766
cuda
4420
cuda
Objective function 44.69 = squared loss an data 32.72 + 0.5*rho*h**2 2.678969 + alpha*h 7.628259 + L2reg 1.49 + L1reg 0.17 ; SHD = 40 ; DAG True
||w||^2 28463.972178434808
exp ma of ||w||^2 2467115.2157788007
||w|| 168.7126912192287
exp ma of ||w|| 635.4953990610138
||w||^2 2.083725991801415
exp ma of ||w||^2 2.676298317092517
||w|| 1.443511687448846
exp ma of ||w|| 1.5871519648016763
||w||^2 4.552422422561565
exp ma of ||w||^2 2.562524999839346
||w|| 2.1336406498193563
exp ma of ||w|| 1.5570676902439384
||w||^2 2.6828042914015806
exp ma of ||w||^2 2.512177862106586
||w|| 1.6379268272427743
exp ma of ||w|| 1.5433895162072082
||w||^2 3.8889230362301355
exp ma of ||w||^2 2.5748241705575445
||w|| 1.9720352522787556
exp ma of ||w|| 1.544052686368644
cuda
Objective function 38.93 = squared loss an data 31.09 + 0.5*rho*h**2 1.117346 + alpha*h 4.926466 + L2reg 1.63 + L1reg 0.17 ; SHD = 42 ; DAG True
Proportion of microbatches that were clipped  0.7794283868884224
iteration 1 in inner loop, alpha 104.21414153257807 rho 1000.0 h 0.047272528811610925
4420
cuda
Objective function 48.99 = squared loss an data 31.09 + 0.5*rho*h**2 11.173460 + alpha*h 4.926466 + L2reg 1.63 + L1reg 0.17 ; SHD = 42 ; DAG True
||w||^2 1646880224212.5835
exp ma of ||w||^2 326162631119.7349
||w|| 1283308.312219859
exp ma of ||w|| 281605.4106594481
||w||^2 30001641544.051933
exp ma of ||w||^2 161620427599.5436
||w|| 173209.81942156726
exp ma of ||w|| 325131.7685473647
||w||^2 1.39861294192937
exp ma of ||w||^2 3.2077537784800847
||w|| 1.1826296723528333
exp ma of ||w|| 1.745999297717048
||w||^2 1.8144011157733781
exp ma of ||w||^2 2.9406339254367846
||w|| 1.346997073409359
exp ma of ||w|| 1.6782958619775274
||w||^2 6.298128038444399
exp ma of ||w||^2 2.7867995376533945
||w|| 2.5096071482294593
exp ma of ||w|| 1.6331718244850226
cuda
Objective function 36.38 = squared loss an data 30.35 + 0.5*rho*h**2 2.027220 + alpha*h 2.098418 + L2reg 1.74 + L1reg 0.17 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.7804489730934565
iteration 2 in inner loop, alpha 104.21414153257807 rho 10000.0 h 0.020135638988300286
4420
cuda
Objective function 54.63 = squared loss an data 30.35 + 0.5*rho*h**2 20.272198 + alpha*h 2.098418 + L2reg 1.74 + L1reg 0.17 ; SHD = 43 ; DAG True
||w||^2 3488456809149.801
exp ma of ||w||^2 588648440803.5948
||w|| 1867741.0979977392
exp ma of ||w|| 311273.0691704273
||w||^2 8997080112.433716
exp ma of ||w||^2 227197827660.00525
||w|| 94852.93939796339
exp ma of ||w|| 125077.0174444406
v before min max tensor([[ 9.876e-04, -3.142e-06, -3.046e-04,  ..., -9.514e-06,  9.775e-04,
         -2.016e-05],
        [ 3.747e-03, -7.641e-05, -6.443e-05,  ...,  4.950e-06, -6.288e-06,
          2.681e-04],
        [ 1.815e-02, -3.670e-04, -2.247e-06,  ...,  8.377e-06,  1.302e-04,
         -8.884e-06],
        ...,
        [ 1.599e-05, -1.471e-04, -1.246e-03,  ...,  3.289e-04, -8.138e-05,
         -1.491e-03],
        [-2.926e-05, -2.732e-04, -1.111e-04,  ...,  4.927e-06,  4.917e-03,
          1.364e-03],
        [ 1.682e-04, -1.078e-04, -9.604e-05,  ..., -5.690e-04, -1.330e-03,
          1.111e-02]], device='cuda:0')
v tensor([[9.876e-04, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 9.775e-04,
         1.000e-12],
        [3.747e-03, 1.000e-12, 1.000e-12,  ..., 4.950e-06, 1.000e-12,
         2.681e-04],
        [1.815e-02, 1.000e-12, 1.000e-12,  ..., 8.377e-06, 1.302e-04,
         1.000e-12],
        ...,
        [1.599e-05, 1.000e-12, 1.000e-12,  ..., 3.289e-04, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 4.927e-06, 4.917e-03,
         1.364e-03],
        [1.682e-04, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.111e-02]], device='cuda:0')
v before min max tensor([-5.703e-07, -1.441e-06, -3.119e-05, -1.306e-04, -1.279e-05, -1.809e-06,
        -2.111e-05,  1.581e-05,  1.192e-04,  1.756e-03, -3.430e-06, -1.753e-06,
         3.688e-05, -1.259e-04, -1.082e-04, -1.912e-05, -4.205e-05, -1.288e-04,
         6.733e-05,  5.896e-08, -9.672e-04, -6.015e-05,  3.293e-04,  2.651e-05,
         1.460e-06, -4.225e-06, -1.141e-04, -3.251e-05, -1.841e-04, -2.446e-05,
        -2.890e-04,  2.461e-07,  3.538e-06, -2.026e-05,  8.677e-06,  2.123e-05,
        -1.389e-05, -6.152e-06, -1.119e-05,  1.785e-06,  3.326e-05, -1.706e-03,
        -5.373e-05, -1.739e-05, -1.080e-04,  1.180e-04, -1.858e-06,  4.552e-07,
        -3.615e-05, -3.641e-05, -6.216e-05, -1.420e-04, -5.251e-07, -4.556e-07,
        -3.558e-05, -1.912e-05,  1.291e-04, -1.162e-07, -6.976e-05, -4.263e-04,
         5.533e-05, -1.579e-04,  2.825e-04, -4.430e-05, -1.349e-05, -1.130e-04,
        -1.124e-04,  1.929e-04,  4.705e-06,  4.007e-04, -4.672e-07, -9.820e-06,
         1.279e-04,  6.041e-05,  8.612e-04, -4.598e-04, -7.910e-07, -1.912e-05,
        -7.493e-05, -1.341e-05, -1.118e-03,  1.248e-04, -2.865e-06, -1.914e-05,
        -2.364e-05, -1.141e-04, -1.323e-04, -5.838e-07,  1.766e-04, -1.784e-05,
         6.272e-07,  3.530e-05, -4.246e-05, -2.003e-06, -4.276e-05,  1.052e-03,
         9.073e-05, -8.384e-07, -3.982e-05, -1.731e-05, -6.231e-05, -6.690e-06,
        -3.952e-05,  4.125e-04, -2.022e-06, -1.551e-05, -7.791e-05,  8.531e-05,
        -3.445e-04,  4.365e-07,  5.592e-05,  1.884e-04, -4.177e-06,  5.346e-06,
        -8.660e-05, -1.651e-06, -1.495e-06, -2.374e-06, -3.325e-05,  4.874e-04,
         4.294e-04, -1.296e-04, -2.714e-04, -5.957e-06, -1.084e-06, -3.820e-05,
        -2.286e-04, -7.466e-05, -1.928e-03,  5.176e-05, -4.323e-04, -2.480e-05,
        -2.262e-05,  2.137e-05,  5.735e-05, -2.444e-06, -2.552e-06, -1.707e-04,
        -5.285e-05, -3.520e-05,  1.540e-05, -7.102e-06, -1.581e-04,  2.634e-07,
        -3.273e-04, -2.721e-06, -2.710e-05, -2.353e-04, -8.806e-05, -7.858e-05,
        -1.771e-03, -9.880e-05, -2.538e-05, -2.784e-06, -2.148e-05, -2.327e-05,
         1.427e-03,  4.938e-05, -2.484e-05,  9.601e-05, -2.341e-05,  1.905e-05,
        -7.126e-06, -1.852e-04, -5.344e-05, -4.301e-06, -4.188e-04,  3.064e-04,
        -1.107e-04,  1.560e-06, -1.052e-06,  8.387e-05, -3.818e-04, -5.269e-05,
        -1.067e-04, -7.537e-07, -1.934e-04, -1.411e-04, -5.361e-06, -4.676e-05,
        -4.742e-05,  2.984e-05,  1.413e-04,  4.611e-06, -1.396e-06, -1.583e-05,
        -8.453e-05,  2.036e-03, -4.761e-05, -8.266e-06,  1.340e-03, -1.426e-04,
         6.691e-04,  2.076e-05,  4.397e-04,  1.037e-04, -3.472e-06,  2.240e-04,
        -1.688e-04, -4.305e-04], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.581e-05, 1.192e-04, 1.756e-03, 1.000e-12, 1.000e-12,
        3.688e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        6.733e-05, 5.896e-08, 1.000e-12, 1.000e-12, 3.293e-04, 2.651e-05,
        1.460e-06, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.461e-07, 3.538e-06, 1.000e-12, 8.677e-06, 2.123e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.785e-06, 3.326e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.180e-04, 1.000e-12, 4.552e-07,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.291e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        5.533e-05, 1.000e-12, 2.825e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.929e-04, 4.705e-06, 4.007e-04, 1.000e-12, 1.000e-12,
        1.279e-04, 6.041e-05, 8.612e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.248e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.766e-04, 1.000e-12,
        6.272e-07, 3.530e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.052e-03,
        9.073e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 4.125e-04, 1.000e-12, 1.000e-12, 1.000e-12, 8.531e-05,
        1.000e-12, 4.365e-07, 5.592e-05, 1.884e-04, 1.000e-12, 5.346e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.874e-04,
        4.294e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 5.176e-05, 1.000e-12, 1.000e-12,
        1.000e-12, 2.137e-05, 5.735e-05, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.540e-05, 1.000e-12, 1.000e-12, 2.634e-07,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.427e-03, 4.938e-05, 1.000e-12, 9.601e-05, 1.000e-12, 1.905e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.064e-04,
        1.000e-12, 1.560e-06, 1.000e-12, 8.387e-05, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.984e-05, 1.413e-04, 4.611e-06, 1.000e-12, 1.000e-12,
        1.000e-12, 2.036e-03, 1.000e-12, 1.000e-12, 1.340e-03, 1.000e-12,
        6.691e-04, 2.076e-05, 4.397e-04, 1.037e-04, 1.000e-12, 2.240e-04,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 4.066e-05],
         [ 6.958e-04],
         [ 1.053e-06],
         [-1.103e-04],
         [-2.055e-05],
         [ 5.683e-07],
         [ 6.335e-06],
         [ 1.704e-05],
         [-2.216e-04],
         [ 2.042e-06]],

        [[-2.667e-06],
         [-1.678e-05],
         [-1.332e-05],
         [-4.585e-07],
         [-7.130e-07],
         [-2.492e-04],
         [-9.908e-06],
         [-2.259e-06],
         [-3.471e-06],
         [-1.816e-05]],

        [[-4.643e-04],
         [ 1.718e-05],
         [-1.784e-05],
         [-2.039e-05],
         [-1.978e-04],
         [ 5.772e-05],
         [ 2.211e-04],
         [ 1.849e-04],
         [-5.081e-05],
         [-1.062e-04]],

        [[-3.893e-05],
         [ 2.140e-05],
         [ 1.209e-03],
         [-1.851e-04],
         [-2.208e-05],
         [-4.070e-05],
         [ 1.077e-05],
         [ 5.063e-06],
         [-2.110e-05],
         [ 3.358e-05]],

        [[-6.444e-05],
         [ 1.251e-03],
         [ 2.825e-05],
         [-6.102e-06],
         [-1.685e-05],
         [-1.604e-05],
         [-3.752e-05],
         [-6.636e-06],
         [ 1.034e-04],
         [-4.264e-05]],

        [[-3.966e-04],
         [ 1.231e-03],
         [-2.559e-06],
         [-1.013e-05],
         [-1.479e-04],
         [-9.035e-05],
         [ 6.004e-06],
         [ 6.958e-07],
         [-1.929e-05],
         [ 1.030e-04]],

        [[-1.391e-04],
         [ 5.507e-04],
         [ 1.305e-06],
         [-1.318e-06],
         [-2.072e-04],
         [-6.944e-05],
         [-4.043e-06],
         [-7.363e-05],
         [-1.725e-04],
         [-4.075e-05]],

        [[-8.743e-05],
         [-4.234e-04],
         [ 2.710e-04],
         [-6.158e-05],
         [ 7.292e-06],
         [ 4.492e-04],
         [-1.267e-03],
         [-2.736e-07],
         [-9.624e-06],
         [-1.275e-05]],

        [[-6.888e-06],
         [-9.450e-06],
         [-4.926e-05],
         [-9.984e-06],
         [-2.555e-06],
         [-1.014e-06],
         [ 2.152e-06],
         [-4.010e-05],
         [-7.820e-06],
         [ 1.915e-04]],

        [[-4.730e-05],
         [-1.853e-04],
         [-1.793e-06],
         [-1.060e-05],
         [-6.890e-05],
         [-2.992e-04],
         [ 6.177e-04],
         [-1.199e-04],
         [-5.947e-05],
         [-2.319e-04]],

        [[ 4.252e-05],
         [-6.787e-08],
         [ 9.430e-06],
         [-5.977e-05],
         [-1.961e-04],
         [-7.061e-04],
         [-3.019e-06],
         [-4.826e-06],
         [ 2.057e-04],
         [-5.057e-05]],

        [[-2.792e-04],
         [-5.444e-05],
         [-3.447e-04],
         [ 2.323e-05],
         [-1.005e-04],
         [-8.413e-04],
         [-2.323e-04],
         [ 6.674e-06],
         [-5.178e-04],
         [-2.117e-05]],

        [[ 9.427e-06],
         [-3.110e-04],
         [-1.816e-06],
         [ 8.363e-04],
         [ 4.463e-05],
         [-3.289e-06],
         [-1.278e-05],
         [ 5.279e-07],
         [ 5.677e-05],
         [-8.110e-05]],

        [[-2.466e-06],
         [-1.410e-04],
         [ 4.624e-05],
         [-1.462e-04],
         [-1.848e-03],
         [ 3.133e-04],
         [ 7.962e-06],
         [-1.599e-05],
         [-8.668e-07],
         [-9.832e-05]],

        [[-9.424e-05],
         [ 2.655e-05],
         [-5.329e-05],
         [-3.124e-05],
         [ 5.154e-05],
         [-3.527e-05],
         [ 1.293e-04],
         [ 1.041e-04],
         [ 7.922e-05],
         [ 1.796e-04]],

        [[-5.935e-05],
         [-9.040e-05],
         [ 1.049e-04],
         [ 1.050e-04],
         [ 6.305e-05],
         [-3.528e-06],
         [-3.457e-04],
         [ 1.331e-05],
         [-3.073e-04],
         [-3.786e-05]],

        [[ 5.161e-06],
         [ 1.777e-05],
         [-3.229e-04],
         [-5.604e-05],
         [-1.628e-05],
         [-1.609e-07],
         [-2.614e-04],
         [ 1.128e-04],
         [ 3.578e-05],
         [ 7.626e-04]],

        [[ 3.562e-05],
         [ 2.102e-04],
         [-1.399e-05],
         [-1.950e-05],
         [ 5.388e-04],
         [-2.593e-04],
         [ 6.090e-05],
         [-7.623e-05],
         [ 1.788e-07],
         [-1.242e-04]],

        [[-1.524e-04],
         [-6.699e-04],
         [-2.868e-04],
         [-8.798e-05],
         [-3.271e-06],
         [ 6.610e-07],
         [-3.171e-04],
         [ 1.845e-05],
         [ 5.327e-04],
         [ 8.712e-06]],

        [[-3.777e-04],
         [-5.320e-05],
         [-3.388e-05],
         [-1.756e-05],
         [-2.129e-05],
         [ 2.534e-06],
         [ 1.330e-04],
         [-2.127e-04],
         [-3.934e-04],
         [ 1.079e-06]]], device='cuda:0')
v tensor([[[4.066e-05],
         [6.958e-04],
         [1.053e-06],
         [1.000e-12],
         [1.000e-12],
         [5.683e-07],
         [6.335e-06],
         [1.704e-05],
         [1.000e-12],
         [2.042e-06]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.718e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.772e-05],
         [2.211e-04],
         [1.849e-04],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [2.140e-05],
         [1.209e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.077e-05],
         [5.063e-06],
         [1.000e-12],
         [3.358e-05]],

        [[1.000e-12],
         [1.251e-03],
         [2.825e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.034e-04],
         [1.000e-12]],

        [[1.000e-12],
         [1.231e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.004e-06],
         [6.958e-07],
         [1.000e-12],
         [1.030e-04]],

        [[1.000e-12],
         [5.507e-04],
         [1.305e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [2.710e-04],
         [1.000e-12],
         [7.292e-06],
         [4.492e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.152e-06],
         [1.000e-12],
         [1.000e-12],
         [1.915e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.177e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[4.252e-05],
         [1.000e-12],
         [9.430e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.057e-04],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.323e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.674e-06],
         [1.000e-12],
         [1.000e-12]],

        [[9.427e-06],
         [1.000e-12],
         [1.000e-12],
         [8.363e-04],
         [4.463e-05],
         [1.000e-12],
         [1.000e-12],
         [5.279e-07],
         [5.677e-05],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [4.624e-05],
         [1.000e-12],
         [1.000e-12],
         [3.133e-04],
         [7.962e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [2.655e-05],
         [1.000e-12],
         [1.000e-12],
         [5.154e-05],
         [1.000e-12],
         [1.293e-04],
         [1.041e-04],
         [7.922e-05],
         [1.796e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.049e-04],
         [1.050e-04],
         [6.305e-05],
         [1.000e-12],
         [1.000e-12],
         [1.331e-05],
         [1.000e-12],
         [1.000e-12]],

        [[5.161e-06],
         [1.777e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.128e-04],
         [3.578e-05],
         [7.626e-04]],

        [[3.562e-05],
         [2.102e-04],
         [1.000e-12],
         [1.000e-12],
         [5.388e-04],
         [1.000e-12],
         [6.090e-05],
         [1.000e-12],
         [1.788e-07],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.610e-07],
         [1.000e-12],
         [1.845e-05],
         [5.327e-04],
         [8.712e-06]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.534e-06],
         [1.330e-04],
         [1.000e-12],
         [1.000e-12],
         [1.079e-06]]], device='cuda:0')
v before min max tensor([[-1.966e-05],
        [-8.564e-06],
        [-1.077e-05],
        [ 6.943e-05],
        [-3.087e-05],
        [-5.280e-05],
        [-1.074e-04],
        [-8.893e-06],
        [ 3.271e-06],
        [-1.077e-04],
        [ 3.213e-04],
        [-7.271e-04],
        [-7.304e-06],
        [-6.497e-04],
        [-2.240e-04],
        [-5.563e-05],
        [ 1.188e-05],
        [-1.561e-06],
        [-1.357e-03],
        [ 3.113e-05]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [6.943e-05],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [3.271e-06],
        [1.000e-12],
        [3.213e-04],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.188e-05],
        [1.000e-12],
        [1.000e-12],
        [3.113e-05]], device='cuda:0')
a after update for 1 param tensor([[ 9.924e-06, -4.716e-07, -1.489e-05,  ...,  1.366e-06, -2.915e-05,
          1.706e-06],
        [ 5.791e-05,  3.620e-06,  4.101e-06,  ...,  2.729e-06,  1.678e-07,
         -2.263e-05],
        [-1.024e-04, -1.574e-05,  1.101e-07,  ..., -4.850e-06,  1.290e-05,
          1.456e-06],
        ...,
        [-1.216e-05, -1.540e-05, -3.465e-05,  ...,  2.961e-05, -5.199e-05,
         -3.142e-05],
        [ 7.202e-06, -2.456e-06,  2.308e-06,  ..., -3.035e-06,  5.482e-05,
          6.918e-05],
        [-2.120e-05,  1.840e-05, -1.082e-05,  ..., -3.612e-06,  7.816e-06,
         -9.003e-05]], device='cuda:0')
s after update for 1 param tensor([[1.467e-02, 1.459e-05, 1.113e-03,  ..., 5.955e-05, 9.913e-03,
         9.180e-05],
        [1.961e-02, 3.583e-04, 2.514e-04,  ..., 7.038e-04, 2.757e-05,
         5.205e-03],
        [4.310e-02, 1.644e-03, 8.439e-06,  ..., 9.158e-04, 3.634e-03,
         3.064e-05],
        ...,
        [1.529e-03, 5.783e-04, 4.856e-03,  ..., 5.855e-03, 5.387e-03,
         9.289e-03],
        [1.345e-04, 9.832e-04, 3.859e-04,  ..., 7.020e-04, 2.223e-02,
         1.453e-02],
        [4.607e-03, 3.364e-03, 3.670e-04,  ..., 1.977e-03, 6.314e-03,
         3.404e-02]], device='cuda:0')
b after update for 1 param tensor([[0.539, 0.017, 0.148,  ..., 0.034, 0.443, 0.043],
        [0.623, 0.084, 0.071,  ..., 0.118, 0.023, 0.321],
        [0.924, 0.180, 0.013,  ..., 0.135, 0.268, 0.025],
        ...,
        [0.174, 0.107, 0.310,  ..., 0.340, 0.327, 0.429],
        [0.052, 0.139, 0.087,  ..., 0.118, 0.663, 0.536],
        [0.302, 0.258, 0.085,  ..., 0.198, 0.353, 0.821]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 4.700e-07,  4.185e-06,  2.642e-06, -1.471e-07,  6.310e-06,  8.484e-08,
        -6.353e-07,  8.041e-06, -6.924e-06,  6.528e-05,  1.318e-06,  8.720e-07,
        -9.442e-06,  2.761e-06,  8.985e-07,  4.550e-07,  1.386e-06, -3.828e-06,
        -8.923e-06, -3.853e-06, -8.181e-06, -7.749e-06, -2.804e-05, -9.025e-06,
         1.597e-06, -3.365e-06,  6.378e-06, -8.410e-06,  4.662e-06, -2.193e-06,
         2.073e-06, -1.205e-05,  3.742e-06, -1.771e-06, -4.186e-06, -1.426e-07,
        -5.105e-06,  3.931e-07, -2.617e-06,  6.253e-07, -1.648e-05, -3.005e-05,
         2.024e-06, -2.501e-06,  7.469e-06,  1.591e-05, -1.879e-06, -2.296e-06,
         5.944e-06,  3.505e-06, -1.120e-05,  3.419e-06,  1.523e-06, -6.037e-07,
        -2.367e-06,  5.052e-08,  1.128e-05,  2.036e-06, -7.718e-07,  2.988e-06,
         5.357e-06,  1.768e-06, -2.959e-05, -4.688e-07, -4.320e-06,  7.301e-06,
         2.097e-05, -1.970e-05,  7.145e-06,  2.503e-05,  3.303e-06, -2.004e-06,
         1.978e-05,  8.316e-06, -3.836e-05, -8.952e-06, -1.026e-07,  8.874e-07,
         1.784e-06, -2.308e-06,  2.515e-05, -2.419e-05, -4.369e-07, -4.146e-06,
        -1.339e-06,  6.224e-06,  1.984e-06, -6.478e-07,  1.272e-05, -4.510e-05,
        -3.659e-06,  7.118e-06, -2.878e-07, -1.825e-06,  6.814e-06, -3.203e-05,
         1.081e-05,  3.507e-07,  1.114e-05, -7.518e-10,  1.229e-05, -2.420e-06,
        -1.596e-06, -3.002e-05, -1.220e-06,  2.932e-06, -5.932e-06, -1.028e-05,
         1.780e-06, -7.630e-07, -9.933e-06, -1.235e-05, -6.542e-06, -2.764e-06,
        -5.705e-06,  1.802e-06,  3.293e-06,  3.437e-06, -9.904e-06, -2.569e-05,
         2.205e-05, -7.460e-06,  1.616e-05, -4.086e-07, -4.829e-07,  1.047e-05,
        -1.652e-06,  4.368e-06, -2.482e-05,  1.420e-05,  9.389e-06,  3.982e-06,
        -2.069e-05, -4.006e-06, -1.342e-05,  4.518e-07,  1.067e-06, -9.175e-07,
        -4.852e-07, -1.234e-06, -7.789e-06, -3.868e-07,  8.588e-06,  7.428e-06,
        -1.252e-05,  4.667e-06, -5.821e-06, -1.349e-05, -1.220e-06,  3.098e-06,
         2.292e-05,  6.921e-06,  1.485e-05, -9.497e-07,  2.974e-06, -1.018e-05,
        -3.001e-05, -9.818e-06,  2.639e-05, -9.991e-06, -3.111e-06,  3.699e-06,
        -7.617e-06,  4.043e-06, -1.655e-05, -1.159e-06, -2.325e-05,  3.193e-05,
        -1.410e-05, -1.621e-06,  1.066e-06,  2.049e-05,  4.433e-06,  6.477e-06,
        -7.635e-06, -1.640e-06,  5.986e-07,  7.908e-06, -5.084e-07, -1.004e-05,
         7.112e-06,  6.614e-06,  1.573e-05,  1.442e-06, -3.440e-06, -1.794e-05,
         4.022e-06, -6.555e-05, -1.602e-06,  1.189e-06,  4.071e-05, -2.065e-05,
         3.406e-05,  8.285e-06,  4.258e-05,  1.265e-05,  5.606e-06, -2.845e-05,
        -3.939e-05, -2.683e-05], device='cuda:0')
s after update for 1 param tensor([2.395e-06, 1.717e-04, 1.075e-04, 4.819e-04, 8.325e-05, 7.142e-06,
        1.162e-04, 1.270e-03, 3.523e-03, 1.370e-02, 1.978e-05, 6.044e-06,
        1.946e-03, 4.396e-04, 3.840e-04, 7.940e-05, 1.465e-04, 6.656e-04,
        2.596e-03, 8.691e-05, 4.878e-03, 2.927e-04, 5.851e-03, 1.641e-03,
        3.822e-04, 3.954e-05, 4.531e-04, 2.877e-04, 7.279e-04, 9.597e-05,
        1.047e-03, 3.960e-04, 5.949e-04, 7.951e-05, 9.375e-04, 1.461e-03,
        1.168e-04, 2.360e-05, 5.910e-05, 4.225e-04, 2.399e-03, 5.915e-03,
        1.858e-04, 1.405e-04, 3.712e-04, 3.597e-03, 9.676e-06, 2.134e-04,
        1.337e-04, 2.320e-04, 8.497e-04, 6.570e-04, 5.031e-06, 1.882e-06,
        1.273e-04, 6.908e-05, 4.376e-03, 1.892e-05, 2.689e-04, 1.475e-03,
        2.557e-03, 5.582e-04, 5.605e-03, 1.998e-04, 9.753e-05, 9.038e-04,
        6.877e-04, 4.607e-03, 6.928e-04, 6.350e-03, 1.246e-05, 5.876e-05,
        3.585e-03, 2.462e-03, 9.333e-03, 1.620e-03, 3.208e-06, 7.964e-05,
        3.082e-04, 7.998e-05, 3.851e-03, 3.710e-03, 1.014e-05, 7.356e-05,
        9.755e-05, 4.537e-04, 5.302e-04, 2.678e-06, 4.206e-03, 3.399e-03,
        2.519e-04, 1.881e-03, 1.617e-04, 4.127e-05, 2.177e-04, 1.064e-02,
        3.016e-03, 4.910e-06, 2.907e-04, 6.263e-05, 3.121e-04, 3.015e-05,
        1.359e-04, 6.494e-03, 7.010e-06, 6.949e-05, 3.397e-04, 2.925e-03,
        1.274e-03, 2.089e-04, 2.451e-03, 4.376e-03, 1.991e-04, 7.314e-04,
        3.506e-04, 7.083e-06, 2.675e-05, 3.149e-05, 2.617e-04, 7.082e-03,
        6.594e-03, 4.754e-04, 1.262e-03, 2.372e-05, 4.654e-06, 1.799e-04,
        8.969e-04, 2.590e-04, 7.709e-03, 2.415e-03, 2.670e-03, 1.169e-04,
        8.343e-04, 1.462e-03, 2.410e-03, 4.618e-05, 1.555e-05, 5.920e-04,
        2.143e-04, 1.456e-04, 1.243e-03, 3.774e-05, 7.710e-04, 2.345e-04,
        1.238e-03, 8.498e-05, 1.898e-04, 1.069e-03, 4.224e-04, 2.874e-04,
        6.086e-03, 3.583e-04, 2.072e-04, 1.055e-05, 9.629e-05, 3.374e-04,
        1.208e-02, 2.225e-03, 8.238e-04, 3.101e-03, 1.008e-04, 1.382e-03,
        6.531e-04, 6.486e-04, 4.517e-04, 2.124e-05, 2.261e-03, 5.602e-03,
        8.454e-04, 3.950e-04, 5.059e-06, 2.975e-03, 1.367e-03, 2.200e-04,
        3.738e-04, 1.063e-05, 6.810e-04, 6.412e-04, 1.842e-05, 7.049e-04,
        2.458e-04, 1.729e-03, 3.770e-03, 6.792e-04, 3.638e-05, 1.325e-03,
        3.132e-04, 1.448e-02, 2.170e-04, 3.130e-05, 1.252e-02, 7.495e-04,
        8.201e-03, 1.466e-03, 6.745e-03, 3.240e-03, 9.669e-05, 4.891e-03,
        3.771e-03, 1.894e-03], device='cuda:0')
b after update for 1 param tensor([0.007, 0.058, 0.046, 0.098, 0.041, 0.012, 0.048, 0.159, 0.264, 0.521,
        0.020, 0.011, 0.196, 0.093, 0.087, 0.040, 0.054, 0.115, 0.227, 0.041,
        0.311, 0.076, 0.340, 0.180, 0.087, 0.028, 0.095, 0.075, 0.120, 0.044,
        0.144, 0.089, 0.109, 0.040, 0.136, 0.170, 0.048, 0.022, 0.034, 0.091,
        0.218, 0.342, 0.061, 0.053, 0.086, 0.267, 0.014, 0.065, 0.051, 0.068,
        0.130, 0.114, 0.010, 0.006, 0.050, 0.037, 0.294, 0.019, 0.073, 0.171,
        0.225, 0.105, 0.333, 0.063, 0.044, 0.134, 0.117, 0.302, 0.117, 0.354,
        0.016, 0.034, 0.266, 0.221, 0.430, 0.179, 0.008, 0.040, 0.078, 0.040,
        0.276, 0.271, 0.014, 0.038, 0.044, 0.095, 0.102, 0.007, 0.289, 0.259,
        0.071, 0.193, 0.057, 0.029, 0.066, 0.459, 0.244, 0.010, 0.076, 0.035,
        0.079, 0.024, 0.052, 0.358, 0.012, 0.037, 0.082, 0.241, 0.159, 0.064,
        0.220, 0.294, 0.063, 0.120, 0.083, 0.012, 0.023, 0.025, 0.072, 0.374,
        0.361, 0.097, 0.158, 0.022, 0.010, 0.060, 0.133, 0.072, 0.391, 0.219,
        0.230, 0.048, 0.128, 0.170, 0.218, 0.030, 0.018, 0.108, 0.065, 0.054,
        0.157, 0.027, 0.124, 0.068, 0.157, 0.041, 0.061, 0.145, 0.091, 0.075,
        0.347, 0.084, 0.064, 0.014, 0.044, 0.082, 0.489, 0.210, 0.128, 0.248,
        0.045, 0.165, 0.114, 0.113, 0.095, 0.021, 0.212, 0.333, 0.129, 0.088,
        0.010, 0.243, 0.164, 0.066, 0.086, 0.015, 0.116, 0.113, 0.019, 0.118,
        0.070, 0.185, 0.273, 0.116, 0.027, 0.162, 0.079, 0.535, 0.066, 0.025,
        0.498, 0.122, 0.403, 0.170, 0.365, 0.253, 0.044, 0.311, 0.273, 0.194],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 1.940e-05],
         [-2.844e-05],
         [ 3.444e-06],
         [ 6.983e-06],
         [-1.970e-06],
         [ 2.090e-06],
         [ 9.243e-06],
         [ 4.558e-06],
         [-6.564e-06],
         [-1.424e-06]],

        [[ 2.927e-06],
         [ 3.637e-06],
         [ 2.543e-06],
         [-1.776e-06],
         [ 2.443e-07],
         [ 1.207e-05],
         [ 1.350e-06],
         [-1.126e-06],
         [ 4.872e-07],
         [ 1.086e-05]],

        [[ 2.455e-07],
         [-8.642e-06],
         [-2.850e-06],
         [-9.711e-06],
         [ 2.697e-06],
         [ 9.613e-06],
         [ 1.172e-05],
         [-3.890e-05],
         [ 1.389e-06],
         [-6.770e-06]],

        [[ 5.388e-10],
         [ 3.955e-06],
         [ 3.610e-05],
         [ 1.863e-06],
         [ 2.145e-06],
         [ 1.148e-05],
         [ 4.770e-06],
         [ 2.704e-06],
         [ 2.825e-06],
         [ 8.342e-06]],

        [[-6.711e-06],
         [ 3.585e-05],
         [ 4.923e-06],
         [-2.426e-06],
         [-1.314e-06],
         [-2.732e-06],
         [-1.454e-06],
         [ 2.338e-06],
         [-1.352e-05],
         [ 4.432e-06]],

        [[ 7.279e-06],
         [-4.860e-05],
         [ 1.088e-06],
         [-1.283e-06],
         [-6.265e-06],
         [ 4.266e-06],
         [-2.828e-06],
         [-3.037e-06],
         [ 5.009e-06],
         [ 1.626e-05]],

        [[ 2.640e-05],
         [ 2.972e-05],
         [-1.094e-05],
         [ 1.621e-06],
         [-6.613e-06],
         [ 1.834e-06],
         [ 2.378e-06],
         [ 8.782e-06],
         [-8.209e-06],
         [ 7.694e-07]],

        [[-1.157e-06],
         [-1.027e-06],
         [ 2.090e-05],
         [ 1.249e-05],
         [-5.931e-06],
         [-2.173e-05],
         [-2.337e-05],
         [ 6.673e-07],
         [-4.525e-07],
         [-2.304e-07]],

        [[-6.399e-06],
         [-2.358e-06],
         [-1.787e-05],
         [-4.577e-06],
         [-1.430e-06],
         [-6.080e-07],
         [-2.948e-06],
         [ 4.748e-07],
         [-1.941e-06],
         [ 1.429e-05]],

        [[ 1.915e-06],
         [-6.448e-06],
         [ 1.788e-06],
         [ 6.025e-08],
         [-4.008e-06],
         [ 8.889e-06],
         [-5.036e-05],
         [-7.868e-06],
         [-8.625e-06],
         [-5.500e-06]],

        [[-8.287e-06],
         [ 1.002e-06],
         [-9.420e-06],
         [-8.197e-06],
         [-8.653e-06],
         [ 6.684e-06],
         [ 2.964e-06],
         [-2.264e-06],
         [-2.108e-05],
         [ 3.064e-06]],

        [[ 2.870e-05],
         [ 2.026e-05],
         [-4.801e-06],
         [-7.644e-06],
         [ 7.076e-06],
         [ 1.873e-05],
         [-1.825e-06],
         [-2.954e-06],
         [-2.275e-05],
         [ 2.990e-07]],

        [[ 4.207e-06],
         [ 1.105e-05],
         [-5.074e-07],
         [ 2.477e-05],
         [ 7.975e-06],
         [-3.661e-06],
         [-7.604e-07],
         [-6.534e-06],
         [ 5.538e-06],
         [-6.211e-06]],

        [[-6.691e-06],
         [ 2.155e-05],
         [ 9.061e-06],
         [-1.637e-05],
         [-1.235e-05],
         [ 3.354e-05],
         [ 4.639e-06],
         [-3.167e-06],
         [-2.740e-07],
         [-3.684e-06]],

        [[ 8.019e-06],
         [ 6.753e-06],
         [ 4.420e-06],
         [-5.887e-07],
         [ 5.744e-06],
         [-2.134e-05],
         [ 6.111e-06],
         [-1.538e-05],
         [-2.007e-05],
         [-3.689e-05]],

        [[-9.101e-06],
         [-1.592e-06],
         [ 1.425e-05],
         [ 7.432e-06],
         [-1.151e-05],
         [-9.465e-07],
         [ 3.683e-06],
         [-3.803e-06],
         [-2.825e-05],
         [-1.118e-06]],

        [[ 3.873e-06],
         [-3.059e-06],
         [-4.195e-05],
         [-1.260e-05],
         [-2.111e-06],
         [-1.725e-06],
         [ 1.384e-05],
         [-1.668e-05],
         [-7.336e-06],
         [-2.789e-05]],

        [[-6.262e-06],
         [-1.118e-05],
         [ 2.627e-06],
         [ 3.519e-07],
         [ 2.472e-05],
         [-5.200e-06],
         [ 1.828e-05],
         [-4.057e-06],
         [-1.161e-06],
         [ 9.790e-06]],

        [[ 1.112e-05],
         [-5.651e-06],
         [ 1.076e-06],
         [ 2.112e-06],
         [ 9.928e-07],
         [-2.690e-06],
         [-6.114e-06],
         [-8.287e-06],
         [-2.868e-05],
         [ 1.096e-05]],

        [[ 2.553e-06],
         [ 8.541e-06],
         [-3.896e-07],
         [-5.716e-06],
         [-1.777e-06],
         [ 8.774e-06],
         [ 1.357e-05],
         [-1.278e-06],
         [-2.801e-06],
         [ 4.476e-06]]], device='cuda:0')
s after update for 1 param tensor([[[2.124e-03],
         [8.385e-03],
         [3.273e-04],
         [4.526e-04],
         [7.227e-05],
         [2.385e-04],
         [8.288e-04],
         [1.306e-03],
         [7.919e-04],
         [4.519e-04]],

        [[7.130e-05],
         [1.485e-04],
         [9.129e-05],
         [5.131e-06],
         [2.470e-06],
         [9.716e-04],
         [5.949e-05],
         [7.814e-06],
         [1.283e-05],
         [1.020e-03]],

        [[1.850e-03],
         [1.321e-03],
         [1.090e-04],
         [1.660e-04],
         [6.927e-04],
         [2.404e-03],
         [4.709e-03],
         [4.504e-03],
         [1.746e-04],
         [3.692e-04]],

        [[1.465e-04],
         [1.463e-03],
         [1.110e-02],
         [7.651e-04],
         [9.999e-05],
         [2.113e-04],
         [1.040e-03],
         [7.118e-04],
         [1.131e-04],
         [1.854e-03]],

        [[5.743e-04],
         [1.128e-02],
         [1.682e-03],
         [3.544e-05],
         [6.089e-05],
         [9.869e-05],
         [1.350e-04],
         [2.666e-05],
         [3.306e-03],
         [1.675e-04]],

        [[2.093e-03],
         [1.127e-02],
         [1.429e-05],
         [7.089e-05],
         [6.523e-04],
         [4.765e-04],
         [7.749e-04],
         [2.686e-04],
         [9.379e-05],
         [3.220e-03]],

        [[1.701e-03],
         [7.461e-03],
         [4.175e-04],
         [2.821e-05],
         [7.282e-04],
         [2.480e-04],
         [2.006e-05],
         [2.953e-04],
         [1.026e-03],
         [2.434e-04]],

        [[3.016e-04],
         [1.497e-03],
         [5.318e-03],
         [5.076e-04],
         [8.578e-04],
         [6.741e-03],
         [4.368e-03],
         [1.422e-05],
         [3.320e-05],
         [4.956e-05]],

        [[5.722e-05],
         [4.610e-05],
         [8.184e-04],
         [5.880e-05],
         [1.388e-05],
         [4.201e-06],
         [4.640e-04],
         [1.570e-04],
         [2.696e-05],
         [4.418e-03]],

        [[1.971e-04],
         [6.367e-04],
         [8.912e-06],
         [3.653e-05],
         [2.536e-04],
         [1.028e-03],
         [8.559e-03],
         [1.102e-03],
         [2.130e-04],
         [7.988e-04]],

        [[2.065e-03],
         [1.882e-06],
         [1.011e-03],
         [4.941e-04],
         [1.632e-03],
         [3.581e-03],
         [3.202e-05],
         [1.694e-05],
         [4.546e-03],
         [2.796e-04]],

        [[1.161e-03],
         [4.242e-04],
         [1.407e-03],
         [1.609e-03],
         [6.778e-04],
         [2.891e-03],
         [8.701e-04],
         [8.171e-04],
         [2.555e-03],
         [8.337e-05]],

        [[9.710e-04],
         [1.130e-03],
         [1.188e-05],
         [9.184e-03],
         [2.144e-03],
         [3.261e-05],
         [4.582e-05],
         [3.951e-04],
         [2.396e-03],
         [1.106e-03]],

        [[1.012e-04],
         [1.097e-03],
         [2.151e-03],
         [1.017e-03],
         [6.590e-03],
         [5.800e-03],
         [9.239e-04],
         [7.675e-05],
         [3.006e-06],
         [3.571e-04]],

        [[3.429e-04],
         [1.631e-03],
         [1.839e-04],
         [1.397e-04],
         [2.271e-03],
         [8.494e-04],
         [3.628e-03],
         [3.234e-03],
         [2.921e-03],
         [5.073e-03]],

        [[2.980e-04],
         [5.106e-04],
         [3.262e-03],
         [3.262e-03],
         [2.516e-03],
         [1.283e-05],
         [1.215e-03],
         [1.154e-03],
         [2.372e-03],
         [1.561e-04]],

        [[7.188e-04],
         [1.333e-03],
         [3.569e-03],
         [4.572e-04],
         [5.853e-05],
         [5.504e-06],
         [1.224e-03],
         [3.597e-03],
         [1.892e-03],
         [8.900e-03]],

        [[1.888e-03],
         [4.630e-03],
         [7.722e-05],
         [6.716e-05],
         [7.410e-03],
         [8.977e-04],
         [2.482e-03],
         [3.292e-04],
         [1.337e-04],
         [5.879e-04]],

        [[5.248e-04],
         [2.507e-03],
         [1.034e-03],
         [4.239e-04],
         [1.151e-05],
         [2.573e-04],
         [1.123e-03],
         [1.359e-03],
         [7.457e-03],
         [1.181e-03]],

        [[2.136e-03],
         [2.011e-04],
         [1.192e-04],
         [9.645e-05],
         [7.490e-05],
         [5.319e-04],
         [3.706e-03],
         [1.106e-03],
         [1.352e-03],
         [3.304e-04]]], device='cuda:0')
b after update for 1 param tensor([[[0.205],
         [0.407],
         [0.080],
         [0.095],
         [0.038],
         [0.069],
         [0.128],
         [0.161],
         [0.125],
         [0.095]],

        [[0.038],
         [0.054],
         [0.043],
         [0.010],
         [0.007],
         [0.139],
         [0.034],
         [0.012],
         [0.016],
         [0.142]],

        [[0.191],
         [0.162],
         [0.046],
         [0.057],
         [0.117],
         [0.218],
         [0.305],
         [0.299],
         [0.059],
         [0.085]],

        [[0.054],
         [0.170],
         [0.469],
         [0.123],
         [0.044],
         [0.065],
         [0.143],
         [0.119],
         [0.047],
         [0.192]],

        [[0.107],
         [0.472],
         [0.182],
         [0.026],
         [0.035],
         [0.044],
         [0.052],
         [0.023],
         [0.256],
         [0.058]],

        [[0.203],
         [0.472],
         [0.017],
         [0.037],
         [0.114],
         [0.097],
         [0.124],
         [0.073],
         [0.043],
         [0.252]],

        [[0.183],
         [0.384],
         [0.091],
         [0.024],
         [0.120],
         [0.070],
         [0.020],
         [0.076],
         [0.143],
         [0.069]],

        [[0.077],
         [0.172],
         [0.324],
         [0.100],
         [0.130],
         [0.365],
         [0.294],
         [0.017],
         [0.026],
         [0.031]],

        [[0.034],
         [0.030],
         [0.127],
         [0.034],
         [0.017],
         [0.009],
         [0.096],
         [0.056],
         [0.023],
         [0.296]],

        [[0.062],
         [0.112],
         [0.013],
         [0.027],
         [0.071],
         [0.143],
         [0.412],
         [0.148],
         [0.065],
         [0.126]],

        [[0.202],
         [0.006],
         [0.141],
         [0.099],
         [0.180],
         [0.266],
         [0.025],
         [0.018],
         [0.300],
         [0.074]],

        [[0.152],
         [0.092],
         [0.167],
         [0.178],
         [0.116],
         [0.239],
         [0.131],
         [0.127],
         [0.225],
         [0.041]],

        [[0.139],
         [0.150],
         [0.015],
         [0.426],
         [0.206],
         [0.025],
         [0.030],
         [0.088],
         [0.218],
         [0.148]],

        [[0.045],
         [0.147],
         [0.206],
         [0.142],
         [0.361],
         [0.339],
         [0.135],
         [0.039],
         [0.008],
         [0.084]],

        [[0.082],
         [0.180],
         [0.060],
         [0.053],
         [0.212],
         [0.130],
         [0.268],
         [0.253],
         [0.240],
         [0.317]],

        [[0.077],
         [0.101],
         [0.254],
         [0.254],
         [0.223],
         [0.016],
         [0.155],
         [0.151],
         [0.217],
         [0.056]],

        [[0.119],
         [0.162],
         [0.266],
         [0.095],
         [0.034],
         [0.010],
         [0.156],
         [0.267],
         [0.193],
         [0.420]],

        [[0.193],
         [0.303],
         [0.039],
         [0.036],
         [0.383],
         [0.133],
         [0.222],
         [0.081],
         [0.051],
         [0.108]],

        [[0.102],
         [0.223],
         [0.143],
         [0.092],
         [0.015],
         [0.071],
         [0.149],
         [0.164],
         [0.384],
         [0.153]],

        [[0.206],
         [0.063],
         [0.049],
         [0.044],
         [0.038],
         [0.103],
         [0.271],
         [0.148],
         [0.164],
         [0.081]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 9.302e-07],
        [ 7.259e-06],
        [ 5.725e-06],
        [ 1.202e-05],
        [ 7.580e-06],
        [-4.681e-06],
        [-5.753e-07],
        [ 2.458e-06],
        [ 5.269e-06],
        [ 4.768e-06],
        [ 1.534e-05],
        [ 1.624e-05],
        [-1.242e-06],
        [ 8.110e-06],
        [-6.550e-06],
        [ 6.634e-08],
        [-1.954e-06],
        [-5.602e-06],
        [-2.873e-05],
        [ 1.128e-05]], device='cuda:0')
s after update for 1 param tensor([[1.255e-04],
        [1.115e-03],
        [7.177e-05],
        [2.637e-03],
        [1.476e-04],
        [2.136e-04],
        [3.997e-04],
        [4.345e-05],
        [6.442e-04],
        [1.287e-03],
        [5.670e-03],
        [3.078e-03],
        [2.653e-05],
        [2.639e-03],
        [7.702e-04],
        [1.926e-04],
        [1.094e-03],
        [1.327e-04],
        [5.027e-03],
        [3.091e-03]], device='cuda:0')
b after update for 1 param tensor([[0.050],
        [0.149],
        [0.038],
        [0.228],
        [0.054],
        [0.065],
        [0.089],
        [0.029],
        [0.113],
        [0.160],
        [0.335],
        [0.247],
        [0.023],
        [0.229],
        [0.123],
        [0.062],
        [0.147],
        [0.051],
        [0.315],
        [0.247]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 4.085817585690248
exp ma of ||w||^2 561.9736558837573
||w|| 2.0213405417421004
exp ma of ||w|| 3.011181360904196
||w||^2 3.7840191928288984
exp ma of ||w||^2 4.735257793182592
||w|| 1.9452555597732906
exp ma of ||w|| 2.063490890117608
||w||^2 2.9217721695708354
exp ma of ||w||^2 3.132732117812608
||w|| 1.7093192123096363
exp ma of ||w|| 1.736837479458231
cuda
Objective function 34.88 = squared loss an data 29.45 + 0.5*rho*h**2 2.677646 + alpha*h 0.762638 + L2reg 1.83 + L1reg 0.16 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.7844729576092253
iteration 3 in inner loop, alpha 104.21414153257807 rho 100000.0 h 0.007317985875182842
iteration 4 in outer loop, alpha = 836.0127290508623, rho = 100000.0, h = 0.007317985875182842
cuda
4420
cuda
Objective function 40.23 = squared loss an data 29.45 + 0.5*rho*h**2 2.677646 + alpha*h 6.117929 + L2reg 1.83 + L1reg 0.16 ; SHD = 36 ; DAG True
||w||^2 1303309210331.6619
exp ma of ||w||^2 353040564953.7392
||w|| 1141625.6874876553
exp ma of ||w|| 300956.2727135325
||w||^2 11663303857.722282
exp ma of ||w||^2 8275621904168.274
||w|| 107996.77707099542
exp ma of ||w|| 1068833.2523161115
||w||^2 1246799364.6810644
exp ma of ||w||^2 308365773914.9565
||w|| 35310.04622881517
exp ma of ||w|| 77336.99918769494
||w||^2 229.1543364885593
exp ma of ||w||^2 5156465.178690474
||w|| 15.137844512629904
exp ma of ||w|| 42.23204637098457
||w||^2 10.36254915539704
exp ma of ||w||^2 30358.030377556945
||w|| 3.2190913555531537
exp ma of ||w|| 4.919411987473214
||w||^2 8.816985180715298
exp ma of ||w||^2 5344.566764937903
||w|| 2.9693408663734275
exp ma of ||w|| 3.601076398607459
||w||^2 6.317234591582398
exp ma of ||w||^2 338.0351724383608
||w|| 2.5134109476133024
exp ma of ||w|| 2.897005866732776
||w||^2 3.9066268935787747
exp ma of ||w||^2 3.158532089429175
||w|| 1.9765188826770097
exp ma of ||w|| 1.743551414592799
||w||^2 2.706850007868638
exp ma of ||w||^2 3.159015217941563
||w|| 1.645250743160032
exp ma of ||w|| 1.7456680342436774
cuda
Objective function 35.46 = squared loss an data 29.15 + 0.5*rho*h**2 0.838390 + alpha*h 3.423347 + L2reg 1.90 + L1reg 0.15 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.782071840923669
iteration 1 in inner loop, alpha 836.0127290508623 rho 100000.0 h 0.004094849966705283
iteration 5 in outer loop, alpha = 4930.8626957561455, rho = 1000000.0, h = 0.004094849966705283
Threshold 0.3
[[0.003 0.065 0.048 0.134 0.044 0.03  0.124 0.014 0.228 0.151 0.236 0.055
  0.052 0.129 0.267 0.125 0.049 0.012 0.218 0.057]
 [0.057 0.007 0.02  0.054 0.025 0.039 0.121 0.02  0.844 0.06  0.031 0.014
  0.03  0.092 0.493 0.168 0.005 0.002 0.105 0.028]
 [0.106 0.199 0.004 0.428 0.128 0.233 0.199 0.029 0.693 0.266 0.094 0.018
  0.1   0.035 0.697 0.335 0.1   0.027 2.267 0.649]
 [0.02  0.069 0.006 0.007 0.007 0.045 0.062 0.01  0.138 0.099 0.016 0.013
  0.075 0.066 0.068 0.102 0.028 0.01  0.163 0.002]
 [0.095 0.175 0.036 0.916 0.004 0.094 0.157 0.016 0.953 0.245 0.12  0.101
  0.064 0.23  0.165 0.177 0.032 0.014 0.223 0.118]
 [0.2   0.099 0.009 0.08  0.05  0.004 0.223 0.017 0.343 0.055 0.168 0.033
  0.033 0.072 0.269 0.137 0.102 0.019 0.894 0.051]
 [0.023 0.037 0.016 0.048 0.024 0.021 0.006 0.001 0.129 0.029 0.026 0.014
  0.011 0.039 0.057 0.26  0.03  0.006 0.053 0.021]
 [0.16  0.198 0.087 0.261 0.17  0.161 3.564 0.005 0.409 0.165 0.144 0.242
  0.134 0.151 0.544 0.949 0.119 0.058 0.16  0.144]
 [0.018 0.007 0.006 0.027 0.004 0.01  0.032 0.009 0.004 0.022 0.007 0.01
  0.023 0.043 0.048 0.05  0.01  0.003 0.053 0.019]
 [0.03  0.078 0.014 0.048 0.009 0.055 0.133 0.011 0.171 0.005 0.027 0.014
  0.002 0.043 0.105 0.095 0.047 0.018 0.11  0.014]
 [0.024 0.152 0.042 0.249 0.042 0.034 0.2   0.035 0.428 0.097 0.003 0.043
  0.03  0.094 0.14  0.137 0.054 0.013 0.132 0.074]
 [0.061 0.205 0.343 0.097 0.052 0.115 0.288 0.016 0.236 0.174 0.089 0.004
  0.133 0.084 0.348 0.219 0.099 0.024 0.546 0.235]
 [0.086 0.151 0.082 0.079 0.06  0.122 0.264 0.034 0.142 1.229 0.155 0.038
  0.004 0.103 0.33  0.252 0.114 0.032 0.309 0.033]
 [0.033 0.066 0.053 0.081 0.013 0.064 0.083 0.016 0.153 0.117 0.054 0.056
  0.051 0.002 0.131 0.086 0.059 0.011 0.373 0.02 ]
 [0.015 0.005 0.004 0.065 0.017 0.014 0.083 0.006 0.097 0.036 0.038 0.008
  0.014 0.023 0.003 0.19  0.007 0.002 0.064 0.016]
 [0.038 0.035 0.008 0.052 0.03  0.024 0.017 0.003 0.059 0.056 0.027 0.016
  0.011 0.065 0.029 0.007 0.045 0.01  0.069 0.017]
 [0.091 0.674 0.035 0.13  0.133 0.039 0.131 0.023 0.315 0.076 0.073 0.028
  0.034 0.058 0.96  0.103 0.006 0.001 0.266 0.167]
 [0.189 1.7   0.162 0.224 0.224 0.167 0.249 0.087 0.512 0.29  0.25  0.183
  0.104 0.18  0.535 0.377 2.133 0.003 0.449 0.145]
 [0.015 0.04  0.002 0.028 0.018 0.004 0.068 0.016 0.133 0.04  0.028 0.006
  0.012 0.011 0.084 0.057 0.019 0.007 0.003 0.01 ]
 [0.062 0.126 0.005 1.247 0.051 0.071 0.147 0.029 0.144 0.26  0.072 0.018
  0.148 0.138 0.226 0.305 0.028 0.012 0.371 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.844 0.    0.    0.
  0.    0.    0.493 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.428 0.    0.    0.    0.    0.693 0.    0.    0.
  0.    0.    0.697 0.335 0.    0.    2.267 0.649]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.916 0.    0.    0.    0.    0.953 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.343 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.894 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    3.564 0.    0.409 0.    0.    0.
  0.    0.    0.544 0.949 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.428 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.343 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.348 0.    0.    0.    0.546 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.229 0.    0.
  0.    0.    0.33  0.    0.    0.    0.309 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.373 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.674 0.    0.    0.    0.    0.    0.    0.315 0.    0.    0.
  0.    0.    0.96  0.    0.    0.    0.    0.   ]
 [0.    1.7   0.    0.    0.    0.    0.    0.    0.512 0.    0.    0.
  0.    0.    0.535 0.377 2.133 0.    0.449 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.247 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.305 0.    0.    0.371 0.   ]]
{'fdr': 0.4722222222222222, 'tpr': 0.475, 'fpr': 0.11333333333333333, 'f1': 0.5, 'shd': 36, 'npred': 36, 'ntrue': 40}
[6.521e-02 4.772e-02 1.342e-01 4.442e-02 2.972e-02 1.244e-01 1.405e-02
 2.278e-01 1.512e-01 2.357e-01 5.485e-02 5.176e-02 1.287e-01 2.672e-01
 1.251e-01 4.851e-02 1.200e-02 2.177e-01 5.691e-02 5.734e-02 2.000e-02
 5.373e-02 2.476e-02 3.864e-02 1.206e-01 2.049e-02 8.440e-01 5.990e-02
 3.069e-02 1.368e-02 2.956e-02 9.169e-02 4.934e-01 1.677e-01 5.349e-03
 1.738e-03 1.053e-01 2.825e-02 1.060e-01 1.991e-01 4.276e-01 1.282e-01
 2.335e-01 1.993e-01 2.932e-02 6.930e-01 2.661e-01 9.359e-02 1.816e-02
 1.003e-01 3.510e-02 6.966e-01 3.353e-01 9.972e-02 2.706e-02 2.267e+00
 6.493e-01 2.026e-02 6.876e-02 5.899e-03 6.778e-03 4.516e-02 6.215e-02
 1.031e-02 1.381e-01 9.937e-02 1.648e-02 1.338e-02 7.465e-02 6.606e-02
 6.810e-02 1.023e-01 2.823e-02 9.637e-03 1.632e-01 2.151e-03 9.494e-02
 1.746e-01 3.596e-02 9.163e-01 9.392e-02 1.567e-01 1.590e-02 9.527e-01
 2.450e-01 1.197e-01 1.007e-01 6.426e-02 2.301e-01 1.646e-01 1.765e-01
 3.210e-02 1.411e-02 2.229e-01 1.179e-01 2.004e-01 9.904e-02 9.408e-03
 8.031e-02 5.036e-02 2.232e-01 1.727e-02 3.429e-01 5.475e-02 1.677e-01
 3.320e-02 3.341e-02 7.174e-02 2.692e-01 1.374e-01 1.024e-01 1.938e-02
 8.940e-01 5.077e-02 2.298e-02 3.740e-02 1.635e-02 4.802e-02 2.383e-02
 2.125e-02 1.014e-03 1.293e-01 2.913e-02 2.582e-02 1.437e-02 1.145e-02
 3.880e-02 5.679e-02 2.602e-01 3.014e-02 6.211e-03 5.294e-02 2.073e-02
 1.603e-01 1.979e-01 8.680e-02 2.605e-01 1.696e-01 1.614e-01 3.564e+00
 4.085e-01 1.649e-01 1.440e-01 2.415e-01 1.338e-01 1.515e-01 5.439e-01
 9.490e-01 1.188e-01 5.848e-02 1.602e-01 1.437e-01 1.751e-02 6.806e-03
 5.641e-03 2.735e-02 3.852e-03 9.545e-03 3.209e-02 8.967e-03 2.226e-02
 7.303e-03 9.795e-03 2.284e-02 4.267e-02 4.773e-02 5.010e-02 9.800e-03
 2.563e-03 5.301e-02 1.873e-02 3.041e-02 7.776e-02 1.382e-02 4.790e-02
 8.921e-03 5.464e-02 1.328e-01 1.125e-02 1.714e-01 2.725e-02 1.429e-02
 2.088e-03 4.298e-02 1.055e-01 9.522e-02 4.663e-02 1.814e-02 1.100e-01
 1.422e-02 2.422e-02 1.517e-01 4.177e-02 2.494e-01 4.158e-02 3.358e-02
 2.004e-01 3.496e-02 4.281e-01 9.664e-02 4.299e-02 3.037e-02 9.371e-02
 1.396e-01 1.373e-01 5.378e-02 1.328e-02 1.324e-01 7.392e-02 6.114e-02
 2.052e-01 3.429e-01 9.743e-02 5.198e-02 1.151e-01 2.876e-01 1.571e-02
 2.363e-01 1.740e-01 8.904e-02 1.326e-01 8.418e-02 3.478e-01 2.186e-01
 9.880e-02 2.351e-02 5.464e-01 2.347e-01 8.648e-02 1.507e-01 8.228e-02
 7.892e-02 5.957e-02 1.216e-01 2.639e-01 3.381e-02 1.419e-01 1.229e+00
 1.554e-01 3.842e-02 1.025e-01 3.296e-01 2.518e-01 1.139e-01 3.207e-02
 3.095e-01 3.308e-02 3.260e-02 6.567e-02 5.328e-02 8.069e-02 1.332e-02
 6.380e-02 8.291e-02 1.628e-02 1.531e-01 1.174e-01 5.445e-02 5.577e-02
 5.085e-02 1.312e-01 8.595e-02 5.863e-02 1.148e-02 3.729e-01 2.010e-02
 1.502e-02 5.187e-03 4.298e-03 6.534e-02 1.682e-02 1.401e-02 8.293e-02
 5.618e-03 9.723e-02 3.636e-02 3.781e-02 8.233e-03 1.363e-02 2.289e-02
 1.896e-01 7.480e-03 1.835e-03 6.357e-02 1.556e-02 3.753e-02 3.462e-02
 7.910e-03 5.202e-02 3.005e-02 2.430e-02 1.674e-02 3.173e-03 5.926e-02
 5.566e-02 2.739e-02 1.608e-02 1.076e-02 6.476e-02 2.910e-02 4.460e-02
 9.523e-03 6.931e-02 1.713e-02 9.110e-02 6.742e-01 3.536e-02 1.296e-01
 1.329e-01 3.914e-02 1.306e-01 2.278e-02 3.152e-01 7.594e-02 7.330e-02
 2.841e-02 3.408e-02 5.841e-02 9.602e-01 1.032e-01 1.308e-03 2.665e-01
 1.670e-01 1.889e-01 1.700e+00 1.622e-01 2.237e-01 2.239e-01 1.672e-01
 2.492e-01 8.711e-02 5.123e-01 2.897e-01 2.505e-01 1.826e-01 1.037e-01
 1.801e-01 5.353e-01 3.769e-01 2.133e+00 4.490e-01 1.446e-01 1.527e-02
 3.960e-02 1.608e-03 2.834e-02 1.808e-02 3.613e-03 6.788e-02 1.555e-02
 1.331e-01 4.048e-02 2.795e-02 5.850e-03 1.221e-02 1.126e-02 8.377e-02
 5.747e-02 1.926e-02 6.654e-03 1.049e-02 6.218e-02 1.260e-01 5.091e-03
 1.247e+00 5.131e-02 7.107e-02 1.467e-01 2.899e-02 1.435e-01 2.600e-01
 7.242e-02 1.759e-02 1.485e-01 1.384e-01 2.264e-01 3.055e-01 2.838e-02
 1.166e-02 3.706e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8234558823529411, 0.5315935151541425)
Iterations 630
Achieves (14.15454886112812, 1e-05)-DP
