samples  5000  graph  20 80 ER mlp  minibatch size  50  noise  0.8  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7622777646747636
iteration 1 in outer loop, alpha = 1.7622777646747636, rho = 1.0, h = 1.7622777646747636
cuda
iteration 1 in inner loop,alpha 1.7622777646747636 rho 1.0 h 1.1780819441533836
iteration 2 in inner loop,alpha 1.7622777646747636 rho 10.0 h 0.5435478850940001
iteration 3 in inner loop,alpha 1.7622777646747636 rho 100.0 h 0.1859843346820469
iteration 2 in outer loop, alpha = 20.360711232879453, rho = 100.0, h = 0.1859843346820469
cuda
iteration 1 in inner loop,alpha 20.360711232879453 rho 100.0 h 0.11004940531644536
iteration 2 in inner loop,alpha 20.360711232879453 rho 1000.0 h 0.04359706294061283
iteration 3 in outer loop, alpha = 63.95777417349228, rho = 1000.0, h = 0.04359706294061283
cuda
iteration 1 in inner loop,alpha 63.95777417349228 rho 1000.0 h 0.023356320581381596
iteration 2 in inner loop,alpha 63.95777417349228 rho 10000.0 h 0.008107539273638054
iteration 4 in outer loop, alpha = 145.03316690987282, rho = 10000.0, h = 0.008107539273638054
cuda
iteration 1 in inner loop,alpha 145.03316690987282 rho 10000.0 h 0.004082852139219995
iteration 2 in inner loop,alpha 145.03316690987282 rho 100000.0 h 0.0013557680971203467
iteration 5 in outer loop, alpha = 280.6099766219075, rho = 100000.0, h = 0.0013557680971203467
cuda
iteration 1 in inner loop,alpha 280.6099766219075 rho 100000.0 h 0.0006771204309288237
iteration 6 in outer loop, alpha = 957.7304075507311, rho = 1000000.0, h = 0.0006771204309288237
Threshold 0.3
[[0.001 3.134 0.078 0.244 0.918 1.115 0.275 0.    1.945 1.094 0.124 1.121
  0.259 0.002 0.353 0.441 0.287 1.211 1.302 0.47 ]
 [0.    0.001 0.023 0.475 0.505 1.656 1.805 0.    0.578 0.464 0.092 0.677
  0.304 0.001 1.181 0.26  0.305 0.861 0.511 1.905]
 [0.002 0.009 0.    0.059 0.394 0.555 1.548 0.    0.182 0.494 0.092 0.193
  1.306 0.    1.522 0.317 0.611 0.    1.22  0.33 ]
 [0.    0.    0.    0.021 0.002 0.003 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.198 0.262 0.151 0.    0.001 0.716]
 [0.    0.    0.    0.585 0.004 0.316 0.2   0.    0.    0.401 0.002 0.001
  0.002 0.    0.212 0.368 1.232 0.    0.33  1.066]
 [0.    0.    0.    0.502 0.002 0.003 0.001 0.    0.    0.    0.    0.
  0.    0.    0.341 1.645 1.812 0.    0.251 0.281]
 [0.    0.    0.    0.196 0.001 0.223 0.002 0.    0.    0.008 0.    0.
  0.    0.    0.325 0.335 0.251 0.    0.163 0.205]
 [2.479 0.136 0.    0.102 0.705 0.217 0.692 0.001 0.297 0.457 0.135 0.381
  1.954 0.692 0.178 0.165 0.156 1.129 0.483 0.452]
 [0.    0.    0.002 0.369 1.525 0.943 1.308 0.    0.001 0.813 0.443 0.297
  1.772 0.    1.14  1.138 0.839 0.    1.296 0.675]
 [0.    0.    0.    0.109 0.018 2.199 0.13  0.    0.    0.008 0.001 0.001
  0.    0.    1.173 1.43  1.791 0.    0.851 0.231]
 [0.    0.    0.    0.973 0.111 0.579 2.36  0.    0.    0.251 0.001 0.
  0.    0.    1.504 0.509 1.008 0.    0.294 1.301]
 [0.    0.    0.    0.154 0.447 0.268 1.668 0.    0.    0.271 0.127 0.001
  0.127 0.    0.161 0.858 0.267 0.    0.146 0.235]
 [0.    0.    0.    0.101 0.157 1.384 1.107 0.    0.    0.177 2.143 0.001
  0.001 0.    0.298 0.34  0.303 0.    0.509 1.082]
 [0.141 0.034 0.033 0.771 0.319 1.639 0.272 0.001 0.281 0.347 1.711 0.533
  2.063 0.001 0.424 0.426 0.996 0.351 0.661 1.968]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.221 0.003 0.    0.001 0.002]
 [0.    0.    0.    0.003 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.005 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.003 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.076 1.242 0.004 0.    0.001 0.002]
 [0.    0.    0.031 0.046 0.39  0.296 1.403 0.    3.183 0.559 0.093 0.726
  0.076 0.    0.337 0.17  0.174 0.001 0.407 0.253]
 [0.    0.    0.    0.748 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.343 1.075 0.377 0.    0.001 0.244]
 [0.    0.    0.    0.003 0.    0.    0.    0.    0.    0.002 0.    0.
  0.    0.    0.113 1.788 0.257 0.    0.001 0.002]]
[[0.    3.134 0.    0.    0.918 1.115 0.    0.    1.945 1.094 0.    1.121
  0.    0.    0.353 0.441 0.    1.211 1.302 0.47 ]
 [0.    0.    0.    0.475 0.505 1.656 1.805 0.    0.578 0.464 0.    0.677
  0.304 0.    1.181 0.    0.305 0.861 0.511 1.905]
 [0.    0.    0.    0.    0.394 0.555 1.548 0.    0.    0.494 0.    0.
  1.306 0.    1.522 0.317 0.611 0.    1.22  0.33 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.716]
 [0.    0.    0.    0.585 0.    0.316 0.    0.    0.    0.401 0.    0.
  0.    0.    0.    0.368 1.232 0.    0.33  1.066]
 [0.    0.    0.    0.502 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.341 1.645 1.812 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.325 0.335 0.    0.    0.    0.   ]
 [2.479 0.    0.    0.    0.705 0.    0.692 0.    0.    0.457 0.    0.381
  1.954 0.692 0.    0.    0.    1.129 0.483 0.452]
 [0.    0.    0.    0.369 1.525 0.943 1.308 0.    0.    0.813 0.443 0.
  1.772 0.    1.14  1.138 0.839 0.    1.296 0.675]
 [0.    0.    0.    0.    0.    2.199 0.    0.    0.    0.    0.    0.
  0.    0.    1.173 1.43  1.791 0.    0.851 0.   ]
 [0.    0.    0.    0.973 0.    0.579 2.36  0.    0.    0.    0.    0.
  0.    0.    1.504 0.509 1.008 0.    0.    1.301]
 [0.    0.    0.    0.    0.447 0.    1.668 0.    0.    0.    0.    0.
  0.    0.    0.    0.858 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.384 1.107 0.    0.    0.    2.143 0.
  0.    0.    0.    0.34  0.303 0.    0.509 1.082]
 [0.    0.    0.    0.771 0.319 1.639 0.    0.    0.    0.347 1.711 0.533
  2.063 0.    0.424 0.426 0.996 0.351 0.661 1.968]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.076 1.242 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.39  0.    1.403 0.    3.183 0.559 0.    0.726
  0.    0.    0.337 0.    0.    0.    0.407 0.   ]
 [0.    0.    0.    0.748 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.343 1.075 0.377 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.788 0.    0.    0.    0.   ]]
{'fdr': 0.4117647058823529, 'tpr': 0.875, 'fpr': 0.44545454545454544, 'f1': 0.7035175879396985, 'shd': 54, 'npred': 119, 'ntrue': 80}
[3.134e+00 7.800e-02 2.442e-01 9.181e-01 1.115e+00 2.749e-01 2.945e-04
 1.945e+00 1.094e+00 1.244e-01 1.121e+00 2.587e-01 1.692e-03 3.529e-01
 4.410e-01 2.872e-01 1.211e+00 1.302e+00 4.703e-01 4.509e-04 2.341e-02
 4.748e-01 5.049e-01 1.656e+00 1.805e+00 1.875e-04 5.775e-01 4.637e-01
 9.249e-02 6.767e-01 3.039e-01 1.431e-03 1.181e+00 2.597e-01 3.053e-01
 8.611e-01 5.108e-01 1.905e+00 2.351e-03 9.353e-03 5.934e-02 3.938e-01
 5.554e-01 1.548e+00 1.739e-04 1.816e-01 4.940e-01 9.184e-02 1.926e-01
 1.306e+00 3.372e-04 1.522e+00 3.172e-01 6.107e-01 3.730e-04 1.220e+00
 3.305e-01 8.443e-05 1.903e-04 8.516e-05 1.758e-03 2.646e-03 1.046e-03
 7.132e-06 9.620e-05 2.156e-03 3.880e-04 2.226e-04 8.402e-05 4.484e-06
 1.985e-01 2.618e-01 1.506e-01 1.007e-04 1.195e-03 7.161e-01 7.475e-06
 1.602e-04 3.549e-05 5.847e-01 3.164e-01 2.003e-01 2.104e-05 1.627e-04
 4.007e-01 1.608e-03 1.157e-03 2.310e-03 7.801e-05 2.116e-01 3.677e-01
 1.232e+00 2.794e-05 3.296e-01 1.066e+00 8.848e-05 2.088e-04 7.568e-05
 5.016e-01 2.155e-03 1.378e-03 5.416e-05 5.804e-05 4.495e-04 5.770e-05
 3.067e-04 2.646e-04 4.075e-05 3.415e-01 1.645e+00 1.812e+00 1.464e-05
 2.507e-01 2.811e-01 5.854e-05 1.472e-04 4.297e-05 1.960e-01 6.229e-04
 2.231e-01 3.729e-05 5.349e-05 8.329e-03 1.394e-04 2.169e-04 6.919e-05
 3.935e-06 3.250e-01 3.346e-01 2.507e-01 1.884e-06 1.627e-01 2.055e-01
 2.479e+00 1.357e-01 4.216e-04 1.018e-01 7.053e-01 2.168e-01 6.916e-01
 2.965e-01 4.573e-01 1.353e-01 3.814e-01 1.954e+00 6.917e-01 1.784e-01
 1.650e-01 1.559e-01 1.129e+00 4.832e-01 4.520e-01 1.038e-05 8.902e-05
 2.009e-03 3.690e-01 1.525e+00 9.431e-01 1.308e+00 1.147e-05 8.129e-01
 4.427e-01 2.973e-01 1.772e+00 2.820e-05 1.140e+00 1.138e+00 8.391e-01
 3.639e-05 1.296e+00 6.753e-01 3.455e-05 3.428e-05 8.825e-05 1.086e-01
 1.772e-02 2.199e+00 1.305e-01 6.082e-05 1.774e-04 6.313e-04 1.084e-03
 2.994e-04 1.477e-04 1.173e+00 1.430e+00 1.791e+00 1.406e-05 8.510e-01
 2.306e-01 7.496e-06 1.268e-04 6.677e-05 9.731e-01 1.113e-01 5.791e-01
 2.360e+00 2.383e-05 7.487e-05 2.512e-01 3.888e-04 2.307e-04 8.525e-05
 1.504e+00 5.091e-01 1.008e+00 1.412e-05 2.939e-01 1.301e+00 2.801e-05
 4.436e-05 2.989e-04 1.538e-01 4.472e-01 2.684e-01 1.668e+00 4.104e-05
 2.206e-04 2.712e-01 1.267e-01 1.273e-01 1.835e-04 1.607e-01 8.576e-01
 2.669e-01 1.750e-04 1.457e-01 2.354e-01 5.647e-06 2.771e-05 1.854e-04
 1.013e-01 1.573e-01 1.384e+00 1.107e+00 3.351e-05 3.422e-04 1.770e-01
 2.143e+00 1.398e-03 8.593e-05 2.976e-01 3.396e-01 3.029e-01 9.861e-05
 5.085e-01 1.082e+00 1.406e-01 3.408e-02 3.282e-02 7.707e-01 3.189e-01
 1.639e+00 2.723e-01 7.101e-04 2.814e-01 3.468e-01 1.711e+00 5.330e-01
 2.063e+00 4.241e-01 4.261e-01 9.958e-01 3.514e-01 6.606e-01 1.968e+00
 4.883e-05 5.357e-05 3.325e-05 4.214e-03 1.268e-03 7.581e-04 4.606e-04
 3.400e-05 7.915e-06 5.139e-05 9.379e-05 2.433e-04 5.101e-05 1.738e-05
 2.213e-01 2.631e-03 3.956e-06 6.041e-04 2.412e-03 2.849e-05 1.999e-05
 8.318e-05 2.585e-03 6.066e-04 1.135e-04 3.989e-04 5.079e-05 2.298e-05
 3.023e-04 9.241e-05 6.673e-05 2.256e-05 1.348e-05 1.516e-03 9.644e-04
 5.753e-06 5.614e-04 2.969e-04 5.462e-05 7.446e-06 3.075e-05 3.401e-03
 9.562e-04 6.812e-04 9.755e-04 4.656e-05 2.372e-05 6.680e-05 8.430e-05
 9.929e-05 2.619e-05 3.167e-05 1.076e+00 1.242e+00 5.653e-06 7.190e-04
 1.807e-03 9.714e-05 1.476e-05 3.122e-02 4.642e-02 3.905e-01 2.959e-01
 1.403e+00 9.669e-05 3.183e+00 5.591e-01 9.311e-02 7.263e-01 7.550e-02
 1.387e-04 3.373e-01 1.704e-01 1.742e-01 4.067e-01 2.528e-01 9.316e-05
 1.475e-05 1.510e-04 7.477e-01 1.024e-03 1.075e-03 8.995e-04 5.155e-05
 1.697e-04 5.418e-04 1.689e-04 2.756e-04 1.511e-04 2.550e-05 3.433e-01
 1.075e+00 3.773e-01 2.337e-05 2.441e-01 3.985e-05 1.127e-04 7.164e-05
 2.905e-03 3.021e-04 3.795e-04 4.814e-04 2.097e-05 7.554e-05 1.726e-03
 3.512e-04 1.408e-04 2.292e-04 1.401e-05 1.126e-01 1.788e+00 2.571e-01
 1.628e-05 6.311e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.9382916666666666, 0.888142405103608)
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
total norm for a microbatch 47.01513860766761 clip 48.64320246940336
total norm for a microbatch 52.56142901544627 clip 51.877609711071386
total norm for a microbatch 56.53917011627199 clip 51.56798348801206
total norm for a microbatch 67.67768648750601 clip 49.45067078458778
total norm for a microbatch 49.76768794995849 clip 54.62474311180243
total norm for a microbatch 85.62243874136716 clip 59.06939733381949
total norm for a microbatch 110.53779446332128 clip 64.68235802646493
total norm for a microbatch 53.70046558224526 clip 66.67895172713605
total norm for a microbatch 116.63815999130443 clip 66.254343904401
total norm for a microbatch 91.73608699683072 clip 75.4387032735148
total norm for a microbatch 83.31015147145702 clip 77.2327701896206
total norm for a microbatch 155.08431553895127 clip 78.09679633234205
cuda
Objective function 39.92 = squared loss an data 32.33 + 0.5*rho*h**2 5.994169 + alpha*h 0.000000 + L2reg 1.29 + L1reg 0.30 ; SHD = 129 ; DAG False
Proportion of microbatches that were clipped  0.8144887740268131
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 3.462418049444068
iteration 1 in outer loop, alpha = 3.462418049444068, rho = 1.0, h = 3.462418049444068
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 51.90 = squared loss an data 32.33 + 0.5*rho*h**2 5.994169 + alpha*h 11.988339 + L2reg 1.29 + L1reg 0.30 ; SHD = 129 ; DAG False
total norm for a microbatch 95.05037991340097 clip 1.2048967501798744
total norm for a microbatch 95.60423685235281 clip 5.734361006279214
total norm for a microbatch 99.02216772199905 clip 12.067532816689715
total norm for a microbatch 111.33302275918673 clip 100.13191198793614
total norm for a microbatch 112.79841076218023 clip 94.56847363556626
total norm for a microbatch 172.0618777580798 clip 102.95960293236968
total norm for a microbatch 113.30548667352845 clip 104.89480301415993
total norm for a microbatch 219.7743129477922 clip 109.20528687284771
total norm for a microbatch 172.7710471407561 clip 103.28475818749676
total norm for a microbatch 108.45784389702966 clip 102.59417124359103
total norm for a microbatch 134.96717348190373 clip 107.33498231185645
total norm for a microbatch 108.93780507466636 clip 106.9705743853889
cuda
Objective function 38.92 = squared loss an data 26.20 + 0.5*rho*h**2 2.678805 + alpha*h 8.014290 + L2reg 1.76 + L1reg 0.28 ; SHD = 119 ; DAG False
Proportion of microbatches that were clipped  0.8176341284702434
iteration 1 in inner loop, alpha 3.462418049444068 rho 1.0 h 2.3146512636080026
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 63.03 = squared loss an data 26.20 + 0.5*rho*h**2 26.788052 + alpha*h 8.014290 + L2reg 1.76 + L1reg 0.28 ; SHD = 119 ; DAG False
total norm for a microbatch 188.1333337938964 clip 6.535292006935008
total norm for a microbatch 185.4763433808536 clip 116.43424640428647
total norm for a microbatch 171.26388691449412 clip 129.89744276196984
total norm for a microbatch 179.48093211020623 clip 135.26337248238977
total norm for a microbatch 171.80987090906112 clip 132.26089402995092
total norm for a microbatch 116.61837557317243 clip 124.62405586327344
total norm for a microbatch 130.49901462391762 clip 129.8064122119219
total norm for a microbatch 213.06287025033447 clip 128.57988355924516
total norm for a microbatch 138.4939984649607 clip 130.2715313825275
total norm for a microbatch 221.055945195938 clip 128.91581896495822
total norm for a microbatch 129.45036316864034 clip 125.52824171163691
cuda
Objective function 41.32 = squared loss an data 29.35 + 0.5*rho*h**2 5.931591 + alpha*h 3.771205 + L2reg 2.02 + L1reg 0.25 ; SHD = 101 ; DAG False
Proportion of microbatches that were clipped  0.8224989955805544
iteration 2 in inner loop, alpha 3.462418049444068 rho 10.0 h 1.089182344480495
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 94.71 = squared loss an data 29.35 + 0.5*rho*h**2 59.315909 + alpha*h 3.771205 + L2reg 2.02 + L1reg 0.25 ; SHD = 101 ; DAG False
total norm for a microbatch 151.13099423366714 clip 2.948653418152897
total norm for a microbatch 169.9605919244678 clip 91.56579636572476
total norm for a microbatch 208.48001101518864 clip 169.53917285629223
total norm for a microbatch 314.8668236432361 clip 154.0109479150373
total norm for a microbatch 190.99407629713556 clip 153.18403233895194
total norm for a microbatch 213.90841353344177 clip 142.75432925257027
total norm for a microbatch 206.75123045544333 clip 150.03272808444768
total norm for a microbatch 225.67614898875203 clip 157.40144722565674
cuda
Objective function 45.05 = squared loss an data 32.63 + 0.5*rho*h**2 8.534944 + alpha*h 1.430523 + L2reg 2.23 + L1reg 0.22 ; SHD = 98 ; DAG True
Proportion of microbatches that were clipped  0.8251095601363415
iteration 3 in inner loop, alpha 3.462418049444068 rho 100.0 h 0.4131572103931731
iteration 2 in outer loop, alpha = 44.77813908876138, rho = 100.0, h = 0.4131572103931731
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 62.12 = squared loss an data 32.63 + 0.5*rho*h**2 8.534944 + alpha*h 18.500411 + L2reg 2.23 + L1reg 0.22 ; SHD = 98 ; DAG True
total norm for a microbatch 164.9764368974883 clip 1.8892737511837898
total norm for a microbatch 147.08710862747284 clip 4.232688379914981
total norm for a microbatch 317.22433116179 clip 12.870221525249661
total norm for a microbatch 132.42965548930562 clip 22.389119785841025
total norm for a microbatch 207.412718315522 clip 168.52891694996853
total norm for a microbatch 390.59547889612 clip 181.71171152936876
total norm for a microbatch 192.4723815764788 clip 175.6217518485951
total norm for a microbatch 300.23579319652475 clip 171.86930289857852
total norm for a microbatch 138.07206171828676 clip 168.4533944810115
total norm for a microbatch 244.77686828610737 clip 177.64377708314896
total norm for a microbatch 348.2227680987263 clip 168.6386152470894
total norm for a microbatch 262.99565931832007 clip 181.89178928436993
total norm for a microbatch 262.184662508616 clip 172.5928010903143
total norm for a microbatch 181.67030091061613 clip 163.84033435048096
total norm for a microbatch 194.2692962478989 clip 157.9774480227535
cuda
Objective function 51.45 = squared loss an data 33.24 + 0.5*rho*h**2 3.571062 + alpha*h 11.966846 + L2reg 2.46 + L1reg 0.22 ; SHD = 89 ; DAG True
Proportion of microbatches that were clipped  0.8253762744780709
iteration 1 in inner loop, alpha 44.77813908876138 rho 100.0 h 0.26724751324427487
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 83.59 = squared loss an data 33.24 + 0.5*rho*h**2 35.710617 + alpha*h 11.966846 + L2reg 2.46 + L1reg 0.22 ; SHD = 89 ; DAG True
total norm for a microbatch 137.4192901949849 clip 13.748719700092343
total norm for a microbatch 209.17199799227973 clip 31.423795728973765
total norm for a microbatch 177.8791755723709 clip 192.1650687117902
total norm for a microbatch 203.56164179509295 clip 181.87091222403143
total norm for a microbatch 241.47397108991407 clip 196.04515290281543
total norm for a microbatch 169.8739292125854 clip 184.28174639962174
cuda
Objective function 52.77 = squared loss an data 35.82 + 0.5*rho*h**2 8.337897 + alpha*h 5.782416 + L2reg 2.62 + L1reg 0.21 ; SHD = 91 ; DAG True
Proportion of microbatches that were clipped  0.8281237572576156
iteration 2 in inner loop, alpha 44.77813908876138 rho 1000.0 h 0.12913479364226532
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 127.81 = squared loss an data 35.82 + 0.5*rho*h**2 83.378975 + alpha*h 5.782416 + L2reg 2.62 + L1reg 0.21 ; SHD = 91 ; DAG True
total norm for a microbatch 620.3924259449378 clip 1.1023523130427815
total norm for a microbatch 316.0191340875088 clip 1.724667458972577
total norm for a microbatch 319.6046678328959 clip 5.069901930891112
total norm for a microbatch 291.95694622504215 clip 24.459772588059636
total norm for a microbatch 437.2210601976787 clip 256.05785120025996
total norm for a microbatch 272.10589404468857 clip 263.6804683869053
total norm for a microbatch 307.76357406959033 clip 257.37514908184926
total norm for a microbatch 199.23079343071467 clip 202.88425706372465
total norm for a microbatch 274.07121965829793 clip 213.94424714911992
total norm for a microbatch 271.600675867758 clip 223.5706227827817
total norm for a microbatch 257.3258280363097 clip 221.41321045926847
total norm for a microbatch 325.7369599173942 clip 197.05358326698553
cuda
Objective function 49.84 = squared loss an data 36.30 + 0.5*rho*h**2 8.700307 + alpha*h 1.867877 + L2reg 2.77 + L1reg 0.20 ; SHD = 86 ; DAG True
Proportion of microbatches that were clipped  0.8280564638328415
iteration 3 in inner loop, alpha 44.77813908876138 rho 10000.0 h 0.04171404346982399
iteration 3 in outer loop, alpha = 461.91857378700126, rho = 10000.0, h = 0.04171404346982399
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 67.24 = squared loss an data 36.30 + 0.5*rho*h**2 8.700307 + alpha*h 19.268491 + L2reg 2.77 + L1reg 0.20 ; SHD = 86 ; DAG True
total norm for a microbatch 360.7145088853199 clip 25.04474568717045
total norm for a microbatch 633.9950044335401 clip 61.003513761105026
total norm for a microbatch 470.07762561451506 clip 167.70596421678044
total norm for a microbatch 306.25642136362205 clip 324.1657813690438
total norm for a microbatch 243.56657672002584 clip 287.7420010834193
total norm for a microbatch 268.4963750074646 clip 249.4741340295997
total norm for a microbatch 267.89915917686983 clip 232.35790916917887
total norm for a microbatch 188.0786169759189 clip 205.44544702828512
total norm for a microbatch 295.29755306225917 clip 205.44544702828512
total norm for a microbatch 248.54122147449445 clip 218.7434585265379
total norm for a microbatch 267.9605089838601 clip 231.56132839839387
total norm for a microbatch 453.0463764241811 clip 231.90937921269446
cuda
Objective function 57.17 = squared loss an data 36.24 + 0.5*rho*h**2 4.299878 + alpha*h 13.545919 + L2reg 2.88 + L1reg 0.21 ; SHD = 90 ; DAG True
Proportion of microbatches that were clipped  0.8290605095541401
iteration 1 in inner loop, alpha 461.91857378700126 rho 10000.0 h 0.029325339816821838
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 95.87 = squared loss an data 36.24 + 0.5*rho*h**2 42.998778 + alpha*h 13.545919 + L2reg 2.88 + L1reg 0.21 ; SHD = 90 ; DAG True
total norm for a microbatch 383.3507004479995 clip 7.1683676437658574
total norm for a microbatch 513.2031354355933 clip 73.62502321720677
total norm for a microbatch 580.581692122242 clip 169.08077693103417
total norm for a microbatch 646.2087324373806 clip 593.3260430872006
total norm for a microbatch 427.61595440893996 clip 310.31702300618696
total norm for a microbatch 264.4249763428122 clip 278.1458125963518
total norm for a microbatch 288.47325894933624 clip 239.6334303839376
total norm for a microbatch 207.90435354873563 clip 237.27903214893692
total norm for a microbatch 282.81136885451997 clip 237.93792062332201
total norm for a microbatch 370.80759712439743 clip 222.93390535596436
total norm for a microbatch 362.799518155585 clip 232.47270397400996
cuda
Objective function 48.86 = squared loss an data 36.77 + 0.5*rho*h**2 4.489310 + alpha*h 4.376936 + L2reg 3.01 + L1reg 0.21 ; SHD = 95 ; DAG True
Proportion of microbatches that were clipped  0.8289251547552055
iteration 2 in inner loop, alpha 461.91857378700126 rho 100000.0 h 0.009475557775470378
iteration 4 in outer loop, alpha = 1409.474351334039, rho = 100000.0, h = 0.009475557775470378
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 57.84 = squared loss an data 36.77 + 0.5*rho*h**2 4.489310 + alpha*h 13.355556 + L2reg 3.01 + L1reg 0.21 ; SHD = 95 ; DAG True
total norm for a microbatch 496.231833447889 clip 14.910992334882119
total norm for a microbatch 444.9844211185008 clip 16.170435013909188
total norm for a microbatch 520.2758256254714 clip 23.28079197829579
total norm for a microbatch 627.6953371605085 clip 28.18538658198441
total norm for a microbatch 1183.6105513170914 clip 678.2505316234354
total norm for a microbatch 1226.7595671164377 clip 881.4966459494385
total norm for a microbatch 835.8118869058328 clip 804.803494894881
total norm for a microbatch 478.2745027165236 clip 433.1805397022512
total norm for a microbatch 367.46076467643036 clip 298.633785507712
total norm for a microbatch 315.69578827693266 clip 242.52192804153398
total norm for a microbatch 263.2253393951728 clip 222.47494730521134
cuda
Objective function 48.23 = squared loss an data 38.38 + 0.5*rho*h**2 0.815514 + alpha*h 5.692300 + L2reg 3.12 + L1reg 0.23 ; SHD = 85 ; DAG True
Proportion of microbatches that were clipped  0.8284639398062915
iteration 1 in inner loop, alpha 1409.474351334039 rho 100000.0 h 0.0040385976376526855
iteration 5 in outer loop, alpha = 5448.071988986725, rho = 1000000.0, h = 0.0040385976376526855
Threshold 0.3
[[0.005 1.855 0.472 0.842 1.316 0.742 1.065 0.95  0.456 1.027 0.455 1.054
  1.07  0.781 0.765 1.227 0.884 1.087 1.024 1.059]
 [0.001 0.004 0.079 0.792 0.605 0.431 0.878 0.204 0.028 0.641 0.032 0.212
  0.343 0.262 0.549 0.572 0.23  0.421 0.163 0.468]
 [0.005 0.038 0.003 0.609 0.863 0.043 0.103 0.197 0.018 0.385 0.015 0.244
  0.214 0.149 0.466 0.487 0.123 0.169 0.022 0.056]
 [0.001 0.003 0.005 0.004 0.049 0.004 0.036 0.005 0.005 0.02  0.002 0.008
  0.007 0.008 0.024 0.094 0.024 0.008 0.005 0.006]
 [0.002 0.006 0.005 0.071 0.004 0.005 0.019 0.004 0.003 0.014 0.004 0.007
  0.021 0.013 0.069 0.12  0.007 0.016 0.005 0.022]
 [0.006 0.012 0.101 0.601 1.03  0.003 0.307 0.125 0.019 0.379 0.014 0.265
  0.33  0.249 0.567 1.229 0.949 0.604 0.138 0.249]
 [0.002 0.005 0.033 0.128 0.306 0.009 0.003 0.036 0.004 0.109 0.002 0.034
  0.026 0.015 0.191 0.103 0.015 0.013 0.006 0.02 ]
 [0.004 0.026 0.021 0.764 0.78  0.038 0.118 0.004 0.006 0.415 0.008 0.171
  0.157 0.071 0.58  0.456 0.097 0.098 0.016 0.03 ]
 [0.008 0.184 0.235 0.881 0.983 0.364 0.837 0.507 0.004 0.451 0.01  0.402
  0.547 0.229 0.509 0.867 0.429 0.733 0.302 0.385]
 [0.002 0.006 0.011 0.217 0.324 0.009 0.034 0.008 0.009 0.006 0.002 0.043
  0.037 0.004 0.028 0.205 0.012 0.015 0.006 0.017]
 [0.009 0.132 0.351 1.27  0.852 0.199 1.315 0.486 0.445 1.231 0.005 1.216
  0.623 0.24  0.604 0.503 0.313 0.531 0.189 0.927]
 [0.002 0.016 0.023 0.334 0.65  0.022 0.173 0.024 0.007 0.11  0.002 0.003
  0.057 0.015 0.244 0.363 0.035 0.06  0.018 0.028]
 [0.002 0.012 0.024 0.489 0.215 0.014 0.186 0.028 0.006 0.101 0.007 0.094
  0.004 0.008 0.121 0.112 0.023 0.079 0.017 0.044]
 [0.006 0.009 0.03  0.601 0.354 0.019 0.352 0.078 0.016 0.689 0.017 0.248
  0.477 0.004 0.28  0.193 0.067 0.101 0.103 0.143]
 [0.002 0.007 0.012 0.257 0.076 0.007 0.028 0.007 0.006 0.185 0.005 0.014
  0.042 0.016 0.004 0.062 0.012 0.015 0.014 0.01 ]
 [0.001 0.003 0.005 0.066 0.044 0.003 0.05  0.007 0.004 0.022 0.006 0.008
  0.044 0.013 0.091 0.003 0.004 0.019 0.004 0.008]
 [0.005 0.015 0.04  0.18  0.494 0.005 0.274 0.054 0.008 0.666 0.012 0.115
  0.191 0.085 0.524 0.877 0.003 0.094 0.051 0.044]
 [0.002 0.007 0.034 0.581 0.291 0.009 0.229 0.052 0.005 0.291 0.008 0.082
  0.072 0.039 0.308 0.287 0.063 0.005 0.013 0.046]
 [0.002 0.031 0.157 0.85  0.56  0.031 0.621 0.254 0.015 0.627 0.026 0.25
  0.333 0.039 0.258 1.095 0.089 0.32  0.003 0.228]
 [0.002 0.008 0.06  0.7   0.224 0.017 0.222 0.179 0.011 0.401 0.003 0.153
  0.103 0.038 0.325 0.54  0.133 0.077 0.023 0.005]]
[[0.    1.855 0.472 0.842 1.316 0.742 1.065 0.95  0.456 1.027 0.455 1.054
  1.07  0.781 0.765 1.227 0.884 1.087 1.024 1.059]
 [0.    0.    0.    0.792 0.605 0.431 0.878 0.    0.    0.641 0.    0.
  0.343 0.    0.549 0.572 0.    0.421 0.    0.468]
 [0.    0.    0.    0.609 0.863 0.    0.    0.    0.    0.385 0.    0.
  0.    0.    0.466 0.487 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.601 1.03  0.    0.307 0.    0.    0.379 0.    0.
  0.33  0.    0.567 1.229 0.949 0.604 0.    0.   ]
 [0.    0.    0.    0.    0.306 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.764 0.78  0.    0.    0.    0.    0.415 0.    0.
  0.    0.    0.58  0.456 0.    0.    0.    0.   ]
 [0.    0.    0.    0.881 0.983 0.364 0.837 0.507 0.    0.451 0.    0.402
  0.547 0.    0.509 0.867 0.429 0.733 0.302 0.385]
 [0.    0.    0.    0.    0.324 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.351 1.27  0.852 0.    1.315 0.486 0.445 1.231 0.    1.216
  0.623 0.    0.604 0.503 0.313 0.531 0.    0.927]
 [0.    0.    0.    0.334 0.65  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.363 0.    0.    0.    0.   ]
 [0.    0.    0.    0.489 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.601 0.354 0.    0.352 0.    0.    0.689 0.    0.
  0.477 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.494 0.    0.    0.    0.    0.666 0.    0.
  0.    0.    0.524 0.877 0.    0.    0.    0.   ]
 [0.    0.    0.    0.581 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.308 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.85  0.56  0.    0.621 0.    0.    0.627 0.    0.
  0.333 0.    0.    1.095 0.    0.32  0.    0.   ]
 [0.    0.    0.    0.7   0.    0.    0.    0.    0.    0.401 0.    0.
  0.    0.    0.325 0.54  0.    0.    0.    0.   ]]
{'fdr': 0.5576923076923077, 'tpr': 0.575, 'fpr': 0.5272727272727272, 'f1': 0.5, 'shd': 85, 'npred': 104, 'ntrue': 80}
[1.855e+00 4.716e-01 8.415e-01 1.316e+00 7.416e-01 1.065e+00 9.500e-01
 4.556e-01 1.027e+00 4.552e-01 1.054e+00 1.070e+00 7.807e-01 7.649e-01
 1.227e+00 8.835e-01 1.087e+00 1.024e+00 1.059e+00 1.250e-03 7.906e-02
 7.922e-01 6.054e-01 4.312e-01 8.778e-01 2.038e-01 2.765e-02 6.408e-01
 3.203e-02 2.120e-01 3.426e-01 2.623e-01 5.493e-01 5.721e-01 2.296e-01
 4.209e-01 1.633e-01 4.676e-01 5.414e-03 3.778e-02 6.091e-01 8.633e-01
 4.274e-02 1.031e-01 1.967e-01 1.840e-02 3.846e-01 1.520e-02 2.438e-01
 2.139e-01 1.488e-01 4.658e-01 4.868e-01 1.230e-01 1.690e-01 2.160e-02
 5.588e-02 1.320e-03 3.457e-03 4.772e-03 4.928e-02 4.449e-03 3.633e-02
 4.613e-03 4.946e-03 2.014e-02 2.439e-03 8.392e-03 6.968e-03 7.690e-03
 2.382e-02 9.405e-02 2.369e-02 7.912e-03 5.317e-03 6.432e-03 1.834e-03
 5.941e-03 4.508e-03 7.090e-02 5.055e-03 1.932e-02 3.513e-03 2.815e-03
 1.406e-02 4.293e-03 6.633e-03 2.057e-02 1.305e-02 6.874e-02 1.196e-01
 6.688e-03 1.599e-02 4.535e-03 2.173e-02 5.724e-03 1.228e-02 1.007e-01
 6.010e-01 1.030e+00 3.071e-01 1.248e-01 1.910e-02 3.793e-01 1.400e-02
 2.654e-01 3.305e-01 2.493e-01 5.672e-01 1.229e+00 9.493e-01 6.036e-01
 1.379e-01 2.493e-01 1.711e-03 5.323e-03 3.308e-02 1.283e-01 3.056e-01
 8.857e-03 3.638e-02 4.362e-03 1.090e-01 2.123e-03 3.450e-02 2.594e-02
 1.498e-02 1.907e-01 1.030e-01 1.500e-02 1.251e-02 6.069e-03 1.998e-02
 4.358e-03 2.581e-02 2.059e-02 7.643e-01 7.796e-01 3.756e-02 1.178e-01
 6.431e-03 4.147e-01 7.834e-03 1.714e-01 1.567e-01 7.067e-02 5.802e-01
 4.558e-01 9.707e-02 9.844e-02 1.590e-02 3.017e-02 8.367e-03 1.839e-01
 2.350e-01 8.813e-01 9.828e-01 3.643e-01 8.370e-01 5.075e-01 4.513e-01
 9.866e-03 4.018e-01 5.472e-01 2.290e-01 5.086e-01 8.673e-01 4.286e-01
 7.332e-01 3.018e-01 3.851e-01 2.057e-03 6.245e-03 1.145e-02 2.167e-01
 3.240e-01 8.854e-03 3.404e-02 8.130e-03 8.570e-03 2.180e-03 4.254e-02
 3.665e-02 4.321e-03 2.808e-02 2.050e-01 1.182e-02 1.451e-02 6.052e-03
 1.721e-02 8.953e-03 1.323e-01 3.513e-01 1.270e+00 8.517e-01 1.992e-01
 1.315e+00 4.857e-01 4.450e-01 1.231e+00 1.216e+00 6.232e-01 2.395e-01
 6.044e-01 5.028e-01 3.133e-01 5.311e-01 1.886e-01 9.266e-01 2.397e-03
 1.551e-02 2.335e-02 3.345e-01 6.499e-01 2.162e-02 1.727e-01 2.440e-02
 7.245e-03 1.103e-01 2.341e-03 5.683e-02 1.510e-02 2.436e-01 3.634e-01
 3.480e-02 6.027e-02 1.778e-02 2.829e-02 2.136e-03 1.199e-02 2.363e-02
 4.891e-01 2.155e-01 1.438e-02 1.863e-01 2.762e-02 6.317e-03 1.008e-01
 7.190e-03 9.362e-02 7.600e-03 1.209e-01 1.119e-01 2.300e-02 7.923e-02
 1.737e-02 4.374e-02 5.591e-03 9.476e-03 3.005e-02 6.011e-01 3.543e-01
 1.919e-02 3.520e-01 7.785e-02 1.616e-02 6.886e-01 1.741e-02 2.484e-01
 4.767e-01 2.802e-01 1.929e-01 6.695e-02 1.012e-01 1.030e-01 1.433e-01
 2.190e-03 6.505e-03 1.153e-02 2.566e-01 7.642e-02 7.245e-03 2.773e-02
 7.451e-03 6.079e-03 1.852e-01 5.136e-03 1.364e-02 4.174e-02 1.600e-02
 6.187e-02 1.152e-02 1.517e-02 1.369e-02 9.869e-03 1.470e-03 3.132e-03
 5.297e-03 6.613e-02 4.408e-02 2.739e-03 4.995e-02 6.752e-03 3.962e-03
 2.190e-02 5.701e-03 8.321e-03 4.406e-02 1.290e-02 9.104e-02 4.255e-03
 1.937e-02 3.909e-03 7.770e-03 5.155e-03 1.472e-02 3.973e-02 1.796e-01
 4.937e-01 4.863e-03 2.738e-01 5.381e-02 8.014e-03 6.659e-01 1.208e-02
 1.146e-01 1.909e-01 8.452e-02 5.237e-01 8.772e-01 9.370e-02 5.136e-02
 4.350e-02 2.102e-03 7.024e-03 3.417e-02 5.808e-01 2.915e-01 8.737e-03
 2.286e-01 5.187e-02 4.674e-03 2.913e-01 8.175e-03 8.152e-02 7.159e-02
 3.864e-02 3.077e-01 2.871e-01 6.336e-02 1.285e-02 4.591e-02 2.325e-03
 3.080e-02 1.566e-01 8.501e-01 5.598e-01 3.091e-02 6.205e-01 2.537e-01
 1.523e-02 6.265e-01 2.640e-02 2.497e-01 3.329e-01 3.854e-02 2.582e-01
 1.095e+00 8.862e-02 3.196e-01 2.280e-01 2.017e-03 8.256e-03 6.009e-02
 6.995e-01 2.238e-01 1.686e-02 2.224e-01 1.790e-01 1.117e-02 4.006e-01
 3.067e-03 1.527e-01 1.033e-01 3.758e-02 3.253e-01 5.396e-01 1.334e-01
 7.725e-02 2.342e-02]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.7434583333333332, 0.4600072223156562)
Iterations 2500
Achieves (6.086687062773559, 1e-05)-DP
