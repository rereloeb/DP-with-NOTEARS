samples  5000  graph  20 40 ER mlp  minibatch size  50  noise  0.6  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617290166446
iteration 1 in outer loop, alpha = 1.5634617290166446, rho = 1.0, h = 1.5634617290166446
cuda
iteration 1 in inner loop,alpha 1.5634617290166446 rho 1.0 h 1.0381693524821998
iteration 2 in inner loop,alpha 1.5634617290166446 rho 10.0 h 0.4486130544339133
iteration 3 in inner loop,alpha 1.5634617290166446 rho 100.0 h 0.13645699227387453
iteration 2 in outer loop, alpha = 15.209160956404098, rho = 100.0, h = 0.13645699227387453
cuda
iteration 1 in inner loop,alpha 15.209160956404098 rho 100.0 h 0.07242523875783746
iteration 2 in inner loop,alpha 15.209160956404098 rho 1000.0 h 0.02272817333826893
iteration 3 in outer loop, alpha = 37.93733429467303, rho = 1000.0, h = 0.02272817333826893
cuda
iteration 1 in inner loop,alpha 37.93733429467303 rho 1000.0 h 0.010195562783067658
iteration 2 in inner loop,alpha 37.93733429467303 rho 10000.0 h 0.0032923257204373613
iteration 4 in outer loop, alpha = 70.86059149904665, rho = 10000.0, h = 0.0032923257204373613
cuda
iteration 1 in inner loop,alpha 70.86059149904665 rho 10000.0 h 0.0015081687033671187
iteration 2 in inner loop,alpha 70.86059149904665 rho 100000.0 h 0.0005712122161831701
iteration 5 in outer loop, alpha = 127.98181311736366, rho = 100000.0, h = 0.0005712122161831701
cuda
iteration 1 in inner loop,alpha 127.98181311736366 rho 100000.0 h 0.0002711641382049379
iteration 6 in outer loop, alpha = 399.1459513223016, rho = 1000000.0, h = 0.0002711641382049379
Threshold 0.3
[[0.    0.011 0.039 2.292 0.034 0.032 0.042 0.006 1.993 0.015 1.843 0.005
  0.101 0.009 0.208 0.915 0.006 0.001 0.036 0.218]
 [0.031 0.005 0.217 0.058 0.378 0.112 0.03  0.002 3.05  0.05  0.017 0.009
  0.601 0.002 1.802 1.569 0.286 0.    0.226 0.3  ]
 [0.002 0.002 0.003 0.068 0.129 1.144 0.    0.    2.291 0.076 0.061 0.
  2.288 0.001 1.341 0.877 0.    0.    3.745 2.185]
 [0.    0.    0.    0.003 0.001 0.002 0.    0.    0.001 0.    0.    0.
  0.    0.    0.148 1.478 0.    0.    0.    0.001]
 [0.    0.    0.    1.656 0.001 0.197 0.001 0.    0.001 0.034 0.016 0.
  0.375 0.001 0.09  0.341 0.001 0.    0.035 0.175]
 [0.    0.001 0.    0.103 0.002 0.001 0.    0.    0.001 0.039 0.065 0.
  0.23  0.    1.898 0.458 0.    0.    0.001 0.105]
 [0.001 0.017 0.575 0.056 0.219 0.106 0.005 0.    0.035 0.003 0.017 0.
  0.036 0.001 0.066 1.648 0.008 0.002 0.062 0.029]
 [0.011 0.015 0.606 0.048 0.182 0.096 4.457 0.001 0.146 0.01  0.008 0.048
  0.01  0.01  0.126 0.239 0.065 0.008 0.061 0.047]
 [0.    0.    0.    0.059 0.975 0.123 0.001 0.    0.002 0.051 0.046 0.
  0.277 0.001 0.296 0.466 0.    0.    0.017 0.256]
 [0.    0.    0.    0.096 0.    0.    0.    0.    0.    0.001 0.822 0.
  0.    0.    0.177 0.243 0.    0.    0.    0.185]
 [0.    0.    0.    0.101 0.001 0.    0.    0.    0.001 0.    0.001 0.
  0.    0.001 1.315 0.274 0.    0.    0.    2.276]
 [0.01  0.002 2.686 0.065 0.021 0.21  3.589 0.005 0.218 0.015 0.014 0.001
  0.228 0.005 0.58  0.357 0.009 0.032 0.21  0.152]
 [0.    0.    0.    0.103 0.    0.002 0.    0.    0.001 3.198 0.133 0.
  0.002 0.    0.366 0.526 0.    0.    0.    0.408]
 [0.008 0.009 0.019 0.051 0.03  0.117 0.003 0.013 0.095 0.027 0.077 0.026
  0.275 0.    0.172 0.163 0.009 0.025 2.628 0.246]
 [0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.005 0.264 0.    0.    0.    0.004]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.006 0.009 0.    0.    0.    0.   ]
 [0.009 0.006 0.33  0.064 0.308 0.2   0.032 0.002 0.268 0.05  0.054 0.009
  0.4   0.001 1.83  1.501 0.003 0.    0.327 0.438]
 [0.011 4.265 0.007 0.053 0.217 0.002 0.012 0.003 0.59  0.012 0.008 0.004
  1.389 0.006 0.154 0.241 2.995 0.001 1.41  0.979]
 [0.003 0.001 0.    0.059 0.015 1.211 0.    0.    0.083 0.05  0.017 0.
  1.033 0.    0.362 1.336 0.    0.    0.003 0.464]
 [0.    0.    0.    2.069 0.    0.002 0.    0.    0.    0.    0.    0.
  0.    0.    0.219 0.321 0.    0.    0.    0.003]]
[[0.    0.    0.    2.292 0.    0.    0.    0.    1.993 0.    1.843 0.
  0.    0.    0.    0.915 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.378 0.    0.    0.    3.05  0.    0.    0.
  0.601 0.    1.802 1.569 0.    0.    0.    0.3  ]
 [0.    0.    0.    0.    0.    1.144 0.    0.    2.291 0.    0.    0.
  2.288 0.    1.341 0.877 0.    0.    3.745 2.185]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.478 0.    0.    0.    0.   ]
 [0.    0.    0.    1.656 0.    0.    0.    0.    0.    0.    0.    0.
  0.375 0.    0.    0.341 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.898 0.458 0.    0.    0.    0.   ]
 [0.    0.    0.575 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.648 0.    0.    0.    0.   ]
 [0.    0.    0.606 0.    0.    0.    4.457 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.975 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.466 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.822 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.315 0.    0.    0.    0.    2.276]
 [0.    0.    2.686 0.    0.    0.    3.589 0.    0.    0.    0.    0.
  0.    0.    0.58  0.357 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.198 0.    0.
  0.    0.    0.366 0.526 0.    0.    0.    0.408]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.628 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.33  0.    0.308 0.    0.    0.    0.    0.    0.    0.
  0.4   0.    1.83  1.501 0.    0.    0.327 0.438]
 [0.    4.265 0.    0.    0.    0.    0.    0.    0.59  0.    0.    0.
  1.389 0.    0.    0.    2.995 0.    1.41  0.979]
 [0.    0.    0.    0.    0.    1.211 0.    0.    0.    0.    0.    0.
  1.033 0.    0.362 1.336 0.    0.    0.    0.464]
 [0.    0.    0.    2.069 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.321 0.    0.    0.    0.   ]]
{'fdr': 0.3442622950819672, 'tpr': 1.0, 'fpr': 0.14, 'f1': 0.7920792079207921, 'shd': 21, 'npred': 61, 'ntrue': 40}
[1.080e-02 3.899e-02 2.292e+00 3.404e-02 3.212e-02 4.223e-02 6.263e-03
 1.993e+00 1.482e-02 1.843e+00 5.206e-03 1.010e-01 9.081e-03 2.084e-01
 9.149e-01 6.020e-03 1.227e-03 3.639e-02 2.176e-01 3.107e-02 2.165e-01
 5.834e-02 3.782e-01 1.121e-01 3.012e-02 2.302e-03 3.050e+00 4.958e-02
 1.705e-02 8.611e-03 6.006e-01 1.568e-03 1.802e+00 1.569e+00 2.860e-01
 3.866e-04 2.265e-01 3.005e-01 1.651e-03 1.948e-03 6.849e-02 1.286e-01
 1.144e+00 2.342e-04 6.262e-05 2.291e+00 7.636e-02 6.117e-02 3.123e-04
 2.288e+00 5.109e-04 1.341e+00 8.770e-01 3.519e-04 6.529e-05 3.745e+00
 2.185e+00 1.089e-05 3.547e-04 1.976e-04 5.960e-04 1.537e-03 3.576e-04
 4.340e-05 6.414e-04 2.656e-05 4.595e-05 1.045e-04 6.900e-05 5.277e-05
 1.477e-01 1.478e+00 2.745e-04 9.907e-05 2.766e-04 1.126e-03 6.182e-05
 8.849e-05 1.799e-04 1.656e+00 1.966e-01 5.639e-04 4.977e-06 7.635e-04
 3.374e-02 1.638e-02 6.692e-05 3.746e-01 1.498e-03 9.040e-02 3.406e-01
 7.420e-04 3.428e-05 3.474e-02 1.745e-01 4.460e-04 5.473e-04 8.798e-05
 1.030e-01 2.026e-03 2.151e-04 1.317e-05 1.218e-03 3.937e-02 6.500e-02
 3.572e-05 2.301e-01 3.190e-05 1.898e+00 4.576e-01 3.045e-04 2.002e-05
 5.005e-04 1.050e-01 8.277e-04 1.718e-02 5.746e-01 5.574e-02 2.186e-01
 1.055e-01 1.766e-04 3.512e-02 2.632e-03 1.732e-02 2.713e-05 3.554e-02
 6.459e-04 6.625e-02 1.648e+00 8.085e-03 2.214e-03 6.188e-02 2.902e-02
 1.142e-02 1.514e-02 6.064e-01 4.838e-02 1.816e-01 9.582e-02 4.457e+00
 1.458e-01 9.827e-03 8.217e-03 4.775e-02 1.018e-02 9.899e-03 1.257e-01
 2.388e-01 6.453e-02 7.835e-03 6.148e-02 4.697e-02 4.407e-05 1.891e-04
 3.493e-04 5.917e-02 9.755e-01 1.233e-01 5.804e-04 2.478e-05 5.060e-02
 4.636e-02 9.077e-05 2.766e-01 8.548e-04 2.960e-01 4.658e-01 4.408e-04
 2.549e-05 1.670e-02 2.558e-01 3.528e-04 1.339e-05 4.626e-05 9.623e-02
 1.338e-04 3.616e-04 2.544e-05 9.549e-06 2.602e-04 8.221e-01 1.467e-04
 1.731e-04 1.317e-04 1.771e-01 2.434e-01 5.894e-05 1.273e-05 2.951e-05
 1.854e-01 5.488e-05 2.464e-04 1.777e-04 1.007e-01 5.813e-04 4.784e-04
 2.319e-04 5.696e-06 7.839e-04 8.297e-05 1.169e-04 4.553e-05 7.201e-04
 1.315e+00 2.742e-01 2.581e-04 4.133e-05 7.102e-05 2.276e+00 1.033e-02
 2.015e-03 2.686e+00 6.548e-02 2.075e-02 2.098e-01 3.589e+00 4.863e-03
 2.181e-01 1.474e-02 1.391e-02 2.279e-01 5.052e-03 5.800e-01 3.566e-01
 8.578e-03 3.196e-02 2.097e-01 1.523e-01 2.099e-04 8.668e-05 1.599e-04
 1.027e-01 1.149e-04 1.512e-03 1.984e-04 4.410e-05 6.181e-04 3.198e+00
 1.328e-01 5.032e-05 2.874e-05 3.661e-01 5.260e-01 3.834e-04 4.288e-05
 2.097e-04 4.079e-01 7.873e-03 9.108e-03 1.906e-02 5.132e-02 3.011e-02
 1.173e-01 3.220e-03 1.315e-02 9.506e-02 2.745e-02 7.668e-02 2.646e-02
 2.749e-01 1.717e-01 1.630e-01 9.448e-03 2.477e-02 2.628e+00 2.457e-01
 9.481e-06 3.964e-04 9.684e-05 4.388e-03 4.196e-04 1.289e-04 3.525e-04
 1.217e-05 3.732e-04 6.200e-05 1.747e-04 3.584e-05 9.148e-05 1.646e-05
 2.644e-01 3.922e-04 8.171e-05 4.598e-05 4.337e-03 3.942e-05 2.811e-04
 5.142e-05 1.622e-04 4.368e-05 4.065e-04 3.714e-04 3.552e-05 8.923e-05
 8.710e-05 4.920e-05 1.497e-05 6.917e-05 7.032e-05 6.476e-03 2.934e-05
 3.287e-05 4.033e-05 7.315e-05 9.390e-03 5.900e-03 3.305e-01 6.450e-02
 3.083e-01 2.000e-01 3.153e-02 2.490e-03 2.681e-01 4.975e-02 5.438e-02
 9.142e-03 3.998e-01 5.501e-04 1.830e+00 1.501e+00 2.420e-04 3.266e-01
 4.379e-01 1.083e-02 4.265e+00 7.270e-03 5.302e-02 2.174e-01 2.001e-03
 1.224e-02 3.384e-03 5.900e-01 1.194e-02 8.431e-03 3.606e-03 1.389e+00
 6.168e-03 1.538e-01 2.406e-01 2.995e+00 1.410e+00 9.791e-01 2.540e-03
 9.372e-04 4.085e-04 5.883e-02 1.486e-02 1.211e+00 1.669e-04 2.779e-05
 8.258e-02 5.010e-02 1.694e-02 1.006e-04 1.033e+00 2.701e-05 3.616e-01
 1.336e+00 1.838e-04 9.972e-05 4.637e-01 2.714e-04 1.146e-04 1.927e-04
 2.069e+00 3.350e-04 1.811e-03 3.214e-04 3.046e-05 3.690e-04 1.083e-04
 5.047e-05 7.632e-05 2.915e-05 1.457e-04 2.193e-01 3.212e-01 4.502e-05
 1.054e-04 1.625e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9992647058823529, 0.9934713912844813)
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
total norm for a microbatch 50.744106348248884 clip 49.42775337917733
total norm for a microbatch 71.3192724813848 clip 49.84345953886338
total norm for a microbatch 63.94716675582872 clip 56.311625445855384
total norm for a microbatch 45.33934710508183 clip 57.109793845017656
total norm for a microbatch 69.57573162363244 clip 62.58257912546261
total norm for a microbatch 169.13703076295064 clip 73.31116955713915
total norm for a microbatch 84.35170815470424 clip 86.9635000401081
total norm for a microbatch 91.2645535805997 clip 77.62498420770791
total norm for a microbatch 82.32091492380353 clip 80.92323838492756
total norm for a microbatch 163.8973881805418 clip 83.7068194705334
total norm for a microbatch 74.15003595170396 clip 82.33726923444843
total norm for a microbatch 73.20993116817155 clip 82.59499090833454
cuda
Objective function 35.17 = squared loss an data 28.15 + 0.5*rho*h**2 5.523880 + alpha*h 0.000000 + L2reg 1.21 + L1reg 0.28 ; SHD = 109 ; DAG False
Proportion of microbatches that were clipped  0.8150541108060088
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 3.3238171601031574
iteration 1 in outer loop, alpha = 3.3238171601031574, rho = 1.0, h = 3.3238171601031574
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 46.22 = squared loss an data 28.15 + 0.5*rho*h**2 5.523880 + alpha*h 11.047761 + L2reg 1.21 + L1reg 0.28 ; SHD = 109 ; DAG False
total norm for a microbatch 94.889038743023 clip 1.2048967501798744
total norm for a microbatch 225.94008729234096 clip 5.734361006279214
total norm for a microbatch 75.60662180618422 clip 12.067532816689715
total norm for a microbatch 152.94327959671347 clip 110.221109715768
total norm for a microbatch 144.727431841864 clip 104.09710452242511
total norm for a microbatch 98.99431083797454 clip 113.33371615302678
total norm for a microbatch 150.6744934861455 clip 110.93650070378338
total norm for a microbatch 125.72373763803287 clip 107.471906306044
total norm for a microbatch 107.30075059820267 clip 104.95060555954724
total norm for a microbatch 97.70321814044335 clip 105.9302777512785
total norm for a microbatch 105.05095096561966 clip 106.47972803183268
total norm for a microbatch 152.5315126441182 clip 106.11822373875721
cuda
Objective function 31.05 = squared loss an data 20.06 + 0.5*rho*h**2 2.108463 + alpha*h 6.825510 + L2reg 1.78 + L1reg 0.27 ; SHD = 99 ; DAG False
Proportion of microbatches that were clipped  0.8177969551412522
iteration 1 in inner loop, alpha 3.3238171601031574 rho 1.0 h 2.053515502096559
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 50.02 = squared loss an data 20.06 + 0.5*rho*h**2 21.084630 + alpha*h 6.825510 + L2reg 1.78 + L1reg 0.27 ; SHD = 99 ; DAG False
total norm for a microbatch 188.51063867440405 clip 6.535292006935008
total norm for a microbatch 264.1467650190675 clip 117.36945622703136
total norm for a microbatch 155.54977703686697 clip 141.84646457790865
total norm for a microbatch 261.8774568967405 clip 139.66179991467394
total norm for a microbatch 168.10575634266362 clip 138.76424886320382
total norm for a microbatch 262.1705203188995 clip 138.28290907931304
total norm for a microbatch 160.30180964883385 clip 142.88558480681158
total norm for a microbatch 108.13669921990979 clip 134.90224069539715
total norm for a microbatch 192.76059036965606 clip 132.37263992956628
total norm for a microbatch 134.27895793693722 clip 130.99506165291282
total norm for a microbatch 108.46504671923176 clip 120.6062089519007
cuda
Objective function 31.84 = squared loss an data 21.87 + 0.5*rho*h**2 4.469214 + alpha*h 3.142445 + L2reg 2.10 + L1reg 0.26 ; SHD = 91 ; DAG False
Proportion of microbatches that were clipped  0.8219365206910406
iteration 2 in inner loop, alpha 3.3238171601031574 rho 10.0 h 0.9454326473005565
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 72.06 = squared loss an data 21.87 + 0.5*rho*h**2 44.692145 + alpha*h 3.142445 + L2reg 2.10 + L1reg 0.26 ; SHD = 91 ; DAG False
total norm for a microbatch 327.21875800938767 clip 2.948653418152897
total norm for a microbatch 258.30700141984755 clip 92.30126067140115
total norm for a microbatch 183.12726982169158 clip 168.18827028851905
total norm for a microbatch 292.18893064254246 clip 151.56638143079974
total norm for a microbatch 251.68116581182252 clip 153.18403233895197
total norm for a microbatch 154.11747397055046 clip 140.48843547362162
total norm for a microbatch 197.0601583425202 clip 148.8372545298425
total norm for a microbatch 152.72904852445512 clip 141.8541660904704
cuda
Objective function 32.70 = squared loss an data 22.98 + 0.5*rho*h**2 5.913391 + alpha*h 1.143064 + L2reg 2.40 + L1reg 0.25 ; SHD = 98 ; DAG True
Proportion of microbatches that were clipped  0.8243791592273981
iteration 3 in inner loop, alpha 3.3238171601031574 rho 100.0 h 0.3439008828080361
iteration 2 in outer loop, alpha = 37.71390544090677, rho = 100.0, h = 0.3439008828080361
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 44.52 = squared loss an data 22.98 + 0.5*rho*h**2 5.913391 + alpha*h 12.969845 + L2reg 2.40 + L1reg 0.25 ; SHD = 98 ; DAG True
total norm for a microbatch 215.29896150170347 clip 1.8892737511837898
total norm for a microbatch 197.32014499575732 clip 4.232688379914981
total norm for a microbatch 123.45698318536775 clip 12.870221525249661
total norm for a microbatch 288.96200976975786 clip 22.389119785841025
total norm for a microbatch 148.15095363641004 clip 167.1860641872932
total norm for a microbatch 235.79283673003738 clip 162.45831221339344
total norm for a microbatch 152.20408828876504 clip 158.274765543383
total norm for a microbatch 145.82922091690781 clip 153.65876329706344
total norm for a microbatch 185.6268493405053 clip 160.55862182774712
total norm for a microbatch 123.86298105550125 clip 158.82139878186268
total norm for a microbatch 191.74006469826924 clip 149.56903428232496
total norm for a microbatch 178.1677608338913 clip 163.92548189628107
total norm for a microbatch 238.28821028004103 clip 154.30560270069668
total norm for a microbatch 173.80285202056487 clip 144.15547181845878
total norm for a microbatch 154.47400295343024 clip 142.3732725801487
cuda
Objective function 35.30 = squared loss an data 22.49 + 0.5*rho*h**2 2.133192 + alpha*h 7.789892 + L2reg 2.63 + L1reg 0.26 ; SHD = 87 ; DAG True
Proportion of microbatches that were clipped  0.8240006473539407
iteration 1 in inner loop, alpha 37.71390544090677 rho 100.0 h 0.20655225577272418
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 54.50 = squared loss an data 22.49 + 0.5*rho*h**2 21.331917 + alpha*h 7.789892 + L2reg 2.63 + L1reg 0.26 ; SHD = 87 ; DAG True
total norm for a microbatch 338.9746565287328 clip 13.748719700092343
total norm for a microbatch 180.94302647897607 clip 31.423795728973765
total norm for a microbatch 142.89048488626932 clip 161.15308239091183
total norm for a microbatch 154.20653101425205 clip 160.01973668843306
total norm for a microbatch 166.28366194916973 clip 169.7530959649413
total norm for a microbatch 156.081514636451 clip 150.87713300828796
cuda
Objective function 32.89 = squared loss an data 22.64 + 0.5*rho*h**2 3.792153 + alpha*h 3.284426 + L2reg 2.91 + L1reg 0.27 ; SHD = 86 ; DAG True
Proportion of microbatches that were clipped  0.8267716535433071
iteration 2 in inner loop, alpha 37.71390544090677 rho 1000.0 h 0.08708792320031478
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 67.02 = squared loss an data 22.64 + 0.5*rho*h**2 37.921532 + alpha*h 3.284426 + L2reg 2.91 + L1reg 0.27 ; SHD = 86 ; DAG True
total norm for a microbatch 1356.906598605079 clip 1.1023523130427815
total norm for a microbatch 261.3771758842348 clip 1.724667458972577
total norm for a microbatch 193.2541056440482 clip 5.069901930891112
total norm for a microbatch 242.0623124374569 clip 24.459772588059636
total norm for a microbatch 249.35196494529907 clip 251.9935268808268
total norm for a microbatch 242.69691104879124 clip 245.3628224559853
total norm for a microbatch 287.88906927835046 clip 221.08223681530797
total norm for a microbatch 294.60097106216716 clip 175.67499246557318
total norm for a microbatch 186.61568010627502 clip 175.16273458510045
total norm for a microbatch 208.03941382953226 clip 178.70338239066143
total norm for a microbatch 198.01107215492343 clip 176.97893006940234
total norm for a microbatch 271.9384256593721 clip 170.62628346204684
cuda
Objective function 30.59 = squared loss an data 21.57 + 0.5*rho*h**2 4.487449 + alpha*h 1.129838 + L2reg 3.13 + L1reg 0.28 ; SHD = 80 ; DAG True
Proportion of microbatches that were clipped  0.8260626844245953
iteration 3 in inner loop, alpha 37.71390544090677 rho 10000.0 h 0.029958134763312216
iteration 3 in outer loop, alpha = 337.29525307402895, rho = 10000.0, h = 0.029958134763312216
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 39.56 = squared loss an data 21.57 + 0.5*rho*h**2 4.487449 + alpha*h 10.104737 + L2reg 3.13 + L1reg 0.28 ; SHD = 80 ; DAG True
total norm for a microbatch 280.6217889567576 clip 25.04474568717045
total norm for a microbatch 322.7962766669469 clip 61.003513761105026
total norm for a microbatch 300.5349130867473 clip 167.70596421678044
total norm for a microbatch 392.2462832028874 clip 348.3664893108648
total norm for a microbatch 263.5634814952849 clip 233.70607803752748
total norm for a microbatch 207.5785468643395 clip 216.01659611391042
total norm for a microbatch 188.75285718673106 clip 201.19586671424565
total norm for a microbatch 174.07049622238807 clip 175.06906165653996
total norm for a microbatch 208.76703659717717 clip 175.06906165653996
total norm for a microbatch 215.05614429103815 clip 180.53047875957725
total norm for a microbatch 191.89962138538075 clip 205.37647450990872
total norm for a microbatch 182.57206936771337 clip 210.68132600356503
cuda
Objective function 34.90 = squared loss an data 21.65 + 0.5*rho*h**2 2.378183 + alpha*h 7.356104 + L2reg 3.24 + L1reg 0.28 ; SHD = 84 ; DAG True
Proportion of microbatches that were clipped  0.8273885350318472
iteration 1 in inner loop, alpha 337.29525307402895 rho 10000.0 h 0.02180909466525449
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 56.30 = squared loss an data 21.65 + 0.5*rho*h**2 23.781831 + alpha*h 7.356104 + L2reg 3.24 + L1reg 0.28 ; SHD = 84 ; DAG True
total norm for a microbatch 897.4677521144049 clip 7.1683676437658574
total norm for a microbatch 1126.2492997523598 clip 73.62502321720677
total norm for a microbatch 1246.1137565471877 clip 169.08077693103417
total norm for a microbatch 1889.7141041033703 clip 1587.2275081289345
total norm for a microbatch 412.7522843719232 clip 300.54407931271373
total norm for a microbatch 290.8858994432043 clip 248.6746664995388
total norm for a microbatch 170.6499273761226 clip 163.22186043734217
total norm for a microbatch 266.7084950885177 clip 160.33041817963243
total norm for a microbatch 174.62349598944238 clip 166.00365429346
total norm for a microbatch 169.4411467086011 clip 155.53570804541624
total norm for a microbatch 238.20647165108602 clip 159.61630091720755
cuda
Objective function 28.53 = squared loss an data 23.04 + 0.5*rho*h**2 0.671148 + alpha*h 1.235761 + L2reg 3.32 + L1reg 0.27 ; SHD = 84 ; DAG True
Proportion of microbatches that were clipped  0.8249859313449635
iteration 2 in inner loop, alpha 337.29525307402895 rho 100000.0 h 0.0036637363704450365
iteration 4 in outer loop, alpha = 703.6688901185325, rho = 100000.0, h = 0.0036637363704450365
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 29.88 = squared loss an data 23.04 + 0.5*rho*h**2 0.671148 + alpha*h 2.578057 + L2reg 3.32 + L1reg 0.27 ; SHD = 84 ; DAG True
total norm for a microbatch 823.4247252657759 clip 14.910992334882119
total norm for a microbatch 728.1715913198888 clip 16.170435013909188
total norm for a microbatch 694.3046666235662 clip 23.28079197829579
total norm for a microbatch 733.1872785535991 clip 28.18538658198441
total norm for a microbatch 2110.0108469104257 clip 678.2505316234354
total norm for a microbatch 1907.0246943566415 clip 1263.4750714756124
total norm for a microbatch 1499.018078995755 clip 1342.9153203097603
total norm for a microbatch 360.1356878830563 clip 378.09844261659714
total norm for a microbatch 256.947551588512 clip 250.43966295879372
total norm for a microbatch 277.9914874102842 clip 187.74640284936493
total norm for a microbatch 253.33182273525162 clip 177.82759239908236
cuda
Objective function 31.44 = squared loss an data 25.63 + 0.5*rho*h**2 0.323617 + alpha*h 1.790190 + L2reg 3.42 + L1reg 0.27 ; SHD = 85 ; DAG True
Proportion of microbatches that were clipped  0.8254222364524133
iteration 1 in inner loop, alpha 703.6688901185325 rho 100000.0 h 0.0025440800244425077
iteration 5 in outer loop, alpha = 3247.7489145610402, rho = 1000000.0, h = 0.0025440800244425077
Threshold 0.3
[[0.003 0.034 0.004 0.456 0.018 0.018 0.088 0.005 0.825 0.375 0.018 0.04
  0.006 0.029 0.408 0.745 0.016 0.003 0.481 0.023]
 [0.056 0.005 0.005 0.522 0.045 0.054 0.117 0.011 1.864 0.473 0.038 0.091
  0.018 0.093 1.038 0.333 0.318 0.001 0.457 0.043]
 [0.719 0.581 0.004 1.062 1.248 0.735 1.18  0.173 2.178 1.285 0.445 0.752
  0.471 1.107 2.304 0.855 0.369 0.024 2.591 0.638]
 [0.012 0.005 0.002 0.003 0.002 0.007 0.048 0.001 0.039 0.027 0.007 0.008
  0.003 0.009 0.054 0.132 0.015 0.001 0.041 0.002]
 [0.171 0.114 0.002 1.032 0.002 0.015 0.137 0.005 0.358 0.288 0.043 0.11
  0.025 0.174 0.353 0.435 0.096 0.004 0.206 0.041]
 [0.145 0.063 0.008 0.454 0.208 0.002 0.183 0.006 0.686 0.338 0.045 0.072
  0.013 0.337 1.156 0.468 0.021 0.002 0.591 0.028]
 [0.05  0.021 0.002 0.109 0.025 0.018 0.003 0.    0.225 0.153 0.016 0.011
  0.006 0.022 0.216 0.395 0.011 0.002 0.155 0.015]
 [0.786 0.42  0.019 1.477 0.767 0.582 4.288 0.004 1.074 1.174 0.562 0.309
  0.207 0.427 1.179 1.564 0.625 0.028 0.978 0.633]
 [0.004 0.001 0.001 0.078 0.012 0.003 0.019 0.002 0.003 0.027 0.005 0.003
  0.004 0.006 0.042 0.018 0.008 0.    0.046 0.003]
 [0.007 0.005 0.001 0.105 0.014 0.007 0.026 0.002 0.117 0.002 0.004 0.003
  0.001 0.005 0.046 0.079 0.003 0.001 0.024 0.006]
 [0.178 0.083 0.004 0.406 0.098 0.076 0.277 0.007 0.664 0.772 0.004 0.047
  0.038 0.075 0.505 0.15  0.014 0.002 0.248 0.09 ]
 [0.095 0.04  0.005 0.336 0.035 0.05  0.304 0.01  0.929 0.671 0.083 0.003
  0.034 0.073 0.785 0.54  0.021 0.004 0.392 0.035]
 [0.442 0.248 0.013 0.776 0.134 0.113 0.481 0.015 0.612 2.21  0.064 0.14
  0.002 0.064 0.477 0.386 0.084 0.007 0.308 0.18 ]
 [0.121 0.04  0.002 0.486 0.01  0.009 0.204 0.008 0.471 0.624 0.065 0.046
  0.031 0.003 0.356 0.311 0.015 0.003 0.27  0.03 ]
 [0.008 0.002 0.001 0.08  0.01  0.002 0.017 0.002 0.112 0.115 0.005 0.003
  0.007 0.011 0.002 0.022 0.003 0.    0.041 0.007]
 [0.004 0.01  0.002 0.047 0.009 0.006 0.01  0.001 0.266 0.026 0.017 0.004
  0.004 0.005 0.212 0.005 0.004 0.002 0.029 0.01 ]
 [0.155 0.019 0.016 0.242 0.061 0.156 0.34  0.004 0.335 1.214 0.21  0.117
  0.048 0.229 1.235 0.907 0.002 0.002 0.229 0.039]
 [0.88  3.107 0.183 1.238 0.78  0.668 0.942 0.113 1.178 1.241 0.734 0.707
  0.391 0.609 1.027 1.355 1.4   0.002 1.236 0.604]
 [0.005 0.007 0.001 0.113 0.013 0.006 0.02  0.003 0.085 0.158 0.015 0.011
  0.012 0.015 0.103 0.142 0.012 0.001 0.003 0.004]
 [0.179 0.105 0.007 1.721 0.111 0.135 0.185 0.006 1.08  0.628 0.048 0.115
  0.035 0.132 0.396 0.271 0.121 0.005 0.577 0.005]]
[[0.    0.    0.    0.456 0.    0.    0.    0.    0.825 0.375 0.    0.
  0.    0.    0.408 0.745 0.    0.    0.481 0.   ]
 [0.    0.    0.    0.522 0.    0.    0.    0.    1.864 0.473 0.    0.
  0.    0.    1.038 0.333 0.318 0.    0.457 0.   ]
 [0.719 0.581 0.    1.062 1.248 0.735 1.18  0.    2.178 1.285 0.445 0.752
  0.471 1.107 2.304 0.855 0.369 0.    2.591 0.638]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.032 0.    0.    0.    0.    0.358 0.    0.    0.
  0.    0.    0.353 0.435 0.    0.    0.    0.   ]
 [0.    0.    0.    0.454 0.    0.    0.    0.    0.686 0.338 0.    0.
  0.    0.337 1.156 0.468 0.    0.    0.591 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.395 0.    0.    0.    0.   ]
 [0.786 0.42  0.    1.477 0.767 0.582 4.288 0.    1.074 1.174 0.562 0.309
  0.    0.427 1.179 1.564 0.625 0.    0.978 0.633]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.406 0.    0.    0.    0.    0.664 0.772 0.    0.
  0.    0.    0.505 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.336 0.    0.    0.304 0.    0.929 0.671 0.    0.
  0.    0.    0.785 0.54  0.    0.    0.392 0.   ]
 [0.442 0.    0.    0.776 0.    0.    0.481 0.    0.612 2.21  0.    0.
  0.    0.    0.477 0.386 0.    0.    0.308 0.   ]
 [0.    0.    0.    0.486 0.    0.    0.    0.    0.471 0.624 0.    0.
  0.    0.    0.356 0.311 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.34  0.    0.335 1.214 0.    0.
  0.    0.    1.235 0.907 0.    0.    0.    0.   ]
 [0.88  3.107 0.    1.238 0.78  0.668 0.942 0.    1.178 1.241 0.734 0.707
  0.391 0.609 1.027 1.355 1.4   0.    1.236 0.604]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.721 0.    0.    0.    0.    1.08  0.628 0.    0.
  0.    0.    0.396 0.    0.    0.    0.577 0.   ]]
{'fdr': 0.7247706422018348, 'tpr': 0.75, 'fpr': 0.5266666666666666, 'f1': 0.40268456375838924, 'shd': 85, 'npred': 109, 'ntrue': 40}
[3.374e-02 3.808e-03 4.564e-01 1.802e-02 1.778e-02 8.759e-02 4.680e-03
 8.254e-01 3.754e-01 1.782e-02 4.039e-02 6.214e-03 2.940e-02 4.076e-01
 7.450e-01 1.591e-02 3.023e-03 4.814e-01 2.274e-02 5.551e-02 5.119e-03
 5.220e-01 4.544e-02 5.372e-02 1.170e-01 1.057e-02 1.864e+00 4.726e-01
 3.820e-02 9.142e-02 1.793e-02 9.310e-02 1.038e+00 3.333e-01 3.180e-01
 5.400e-04 4.570e-01 4.302e-02 7.189e-01 5.809e-01 1.062e+00 1.248e+00
 7.345e-01 1.180e+00 1.727e-01 2.178e+00 1.285e+00 4.451e-01 7.515e-01
 4.709e-01 1.107e+00 2.304e+00 8.551e-01 3.694e-01 2.370e-02 2.591e+00
 6.380e-01 1.177e-02 5.046e-03 1.647e-03 2.091e-03 6.816e-03 4.810e-02
 9.255e-04 3.891e-02 2.737e-02 6.851e-03 8.440e-03 3.355e-03 9.309e-03
 5.410e-02 1.325e-01 1.527e-02 7.539e-04 4.079e-02 1.655e-03 1.709e-01
 1.142e-01 1.888e-03 1.032e+00 1.490e-02 1.369e-01 4.874e-03 3.577e-01
 2.884e-01 4.309e-02 1.096e-01 2.521e-02 1.739e-01 3.530e-01 4.354e-01
 9.581e-02 3.595e-03 2.057e-01 4.054e-02 1.445e-01 6.314e-02 8.031e-03
 4.536e-01 2.084e-01 1.828e-01 6.243e-03 6.857e-01 3.382e-01 4.471e-02
 7.190e-02 1.266e-02 3.375e-01 1.156e+00 4.676e-01 2.150e-02 2.371e-03
 5.914e-01 2.831e-02 4.991e-02 2.104e-02 1.694e-03 1.088e-01 2.526e-02
 1.836e-02 2.825e-04 2.249e-01 1.528e-01 1.586e-02 1.135e-02 5.600e-03
 2.242e-02 2.159e-01 3.954e-01 1.063e-02 2.242e-03 1.548e-01 1.481e-02
 7.862e-01 4.201e-01 1.919e-02 1.477e+00 7.674e-01 5.825e-01 4.288e+00
 1.074e+00 1.174e+00 5.621e-01 3.086e-01 2.071e-01 4.275e-01 1.179e+00
 1.564e+00 6.252e-01 2.811e-02 9.780e-01 6.330e-01 4.283e-03 1.240e-03
 8.705e-04 7.752e-02 1.211e-02 3.252e-03 1.902e-02 1.581e-03 2.693e-02
 4.552e-03 2.759e-03 4.043e-03 6.014e-03 4.192e-02 1.753e-02 7.788e-03
 2.742e-04 4.585e-02 2.780e-03 6.726e-03 5.263e-03 1.045e-03 1.053e-01
 1.368e-02 7.375e-03 2.629e-02 1.788e-03 1.171e-01 3.798e-03 2.874e-03
 8.149e-04 5.083e-03 4.626e-02 7.858e-02 2.825e-03 7.937e-04 2.380e-02
 5.961e-03 1.778e-01 8.277e-02 3.859e-03 4.057e-01 9.752e-02 7.589e-02
 2.773e-01 6.973e-03 6.643e-01 7.719e-01 4.734e-02 3.811e-02 7.547e-02
 5.049e-01 1.502e-01 1.426e-02 2.250e-03 2.478e-01 8.977e-02 9.517e-02
 3.982e-02 4.799e-03 3.365e-01 3.517e-02 5.043e-02 3.043e-01 1.047e-02
 9.295e-01 6.708e-01 8.266e-02 3.438e-02 7.288e-02 7.852e-01 5.398e-01
 2.054e-02 3.591e-03 3.922e-01 3.465e-02 4.417e-01 2.480e-01 1.341e-02
 7.759e-01 1.340e-01 1.128e-01 4.809e-01 1.542e-02 6.120e-01 2.210e+00
 6.414e-02 1.398e-01 6.407e-02 4.766e-01 3.857e-01 8.439e-02 7.055e-03
 3.084e-01 1.796e-01 1.212e-01 3.953e-02 2.459e-03 4.859e-01 1.001e-02
 9.470e-03 2.038e-01 8.334e-03 4.710e-01 6.237e-01 6.482e-02 4.600e-02
 3.122e-02 3.565e-01 3.110e-01 1.464e-02 3.058e-03 2.702e-01 2.966e-02
 8.394e-03 2.447e-03 9.003e-04 7.966e-02 1.005e-02 2.013e-03 1.677e-02
 1.930e-03 1.121e-01 1.147e-01 5.309e-03 3.369e-03 6.869e-03 1.111e-02
 2.161e-02 2.846e-03 3.701e-04 4.083e-02 6.830e-03 3.746e-03 9.966e-03
 2.021e-03 4.651e-02 9.279e-03 6.477e-03 9.722e-03 1.139e-03 2.658e-01
 2.629e-02 1.699e-02 3.818e-03 4.290e-03 5.013e-03 2.120e-01 4.359e-03
 1.774e-03 2.924e-02 9.817e-03 1.554e-01 1.930e-02 1.582e-02 2.422e-01
 6.134e-02 1.562e-01 3.400e-01 3.964e-03 3.355e-01 1.214e+00 2.095e-01
 1.171e-01 4.850e-02 2.287e-01 1.235e+00 9.075e-01 1.565e-03 2.293e-01
 3.906e-02 8.800e-01 3.107e+00 1.833e-01 1.238e+00 7.804e-01 6.677e-01
 9.421e-01 1.127e-01 1.178e+00 1.241e+00 7.338e-01 7.074e-01 3.910e-01
 6.091e-01 1.027e+00 1.355e+00 1.400e+00 1.236e+00 6.039e-01 5.176e-03
 6.740e-03 9.165e-04 1.133e-01 1.298e-02 6.500e-03 1.979e-02 2.559e-03
 8.474e-02 1.582e-01 1.545e-02 1.076e-02 1.205e-02 1.502e-02 1.030e-01
 1.418e-01 1.208e-02 1.105e-03 3.770e-03 1.795e-01 1.053e-01 6.721e-03
 1.721e+00 1.110e-01 1.353e-01 1.851e-01 5.990e-03 1.080e+00 6.277e-01
 4.828e-02 1.155e-01 3.478e-02 1.323e-01 3.959e-01 2.710e-01 1.208e-01
 5.203e-03 5.772e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8044117647058823, 0.4864015455041516)
Iterations 2500
Achieves (13.610683049707138, 1e-05)-DP
