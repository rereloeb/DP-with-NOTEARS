samples  5000  SCM  10 2 RE mim  minibatch size  50  noise  0.6  minibatches per NN training  250  DP methodology  adaclip 0.0  box penalty  0
cpu
max degree  2.0  complexity indicator 1  625.0  complexity indicator 2  1086.9565217391305
model created without box penalty
model created without box penalty
cpu
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.1601221069006407
iteration 1 in outer loop, alpha = 1.1601221069006407, rho = 1.0, h = 1.1601221069006407
model created without box penalty
cpu
iteration 1 in inner loop,alpha 1.1601221069006407 rho 1.0 h 0.697926845742554
iteration 2 in inner loop,alpha 1.1601221069006407 rho 10.0 h 0.29175731529299753
iteration 3 in inner loop,alpha 1.1601221069006407 rho 100.0 h 0.08501950555491611
iteration 2 in outer loop, alpha = 9.662072662392251, rho = 100.0, h = 0.08501950555491611
model created without box penalty
cpu
iteration 1 in inner loop,alpha 9.662072662392251 rho 100.0 h 0.040170488512167424
iteration 2 in inner loop,alpha 9.662072662392251 rho 1000.0 h 0.00868366913203289
iteration 3 in outer loop, alpha = 18.34574179442514, rho = 1000.0, h = 0.00868366913203289
model created without box penalty
cpu
iteration 1 in inner loop,alpha 18.34574179442514 rho 1000.0 h 0.0030868790422431402
iteration 2 in inner loop,alpha 18.34574179442514 rho 10000.0 h 0.0008995705368377571
iteration 4 in outer loop, alpha = 27.341447162802712, rho = 10000.0, h = 0.0008995705368377571
model created without box penalty
cpu
iteration 1 in inner loop,alpha 27.341447162802712 rho 10000.0 h 0.0006484053062099093
iteration 2 in inner loop,alpha 27.341447162802712 rho 100000.0 h 0.0002616647124735749
iteration 5 in outer loop, alpha = 289.0061596363776, rho = 1000000.0, h = 0.0002616647124735749
Threshold 0.3
[[0.01  0.    2.439 0.    0.313 0.    0.201 0.    0.    0.052]
 [3.167 0.    0.255 0.    0.057 0.    0.025 0.    0.    0.049]
 [0.001 0.    0.003 0.    0.02  0.    3.039 0.    0.    0.016]
 [2.626 0.    0.121 0.    0.976 0.006 0.072 0.    0.    0.013]
 [0.001 0.    0.015 0.    0.003 0.    0.011 0.    0.    0.001]
 [0.021 0.007 0.019 0.    0.045 0.    0.033 0.    0.005 0.045]
 [0.    0.    0.    0.    0.004 0.    0.009 0.    0.    0.001]
 [0.045 0.    0.003 0.    0.143 0.009 0.081 0.    0.    3.237]
 [0.04  0.    0.064 0.004 0.117 0.    2.722 0.    0.    2.883]
 [0.016 0.    0.012 0.    2.19  0.    0.301 0.    0.    0.008]]
[[0.    0.    2.439 0.    0.313 0.    0.    0.    0.    0.   ]
 [3.167 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    3.039 0.    0.    0.   ]
 [2.626 0.    0.    0.    0.976 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.237]
 [0.    0.    0.    0.    0.    0.    2.722 0.    0.    2.883]
 [0.    0.    0.    0.    2.19  0.    0.301 0.    0.    0.   ]]
{'fdr': 0.18181818181818182, 'tpr': 1.0, 'fpr': 0.05555555555555555, 'f1': 0.8999999999999999, 'shd': 2, 'npred': 11, 'ntrue': 9}
[3.269e-06 2.439e+00 2.360e-05 3.127e-01 1.057e-04 2.009e-01 1.136e-04
 6.644e-05 5.173e-02 3.167e+00 2.553e-01 1.845e-04 5.678e-02 1.726e-04
 2.526e-02 1.867e-04 1.036e-04 4.862e-02 6.936e-04 1.064e-05 4.596e-06
 2.025e-02 6.305e-05 3.039e+00 1.164e-04 1.147e-04 1.567e-02 2.626e+00
 1.722e-04 1.206e-01 9.764e-01 5.872e-03 7.239e-02 3.245e-04 5.762e-05
 1.254e-02 9.900e-04 2.162e-05 1.487e-02 2.243e-05 6.487e-05 1.146e-02
 5.745e-06 4.002e-06 6.108e-04 2.138e-02 6.722e-03 1.936e-02 2.459e-04
 4.486e-02 3.297e-02 1.213e-04 5.333e-03 4.493e-02 7.180e-05 2.638e-05
 9.349e-05 2.986e-06 4.291e-03 1.041e-04 1.811e-04 4.376e-06 1.285e-03
 4.487e-02 1.554e-04 2.565e-03 8.778e-05 1.427e-01 8.665e-03 8.118e-02
 8.880e-05 3.237e+00 4.033e-02 3.416e-04 6.358e-02 4.082e-03 1.167e-01
 8.093e-05 2.722e+00 2.648e-04 2.883e+00 1.620e-02 1.016e-04 1.176e-02
 1.046e-04 2.190e+00 2.991e-05 3.007e-01 3.538e-06 1.440e-05]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (1.0, 1.0)
model created without box penalty
cpu
1210
cpu
[0.559 0.633 0.599 0.618 0.582 0.65  0.581 0.777 0.565 0.59  0.563 0.595
 0.599 0.652 0.572 0.425 0.563 0.623 0.674 0.547 0.67  0.464 0.587 0.633
 0.513 0.67  0.619 0.664 0.569 0.534 0.421 0.561 0.638 0.437 0.543 0.523
 0.602 0.65  0.636 0.545 0.456 0.5   0.452 0.503 0.559 0.545 0.595 0.518
 0.583 0.615 0.586 0.566 0.608 0.69  0.562 0.643 0.552 0.466 0.638 0.594
 0.626 0.59  0.652 0.749 0.536 0.64  0.519 0.682 0.581 0.683 0.469 0.408
 0.488 0.647 0.596 0.555 0.586 0.613 0.659 0.696 0.579 0.623 0.589 0.663
 0.464 0.539 0.399 0.605 0.732 0.502]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.49382716049382713, 0.11721121018714775)
Objective function 186.49 = squared loss an data 9.43 + 0.5*rho*h**2 176.710652 + alpha*h 0.000000 + L2reg 0.19 + L1reg 0.16 ; SHD = 55 ; DAG False
||w||^2 7909.557555894106
exp ma of ||w||^2 1785038324.5684857
||w|| 88.93569337388733
exp ma of ||w|| 1115.678222368671
||w||^2 1540.2516525647357
exp ma of ||w||^2 29867820.138002288
||w|| 39.246039960290716
exp ma of ||w|| 61.11949363229078
||w||^2 0.47673541874228664
exp ma of ||w||^2 2.189586567351884
||w|| 0.6904602948340235
exp ma of ||w|| 0.9318858285675391
||w||^2 0.10595593965176289
exp ma of ||w||^2 0.08812904600966316
||w|| 0.3255087397471271
exp ma of ||w|| 0.28301470790462124
||w||^2 0.01964983985881717
exp ma of ||w||^2 0.03115455218750287
||w|| 0.1401778864829156
exp ma of ||w|| 0.17018999129225337
||w||^2 0.02632670687444899
exp ma of ||w||^2 0.032350915453245546
||w|| 0.16225506733057365
exp ma of ||w|| 0.17332190301958608
||w||^2 0.034915552025651796
exp ma of ||w||^2 0.03206262285108534
||w|| 0.18685703632898548
exp ma of ||w|| 0.17252434074719675
||w||^2 0.014331831026852922
exp ma of ||w||^2 0.031661773533696855
||w|| 0.11971562565869555
exp ma of ||w|| 0.17093730580709993
||w||^2 0.020029003469732944
exp ma of ||w||^2 0.030953968860784095
||w|| 0.14152386183867702
exp ma of ||w|| 0.1690737846751558
||w||^2 0.011194498586900192
exp ma of ||w||^2 0.029283060262749863
||w|| 0.10580405751624175
exp ma of ||w|| 0.16543271474321178
||w||^2 0.05110120938758027
exp ma of ||w||^2 0.0313603601257549
||w|| 0.22605576610115538
exp ma of ||w|| 0.16942546499967395
||w||^2 0.019948643559534646
exp ma of ||w||^2 0.029365191913394068
||w|| 0.14123966708943578
exp ma of ||w|| 0.1659359526409442
||w||^2 0.053007472243002215
exp ma of ||w||^2 0.0308446070343891
||w|| 0.23023351676722095
exp ma of ||w|| 0.17078380399627102
||w||^2 0.013217477585802929
exp ma of ||w||^2 0.030787710038418986
||w|| 0.11496728919915843
exp ma of ||w|| 0.16942739351655056
cpu
[0.233 0.366 0.245 0.275 0.304 0.499 0.265 0.268 0.209 0.568 0.242 0.471
 0.435 0.242 0.513 0.344 0.228 0.279 0.43  0.317 0.257 0.354 0.317 0.281
 0.238 0.318 0.296 0.586 0.303 0.391 0.352 0.36  0.378 0.37  0.297 0.44
 0.24  0.298 0.308 0.214 0.341 0.525 0.256 0.257 0.656 0.339 0.34  0.338
 0.302 0.374 0.314 0.325 0.236 0.232 0.257 0.254 0.367 0.201 0.31  0.258
 0.172 0.28  0.367 0.424 0.265 0.296 0.365 0.482 0.444 0.444 0.377 0.784
 0.267 0.391 0.415 0.384 0.356 0.304 0.587 0.213 0.414 0.308 0.279 0.271
 0.32  0.257 0.256 0.339 0.17  0.16 ]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7626886145404664, 0.46573744785033877)
Objective function 7.22 = squared loss an data 6.04 + 0.5*rho*h**2 0.883656 + alpha*h 0.000000 + L2reg 0.21 + L1reg 0.09 ; SHD = 36 ; DAG False
Proportion of microbatches that were clipped  0.22391653290529695
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.3294032160106362
iteration 1 in outer loop, alpha = 1.3294032160106362, rho = 1.0, h = 1.3294032160106362
cpu
1210
cpu
[0.233 0.366 0.245 0.275 0.304 0.499 0.265 0.268 0.209 0.568 0.242 0.471
 0.435 0.242 0.513 0.344 0.228 0.279 0.43  0.317 0.257 0.354 0.317 0.281
 0.238 0.318 0.296 0.586 0.303 0.391 0.352 0.36  0.378 0.37  0.297 0.44
 0.24  0.298 0.308 0.214 0.341 0.525 0.256 0.257 0.656 0.339 0.34  0.338
 0.302 0.374 0.314 0.325 0.236 0.232 0.257 0.254 0.367 0.201 0.31  0.258
 0.172 0.28  0.367 0.424 0.265 0.296 0.365 0.482 0.444 0.444 0.377 0.784
 0.267 0.391 0.415 0.384 0.356 0.304 0.587 0.213 0.414 0.308 0.279 0.271
 0.32  0.257 0.256 0.339 0.17  0.16 ]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7626886145404664, 0.46573744785033877)
Objective function 8.99 = squared loss an data 6.04 + 0.5*rho*h**2 0.883656 + alpha*h 1.767313 + L2reg 0.21 + L1reg 0.09 ; SHD = 36 ; DAG False
||w||^2 2103518.0892894235
exp ma of ||w||^2 1728020.9112627888
||w|| 1450.3510227835961
exp ma of ||w|| 1195.7367568588752
||w||^2 938954.2483287606
exp ma of ||w||^2 1326598.003851443
||w|| 968.9965161592485
exp ma of ||w|| 1043.113801601805
||w||^2 5331.032535929063
exp ma of ||w||^2 32145.26979986525
||w|| 73.01392015177012
exp ma of ||w|| 160.63257086923758
||w||^2 2764.266273856068
exp ma of ||w||^2 6288.046185968564
||w|| 52.57629003511058
exp ma of ||w|| 70.93016124414697
||w||^2 5667.110978321196
exp ma of ||w||^2 5095.992647682253
||w|| 75.28021638067464
exp ma of ||w|| 63.200134173398794
||w||^2 504.50241326171573
exp ma of ||w||^2 3149.525186147406
||w|| 22.461131166121525
exp ma of ||w|| 49.53861229886995
||w||^2 0.030648009351407223
exp ma of ||w||^2 0.0986458338831535
||w|| 0.17506572866042977
exp ma of ||w|| 0.2972597675711028
||w||^2 0.04374817977178048
exp ma of ||w||^2 0.06061361502513931
||w|| 0.20916065541057305
exp ma of ||w|| 0.23480197887182194
||w||^2 0.038127319972701836
exp ma of ||w||^2 0.043032415742414995
||w|| 0.19526218264861692
exp ma of ||w|| 0.19805789411517238
||w||^2 0.01378233975524625
exp ma of ||w||^2 0.04638000862363845
||w|| 0.1173982101875759
exp ma of ||w|| 0.20346351838232024
||w||^2 0.033348457632005175
exp ma of ||w||^2 0.04408208611409226
||w|| 0.18261560073554825
exp ma of ||w|| 0.19840741057441635
v before min max tensor([[ 4.811e+01, -2.107e+01, -1.355e+01, -1.612e+01, -1.177e+01, -1.949e+01,
          2.763e+01, -2.144e+01,  1.200e+02, -1.364e+01],
        [-2.172e+01, -1.827e+01,  1.896e+02, -5.517e+00,  6.887e+00, -1.166e+01,
         -5.446e+00, -1.339e+01,  2.552e+01, -2.080e+01],
        [ 2.787e+02, -1.898e+01, -1.723e+01, -2.024e+01, -1.890e+01, -2.037e+01,
          6.517e+01, -2.073e+01,  6.455e+01,  4.299e+01],
        [ 2.925e+01, -7.977e+00, -2.342e+01,  1.444e+01,  1.088e+01, -1.919e+01,
          7.887e+01,  1.632e+01, -1.621e+00, -2.052e+01],
        [ 8.648e+01, -9.323e+00, -9.102e+00, -1.662e+01, -1.125e+01,  3.290e+00,
          5.549e+01, -2.437e+01,  3.259e+01,  6.403e+01],
        [ 1.930e+00,  6.655e+01, -1.926e+01,  5.968e-01,  2.751e+01,  1.354e+01,
          6.941e+01, -1.498e+01,  1.984e+01, -2.114e+01],
        [-1.563e+01, -1.856e+01, -2.251e+01, -1.019e+01, -9.613e+00, -8.279e+00,
         -2.835e+01,  1.131e+02, -7.038e+00, -2.327e+01],
        [-2.127e+01,  6.989e+01,  3.944e+01, -2.283e+01,  5.734e+01, -1.807e+01,
         -2.137e+01, -2.401e+01, -1.713e+01, -2.397e+01],
        [-2.572e+01, -1.035e+01, -1.252e+01, -2.080e+01,  7.891e+00, -1.303e+01,
         -2.417e+01, -1.832e+01, -1.767e+01, -2.228e+01],
        [-1.517e+01, -4.154e+00, -1.716e+01,  1.549e+00, -3.259e+01, -1.014e+01,
         -2.516e+01,  4.965e+01, -1.653e+01, -1.888e+01],
        [-1.123e+01, -1.290e+01, -1.969e+01,  7.873e+01, -1.080e+01, -2.469e+01,
         -1.392e+01, -2.924e+01, -1.794e+01, -2.063e+01],
        [ 5.884e+01,  1.658e+01, -2.025e+01, -1.382e+01, -1.373e+01,  8.530e-01,
         -1.747e+01,  8.035e+01, -2.484e+01,  1.465e+02],
        [-1.165e+01,  2.743e+01, -1.869e+01, -1.157e+01, -2.960e+00, -4.851e+00,
         -1.773e+01, -1.830e+01, -2.473e+01, -8.084e+00],
        [-7.673e+00,  4.483e+00, -2.411e+01, -1.027e+01, -2.284e+01, -1.488e+01,
         -1.582e+01, -2.806e+01, -2.315e+01, -2.220e+01],
        [-1.305e+01, -1.879e+01, -2.502e+01, -3.872e+00, -2.114e+01,  3.134e+01,
         -2.327e+01, -1.879e+01,  2.429e+01,  1.411e+01],
        [-4.585e+00, -1.552e+01,  5.365e+00,  2.285e+00,  1.030e+01, -2.590e+01,
          2.809e+01,  4.281e+01,  9.667e+00, -6.627e+00],
        [-1.866e+01,  1.540e+02,  9.319e+01,  2.604e+01,  1.421e+02, -1.664e+01,
         -9.809e+00, -2.398e+01,  3.084e+01, -2.029e+00],
        [-2.547e+01, -9.025e+00,  2.496e+01, -2.408e+01, -1.164e+01, -9.986e+00,
         -2.156e+01, -1.642e+01, -2.229e+01, -2.339e+01],
        [-2.148e+01,  7.904e+01, -1.942e+00,  2.453e+00, -2.419e+01, -2.121e+01,
         -1.422e+01,  8.356e+00,  6.397e+01,  2.128e+01],
        [-2.333e+01,  1.632e+00, -2.024e+01, -2.473e+01, -1.832e+01,  6.151e+00,
         -1.960e+01, -1.790e+01,  3.942e+01,  6.450e+01],
        [ 2.240e+01, -5.236e+00,  1.719e+00, -1.510e+01, -9.233e+00,  7.623e-01,
          2.533e+01, -1.950e+01,  4.099e+00,  3.768e+01],
        [ 2.480e+00, -2.085e+01, -1.110e+01, -6.498e-01,  4.450e+01, -2.713e+01,
         -1.648e+01,  8.425e+01, -2.627e+01, -1.874e+01],
        [-1.696e+01,  1.869e+01, -1.982e+01, -2.118e+01,  3.329e+01, -2.306e+01,
         -2.917e+01, -6.938e+00, -1.354e+01, -1.868e+01],
        [-1.410e+01,  3.623e+01, -1.731e+01, -1.282e+01,  3.233e+01,  4.296e+01,
         -1.569e+01, -1.864e+01, -1.937e+01, -2.042e+01],
        [ 6.032e+01, -1.600e+00,  1.421e+02, -1.872e+01, -2.189e+01, -1.804e+01,
          1.734e+01, -1.296e+01, -2.459e+01,  1.158e+01],
        [-1.456e+01, -1.872e+01, -2.198e+01,  3.329e+01,  1.737e+01, -9.082e+00,
         -7.241e+00, -1.053e+01, -9.162e+00, -1.607e+01],
        [ 4.212e+01,  1.609e+01, -2.216e+01,  1.461e+01, -2.280e+01, -2.677e-01,
         -1.396e+01,  1.099e+01, -1.218e+01,  3.723e+01],
        [ 2.283e+01,  1.921e+01, -2.094e+01, -4.317e+00, -2.121e+01,  5.123e+00,
         -1.082e+01, -2.393e+01, -2.426e+01,  7.765e+01],
        [-2.184e+01, -1.284e+01, -1.376e+01, -3.371e+01, -2.109e+01, -1.120e+01,
         -8.175e+00, -2.079e+01,  4.361e+00, -1.364e+01],
        [-1.349e+00, -2.928e+01, -2.152e+01, -1.625e+01, -1.344e+01, -2.058e+01,
         -2.087e+01, -2.200e+01, -2.660e+01, -1.625e+01],
        [ 1.079e+00, -1.215e+01,  2.678e+01, -2.134e+00,  1.203e+00, -1.729e+01,
         -1.528e+01, -4.402e+00,  9.578e+00, -2.206e+01],
        [-1.988e+01, -6.530e+00, -1.858e+01, -1.999e+01, -2.107e+01, -5.672e+00,
         -1.372e+01,  3.240e+01,  1.311e+01,  8.059e+01],
        [-3.499e-01, -2.117e+01, -2.190e+01, -1.752e+01, -7.109e+00,  3.744e+01,
         -1.098e+01, -1.616e+01, -4.847e+00,  1.032e+01],
        [-2.256e+01,  3.990e+01,  4.712e+01, -1.761e+01,  3.495e+00, -9.560e+00,
         -2.032e+01, -2.015e+01,  2.019e+00, -2.197e+01],
        [ 6.600e+00, -1.629e+01, -2.092e+01, -4.231e+00, -1.956e+01,  1.978e+01,
          6.150e+01,  6.442e+01, -1.813e+01,  4.862e+01],
        [-8.415e+00, -1.852e+01, -2.536e+01,  2.506e+01,  1.080e+01, -2.045e+01,
          1.233e+01,  1.206e+01, -1.227e+01, -1.324e+01],
        [-1.141e+01,  2.071e+01, -2.366e+01, -2.176e+01, -7.545e+00, -2.628e+01,
         -3.526e+00, -1.336e+01, -2.134e+01,  7.035e+01],
        [-2.410e+01, -2.278e+01, -3.345e+01, -1.556e+01, -1.009e+01, -1.025e+01,
         -7.717e+00,  2.216e+01, -2.503e+01, -2.725e+01],
        [ 6.797e+00, -4.640e+00, -1.934e+01, -1.223e+01, -2.975e+00, -8.012e+00,
         -1.114e+01, -1.423e+01, -8.736e+00, -2.902e+01],
        [-1.727e+01,  3.181e+01,  4.906e+01,  2.564e+00, -1.430e+01,  3.585e+01,
         -2.254e+01, -7.050e+00, -9.690e+00,  6.738e+01],
        [-2.021e+01, -7.813e-01, -2.103e+01, -2.656e+01, -1.015e+01, -2.053e+01,
         -8.852e+00,  1.365e+02, -3.508e+00, -7.808e-01],
        [-2.429e+01, -2.447e+01, -6.700e+00, -1.575e+01, -2.368e+01, -2.372e+01,
         -4.228e+00,  6.882e+00, -1.989e+01, -1.373e+01],
        [-3.013e+01, -9.935e+00, -1.034e+01,  4.754e+01, -2.118e+01, -1.285e+01,
          2.017e+00, -8.654e+00, -1.873e+01, -9.764e+00],
        [-5.486e+00,  1.375e+01, -2.765e+01,  1.941e+01, -1.253e+01, -1.480e+01,
         -1.716e+01,  1.150e+01, -5.856e+00, -1.189e+01],
        [-2.011e+01,  2.253e+01,  3.257e+00, -2.269e+01,  5.334e+01,  4.541e+01,
          2.598e+01,  3.440e+01, -9.978e+00, -2.207e+01],
        [-1.019e+01,  4.072e+01,  1.064e+02,  6.478e+00,  1.912e+02, -1.548e+01,
          1.957e+00, -1.862e+01, -1.858e+01, -1.738e+00],
        [ 1.230e+01, -1.233e+01, -2.653e+01,  1.705e+01, -1.071e+01,  1.189e+01,
          1.070e+01, -2.922e+01,  5.825e+00, -8.555e+00],
        [ 4.491e+01, -1.137e+01, -2.495e+01,  1.623e+01,  4.212e+01, -1.469e+01,
         -2.901e+01, -2.072e+01, -1.941e+01, -2.267e+01],
        [-2.026e+01, -1.802e+01,  1.254e+01, -1.488e+01, -2.532e+01,  5.778e+01,
          6.016e+00, -1.209e+01, -1.949e+01, -2.598e+01],
        [-7.670e+00, -7.331e+00, -2.049e+01, -2.092e+01, -2.517e+01, -2.525e+01,
         -1.775e+01, -1.906e+01,  4.017e+01, -5.249e+00],
        [-1.609e+01, -1.811e+01, -2.669e+01, -1.337e+01, -1.626e+01,  6.327e+01,
         -6.251e+00,  1.067e+01, -2.611e+01, -2.597e+01],
        [-1.365e+01,  7.716e+01, -1.746e+01, -2.487e+01, -8.164e+00,  9.905e+01,
         -2.501e+00, -1.736e+01, -2.194e+01,  4.108e+00],
        [ 8.713e+00, -1.810e+01, -1.374e+01, -2.083e+01,  1.715e+01, -2.525e+01,
         -2.602e+01, -1.759e+01, -2.450e+01, -2.031e+01],
        [-1.343e+01,  3.017e+00,  3.438e-01, -1.907e+00, -2.498e+01,  1.017e+02,
         -2.527e+01,  7.395e+01, -1.935e+01, -2.279e+01],
        [-6.536e+00, -2.516e+01,  3.810e+01, -3.982e+00,  2.083e+01, -5.890e+00,
          1.526e+02,  6.413e+00, -2.293e+01, -2.311e+01],
        [-1.601e+01, -1.604e+01, -9.655e+00, -1.980e+01, -6.806e+00,  2.293e+02,
          1.944e+01, -2.400e+01, -1.145e+01, -6.629e+00],
        [-1.651e+01, -3.897e+00,  3.748e+00,  1.045e+02, -1.486e+01, -2.569e+01,
         -1.260e+01, -1.973e+01, -5.440e+00, -2.560e+01],
        [-2.006e+01,  2.406e+01, -3.000e+00,  1.485e+01,  1.317e+01, -1.695e+01,
         -1.046e+01, -1.357e+01, -2.557e+01,  1.271e+01],
        [-2.105e+01, -7.415e+00, -1.115e+01, -2.118e+01,  2.371e+01,  7.466e+00,
          5.643e+01, -8.560e+00, -1.947e+01,  2.416e+01],
        [-2.028e+01, -5.716e+00, -2.215e+01, -2.135e+01, -2.263e+01,  5.603e+01,
         -1.730e+00, -3.374e+01,  2.963e+00, -1.998e+01],
        [ 2.462e+01, -1.657e+01, -2.275e+01, -2.677e-01, -1.028e+01, -2.204e+01,
          7.999e+00,  1.598e+01,  2.624e+01,  3.048e+01],
        [-3.053e+01, -1.110e+01,  6.552e-01, -9.688e+00,  5.215e+01,  1.194e+01,
          5.077e+01, -2.273e+01,  7.237e+00, -1.141e+01],
        [-1.952e+01, -5.072e+00,  1.046e+01, -7.510e+00, -2.138e+01, -1.238e+01,
         -2.460e+01,  8.191e+00, -2.815e+01,  3.634e+01],
        [-2.149e+01,  2.526e+01, -2.187e+01, -2.163e+01, -1.801e+01, -1.728e+01,
         -2.542e+01, -1.065e+01, -2.374e+01,  1.295e+01],
        [-2.068e+01, -1.821e+01,  6.373e+01, -4.822e+00, -3.102e+01,  4.307e+00,
          3.111e+00,  1.280e+01, -1.480e+01, -7.914e+00],
        [-1.611e+01,  2.465e-01, -2.155e+01,  1.495e-02,  2.205e+01,  6.173e+01,
         -2.809e+01, -2.115e+01,  3.982e+00, -1.552e+01],
        [ 6.069e+01, -2.991e+01, -2.610e+01, -2.413e+01, -1.945e+01,  4.505e+01,
          2.913e+00,  4.528e+01,  7.054e+01, -1.720e+01],
        [-2.129e+01, -2.155e+01,  4.294e+01, -1.667e+01, -2.509e+01,  1.204e+01,
         -3.033e+01, -1.379e+01, -1.885e+01,  3.708e+01],
        [-1.091e+01, -2.668e+01, -1.328e+01, -2.452e+01, -1.975e+01, -1.477e+01,
          2.964e+00, -2.755e+01, -1.701e+01,  3.734e+01],
        [ 6.089e+01, -2.338e+01, -2.009e+01, -2.571e+01, -2.196e+01, -1.308e+01,
         -2.548e+01,  6.682e+01, -9.070e+00, -6.120e+00],
        [-1.696e+01, -8.752e+00, -1.568e+01, -1.097e+01,  5.535e+01,  5.737e+01,
          3.073e+01, -9.238e+00, -2.439e+01, -1.585e+01],
        [-2.482e+01, -3.900e+00, -1.043e+01,  3.138e+02, -1.190e+01,  1.193e+01,
         -2.531e+01, -1.173e+01, -2.629e+01,  9.107e+01],
        [ 2.722e+00, -2.363e+01, -3.434e+00, -2.024e+01, -1.393e+01,  1.112e+01,
          6.140e+00, -1.518e+01,  1.604e+01, -3.797e+00],
        [-5.286e+00, -7.712e+00, -2.849e+01,  2.399e+00,  2.655e-01, -1.525e+01,
          5.028e+00, -8.857e+00, -1.573e+01,  5.707e+01],
        [ 4.939e+01, -1.829e+01, -2.044e+01,  1.988e+01, -5.124e-01, -3.353e-01,
         -9.105e+00,  1.872e+02,  1.076e+02, -1.322e+01],
        [ 1.206e+02, -5.817e+00, -9.932e+00, -2.030e+01, -1.782e+01, -2.107e+01,
         -1.332e+01, -2.110e+01, -1.777e+01, -1.307e+01],
        [-8.404e+00, -2.383e+01, -9.696e+00,  1.574e+02, -1.247e+01, -1.050e+01,
         -2.549e+01, -3.191e+00, -2.311e+01, -1.484e+01],
        [-1.761e+01, -1.011e+01,  1.480e+01, -2.088e+01, -1.818e+01, -3.557e+00,
         -1.768e+01, -1.636e+01, -1.388e+00,  1.107e+01],
        [-1.909e+01, -2.593e+01, -9.474e+00, -2.163e+01, -1.575e+01, -1.474e+01,
         -2.262e+01,  9.554e+00, -1.988e+01, -2.686e+01],
        [-1.700e+01, -1.438e+01, -5.014e+00, -1.793e+01, -1.535e+01, -2.042e+01,
         -2.624e+01, -2.038e+01, -1.212e+01,  9.561e+00],
        [-1.268e+01, -2.197e+01, -3.102e+01, -1.458e+01, -2.368e+01, -2.040e+01,
          5.312e+01, -1.666e+01, -4.315e+00,  1.429e+01],
        [-2.543e+01, -1.744e+01, -1.739e+01,  4.774e+01, -1.984e+01, -1.208e+01,
         -1.932e+01, -1.586e+01, -2.097e+01, -1.892e+01],
        [ 2.691e-01,  1.525e+02,  2.676e+01, -2.436e+01, -2.022e+01,  4.568e+01,
          2.008e+01, -1.507e+01, -2.254e+01, -2.164e+01],
        [-1.456e+01,  5.032e+00, -2.389e+01, -2.350e+01, -1.551e+01, -2.442e+01,
         -1.798e+01, -2.213e+01, -5.280e+00, -2.636e+01],
        [-1.837e+01, -2.810e+00, -1.975e+01, -5.017e+00, -1.279e+01, -1.715e+01,
         -2.366e+01,  4.958e+00, -1.480e+01,  7.591e+01],
        [-1.823e+01, -2.034e+01, -3.597e+00,  5.425e-01,  5.724e+01, -1.757e+01,
          2.213e+01,  1.013e+01,  1.037e+01, -1.558e+01],
        [-2.425e+01, -1.669e+01, -2.256e+01,  3.239e+00,  6.073e+01,  3.774e+00,
         -1.783e+01, -2.041e+01, -1.442e+01,  1.477e+00],
        [-1.680e+01,  2.071e+01, -1.585e+01,  6.747e+01,  1.735e+01, -1.210e+01,
         -7.918e+00,  8.829e+00, -2.256e+01,  2.015e+01],
        [-2.519e+01, -2.680e+01,  8.633e+01, -2.315e+01, -7.174e+00, -1.150e+01,
         -1.830e+01, -8.752e+00, -1.438e+01, -2.841e+01],
        [ 2.536e+01,  3.720e+01,  1.966e+01, -9.579e+00, -1.813e+01, -2.080e+01,
         -2.136e+01,  9.027e+00,  2.489e+01, -8.274e+00],
        [-2.417e+01, -3.513e+00, -1.306e+01, -2.615e+01, -5.166e+00, -2.167e+01,
         -2.035e+01, -1.652e+01, -2.954e+01, -2.140e+01],
        [-1.226e+01, -2.387e+01,  7.966e-01, -2.683e+01, -1.634e+01,  7.701e+00,
         -2.369e+01, -1.660e+01, -2.384e+01, -2.201e+01],
        [ 4.564e+01,  1.012e+02, -9.951e+00, -1.665e+01, -1.242e+01, -2.245e+01,
         -2.080e+00,  1.449e-01, -1.320e+01,  4.062e+01],
        [-1.623e+01, -1.889e+01, -2.784e+01, -2.294e+01, -3.079e+01, -1.601e+01,
         -2.453e+01, -2.001e+01, -1.838e+01, -1.791e+01],
        [-2.015e+01, -2.413e+01, -1.941e+00,  6.101e+01, -9.614e+00, -2.350e+01,
         -1.784e+01,  1.406e+01,  1.744e+01, -1.639e+01],
        [ 4.192e+00, -5.427e+00, -2.293e+01, -2.010e+01, -8.969e+00,  3.730e+01,
         -1.619e+01,  7.911e+01,  5.832e+01,  1.784e+01],
        [-3.166e+01, -2.396e+01, -1.774e+01, -1.942e+01, -2.183e+01, -1.972e+01,
         -3.094e+00, -1.821e+01, -1.500e+01, -8.849e+00],
        [-5.043e+00, -4.719e+00,  5.533e+00, -2.055e+01, -2.367e+01, -1.961e+01,
         -2.218e+00, -2.301e+01, -1.897e+01,  3.743e+00],
        [ 1.872e+00, -5.776e+00, -1.311e+01, -2.086e+01, -7.473e+00,  2.773e+01,
         -2.118e+01,  1.265e+01, -1.417e+01,  2.085e+00],
        [-1.383e+01,  2.784e+01, -1.555e+01,  1.309e+01, -6.288e+00,  1.313e+01,
          1.244e+02, -1.168e+01, -2.691e+01, -4.162e+00]])
v tensor([[1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 6.887e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.290e+00,
         1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01],
        [1.930e+00, 1.000e+01, 1.000e-12, 5.968e-01, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 7.891e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.549e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 8.530e-01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 4.483e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e-12, 5.365e+00, 2.285e+00, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 9.667e+00, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 2.453e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 8.356e+00, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.632e+00, 1.000e-12, 1.000e-12, 1.000e-12, 6.151e+00,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.719e+00, 1.000e-12, 1.000e-12, 7.623e-01,
         1.000e+01, 1.000e-12, 4.099e+00, 1.000e+01],
        [2.480e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 5.123e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 4.361e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.079e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.203e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 9.578e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 3.495e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 2.019e+00, 1.000e-12],
        [6.600e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [6.797e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 2.564e+00, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 6.882e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         2.017e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 3.257e+00, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 6.478e+00, 1.000e+01, 1.000e-12,
         1.957e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 5.825e+00, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         6.016e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 4.108e+00],
        [8.713e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 3.017e+00, 3.438e-01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 6.413e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 3.748e+00, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 7.466e+00,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 2.963e+00, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         7.999e+00, 1.000e+01, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e-12, 6.552e-01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e-12, 7.237e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 8.191e+00, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 4.307e+00,
         3.111e+00, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 2.465e-01, 1.000e-12, 1.495e-02, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e-12, 3.982e+00, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         2.913e+00, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         2.964e+00, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [2.722e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         6.140e+00, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 2.399e+00, 2.655e-01, 1.000e-12,
         5.028e+00, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 9.554e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 9.561e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [2.691e-01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 5.032e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 4.958e+00, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 5.425e-01, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 3.239e+00, 1.000e+01, 3.774e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.477e+00],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 8.829e+00, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 9.027e+00, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 7.966e-01, 1.000e-12, 1.000e-12, 7.701e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.449e-01, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12],
        [4.192e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 5.533e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 3.743e+00],
        [1.872e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 2.085e+00],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12]])
v before min max tensor([-21.385,  19.289, -13.023,  32.204, -21.036, -21.427, -23.875, -11.050,
        -21.051, -19.209, -22.214,  35.226, -26.611, -17.189,  -8.693,  22.987,
        -29.462, -22.553, -26.243,  20.278, -24.885,  16.558,   1.821,  39.014,
         -2.522, -24.694, -20.867,  23.802,  -6.414, -18.901,   7.767,   1.047,
         23.441,   5.573, 144.618, -15.541,   7.920, -13.677,  -3.571,  -5.717,
         40.383,  45.544, -25.135,   4.598, -19.213, -17.413,  36.110, -18.816,
        -19.289, -21.371,   9.938,  -8.537, -18.630, -19.763, -22.712, -22.264,
        -21.026, -21.515, -17.227, -14.896, -14.083,  31.189, -12.713, -11.576,
         -6.190,  23.267,  -6.967, -21.199, -19.817,   9.568,  -1.130, -10.259,
        -10.627, -14.228, 123.769,  99.949,  -9.588,  -9.736, -14.334, -16.609,
        -21.210, -26.477, -19.490,  32.485, -14.786,  22.946, -21.177,  -7.425,
          4.194,  51.932,  -0.243, -20.858, -16.381, -22.474,  -1.723, -18.307,
        -16.512,   5.472,  40.182, -15.264])
v tensor([1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.821e+00, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        7.767e+00, 1.047e+00, 1.000e+01, 5.573e+00, 1.000e+01, 1.000e-12,
        7.920e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 4.598e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 9.938e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 9.568e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 4.194e+00, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 5.472e+00, 1.000e+01, 1.000e-12])
v before min max tensor([[[-2.431e-01],
         [-1.851e+01],
         [ 2.459e+01],
         [-5.156e+00],
         [-1.394e+01],
         [ 2.901e+01],
         [-1.425e+01],
         [-1.084e+01],
         [-1.510e+01],
         [ 2.036e+01]],

        [[-1.565e+01],
         [-1.100e+01],
         [-2.487e+01],
         [ 8.300e+01],
         [-6.603e+00],
         [-2.239e+01],
         [-1.971e+01],
         [ 6.473e+00],
         [-1.377e+01],
         [-1.021e+00]],

        [[-2.137e+01],
         [-7.916e+00],
         [ 1.807e+01],
         [-2.261e+01],
         [-3.719e+00],
         [-1.674e+01],
         [-1.343e+01],
         [-1.762e+01],
         [ 6.125e+00],
         [-9.527e+00]],

        [[-7.472e+00],
         [-5.338e+00],
         [-1.244e+01],
         [ 4.816e+01],
         [-1.883e+01],
         [-1.327e+01],
         [-1.835e+01],
         [-1.842e+01],
         [ 3.574e+00],
         [ 2.234e+00]],

        [[-1.881e+01],
         [ 1.298e+01],
         [-2.294e+01],
         [-2.128e+01],
         [-1.851e+01],
         [-2.678e+01],
         [-7.296e+00],
         [-3.028e+01],
         [ 2.078e+01],
         [-9.256e+00]],

        [[ 3.375e+01],
         [-2.018e+01],
         [-2.548e+01],
         [ 5.513e+01],
         [ 2.738e+01],
         [-2.027e+01],
         [-2.716e+01],
         [ 1.739e+00],
         [-2.240e+01],
         [ 1.118e+01]],

        [[-2.893e+01],
         [-1.862e+01],
         [-4.159e+00],
         [-2.789e+01],
         [-2.009e+01],
         [-2.329e+01],
         [-1.489e+01],
         [-1.929e+01],
         [ 8.624e+00],
         [-1.272e+01]],

        [[ 2.493e+00],
         [-1.821e+01],
         [-2.218e+01],
         [-1.143e+01],
         [ 8.828e+00],
         [-2.674e+01],
         [-2.108e+01],
         [-9.841e+00],
         [ 7.640e+00],
         [-2.267e+01]],

        [[-2.777e+01],
         [-1.856e+01],
         [ 6.266e+01],
         [-4.627e+00],
         [-1.675e+00],
         [-2.781e+01],
         [ 5.742e+00],
         [-1.568e+01],
         [-2.432e+01],
         [ 1.968e-01]],

        [[-2.074e+01],
         [-1.874e+01],
         [ 7.565e+01],
         [-7.190e+00],
         [ 2.338e+01],
         [-2.341e+01],
         [ 2.569e+02],
         [-1.262e+00],
         [ 3.165e+01],
         [-2.339e+01]]])
v tensor([[[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.473e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.125e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.574e+00],
         [2.234e+00]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.739e+00],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.624e+00],
         [1.000e-12]],

        [[2.493e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.828e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.640e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.742e+00],
         [1.000e-12],
         [1.000e-12],
         [1.968e-01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]]])
v before min max tensor([[-22.918],
        [ -2.199],
        [-33.012],
        [-16.080],
        [  1.753],
        [-29.388],
        [ 14.215],
        [ 26.731],
        [-14.122],
        [-27.153]])
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.753e+00],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12]])
a after update for 1 param tensor([[-3.935e-02,  8.210e-03,  1.793e-02, -5.482e-02, -7.547e-02,  2.165e-02,
         -1.583e-03, -2.411e-03,  4.714e-02, -1.858e-02],
        [ 4.106e-02,  4.060e-03,  7.504e-03,  1.385e-02, -4.424e-02, -5.347e-02,
         -3.872e-02, -2.463e-02, -9.104e-02, -6.513e-02],
        [ 4.665e-02,  3.212e-02, -1.567e-02, -2.319e-02, -4.804e-02, -4.897e-02,
          1.557e-02, -4.904e-02,  6.562e-03,  3.782e-02],
        [-2.305e-02, -1.683e-02,  3.414e-02,  2.605e-02, -1.257e-02,  3.638e-02,
         -7.526e-03, -4.515e-02,  3.392e-02, -2.774e-02],
        [-5.524e-02, -1.784e-02, -7.184e-02, -4.872e-02, -2.608e-02,  1.073e-02,
         -4.427e-02, -4.375e-02, -5.202e-02, -2.090e-02],
        [ 5.852e-03, -8.673e-03, -5.311e-02,  3.647e-02, -2.556e-03,  9.282e-02,
          6.618e-02, -1.318e-02, -5.094e-03,  1.458e-03],
        [ 2.838e-03,  1.176e-02,  1.474e-02,  4.052e-02,  1.561e-02,  4.498e-03,
         -3.379e-02,  8.168e-03,  1.036e-02,  3.172e-02],
        [ 3.389e-02, -6.604e-03, -9.048e-02, -6.666e-02, -1.489e-02, -2.496e-02,
         -5.729e-02, -4.643e-02, -2.925e-02,  3.629e-02],
        [ 3.376e-02, -1.798e-02,  2.824e-02, -4.103e-02,  2.505e-02,  7.291e-04,
         -1.096e-02, -1.485e-02, -4.573e-02, -2.679e-02],
        [ 1.881e-02, -2.243e-02, -4.087e-02, -4.249e-02,  5.686e-04,  7.313e-03,
          6.569e-02, -1.353e-02,  2.542e-02,  3.418e-02],
        [ 4.616e-02,  8.483e-03, -4.975e-03, -9.693e-03,  1.915e-02,  2.316e-02,
         -8.783e-03, -1.955e-03,  3.120e-03,  3.799e-03],
        [ 5.260e-03,  4.145e-02,  3.582e-02,  2.710e-02,  5.174e-02, -3.195e-02,
         -2.465e-02, -9.264e-04, -1.598e-02,  3.330e-02],
        [ 1.547e-02, -4.830e-03,  2.406e-02,  4.351e-02, -4.591e-02,  5.504e-03,
         -3.149e-02, -2.920e-02, -1.689e-02, -9.041e-02],
        [-4.816e-03,  1.910e-03, -1.891e-02, -2.511e-02, -3.146e-02,  7.299e-04,
         -3.955e-02,  8.865e-03,  2.896e-02,  5.074e-03],
        [ 4.181e-03, -2.921e-02, -5.904e-02,  1.071e-02,  9.798e-03,  2.255e-02,
          3.657e-02,  4.135e-02,  5.739e-03, -2.072e-02],
        [ 1.780e-02,  1.096e-02,  1.384e-02, -2.997e-02,  7.021e-03,  1.263e-02,
          5.955e-02, -2.375e-02, -2.190e-03,  1.626e-02],
        [ 8.167e-03,  5.917e-03,  8.520e-02,  3.383e-02, -3.245e-02,  2.589e-03,
         -6.885e-02,  4.364e-02,  1.880e-02, -1.139e-02],
        [-2.953e-02, -1.615e-02,  5.222e-02,  3.572e-02, -7.752e-03,  2.538e-02,
         -1.367e-02, -1.472e-02,  3.221e-02,  5.417e-02],
        [ 1.462e-02,  1.900e-03, -4.108e-02,  2.415e-02, -1.585e-02,  2.630e-04,
          5.218e-02,  8.468e-03,  1.027e-01, -1.313e-03],
        [ 1.692e-02, -3.130e-03,  3.064e-03, -5.520e-03, -2.108e-02,  2.106e-02,
         -1.330e-02,  5.727e-03,  1.118e-02,  1.462e-02],
        [-2.010e-02, -2.402e-02, -1.232e-02, -5.536e-02, -2.099e-02,  4.686e-02,
         -1.434e-03,  3.141e-02,  5.366e-03,  1.043e-02],
        [-4.776e-02, -4.199e-02, -2.457e-02,  2.961e-02,  5.037e-02, -1.140e-02,
          1.704e-02, -2.193e-02,  7.658e-02, -1.608e-02],
        [ 2.799e-02, -1.074e-02,  1.169e-02,  3.176e-03,  4.571e-02,  3.437e-03,
          6.185e-02, -3.784e-02,  3.411e-02, -5.518e-02],
        [ 1.451e-02,  8.685e-03, -4.489e-03,  7.583e-04, -2.117e-02, -9.298e-02,
          1.103e-02,  6.812e-02,  3.286e-02, -2.265e-03],
        [ 7.138e-02,  1.926e-02, -1.599e-02, -3.212e-02, -3.231e-02,  1.263e-02,
          2.557e-02, -5.764e-02, -4.342e-02, -4.369e-02],
        [ 2.071e-02, -2.142e-02,  1.650e-02,  4.153e-02, -3.139e-03,  1.925e-03,
          9.477e-03, -1.950e-03, -9.158e-03,  3.478e-02],
        [ 1.061e-02, -3.631e-04,  4.739e-03,  4.434e-04, -4.005e-02,  4.114e-02,
          7.415e-03, -1.709e-02,  1.828e-02,  3.362e-02],
        [ 2.572e-02, -2.956e-02,  1.104e-02, -4.185e-03,  3.652e-02, -2.953e-02,
         -1.696e-02,  1.757e-02, -3.298e-02, -3.701e-02],
        [ 2.210e-02,  8.509e-02, -1.350e-02,  1.884e-03, -9.105e-03, -2.010e-02,
         -1.060e-02,  4.451e-02, -7.651e-02, -4.260e-02],
        [-3.025e-02, -1.525e-02,  1.156e-02,  4.079e-02, -4.097e-02,  1.359e-02,
         -2.844e-02, -7.886e-03,  5.765e-02, -2.748e-02],
        [-2.579e-02, -3.515e-02,  4.497e-02, -7.906e-03, -3.584e-02,  1.819e-02,
         -3.267e-03, -1.231e-02,  1.608e-02,  1.237e-02],
        [ 1.172e-02, -1.018e-02, -2.235e-02,  2.915e-02,  3.504e-02,  6.000e-03,
          8.166e-03, -2.138e-02,  3.339e-03, -5.938e-02],
        [-8.458e-03,  8.342e-04,  4.261e-02, -1.990e-02,  1.805e-02, -1.196e-02,
         -4.235e-02,  8.919e-03,  2.204e-02,  7.780e-03],
        [ 6.401e-02,  2.810e-02, -8.354e-02,  2.182e-04, -2.972e-02, -2.224e-02,
         -2.972e-02,  4.936e-02,  5.510e-03, -1.869e-02],
        [ 2.459e-02,  3.098e-02,  7.064e-03, -4.550e-02,  1.599e-02, -5.941e-02,
         -3.309e-02,  6.234e-03, -1.020e-02, -6.361e-02],
        [-6.717e-03, -8.513e-03,  5.049e-03, -5.080e-02, -1.442e-02, -9.235e-03,
         -6.340e-02, -2.790e-02,  1.427e-02, -4.370e-03],
        [ 1.483e-02,  9.914e-03,  3.103e-02, -2.072e-03,  3.199e-02, -1.327e-03,
          5.173e-03,  2.774e-02, -7.829e-02, -1.935e-02],
        [ 1.188e-02,  4.140e-02, -1.864e-02, -4.694e-02,  2.163e-02,  1.215e-02,
         -1.023e-03,  1.749e-03,  3.463e-02, -3.377e-02],
        [-6.356e-02, -2.202e-02, -2.577e-02,  1.926e-02,  1.389e-02, -4.940e-02,
         -3.603e-03, -2.690e-02,  2.733e-02, -6.611e-02],
        [-1.965e-02, -3.643e-02,  9.375e-03, -2.853e-02,  1.632e-02,  5.186e-02,
         -2.363e-02, -1.144e-02,  2.074e-02,  9.639e-03],
        [-3.579e-02, -2.583e-02, -1.271e-02,  3.905e-02, -3.486e-03, -5.384e-03,
          5.375e-02,  2.243e-03, -1.959e-02,  1.435e-02],
        [ 3.012e-02,  2.467e-03,  2.327e-02, -2.599e-02, -4.766e-02, -5.675e-03,
          5.792e-02, -2.971e-02,  1.195e-02, -4.764e-02],
        [ 2.894e-02,  3.819e-02, -1.117e-02, -7.016e-02, -4.504e-03,  9.431e-03,
         -2.538e-02,  8.864e-03, -1.151e-03, -2.415e-02],
        [-6.122e-02,  2.354e-03,  6.769e-03, -9.273e-03, -2.058e-02,  7.879e-02,
         -7.657e-03,  2.192e-02, -5.022e-03, -1.753e-03],
        [ 2.484e-02,  4.031e-02,  2.901e-02, -1.888e-02, -4.455e-02,  3.215e-02,
         -2.140e-02, -1.071e-02, -2.224e-02,  3.678e-03],
        [ 1.013e-02,  3.693e-02, -2.174e-02,  3.172e-02, -3.814e-02,  3.988e-02,
         -2.313e-02,  8.004e-04,  1.212e-03, -5.702e-02],
        [ 1.341e-02, -4.971e-02,  3.383e-02, -1.490e-02,  3.832e-02, -3.878e-02,
         -8.501e-04, -6.750e-02, -5.616e-03, -6.140e-03],
        [-2.733e-02,  4.647e-03,  1.618e-02, -1.284e-02, -2.539e-03,  5.614e-02,
          6.345e-02, -1.241e-02, -2.628e-02,  1.475e-02],
        [-1.766e-02, -3.771e-02, -3.772e-02,  5.331e-02, -6.188e-03,  2.206e-02,
         -1.704e-02,  3.525e-02, -3.643e-02, -8.603e-02],
        [ 1.277e-03, -3.451e-02,  7.026e-03,  3.831e-02,  2.598e-03,  2.243e-02,
          3.066e-02,  2.463e-02,  1.193e-02,  2.255e-02],
        [ 2.801e-02,  3.117e-02,  3.767e-02, -3.424e-02, -4.450e-02,  3.655e-03,
          3.176e-03,  3.681e-02,  8.166e-02, -4.952e-02],
        [ 2.206e-02,  3.617e-02,  1.120e-02, -1.803e-02,  1.019e-01,  2.293e-02,
          2.651e-02,  2.786e-02,  9.519e-02, -3.208e-02],
        [-3.659e-02,  6.393e-02,  1.658e-02, -1.151e-02,  1.869e-03,  3.160e-02,
          3.039e-02,  6.106e-03, -4.892e-02,  2.418e-02],
        [-3.463e-02, -8.584e-03,  2.848e-02, -3.876e-02, -1.033e-02, -3.525e-03,
         -9.963e-03,  6.377e-02, -6.508e-03,  1.388e-02],
        [-1.120e-02, -1.324e-02, -1.074e-02, -5.863e-02,  4.381e-02, -1.639e-02,
         -3.321e-02, -6.797e-02,  8.049e-03,  1.747e-02],
        [ 4.017e-02, -6.729e-02,  6.838e-02,  1.314e-02,  2.061e-02,  1.185e-02,
         -2.295e-02,  3.813e-02,  2.476e-02,  2.037e-02],
        [-5.641e-02,  4.814e-02,  3.262e-02,  1.652e-02,  3.227e-03, -2.962e-02,
          4.523e-03, -2.499e-02, -2.055e-02, -4.084e-02],
        [ 1.571e-02,  5.320e-03, -2.950e-02, -6.599e-03, -2.737e-02,  1.222e-02,
          6.428e-02,  1.449e-02, -6.646e-02,  1.412e-02],
        [ 1.148e-02, -2.890e-02,  1.021e-02, -4.917e-03, -3.887e-05,  1.867e-02,
          3.053e-02,  2.402e-03,  5.751e-02,  2.057e-02],
        [ 1.414e-02,  6.434e-02,  1.971e-02,  2.595e-02, -8.859e-03, -3.595e-02,
          2.185e-02,  6.776e-02,  7.439e-02,  3.686e-02],
        [ 4.003e-02, -2.803e-04,  1.580e-02, -2.604e-02, -4.365e-02,  1.174e-02,
         -1.089e-02, -5.214e-03, -2.879e-02, -2.717e-03],
        [-5.430e-02, -3.287e-02, -1.024e-01, -5.000e-02,  1.097e-02,  1.687e-02,
         -5.992e-02,  5.804e-02, -5.210e-02, -2.411e-02],
        [ 6.369e-02,  5.675e-02, -1.975e-02,  4.608e-03,  9.035e-03, -2.160e-02,
         -9.047e-03, -5.360e-03, -1.625e-02, -7.725e-03],
        [ 4.245e-02,  1.945e-02, -4.230e-02,  3.762e-02,  1.480e-02, -2.427e-02,
          2.939e-02, -1.896e-02,  1.340e-02, -1.338e-02],
        [ 7.506e-02, -1.592e-02, -2.018e-03,  1.978e-03, -2.980e-02,  3.621e-02,
         -2.906e-04, -2.090e-02, -3.448e-02,  2.614e-02],
        [ 3.381e-02,  3.790e-02, -7.054e-02,  2.040e-03, -1.780e-02, -4.096e-02,
          2.567e-02, -2.841e-02,  1.534e-02,  1.995e-02],
        [-1.983e-02, -2.128e-02, -6.265e-03,  3.382e-02,  2.954e-02, -5.347e-02,
         -2.106e-03, -6.683e-02,  3.593e-02,  4.718e-02],
        [ 1.108e-02,  2.641e-02, -4.198e-02, -9.567e-02, -4.801e-03, -3.920e-03,
         -4.836e-03, -3.457e-02, -9.009e-02,  5.050e-02],
        [-8.407e-03, -4.673e-02, -1.383e-02,  2.658e-02,  3.144e-03,  7.997e-02,
          3.356e-02, -4.498e-02,  4.750e-02,  3.968e-02],
        [-7.972e-02,  3.553e-04, -7.322e-03,  5.318e-03, -6.245e-02,  3.434e-02,
          5.975e-03,  1.868e-02,  1.042e-02,  1.374e-02],
        [-1.230e-01,  4.897e-02,  4.431e-02,  1.665e-03, -2.687e-02, -2.481e-02,
          9.262e-03, -2.875e-02, -3.074e-02,  2.936e-05],
        [ 3.833e-02, -1.599e-02, -3.928e-02,  4.741e-02, -7.427e-03,  1.831e-02,
          1.087e-02,  7.712e-03, -4.194e-02, -2.069e-02],
        [ 2.840e-02,  9.098e-03, -4.520e-03, -2.093e-02,  4.467e-02,  4.136e-02,
          2.173e-02, -3.213e-02,  1.423e-02, -2.597e-02],
        [ 1.683e-02, -3.622e-02,  2.107e-02, -3.565e-03, -2.539e-02,  2.558e-02,
          5.760e-03,  1.677e-02, -1.883e-02,  8.648e-02],
        [-4.543e-02,  7.833e-03,  4.630e-02,  1.567e-02,  4.920e-02,  2.048e-02,
          2.045e-03,  2.185e-02, -1.919e-02, -8.666e-03],
        [-1.518e-02, -1.364e-02, -2.404e-04,  4.081e-02,  2.043e-03,  3.604e-02,
          4.160e-02,  1.682e-02,  4.668e-02,  6.115e-02],
        [-6.137e-03,  2.665e-02,  5.255e-03, -5.977e-02,  3.012e-03, -3.861e-02,
          1.550e-02,  4.847e-03, -7.999e-02, -4.135e-03],
        [-4.532e-02,  6.920e-02, -3.697e-02,  9.225e-02, -6.149e-03, -1.936e-02,
         -5.699e-02, -7.757e-03,  1.692e-02, -2.036e-04],
        [ 3.870e-03, -2.590e-02, -2.529e-02, -3.923e-02, -9.639e-02, -2.590e-02,
         -1.414e-02, -1.805e-03,  2.578e-02, -5.620e-02],
        [-1.581e-02, -2.208e-02, -6.075e-03,  4.986e-02,  1.397e-02, -2.164e-02,
         -4.891e-03,  3.147e-03,  1.853e-02, -4.309e-02],
        [ 5.163e-02, -4.603e-03, -1.198e-02, -7.419e-02, -1.995e-03,  3.558e-02,
          7.622e-02, -2.972e-02,  2.021e-02, -3.023e-02],
        [ 1.839e-02,  3.469e-04,  9.509e-03, -1.346e-02,  1.416e-02,  4.194e-02,
          3.092e-02, -3.926e-03,  2.020e-02, -3.834e-02],
        [-8.563e-04, -7.805e-03,  4.852e-02, -2.437e-02,  5.039e-02,  1.279e-02,
          6.670e-03, -1.990e-04, -3.298e-03,  1.797e-02],
        [ 4.461e-02, -1.165e-02, -1.184e-02,  7.223e-03, -3.203e-02, -6.395e-02,
          4.108e-02,  3.562e-02, -8.050e-04,  1.702e-02],
        [ 4.680e-03,  1.482e-02,  3.880e-03,  4.568e-03,  1.732e-02,  2.477e-02,
          4.520e-03, -2.855e-02,  9.000e-03,  5.563e-02],
        [ 7.619e-03,  2.595e-03,  8.399e-03, -1.192e-02,  1.774e-03, -6.325e-03,
         -4.859e-02, -1.543e-02,  4.568e-03, -3.984e-03],
        [ 2.203e-03,  6.344e-03, -1.828e-02, -1.520e-04,  5.822e-03, -3.166e-03,
         -9.275e-03,  2.892e-02,  2.385e-02,  7.506e-04],
        [-3.787e-03, -2.468e-02, -8.041e-03, -6.919e-03,  1.050e-02, -4.180e-02,
          2.519e-02,  4.279e-02, -9.596e-03,  4.569e-02],
        [ 5.814e-02, -1.980e-02,  2.157e-02, -3.260e-03, -7.952e-03, -2.871e-02,
          2.358e-02, -2.364e-02,  8.704e-03, -5.560e-02],
        [-8.169e-02, -3.362e-02,  1.188e-02, -5.453e-02, -1.211e-02, -1.093e-02,
          3.278e-02, -3.882e-02,  2.494e-02,  3.739e-02],
        [ 9.174e-04, -3.195e-02, -2.623e-02, -4.132e-02,  1.079e-02, -1.814e-02,
          4.485e-04,  3.434e-03,  5.308e-02,  2.416e-02],
        [ 3.727e-03,  3.113e-03, -1.585e-02, -1.967e-02, -1.298e-02,  2.365e-02,
          6.669e-02,  5.375e-02,  4.405e-02,  1.858e-03],
        [ 4.261e-02,  3.864e-02,  2.541e-02, -1.815e-02,  3.775e-02,  2.356e-02,
         -8.483e-02,  7.939e-03,  2.852e-02,  2.236e-02],
        [ 2.970e-02, -1.231e-02,  1.425e-02,  4.796e-03, -6.389e-02,  2.977e-02,
          2.174e-02, -7.002e-02,  1.912e-02, -1.038e-02],
        [ 4.956e-03, -5.388e-02,  2.207e-02, -4.573e-03, -4.137e-02, -1.350e-03,
          1.329e-02, -4.884e-02,  1.037e-02,  2.886e-03],
        [ 7.699e-02, -5.269e-03,  6.656e-02, -3.370e-02,  5.386e-03,  3.189e-02,
         -2.058e-02,  3.204e-02,  3.304e-02, -7.772e-03],
        [-1.467e-04,  9.819e-03, -9.971e-03,  6.214e-02,  1.385e-03, -3.819e-03,
         -4.874e-05,  1.519e-02,  7.377e-02,  2.806e-02],
        [ 3.874e-03, -3.723e-02,  4.183e-02, -5.079e-02,  2.679e-02,  1.825e-02,
          1.085e-03,  5.110e-02,  7.975e-03,  7.426e-03],
        [-1.234e-02, -4.660e-02, -4.117e-02,  1.638e-03,  2.009e-02, -2.343e-02,
         -2.657e-02, -3.192e-03,  1.562e-02,  1.885e-02],
        [ 6.210e-04, -6.288e-02,  3.669e-03,  1.057e-01,  1.402e-02, -2.857e-02,
          4.918e-02,  2.649e-02, -8.342e-02,  5.385e-03]])
s after update for 1 param tensor([[2.076, 1.591, 1.390, 1.645, 1.684, 1.331, 1.597, 1.584, 1.987, 1.515],
        [1.767, 1.250, 2.204, 1.751, 1.593, 0.826, 1.650, 0.998, 1.870, 1.665],
        [2.350, 1.571, 1.305, 1.367, 1.326, 1.800, 1.853, 1.472, 1.947, 1.793],
        [2.239, 1.464, 1.655, 1.782, 2.030, 1.589, 1.521, 1.599, 1.239, 1.383],
        [2.032, 0.764, 1.489, 1.128, 1.305, 2.008, 2.123, 1.659, 1.969, 1.804],
        [1.508, 1.761, 1.389, 2.017, 2.118, 1.918, 1.962, 1.218, 1.745, 1.501],
        [1.627, 1.294, 1.512, 0.690, 1.072, 1.653, 1.907, 1.936, 1.746, 1.596],
        [1.538, 1.918, 2.500, 1.564, 2.001, 1.416, 1.466, 2.011, 1.198, 1.662],
        [1.729, 1.548, 1.208, 1.410, 2.202, 1.201, 1.633, 1.228, 1.567, 1.495],
        [1.763, 1.112, 2.096, 1.589, 2.187, 1.268, 1.733, 1.711, 1.242, 1.824],
        [1.228, 1.328, 1.318, 1.769, 1.635, 1.782, 2.039, 1.957, 1.301, 1.539],
        [1.981, 1.605, 1.531, 1.357, 1.786, 1.632, 1.271, 2.061, 1.826, 2.117],
        [1.687, 1.953, 1.852, 1.794, 1.722, 1.583, 1.188, 1.696, 1.672, 1.670],
        [0.790, 1.571, 1.861, 1.278, 1.669, 1.299, 1.978, 2.110, 1.551, 1.579],
        [0.914, 1.586, 1.858, 0.836, 1.422, 1.500, 1.558, 1.324, 1.931, 2.056],
        [1.697, 1.072, 1.671, 1.908, 1.471, 1.773, 1.795, 1.915, 1.623, 1.493],
        [1.250, 2.140, 1.862, 1.418, 2.129, 1.183, 0.676, 1.608, 2.055, 1.740],
        [1.716, 1.984, 2.133, 1.702, 1.620, 1.340, 1.446, 1.480, 1.628, 1.566],
        [1.547, 1.978, 1.585, 1.753, 1.643, 1.700, 1.240, 1.372, 1.800, 2.322],
        [1.584, 1.322, 1.357, 1.759, 1.940, 1.440, 1.518, 1.313, 1.638, 1.941],
        [1.950, 1.684, 1.694, 1.534, 1.567, 1.450, 1.617, 1.382, 1.625, 1.918],
        [1.240, 1.856, 1.528, 1.158, 2.045, 1.962, 1.708, 1.798, 1.766, 1.715],
        [1.548, 1.932, 1.633, 1.487, 2.161, 1.929, 1.982, 1.873, 1.219, 1.519],
        [0.969, 2.225, 1.192, 1.208, 1.663, 1.603, 1.435, 1.259, 1.695, 1.517],
        [2.176, 1.495, 2.008, 1.286, 1.663, 1.217, 2.228, 1.562, 1.767, 2.143],
        [1.440, 1.773, 1.500, 1.874, 1.725, 1.806, 1.573, 2.076, 1.130, 1.192],
        [1.966, 2.137, 1.491, 1.885, 1.552, 1.334, 1.049, 1.156, 1.309, 1.296],
        [2.138, 2.108, 1.632, 1.497, 1.644, 1.233, 1.693, 1.707, 1.711, 1.635],
        [1.462, 1.384, 1.507, 2.305, 1.453, 1.462, 1.844, 1.406, 1.719, 1.667],
        [1.047, 1.977, 1.562, 1.186, 1.671, 1.639, 1.426, 1.614, 1.785, 1.415],
        [1.656, 1.658, 1.920, 1.295, 1.587, 1.208, 1.226, 1.045, 1.831, 1.854],
        [1.331, 1.317, 1.325, 1.844, 1.449, 1.734, 1.072, 1.553, 1.983, 2.036],
        [1.535, 1.484, 1.553, 1.460, 1.504, 2.151, 0.893, 1.235, 1.476, 2.120],
        [1.540, 1.947, 1.879, 1.190, 1.608, 1.320, 1.399, 1.470, 1.385, 1.505],
        [2.078, 1.329, 1.514, 1.440, 1.920, 1.563, 1.622, 2.119, 1.213, 2.047],
        [1.169, 1.607, 1.748, 2.049, 1.822, 1.382, 1.363, 1.964, 1.333, 1.883],
        [1.687, 2.024, 1.862, 1.765, 1.545, 1.766, 1.646, 1.456, 1.429, 1.925],
        [1.984, 1.558, 2.347, 1.818, 1.381, 1.751, 1.544, 1.672, 1.703, 2.026],
        [1.866, 2.153, 1.329, 1.198, 0.677, 1.488, 0.940, 1.169, 1.509, 1.997],
        [2.229, 2.065, 2.149, 1.484, 1.717, 2.355, 1.614, 1.360, 1.479, 1.861],
        [1.358, 1.532, 1.408, 1.804, 1.463, 1.385, 1.357, 2.195, 0.888, 1.315],
        [1.722, 1.649, 1.743, 1.579, 1.688, 1.684, 0.664, 1.687, 1.354, 0.928],
        [2.043, 1.898, 1.717, 1.905, 1.493, 1.403, 1.590, 1.881, 1.285, 1.419],
        [1.477, 1.837, 1.856, 2.001, 1.880, 1.461, 1.179, 1.930, 1.303, 1.308],
        [1.388, 2.261, 2.061, 1.520, 1.877, 1.903, 1.594, 1.989, 0.800, 1.485],
        [1.908, 1.456, 1.900, 1.663, 2.424, 1.791, 1.184, 1.669, 1.851, 1.988],
        [1.870, 0.828, 1.799, 1.974, 2.020, 1.581, 1.547, 2.050, 1.956, 1.325],
        [1.697, 1.672, 1.702, 1.487, 1.624, 1.525, 1.942, 1.389, 1.300, 1.544],
        [1.430, 1.707, 1.791, 1.327, 1.718, 2.097, 1.694, 1.819, 1.735, 1.937],
        [1.382, 1.254, 1.989, 1.901, 1.693, 1.693, 1.267, 1.628, 1.553, 1.032],
        [1.393, 1.333, 1.912, 2.108, 1.271, 1.824, 1.831, 1.438, 1.950, 1.794],
        [1.519, 1.801, 1.661, 1.668, 1.629, 1.889, 1.781, 1.329, 1.495, 1.574],
        [1.498, 1.904, 1.252, 1.525, 1.774, 1.843, 2.098, 1.403, 1.746, 1.508],
        [1.158, 1.650, 1.310, 1.430, 1.828, 1.995, 1.715, 1.785, 1.631, 1.779],
        [1.361, 1.865, 1.587, 1.728, 2.059, 1.277, 1.892, 1.739, 1.556, 1.581],
        [1.075, 1.677, 0.650, 1.756, 1.212, 2.030, 1.828, 1.672, 1.501, 1.561],
        [1.123, 0.913, 1.829, 1.938, 1.186, 1.721, 1.614, 1.325, 1.506, 1.786],
        [1.514, 1.351, 1.445, 1.957, 1.951, 1.704, 0.718, 1.951, 1.840, 1.662],
        [1.412, 0.842, 1.480, 1.514, 2.254, 1.276, 1.955, 1.218, 1.543, 2.091],
        [1.398, 1.661, 1.766, 1.765, 1.726, 1.673, 1.212, 2.258, 1.578, 1.424],
        [1.761, 1.263, 1.618, 1.130, 1.615, 1.554, 2.063, 1.859, 2.036, 1.749],
        [2.149, 1.421, 1.422, 1.651, 1.813, 2.518, 2.180, 1.955, 1.953, 1.243],
        [1.681, 1.616, 1.947, 1.434, 1.537, 0.963, 1.766, 1.057, 1.944, 2.256],
        [1.451, 1.866, 1.467, 1.742, 1.366, 1.362, 1.866, 0.871, 1.635, 1.915],
        [1.518, 1.219, 1.810, 1.603, 2.205, 1.720, 1.229, 1.723, 1.453, 2.092],
        [1.177, 1.304, 1.627, 1.864, 1.840, 1.695, 1.926, 1.873, 1.206, 1.307],
        [1.867, 2.051, 1.780, 1.760, 1.303, 2.075, 1.587, 1.561, 1.886, 1.165],
        [1.601, 1.454, 1.919, 1.738, 1.764, 2.338, 2.038, 1.744, 1.739, 2.273],
        [1.339, 1.814, 1.153, 1.649, 1.323, 1.345, 1.442, 1.876, 1.511, 1.759],
        [1.684, 1.693, 1.346, 1.941, 1.870, 0.898, 1.759, 1.743, 1.516, 1.570],
        [1.735, 2.070, 1.303, 1.595, 1.812, 1.972, 2.035, 1.122, 1.762, 1.681],
        [1.673, 1.916, 1.302, 2.087, 1.584, 1.760, 1.718, 1.261, 1.782, 1.874],
        [1.544, 1.841, 1.364, 1.378, 1.724, 2.066, 1.850, 1.627, 1.524, 1.578],
        [1.402, 1.079, 1.913, 1.614, 1.530, 1.032, 1.951, 1.641, 1.529, 2.613],
        [1.352, 1.238, 1.431, 2.073, 1.781, 1.718, 1.578, 2.039, 2.052, 2.456],
        [2.518, 1.378, 1.467, 1.400, 2.012, 1.715, 1.257, 1.921, 1.332, 1.490],
        [1.833, 1.647, 1.563, 1.795, 1.208, 1.153, 1.813, 1.562, 1.547, 1.965],
        [1.197, 1.639, 2.327, 1.468, 1.218, 1.601, 1.293, 1.348, 1.642, 1.836],
        [1.514, 1.762, 1.309, 1.579, 1.253, 1.094, 1.725, 2.121, 1.451, 1.845],
        [2.105, 1.726, 1.563, 1.462, 2.068, 1.447, 1.769, 1.546, 2.095, 1.541],
        [1.498, 1.592, 2.085, 1.428, 1.975, 1.474, 1.926, 1.527, 1.449, 2.015],
        [1.841, 1.838, 1.582, 1.864, 1.532, 1.788, 1.808, 1.113, 1.803, 1.332],
        [1.019, 1.814, 1.868, 1.816, 1.365, 1.502, 2.001, 1.042, 1.509, 1.653],
        [2.007, 1.500, 1.695, 1.638, 1.052, 1.781, 1.498, 1.539, 0.975, 1.827],
        [1.261, 1.195, 1.369, 0.947, 1.287, 1.425, 1.589, 1.365, 1.242, 2.167],
        [1.309, 1.627, 2.026, 1.433, 1.996, 1.220, 1.986, 2.011, 1.988, 1.351],
        [1.653, 1.654, 1.693, 1.805, 1.550, 1.780, 1.763, 1.393, 1.684, 1.037],
        [1.145, 1.868, 1.895, 1.793, 1.729, 1.773, 1.826, 1.768, 1.630, 1.672],
        [1.880, 2.060, 1.508, 1.690, 1.244, 1.914, 1.381, 1.319, 1.027, 1.903],
        [1.684, 1.377, 2.251, 1.848, 1.433, 1.921, 1.732, 1.865, 1.690, 1.226],
        [1.825, 1.145, 1.591, 1.790, 1.245, 1.591, 1.556, 1.131, 1.979, 1.446],
        [1.495, 1.599, 1.600, 1.955, 1.234, 1.769, 1.598, 1.380, 1.847, 1.481],
        [1.769, 1.876, 1.880, 1.482, 1.892, 1.768, 1.349, 1.899, 0.892, 1.886],
        [1.506, 1.674, 1.864, 1.551, 2.106, 1.465, 1.752, 1.394, 1.453, 1.387],
        [1.353, 1.635, 1.382, 1.543, 1.560, 1.584, 1.634, 1.851, 2.212, 1.304],
        [1.523, 0.610, 1.595, 1.410, 1.656, 1.778, 1.801, 1.783, 1.808, 1.991],
        [2.163, 1.810, 1.430, 1.477, 1.462, 1.552, 1.269, 1.719, 1.954, 0.999],
        [1.617, 1.246, 1.683, 1.404, 1.626, 1.670, 1.072, 1.540, 1.644, 1.639],
        [1.459, 1.280, 1.334, 1.564, 1.457, 1.886, 1.435, 2.145, 1.306, 1.459],
        [0.974, 1.588, 1.438, 1.829, 1.642, 1.423, 2.355, 1.397, 1.816, 1.335]])
b after update for 1 param tensor([[63.786, 55.840, 52.195, 56.779, 57.451, 51.075, 55.938, 55.710, 62.406,
         54.497],
        [58.840, 49.500, 65.719, 58.580, 55.878, 40.240, 56.855, 44.216, 60.531,
         57.126],
        [67.861, 55.480, 50.569, 51.767, 50.985, 59.390, 60.264, 53.704, 61.770,
         59.277],
        [66.242, 53.563, 56.946, 59.092, 63.080, 55.804, 54.587, 55.976, 49.269,
         52.066],
        [63.105, 38.684, 54.015, 47.022, 50.563, 62.736, 64.495, 57.015, 62.118,
         59.459],
        [54.359, 58.739, 52.168, 62.864, 64.421, 61.305, 62.004, 48.852, 58.482,
         54.241],
        [56.466, 50.357, 54.427, 36.782, 45.845, 56.911, 61.138, 61.602, 58.498,
         55.921],
        [54.905, 61.305, 69.992, 55.360, 62.614, 52.685, 53.607, 62.782, 48.446,
         57.069],
        [58.203, 55.072, 48.648, 52.571, 65.689, 48.520, 56.576, 49.047, 55.417,
         54.127],
        [58.778, 46.688, 64.098, 55.805, 65.469, 49.845, 58.283, 57.900, 49.329,
         59.792],
        [49.050, 51.006, 50.817, 58.884, 56.602, 59.098, 63.219, 61.932, 50.489,
         54.924],
        [62.304, 56.083, 54.783, 51.572, 59.157, 56.554, 49.914, 63.553, 59.819,
         64.410],
        [57.494, 61.868, 60.246, 59.299, 58.089, 55.703, 48.246, 57.650, 57.250,
         57.212],
        [39.357, 55.486, 60.390, 50.039, 57.199, 50.454, 62.252, 64.301, 55.132,
         55.627],
        [42.327, 55.744, 60.343, 40.484, 52.794, 54.217, 55.254, 50.931, 61.520,
         63.483],
        [57.664, 45.824, 57.227, 61.143, 53.688, 58.937, 59.306, 61.265, 56.391,
         54.090],
        [49.494, 64.757, 60.399, 52.715, 64.594, 48.143, 36.394, 56.135, 63.467,
         58.391],
        [57.985, 62.361, 64.654, 57.745, 56.342, 51.246, 53.241, 53.858, 56.479,
         55.400],
        [55.061, 62.257, 55.737, 58.610, 56.750, 57.727, 49.298, 51.855, 59.388,
         67.464],
        [55.713, 50.905, 51.574, 58.713, 61.663, 53.123, 54.535, 50.732, 56.663,
         61.668],
        [61.820, 57.439, 57.618, 54.825, 55.411, 53.300, 56.296, 52.038, 56.425,
         61.301],
        [49.291, 60.304, 54.720, 47.635, 63.301, 62.012, 57.858, 59.361, 58.833,
         57.977],
        [55.083, 61.535, 56.576, 53.974, 65.080, 61.484, 62.325, 60.585, 48.867,
         54.553],
        [43.587, 66.030, 48.333, 48.656, 57.094, 56.055, 53.034, 49.671, 57.627,
         54.527],
        [65.301, 54.120, 62.726, 50.193, 57.085, 48.835, 66.079, 55.330, 58.853,
         64.804],
        [53.130, 58.950, 54.225, 60.607, 58.139, 59.492, 55.518, 63.784, 47.052,
         48.332],
        [62.066, 64.716, 54.061, 60.780, 55.156, 51.125, 45.332, 47.598, 50.657,
         50.395],
        [64.725, 64.273, 56.546, 54.156, 56.764, 49.149, 57.600, 57.844, 57.911,
         56.600],
        [53.526, 52.080, 54.342, 67.216, 53.356, 53.533, 60.106, 52.499, 58.047,
         57.149],
        [45.295, 62.249, 55.322, 48.216, 57.218, 56.666, 52.859, 56.246, 59.141,
         52.658],
        [56.967, 57.005, 61.341, 50.370, 55.766, 48.648, 49.022, 45.251, 59.903,
         60.285],
        [51.075, 50.794, 50.961, 60.113, 53.291, 58.290, 45.832, 55.159, 62.342,
         63.167],
        [54.854, 53.922, 55.165, 53.494, 54.281, 64.918, 41.839, 49.189, 53.784,
         64.454],
        [54.938, 61.764, 60.683, 48.286, 56.140, 50.868, 52.355, 53.665, 52.092,
         54.307],
        [63.822, 51.038, 54.462, 53.128, 61.335, 55.342, 56.387, 64.434, 48.762,
         63.337],
        [47.856, 56.123, 58.521, 63.369, 59.758, 52.049, 51.689, 62.036, 51.115,
         60.750],
        [57.504, 62.985, 60.401, 58.814, 55.026, 58.822, 56.796, 53.422, 52.920,
         61.420],
        [62.350, 55.257, 67.817, 59.691, 52.026, 58.586, 55.001, 57.243, 57.773,
         63.013],
        [60.467, 64.962, 51.041, 48.444, 36.411, 54.008, 42.916, 47.858, 54.384,
         62.560],
        [66.097, 63.622, 64.899, 53.934, 58.009, 67.939, 56.236, 51.620, 53.831,
         60.383],
        [51.592, 54.795, 52.537, 59.457, 53.536, 52.102, 51.576, 65.592, 41.712,
         50.769],
        [58.096, 56.848, 58.449, 55.624, 57.523, 57.444, 36.062, 57.498, 51.516,
         42.641],
        [63.277, 60.989, 58.009, 61.100, 54.084, 52.436, 55.814, 60.715, 50.181,
         52.736],
        [53.801, 59.993, 60.314, 62.619, 60.705, 53.501, 48.075, 61.501, 50.538,
         50.621],
        [52.147, 66.561, 63.553, 54.576, 60.653, 61.071, 55.892, 62.435, 39.594,
         53.951],
        [61.141, 53.416, 61.017, 57.088, 68.925, 59.247, 48.170, 57.198, 60.220,
         62.418],
        [60.537, 40.287, 59.381, 62.201, 62.912, 55.662, 55.069, 63.382, 61.913,
         50.958],
        [57.667, 57.233, 57.751, 53.981, 56.413, 54.672, 61.689, 52.175, 50.479,
         54.998],
        [52.941, 57.834, 59.248, 51.000, 58.022, 64.111, 57.615, 59.705, 58.304,
         61.615],
        [52.038, 49.573, 62.431, 61.043, 57.600, 57.595, 49.832, 56.482, 55.164,
         44.972],
        [52.244, 51.109, 61.216, 64.279, 49.914, 59.779, 59.906, 53.080, 61.822,
         59.298],
        [54.557, 59.401, 57.050, 57.174, 56.509, 60.847, 59.085, 51.040, 54.131,
         55.531],
        [54.181, 61.086, 49.542, 54.659, 58.961, 60.097, 64.121, 52.430, 58.502,
         54.359],
        [47.642, 56.863, 50.667, 52.942, 59.853, 62.534, 57.966, 59.145, 56.531,
         59.049],
        [51.645, 60.453, 55.760, 58.194, 63.529, 50.022, 60.895, 58.379, 55.212,
         55.660],
        [45.897, 57.326, 35.691, 58.660, 48.726, 63.076, 59.846, 57.248, 54.234,
         55.313],
        [46.904, 42.291, 59.875, 61.621, 48.211, 58.071, 56.235, 50.966, 54.317,
         59.161],
        [54.462, 51.459, 53.218, 61.934, 61.828, 57.788, 37.520, 61.836, 60.052,
         57.072],
        [52.598, 40.616, 53.856, 54.465, 66.461, 49.998, 61.898, 48.854, 54.984,
         64.014],
        [52.349, 57.056, 58.825, 58.805, 58.167, 57.267, 48.727, 66.524, 55.617,
         52.821],
        [58.737, 49.745, 56.311, 47.050, 56.259, 55.190, 63.579, 60.361, 63.159,
         58.548],
        [64.891, 52.770, 52.796, 56.884, 59.613, 70.251, 65.366, 61.897, 61.873,
         49.353],
        [57.397, 56.270, 61.778, 53.003, 54.874, 43.446, 58.821, 45.514, 61.720,
         66.497],
        [53.325, 60.472, 53.626, 58.433, 51.747, 51.665, 60.464, 41.325, 56.611,
         61.260],
        [54.549, 48.882, 59.562, 56.055, 65.740, 58.054, 49.069, 58.102, 53.364,
         64.026],
        [48.033, 50.550, 56.468, 60.434, 60.052, 57.631, 61.442, 60.586, 48.620,
         50.612],
        [60.485, 63.405, 59.059, 58.721, 50.523, 63.761, 55.769, 55.306, 60.796,
         47.785],
        [56.011, 53.385, 61.324, 58.352, 58.793, 67.690, 63.198, 58.458, 58.379,
         66.740],
        [51.223, 59.615, 47.529, 56.851, 50.909, 51.349, 53.155, 60.630, 54.415,
         58.711],
        [57.446, 57.597, 51.350, 61.681, 60.543, 41.956, 58.716, 58.450, 54.498,
         55.473],
        [58.310, 63.694, 50.527, 55.900, 59.584, 62.171, 63.147, 46.900, 58.755,
         57.396],
        [57.251, 61.280, 50.516, 63.952, 55.709, 58.725, 58.024, 49.714, 59.087,
         60.597],
        [55.006, 60.059, 51.697, 51.965, 58.127, 63.631, 60.208, 56.469, 54.641,
         55.616],
        [52.418, 45.987, 61.236, 56.243, 54.765, 44.966, 61.826, 56.701, 54.731,
         71.555],
        [51.482, 49.262, 52.951, 63.744, 59.078, 58.028, 55.612, 63.214, 63.407,
         69.370],
        [70.249, 51.960, 53.620, 52.382, 62.794, 57.979, 49.626, 61.362, 51.095,
         54.042],
        [59.929, 56.804, 55.349, 59.315, 48.656, 47.537, 59.614, 55.333, 55.057,
         62.062],
        [48.434, 56.674, 67.534, 53.645, 48.849, 56.021, 50.336, 51.393, 56.729,
         59.982],
        [54.468, 58.764, 50.646, 55.620, 49.555, 46.309, 58.144, 64.476, 53.329,
         60.137],
        [64.226, 58.160, 55.337, 53.528, 63.659, 53.250, 58.873, 55.041, 64.076,
         54.950],
        [54.188, 55.859, 63.922, 52.907, 62.205, 53.743, 61.443, 54.699, 53.279,
         62.833],
        [60.064, 60.014, 55.685, 60.441, 54.797, 59.201, 59.530, 46.711, 59.435,
         51.084],
        [44.685, 59.619, 60.508, 59.656, 51.723, 54.245, 62.623, 45.192, 54.384,
         56.917],
        [62.707, 54.211, 57.639, 56.660, 45.414, 59.086, 54.184, 54.913, 43.702,
         59.839],
        [49.720, 48.396, 51.805, 43.088, 50.230, 52.837, 55.810, 51.727, 49.332,
         65.172],
        [50.651, 56.460, 63.013, 53.001, 62.547, 48.890, 62.387, 62.773, 62.417,
         51.458],
        [56.921, 56.941, 57.603, 59.474, 55.109, 59.056, 58.784, 52.249, 57.443,
         45.074],
        [47.377, 60.499, 60.937, 59.282, 58.211, 58.939, 59.816, 58.867, 56.519,
         57.243],
        [60.700, 63.537, 54.371, 57.543, 49.368, 61.250, 52.025, 50.838, 44.852,
         61.063],
        [57.440, 51.950, 66.410, 60.181, 52.992, 61.361, 58.260, 60.461, 57.541,
         49.019],
        [59.800, 47.369, 55.830, 59.222, 49.392, 55.835, 55.219, 47.080, 62.278,
         53.227],
        [54.125, 55.981, 55.996, 61.895, 49.175, 58.872, 55.968, 52.010, 60.164,
         53.881],
        [58.875, 60.639, 60.702, 53.897, 60.895, 58.867, 51.417, 61.002, 41.799,
         60.797],
        [54.326, 57.268, 60.434, 55.131, 64.239, 53.586, 58.590, 52.266, 53.359,
         52.127],
        [51.487, 56.602, 52.033, 54.993, 55.284, 55.716, 56.583, 60.232, 65.842,
         50.549],
        [54.626, 34.576, 55.900, 52.557, 56.961, 59.036, 59.407, 59.105, 59.518,
         62.467],
        [65.099, 59.565, 52.932, 53.807, 53.527, 55.154, 49.877, 58.037, 61.876,
         44.252],
        [56.295, 49.420, 57.423, 52.447, 56.449, 57.206, 45.843, 54.941, 56.752,
         56.667],
        [53.471, 50.082, 51.129, 55.369, 53.426, 60.791, 53.022, 64.839, 50.589,
         53.479],
        [43.683, 55.782, 53.086, 59.863, 56.729, 52.808, 67.931, 52.320, 59.661,
         51.152]])
a after update for 1 param tensor([ 0.036, -0.073, -0.038,  0.047,  0.048,  0.027,  0.007,  0.030,  0.036,
        -0.029, -0.066, -0.031, -0.037,  0.084, -0.079, -0.028, -0.008, -0.015,
        -0.074, -0.056,  0.043,  0.022,  0.027, -0.020, -0.044, -0.007,  0.015,
         0.040,  0.007, -0.035, -0.054,  0.038, -0.033, -0.012, -0.011, -0.056,
         0.062, -0.006,  0.012,  0.005, -0.039, -0.025, -0.069,  0.005,  0.006,
         0.056, -0.069,  0.138, -0.030, -0.026, -0.016, -0.034,  0.024,  0.010,
        -0.038,  0.112, -0.010, -0.010, -0.002, -0.010, -0.015, -0.021, -0.092,
        -0.056,  0.075,  0.082,  0.035,  0.043,  0.051,  0.001, -0.012, -0.026,
        -0.020,  0.015,  0.039, -0.014,  0.039, -0.015, -0.014,  0.004, -0.005,
        -0.045,  0.042,  0.022, -0.027, -0.012,  0.056, -0.055, -0.001, -0.033,
        -0.052, -0.016,  0.036,  0.046,  0.035,  0.049,  0.004,  0.032, -0.017,
         0.057])
s after update for 1 param tensor([1.540, 1.645, 1.744, 1.811, 1.813, 1.459, 1.921, 1.364, 1.675, 1.302,
        1.704, 1.972, 1.866, 1.372, 1.114, 2.024, 1.979, 1.513, 1.781, 1.487,
        1.785, 1.489, 1.502, 1.959, 1.727, 1.893, 1.412, 1.747, 1.560, 1.333,
        1.435, 1.656, 1.665, 2.098, 1.754, 1.929, 2.026, 1.614, 1.419, 1.203,
        2.041, 1.954, 1.719, 1.150, 1.417, 1.629, 1.900, 1.288, 1.469, 1.607,
        1.988, 1.177, 1.707, 1.406, 1.579, 1.490, 1.848, 1.444, 1.866, 1.612,
        1.523, 1.624, 1.537, 1.059, 1.620, 1.452, 1.210, 1.601, 1.645, 1.902,
        0.984, 1.149, 1.528, 1.713, 2.345, 1.527, 1.343, 0.845, 1.455, 1.807,
        1.594, 1.775, 1.989, 1.769, 1.280, 1.564, 1.670, 1.576, 1.583, 1.724,
        1.190, 1.573, 1.294, 1.558, 1.165, 1.226, 1.620, 1.422, 2.318, 1.530])
b after update for 1 param tensor([54.938, 56.775, 58.466, 59.580, 59.604, 53.480, 61.355, 51.709, 57.300,
        50.511, 57.781, 62.164, 60.475, 51.846, 46.726, 62.974, 62.276, 54.457,
        59.086, 53.988, 59.140, 54.010, 54.261, 61.963, 58.171, 60.900, 52.600,
        58.519, 55.291, 51.115, 53.033, 56.963, 57.128, 64.117, 58.623, 61.477,
        63.003, 56.248, 52.740, 48.558, 63.239, 61.883, 58.042, 47.478, 52.693,
        56.497, 61.013, 50.248, 53.647, 56.118, 62.411, 48.033, 57.842, 52.493,
        55.634, 54.039, 60.185, 53.192, 60.472, 56.201, 54.623, 56.414, 54.888,
        45.554, 56.346, 53.347, 48.693, 56.010, 56.774, 61.057, 43.906, 47.460,
        54.714, 57.938, 67.786, 54.701, 51.295, 40.699, 53.389, 59.511, 55.885,
        58.974, 62.433, 58.881, 50.078, 55.357, 57.206, 55.578, 55.698, 58.131,
        48.289, 55.528, 50.360, 55.258, 47.774, 49.009, 56.338, 52.780, 67.404,
        54.755])
a after update for 1 param tensor([[[-0.024],
         [ 0.025],
         [ 0.021],
         [ 0.002],
         [-0.029],
         [-0.071],
         [ 0.008],
         [-0.017],
         [ 0.019],
         [ 0.019]],

        [[-0.020],
         [ 0.034],
         [ 0.020],
         [ 0.045],
         [ 0.064],
         [ 0.027],
         [-0.022],
         [-0.075],
         [ 0.011],
         [ 0.075]],

        [[-0.093],
         [ 0.014],
         [ 0.066],
         [ 0.002],
         [ 0.006],
         [-0.055],
         [ 0.025],
         [-0.001],
         [-0.025],
         [-0.043]],

        [[ 0.050],
         [-0.072],
         [ 0.025],
         [-0.021],
         [-0.032],
         [-0.060],
         [-0.021],
         [ 0.018],
         [-0.008],
         [-0.033]],

        [[-0.032],
         [ 0.001],
         [-0.098],
         [ 0.049],
         [-0.005],
         [-0.030],
         [ 0.028],
         [-0.023],
         [ 0.079],
         [ 0.018]],

        [[ 0.004],
         [ 0.081],
         [-0.029],
         [-0.016],
         [-0.009],
         [-0.025],
         [-0.044],
         [ 0.007],
         [-0.035],
         [ 0.111]],

        [[ 0.009],
         [ 0.008],
         [-0.046],
         [-0.021],
         [-0.033],
         [ 0.075],
         [ 0.008],
         [ 0.019],
         [ 0.107],
         [ 0.035]],

        [[-0.013],
         [-0.012],
         [-0.035],
         [-0.014],
         [ 0.009],
         [ 0.012],
         [ 0.031],
         [-0.009],
         [ 0.084],
         [ 0.077]],

        [[ 0.041],
         [-0.063],
         [ 0.072],
         [-0.040],
         [-0.004],
         [-0.052],
         [-0.032],
         [-0.010],
         [ 0.097],
         [ 0.044]],

        [[-0.008],
         [ 0.025],
         [ 0.021],
         [ 0.016],
         [ 0.035],
         [ 0.011],
         [ 0.009],
         [-0.095],
         [-0.080],
         [ 0.046]]])
s after update for 1 param tensor([[[1.364],
         [1.259],
         [1.533],
         [1.428],
         [1.476],
         [1.758],
         [1.026],
         [1.771],
         [1.940],
         [2.326]],

        [[1.269],
         [1.069],
         [1.725],
         [1.811],
         [0.816],
         [1.566],
         [1.330],
         [2.013],
         [1.393],
         [1.198]],

        [[1.432],
         [1.473],
         [1.758],
         [1.583],
         [1.364],
         [1.133],
         [1.334],
         [1.216],
         [1.597],
         [1.142]],

        [[1.391],
         [1.588],
         [1.061],
         [1.946],
         [1.560],
         [1.487],
         [1.479],
         [1.506],
         [1.155],
         [1.754]],

        [[1.304],
         [1.947],
         [1.797],
         [1.750],
         [1.485],
         [1.792],
         [1.409],
         [2.085],
         [1.901],
         [1.607]],

        [[1.847],
         [1.779],
         [1.706],
         [1.611],
         [2.005],
         [1.399],
         [1.873],
         [1.312],
         [1.726],
         [1.769]],

        [[2.044],
         [1.797],
         [1.631],
         [1.924],
         [1.361],
         [1.560],
         [1.259],
         [1.292],
         [1.466],
         [1.222]],

        [[1.614],
         [1.220],
         [1.850],
         [0.844],
         [1.795],
         [1.793],
         [1.548],
         [1.145],
         [1.853],
         [1.763]],

        [[1.924],
         [1.338],
         [1.663],
         [1.860],
         [1.751],
         [2.137],
         [1.765],
         [1.533],
         [1.703],
         [0.940]],

        [[1.617],
         [1.528],
         [1.743],
         [1.867],
         [1.520],
         [1.578],
         [2.286],
         [1.859],
         [1.584],
         [1.653]]])
b after update for 1 param tensor([[[51.702],
         [49.665],
         [54.805],
         [52.907],
         [53.783],
         [58.692],
         [44.838],
         [58.915],
         [61.652],
         [67.518]],

        [[49.864],
         [45.779],
         [58.150],
         [59.570],
         [39.977],
         [55.393],
         [51.056],
         [62.812],
         [52.248],
         [48.445]],

        [[52.971],
         [53.730],
         [58.703],
         [55.702],
         [51.705],
         [47.129],
         [51.124],
         [48.819],
         [55.937],
         [47.308]],

        [[52.213],
         [55.783],
         [45.599],
         [61.759],
         [55.287],
         [53.985],
         [53.844],
         [54.328],
         [47.584],
         [58.637]],

        [[50.557],
         [61.764],
         [59.346],
         [58.558],
         [53.955],
         [59.268],
         [52.540],
         [63.922],
         [61.035],
         [56.118]],

        [[60.160],
         [59.049],
         [57.814],
         [56.185],
         [62.677],
         [52.354],
         [60.582],
         [50.704],
         [58.159],
         [58.875]],

        [[63.296],
         [59.339],
         [56.532],
         [61.396],
         [51.647],
         [55.298],
         [49.664],
         [50.327],
         [53.599],
         [48.942]],

        [[56.236],
         [48.897],
         [60.209],
         [40.662],
         [59.306],
         [59.270],
         [55.073],
         [47.374],
         [60.263],
         [58.777]],

        [[61.401],
         [51.200],
         [57.086],
         [60.378],
         [58.571],
         [64.721],
         [58.806],
         [54.819],
         [57.773],
         [42.930]],

        [[56.290],
         [54.716],
         [58.445],
         [60.492],
         [54.581],
         [55.602],
         [66.937],
         [60.359],
         [55.719],
         [56.918]]])
a after update for 1 param tensor([[-0.024],
        [-0.050],
        [-0.080],
        [ 0.001],
        [-0.020],
        [-0.021],
        [-0.013],
        [ 0.011],
        [-0.044],
        [ 0.009]])
s after update for 1 param tensor([[1.829],
        [1.874],
        [2.220],
        [1.077],
        [1.491],
        [1.967],
        [1.905],
        [1.550],
        [1.525],
        [1.972]])
b after update for 1 param tensor([[59.868],
        [60.600],
        [65.960],
        [45.935],
        [54.060],
        [62.094],
        [61.093],
        [55.122],
        [54.661],
        [62.166]])
||w||^2 0.008527440834277934
exp ma of ||w||^2 0.04520853462678584
||w|| 0.09234414347579349
exp ma of ||w|| 0.2023695547361215
cpu
[0.233 0.288 0.27  0.287 0.211 0.205 0.409 0.27  0.251 0.323 0.406 0.206
 0.295 0.384 0.268 0.229 0.254 0.302 0.432 0.21  0.178 0.29  0.275 0.457
 0.243 0.213 0.201 0.46  0.277 0.339 0.319 0.358 0.326 0.401 0.384 0.342
 0.328 0.304 0.28  0.273 0.337 0.167 0.14  0.115 0.262 0.282 0.215 0.451
 0.253 0.342 0.333 0.217 0.151 0.175 0.332 0.376 0.199 0.318 0.479 0.202
 0.124 0.226 0.255 0.141 0.189 0.19  0.256 0.289 0.372 0.47  0.264 0.473
 0.265 0.251 0.276 0.242 0.655 0.574 0.563 0.344 0.439 0.296 0.281 0.311
 0.324 0.415 0.446 0.265 0.257 0.144]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.850480109739369, 0.3614942927442927)
Objective function 7.31 = squared loss an data 5.19 + 0.5*rho*h**2 0.411008 + alpha*h 1.205304 + L2reg 0.44 + L1reg 0.08 ; SHD = 28 ; DAG False
Proportion of microbatches that were clipped  0.3808109193095143
iteration 1 in inner loop, alpha 1.3294032160106362 rho 1.0 h 0.9066506903808076
1210
cpu
[0.233 0.288 0.27  0.287 0.211 0.205 0.409 0.27  0.251 0.323 0.406 0.206
 0.295 0.384 0.268 0.229 0.254 0.302 0.432 0.21  0.178 0.29  0.275 0.457
 0.243 0.213 0.201 0.46  0.277 0.339 0.319 0.358 0.326 0.401 0.384 0.342
 0.328 0.304 0.28  0.273 0.337 0.167 0.14  0.115 0.262 0.282 0.215 0.451
 0.253 0.342 0.333 0.217 0.151 0.175 0.332 0.376 0.199 0.318 0.479 0.202
 0.124 0.226 0.255 0.141 0.189 0.19  0.256 0.289 0.372 0.47  0.264 0.473
 0.265 0.251 0.276 0.242 0.655 0.574 0.563 0.344 0.439 0.296 0.281 0.311
 0.324 0.415 0.446 0.265 0.257 0.144]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.850480109739369, 0.3614942927442927)
Objective function 11.01 = squared loss an data 5.19 + 0.5*rho*h**2 4.110077 + alpha*h 1.205304 + L2reg 0.44 + L1reg 0.08 ; SHD = 28 ; DAG False
||w||^2 19379760.506331168
exp ma of ||w||^2 34970770.76991292
||w|| 4402.2449393839015
exp ma of ||w|| 5279.886209850882
||w||^2 0.45098732176668965
exp ma of ||w||^2 2.6314191357168686
||w|| 0.6715558962340288
exp ma of ||w|| 1.4291287142740075
||w||^2 0.05960688642530963
exp ma of ||w||^2 0.0707916060599004
||w|| 0.24414521585587057
exp ma of ||w|| 0.25167217248553586
||w||^2 0.07254762352313854
exp ma of ||w||^2 0.06827550267453142
||w|| 0.26934666050118117
exp ma of ||w|| 0.24787614752900777
||w||^2 0.037924770554618015
exp ma of ||w||^2 0.065683013727936
||w|| 0.1947428318439937
exp ma of ||w|| 0.24369957177777543
cpu
[0.334 0.237 0.282 0.33  0.138 0.132 0.183 0.096 0.32  0.182 0.46  0.354
 0.123 0.21  0.162 0.264 0.288 0.229 0.373 0.106 0.193 0.224 0.2   0.196
 0.176 0.134 0.256 0.376 0.18  0.265 0.167 0.351 0.202 0.21  0.244 0.502
 0.145 0.209 0.178 0.18  0.205 0.297 0.305 0.128 0.263 0.339 0.231 0.257
 0.143 0.24  0.22  0.255 0.165 0.143 0.479 0.321 0.408 0.341 0.213 0.307
 0.438 0.237 0.336 0.332 0.306 0.321 0.272 0.158 0.212 0.142 0.265 0.36
 0.426 0.169 0.433 0.272 0.35  0.534 0.392 0.203 0.473 0.174 0.137 0.193
 0.082 0.298 0.345 0.156 0.186 0.115]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.6364883401920439, 0.18835146335146336)
Objective function 7.52 = squared loss an data 5.77 + 0.5*rho*h**2 0.627617 + alpha*h 0.470998 + L2reg 0.59 + L1reg 0.06 ; SHD = 27 ; DAG False
Proportion of microbatches that were clipped  0.3587514816278151
iteration 2 in inner loop, alpha 1.3294032160106362 rho 10.0 h 0.3542929005863549
1210
cpu
[0.334 0.237 0.282 0.33  0.138 0.132 0.183 0.096 0.32  0.182 0.46  0.354
 0.123 0.21  0.162 0.264 0.288 0.229 0.373 0.106 0.193 0.224 0.2   0.196
 0.176 0.134 0.256 0.376 0.18  0.265 0.167 0.351 0.202 0.21  0.244 0.502
 0.145 0.209 0.178 0.18  0.205 0.297 0.305 0.128 0.263 0.339 0.231 0.257
 0.143 0.24  0.22  0.255 0.165 0.143 0.479 0.321 0.408 0.341 0.213 0.307
 0.438 0.237 0.336 0.332 0.306 0.321 0.272 0.158 0.212 0.142 0.265 0.36
 0.426 0.169 0.433 0.272 0.35  0.534 0.392 0.203 0.473 0.174 0.137 0.193
 0.082 0.298 0.345 0.156 0.186 0.115]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.6364883401920439, 0.18835146335146336)
Objective function 13.17 = squared loss an data 5.77 + 0.5*rho*h**2 6.276173 + alpha*h 0.470998 + L2reg 0.59 + L1reg 0.06 ; SHD = 27 ; DAG False
||w||^2 91308425182.25241
exp ma of ||w||^2 18025683467.901226
||w|| 302172.8399149275
exp ma of ||w|| 57248.638224807364
||w||^2 23724908.85711456
exp ma of ||w||^2 258932654.39096588
||w|| 4870.822195185794
exp ma of ||w|| 13612.38667581625
||w||^2 151796904.6065756
exp ma of ||w||^2 150370532.38042885
||w|| 12320.588646918442
exp ma of ||w|| 10470.409275728412
||w||^2 78165.18032547837
exp ma of ||w||^2 403569.5525231178
||w|| 279.5803646994516
exp ma of ||w|| 562.2273673751339
||w||^2 54.277545137241724
exp ma of ||w||^2 44.66901221493795
||w|| 7.367329579789526
exp ma of ||w|| 5.844249982689255
||w||^2 0.13639565910845042
exp ma of ||w||^2 0.6245365048915984
||w|| 0.3693178293942095
exp ma of ||w|| 0.703340062999534
||w||^2 0.36494149752882465
exp ma of ||w||^2 0.18960350485120933
||w|| 0.6041038797498528
exp ma of ||w|| 0.40877902746844424
||w||^2 0.17541268643771019
exp ma of ||w||^2 0.09839961071721329
||w|| 0.418822977447167
exp ma of ||w|| 0.29611182552910253
||w||^2 0.07787895393556568
exp ma of ||w||^2 0.09702180995263654
||w|| 0.2790680095166153
exp ma of ||w|| 0.29375704193067237
||w||^2 0.034146121973339945
exp ma of ||w||^2 0.08541915002903719
||w|| 0.1847866931717215
exp ma of ||w|| 0.27430103877682044
||w||^2 0.046424440098465164
exp ma of ||w||^2 0.0975352031249178
||w|| 0.21546331497140103
exp ma of ||w|| 0.29142357357938065
||w||^2 0.06837790263418919
exp ma of ||w||^2 0.09315999216872661
||w|| 0.2614916875049553
exp ma of ||w|| 0.2882491111653591
cpu
[0.071 0.224 0.117 0.181 0.19  0.133 0.098 0.057 0.204 0.504 0.323 0.203
 0.303 0.158 0.231 0.116 0.113 0.266 0.288 0.137 0.103 0.189 0.095 0.283
 0.111 0.109 0.259 0.555 0.128 0.373 0.36  0.373 0.196 0.234 0.182 0.197
 0.206 0.123 0.211 0.132 0.166 0.071 0.096 0.11  0.118 0.243 0.241 0.274
 0.137 0.264 0.108 0.116 0.15  0.316 0.325 0.226 0.128 0.168 0.285 0.198
 0.099 0.119 0.19  0.267 0.306 0.386 0.116 0.353 0.213 0.392 0.07  0.933
 0.47  0.49  0.212 0.202 0.209 0.301 0.283 0.53  0.449 0.179 0.177 0.113
 0.154 0.443 0.179 0.213 0.039 0.095]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9012345679012346, 0.5891990055033534)
Objective function 8.10 = squared loss an data 6.57 + 0.5*rho*h**2 0.696204 + alpha*h 0.156870 + L2reg 0.62 + L1reg 0.05 ; SHD = 17 ; DAG True
Proportion of microbatches that were clipped  0.3588959523617929
iteration 3 in inner loop, alpha 1.3294032160106362 rho 100.0 h 0.11800033994177284
iteration 2 in outer loop, alpha = 13.12943721018792, rho = 100.0, h = 0.11800033994177284
cpu
1210
cpu
[0.071 0.224 0.117 0.181 0.19  0.133 0.098 0.057 0.204 0.504 0.323 0.203
 0.303 0.158 0.231 0.116 0.113 0.266 0.288 0.137 0.103 0.189 0.095 0.283
 0.111 0.109 0.259 0.555 0.128 0.373 0.36  0.373 0.196 0.234 0.182 0.197
 0.206 0.123 0.211 0.132 0.166 0.071 0.096 0.11  0.118 0.243 0.241 0.274
 0.137 0.264 0.108 0.116 0.15  0.316 0.325 0.226 0.128 0.168 0.285 0.198
 0.099 0.119 0.19  0.267 0.306 0.386 0.116 0.353 0.213 0.392 0.07  0.933
 0.47  0.49  0.212 0.202 0.209 0.301 0.283 0.53  0.449 0.179 0.177 0.113
 0.154 0.443 0.179 0.213 0.039 0.095]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9012345679012346, 0.5891990055033534)
Objective function 9.49 = squared loss an data 6.57 + 0.5*rho*h**2 0.696204 + alpha*h 1.549278 + L2reg 0.62 + L1reg 0.05 ; SHD = 17 ; DAG True
||w||^2 5749291640.102181
exp ma of ||w||^2 7037914965.157577
||w|| 75824.08350980697
exp ma of ||w|| 73263.74123613258
||w||^2 89708628.95829248
exp ma of ||w||^2 370172557.4721426
||w|| 9471.463928997064
exp ma of ||w|| 16666.387601857405
||w||^2 25243505.015485395
exp ma of ||w||^2 172629484.2122043
||w|| 5024.291493881042
exp ma of ||w|| 11280.970906054217
||w||^2 5477517.345312041
exp ma of ||w||^2 56327812.6437059
||w|| 2340.409653311155
exp ma of ||w|| 6466.713865045969
||w||^2 416.99704820754573
exp ma of ||w||^2 1658.4802729984992
||w|| 20.42050558158504
exp ma of ||w|| 34.78272135260298
||w||^2 11.847860818217251
exp ma of ||w||^2 30.92454865341594
||w|| 3.44207216923429
exp ma of ||w|| 4.80155225274963
||w||^2 0.10809070792931931
exp ma of ||w||^2 0.10316856639867866
||w|| 0.3287715132570328
exp ma of ||w|| 0.3018453693890421
||w||^2 0.1991832783615929
exp ma of ||w||^2 0.08987990593874406
||w|| 0.44629953883192947
exp ma of ||w|| 0.28364061044445127
||w||^2 0.06804266047596909
exp ma of ||w||^2 0.09721246142332478
||w|| 0.2608498811116637
exp ma of ||w|| 0.2964502075264887
||w||^2 0.1085151501946714
exp ma of ||w||^2 0.0884414493148206
||w|| 0.32941637815183294
exp ma of ||w|| 0.28362756496087155
||w||^2 0.08959374135394736
exp ma of ||w||^2 0.08970771824894609
||w|| 0.2993221364248681
exp ma of ||w|| 0.2856301698946077
||w||^2 0.06361115474802304
exp ma of ||w||^2 0.10416947970080954
||w|| 0.2522125190152603
exp ma of ||w|| 0.3037479857221582
||w||^2 0.10649709461750642
exp ma of ||w||^2 0.09611972602315702
||w|| 0.32633892599183817
exp ma of ||w|| 0.28879740724919206
||w||^2 0.11654793427790093
exp ma of ||w||^2 0.10030166684614189
||w|| 0.34139117486821613
exp ma of ||w|| 0.29851013916993147
cpu
[0.082 0.265 0.038 0.171 0.074 0.17  0.103 0.054 0.205 0.595 0.19  0.186
 0.335 0.117 0.226 0.241 0.086 0.253 0.216 0.175 0.107 0.153 0.117 0.374
 0.099 0.085 0.211 0.744 0.202 0.223 0.335 0.207 0.377 0.186 0.355 0.43
 0.15  0.085 0.14  0.067 0.08  0.119 0.087 0.086 0.188 0.302 0.235 0.311
 0.141 0.555 0.234 0.094 0.057 0.249 0.173 0.135 0.118 0.091 0.217 0.091
 0.196 0.046 0.196 0.205 0.131 0.248 0.148 0.366 0.287 0.178 0.171 0.862
 0.556 0.377 0.314 0.094 0.3   0.367 0.614 0.184 0.633 0.163 0.072 0.146
 0.06  0.232 0.143 0.141 0.046 0.055]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9355281207133059, 0.7429002670381981)
Objective function 8.65 = squared loss an data 6.87 + 0.5*rho*h**2 0.229694 + alpha*h 0.889890 + L2reg 0.61 + L1reg 0.05 ; SHD = 14 ; DAG True
Proportion of microbatches that were clipped  0.3411859620814845
iteration 1 in inner loop, alpha 13.12943721018792 rho 100.0 h 0.06777820270804824
1210
cpu
[0.082 0.265 0.038 0.171 0.074 0.17  0.103 0.054 0.205 0.595 0.19  0.186
 0.335 0.117 0.226 0.241 0.086 0.253 0.216 0.175 0.107 0.153 0.117 0.374
 0.099 0.085 0.211 0.744 0.202 0.223 0.335 0.207 0.377 0.186 0.355 0.43
 0.15  0.085 0.14  0.067 0.08  0.119 0.087 0.086 0.188 0.302 0.235 0.311
 0.141 0.555 0.234 0.094 0.057 0.249 0.173 0.135 0.118 0.091 0.217 0.091
 0.196 0.046 0.196 0.205 0.131 0.248 0.148 0.366 0.287 0.178 0.171 0.862
 0.556 0.377 0.314 0.094 0.3   0.367 0.614 0.184 0.633 0.163 0.072 0.146
 0.06  0.232 0.143 0.141 0.046 0.055]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9355281207133059, 0.7429002670381981)
Objective function 10.72 = squared loss an data 6.87 + 0.5*rho*h**2 2.296942 + alpha*h 0.889890 + L2reg 0.61 + L1reg 0.05 ; SHD = 14 ; DAG True
||w||^2 13132477.002191171
exp ma of ||w||^2 88936452.9120672
||w|| 3623.875963963332
exp ma of ||w|| 8148.100245977992
||w||^2 289.104027244284
exp ma of ||w||^2 617.8690554994579
||w|| 17.003059349548952
exp ma of ||w|| 20.606442789067682
||w||^2 91.98950764151817
exp ma of ||w||^2 139.24310466702522
||w|| 9.59111607903471
exp ma of ||w|| 9.677836700161457
||w||^2 0.2696668596443537
exp ma of ||w||^2 0.25954096519473846
||w|| 0.5192945788705614
exp ma of ||w|| 0.4803873225917465
||w||^2 0.1980009495679808
exp ma of ||w||^2 0.10937615286011339
||w|| 0.4449729762221306
exp ma of ||w|| 0.3195535981754453
||w||^2 0.11912519240524284
exp ma of ||w||^2 0.1174652962811421
||w|| 0.3451451758394471
exp ma of ||w|| 0.3260026561952629
||w||^2 0.038213869406989835
exp ma of ||w||^2 0.10200461541003177
||w|| 0.19548368066667313
exp ma of ||w|| 0.3032090679270688
||w||^2 0.053742022928104455
exp ma of ||w||^2 0.10197267312599646
||w|| 0.23182325795334785
exp ma of ||w|| 0.3072113614329144
||w||^2 0.20190748928747249
exp ma of ||w||^2 0.09662246439637577
||w|| 0.4493411724819711
exp ma of ||w|| 0.29330326258446543
||w||^2 0.07927643392414044
exp ma of ||w||^2 0.09133783022961606
||w|| 0.28156071090288937
exp ma of ||w|| 0.29042749667981854
||w||^2 0.06917033783455234
exp ma of ||w||^2 0.09869268316386917
||w|| 0.2630025433993982
exp ma of ||w|| 0.2956026923520049
cpu
[0.019 0.126 0.029 0.077 0.103 0.145 0.052 0.057 0.143 0.812 0.131 0.15
 0.131 0.137 0.304 0.079 0.03  0.103 0.283 0.174 0.059 0.232 0.067 0.34
 0.154 0.047 0.152 0.866 0.243 0.323 0.191 0.098 0.229 0.06  0.083 0.358
 0.14  0.167 0.083 0.109 0.062 0.113 0.026 0.082 0.393 0.133 0.198 0.28
 0.147 0.305 0.372 0.131 0.112 0.397 0.209 0.065 0.079 0.072 0.234 0.067
 0.058 0.021 0.142 0.22  0.219 0.124 0.194 0.662 0.23  0.323 0.166 0.749
 0.367 0.288 0.326 0.241 0.332 0.173 0.863 0.141 0.517 0.201 0.185 0.109
 0.076 0.069 0.041 0.16  0.03  0.043]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8024691358024691, 0.6455189146365616)
Objective function 8.63 = squared loss an data 7.23 + 0.5*rho*h**2 0.405177 + alpha*h 0.373752 + L2reg 0.57 + L1reg 0.05 ; SHD = 14 ; DAG True
Proportion of microbatches that were clipped  0.30824315722469764
iteration 2 in inner loop, alpha 13.12943721018792 rho 1000.0 h 0.02846671533134959
iteration 3 in outer loop, alpha = 41.596152541537506, rho = 1000.0, h = 0.02846671533134959
cpu
1210
cpu
[0.019 0.126 0.029 0.077 0.103 0.145 0.052 0.057 0.143 0.812 0.131 0.15
 0.131 0.137 0.304 0.079 0.03  0.103 0.283 0.174 0.059 0.232 0.067 0.34
 0.154 0.047 0.152 0.866 0.243 0.323 0.191 0.098 0.229 0.06  0.083 0.358
 0.14  0.167 0.083 0.109 0.062 0.113 0.026 0.082 0.393 0.133 0.198 0.28
 0.147 0.305 0.372 0.131 0.112 0.397 0.209 0.065 0.079 0.072 0.234 0.067
 0.058 0.021 0.142 0.22  0.219 0.124 0.194 0.662 0.23  0.323 0.166 0.749
 0.367 0.288 0.326 0.241 0.332 0.173 0.863 0.141 0.517 0.201 0.185 0.109
 0.076 0.069 0.041 0.16  0.03  0.043]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8024691358024691, 0.6455189146365616)
Objective function 9.44 = squared loss an data 7.23 + 0.5*rho*h**2 0.405177 + alpha*h 1.184106 + L2reg 0.57 + L1reg 0.05 ; SHD = 14 ; DAG True
||w||^2 0.11856969896202449
exp ma of ||w||^2 0.09525754070368667
||w|| 0.3443395111834024
exp ma of ||w|| 0.29633372577546907
||w||^2 0.10934689262308642
exp ma of ||w||^2 0.10234857291194167
||w|| 0.3306764167930432
exp ma of ||w|| 0.3075498263983985
||w||^2 0.14826237070431636
exp ma of ||w||^2 0.10384834323979426
||w|| 0.38504853032353775
exp ma of ||w|| 0.30820556554329864
||w||^2 0.04622026291094957
exp ma of ||w||^2 0.10200469764655538
||w|| 0.21498898323158228
exp ma of ||w|| 0.30759605183765704
||w||^2 0.05011273000014362
exp ma of ||w||^2 0.10331040783877084
||w|| 0.22385872777299443
exp ma of ||w|| 0.30940940875361717
||w||^2 0.1360041465238204
exp ma of ||w||^2 0.09557692689254574
||w|| 0.36878740016955625
exp ma of ||w|| 0.2979153379971408
||w||^2 0.16515205377552608
exp ma of ||w||^2 0.09809976880879309
||w|| 0.4063890423910641
exp ma of ||w|| 0.30121407966979225
||w||^2 0.06855816165240829
exp ma of ||w||^2 0.09693184813503633
||w|| 0.261836135115855
exp ma of ||w|| 0.3009469026206865
||w||^2 0.050702606968711
exp ma of ||w||^2 0.09311623004889065
||w|| 0.22517239388679733
exp ma of ||w|| 0.29328181343584997
cpu
[0.025 0.15  0.014 0.115 0.07  0.099 0.046 0.031 0.053 0.622 0.207 0.079
 0.163 0.163 0.24  0.084 0.077 0.153 0.263 0.09  0.061 0.094 0.264 0.641
 0.122 0.137 0.146 1.067 0.223 0.359 0.216 0.13  0.231 0.115 0.075 0.115
 0.191 0.079 0.134 0.092 0.063 0.24  0.033 0.091 0.222 0.27  0.09  0.076
 0.114 0.288 0.399 0.106 0.079 0.191 0.206 0.07  0.035 0.033 0.089 0.058
 0.045 0.017 0.118 0.411 0.3   0.181 0.137 0.732 0.174 0.32  0.112 0.848
 0.254 0.171 0.14  0.214 0.254 0.359 0.776 0.133 0.451 0.253 0.105 0.126
 0.115 0.077 0.088 0.126 0.024 0.059]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8436213991769547, 0.6774437598751906)
Objective function 8.99 = squared loss an data 7.36 + 0.5*rho*h**2 0.215282 + alpha*h 0.863121 + L2reg 0.51 + L1reg 0.04 ; SHD = 10 ; DAG True
Proportion of microbatches that were clipped  0.2930319538173835
iteration 1 in inner loop, alpha 41.596152541537506 rho 1000.0 h 0.020750016592540277
1210
cpu
[0.025 0.15  0.014 0.115 0.07  0.099 0.046 0.031 0.053 0.622 0.207 0.079
 0.163 0.163 0.24  0.084 0.077 0.153 0.263 0.09  0.061 0.094 0.264 0.641
 0.122 0.137 0.146 1.067 0.223 0.359 0.216 0.13  0.231 0.115 0.075 0.115
 0.191 0.079 0.134 0.092 0.063 0.24  0.033 0.091 0.222 0.27  0.09  0.076
 0.114 0.288 0.399 0.106 0.079 0.191 0.206 0.07  0.035 0.033 0.089 0.058
 0.045 0.017 0.118 0.411 0.3   0.181 0.137 0.732 0.174 0.32  0.112 0.848
 0.254 0.171 0.14  0.214 0.254 0.359 0.776 0.133 0.451 0.253 0.105 0.126
 0.115 0.077 0.088 0.126 0.024 0.059]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8436213991769547, 0.6774437598751906)
Objective function 10.93 = squared loss an data 7.36 + 0.5*rho*h**2 2.152816 + alpha*h 0.863121 + L2reg 0.51 + L1reg 0.04 ; SHD = 10 ; DAG True
||w||^2 2599147766.8990927
exp ma of ||w||^2 42989784080.41829
||w|| 50981.837617911464
exp ma of ||w|| 147948.96080773819
||w||^2 41.20334522302312
exp ma of ||w||^2 52.6697302533764
||w|| 6.418983192299472
exp ma of ||w|| 5.411835191031911
||w||^2 3.1604450352240616
exp ma of ||w||^2 10.876432690779518
||w|| 1.7777640549926927
exp ma of ||w|| 2.4620426227695758
||w||^2 0.08128682139497778
exp ma of ||w||^2 0.12582262175255687
||w|| 0.2851084379582228
exp ma of ||w|| 0.34701447935893015
||w||^2 0.09447261008908453
exp ma of ||w||^2 0.12320781657035042
||w|| 0.30736397005681154
exp ma of ||w|| 0.3439301035885413
||w||^2 0.07784430090223779
exp ma of ||w||^2 0.11566499690611919
||w|| 0.2790059155326958
exp ma of ||w|| 0.3324748068272144
||w||^2 0.06788279537431775
exp ma of ||w||^2 0.10862955105582851
||w|| 0.2605432696776444
exp ma of ||w|| 0.3197090499121116
||w||^2 0.3414299652407575
exp ma of ||w||^2 0.11091946308865308
||w|| 0.5843200880003677
exp ma of ||w|| 0.3242949592938003
cpu
[0.015 0.024 0.012 0.113 0.042 0.093 0.033 0.047 0.09  0.691 0.176 0.048
 0.086 0.109 0.198 0.085 0.036 0.203 0.374 0.076 0.105 0.172 0.178 0.681
 0.067 0.103 0.396 0.809 0.256 0.12  0.101 0.13  0.484 0.14  0.135 0.325
 0.13  0.115 0.085 0.084 0.097 0.229 0.051 0.064 0.51  0.249 0.085 0.062
 0.087 0.137 0.133 0.043 0.06  0.148 0.111 0.06  0.019 0.019 0.077 0.071
 0.043 0.008 0.109 0.373 0.162 0.152 0.116 0.287 0.235 0.262 0.092 0.981
 0.237 0.39  0.07  0.067 0.165 0.24  1.332 0.144 0.615 0.098 0.064 0.041
 0.038 0.024 0.078 0.134 0.013 0.023]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7407407407407408, 0.7057586715425388)
Objective function 8.67 = squared loss an data 7.46 + 0.5*rho*h**2 0.336695 + alpha*h 0.341340 + L2reg 0.49 + L1reg 0.04 ; SHD = 8 ; DAG True
Proportion of microbatches that were clipped  0.2571056274940956
iteration 2 in inner loop, alpha 41.596152541537506 rho 10000.0 h 0.008206035699217296
1210
cpu
[0.015 0.024 0.012 0.113 0.042 0.093 0.033 0.047 0.09  0.691 0.176 0.048
 0.086 0.109 0.198 0.085 0.036 0.203 0.374 0.076 0.105 0.172 0.178 0.681
 0.067 0.103 0.396 0.809 0.256 0.12  0.101 0.13  0.484 0.14  0.135 0.325
 0.13  0.115 0.085 0.084 0.097 0.229 0.051 0.064 0.51  0.249 0.085 0.062
 0.087 0.137 0.133 0.043 0.06  0.148 0.111 0.06  0.019 0.019 0.077 0.071
 0.043 0.008 0.109 0.373 0.162 0.152 0.116 0.287 0.235 0.262 0.092 0.981
 0.237 0.39  0.07  0.067 0.165 0.24  1.332 0.144 0.615 0.098 0.064 0.041
 0.038 0.024 0.078 0.134 0.013 0.023]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7407407407407408, 0.7057586715425388)
Objective function 11.70 = squared loss an data 7.46 + 0.5*rho*h**2 3.366951 + alpha*h 0.341340 + L2reg 0.49 + L1reg 0.04 ; SHD = 8 ; DAG True
||w||^2 52096.524222717286
exp ma of ||w||^2 15581461.116432859
||w|| 228.24663025490057
exp ma of ||w|| 521.8075840206864
||w||^2 0.2884133426961697
exp ma of ||w||^2 0.35120213253689064
||w|| 0.5370412858395244
exp ma of ||w|| 0.5066392218938359
||w||^2 0.12287251258222262
exp ma of ||w||^2 0.18045530822113862
||w|| 0.35053175688120275
exp ma of ||w|| 0.41980555863608504
||w||^2 0.0923108141102985
exp ma of ||w||^2 0.14844907532063528
||w|| 0.30382694763680607
exp ma of ||w|| 0.38067363141512284
||w||^2 0.11756956087970988
exp ma of ||w||^2 0.14393460100962638
||w|| 0.34288417997876464
exp ma of ||w|| 0.37233100968640015
||w||^2 0.12147544506200073
exp ma of ||w||^2 0.1399773634943419
||w|| 0.34853327683594393
exp ma of ||w|| 0.36785644654568334
||w||^2 0.10441795182244604
exp ma of ||w||^2 0.13575622629290077
||w|| 0.3231376669818083
exp ma of ||w|| 0.3635043815592344
||w||^2 0.10189885499900474
exp ma of ||w||^2 0.12686339936538796
||w|| 0.3192160005372612
exp ma of ||w|| 0.34956683943500877
||w||^2 0.11329369789314145
exp ma of ||w||^2 0.13670960882684152
||w|| 0.3365912920637452
exp ma of ||w|| 0.3631270698861028
||w||^2 0.0793892351376699
exp ma of ||w||^2 0.13437213971113907
||w|| 0.281760953891184
exp ma of ||w|| 0.3619750005004875
||w||^2 0.05770598128500402
exp ma of ||w||^2 0.13372158338611742
||w|| 0.24022069287429013
exp ma of ||w|| 0.3601039029590121
||w||^2 0.11007484841637498
exp ma of ||w||^2 0.1314611192810598
||w|| 0.3317752980804553
exp ma of ||w|| 0.35696190477169937
||w||^2 0.10657198293753867
exp ma of ||w||^2 0.14004869239756315
||w|| 0.3264536459247142
exp ma of ||w|| 0.3690927979265294
||w||^2 0.15981284783058444
exp ma of ||w||^2 0.13393807124663193
||w|| 0.39976599133816326
exp ma of ||w|| 0.36094469691064734
||w||^2 0.2733911052064698
exp ma of ||w||^2 0.12765234762665997
||w|| 0.5228681527942487
exp ma of ||w|| 0.35268955718561973
||w||^2 0.14264084439336522
exp ma of ||w||^2 0.13485584482707272
||w|| 0.3776782286462449
exp ma of ||w|| 0.36194860747799895
cpu
[0.009 0.014 0.008 0.051 0.055 0.11  0.018 0.02  0.078 0.831 0.29  0.027
 0.071 0.101 0.267 0.077 0.082 0.241 0.551 0.029 0.025 0.067 0.111 0.982
 0.091 0.024 0.264 0.771 0.24  0.279 0.275 0.116 0.266 0.082 0.02  0.512
 0.204 0.072 0.078 0.029 0.171 0.319 0.012 0.021 0.6   0.084 0.08  0.055
 0.059 0.071 0.086 0.049 0.028 0.346 0.078 0.026 0.008 0.013 0.018 0.081
 0.008 0.004 0.123 0.569 0.084 0.065 0.092 0.538 0.131 0.64  0.023 0.982
 0.277 0.17  0.366 0.231 0.323 0.297 1.218 0.411 0.772 0.086 0.041 0.023
 0.015 0.01  0.02  0.076 0.01  0.006]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7791495198902607, 0.7251795059023975)
Objective function 8.51 = squared loss an data 7.48 + 0.5*rho*h**2 0.396232 + alpha*h 0.117096 + L2reg 0.48 + L1reg 0.04 ; SHD = 12 ; DAG True
Proportion of microbatches that were clipped  0.18850427692061716
iteration 3 in inner loop, alpha 41.596152541537506 rho 100000.0 h 0.002815072339004132
iteration 4 in outer loop, alpha = 323.1033864419507, rho = 100000.0, h = 0.002815072339004132
cpu
1210
cpu
[0.009 0.014 0.008 0.051 0.055 0.11  0.018 0.02  0.078 0.831 0.29  0.027
 0.071 0.101 0.267 0.077 0.082 0.241 0.551 0.029 0.025 0.067 0.111 0.982
 0.091 0.024 0.264 0.771 0.24  0.279 0.275 0.116 0.266 0.082 0.02  0.512
 0.204 0.072 0.078 0.029 0.171 0.319 0.012 0.021 0.6   0.084 0.08  0.055
 0.059 0.071 0.086 0.049 0.028 0.346 0.078 0.026 0.008 0.013 0.018 0.081
 0.008 0.004 0.123 0.569 0.084 0.065 0.092 0.538 0.131 0.64  0.023 0.982
 0.277 0.17  0.366 0.231 0.323 0.297 1.218 0.411 0.772 0.086 0.041 0.023
 0.015 0.01  0.02  0.076 0.01  0.006]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7791495198902607, 0.7251795059023975)
Objective function 9.31 = squared loss an data 7.48 + 0.5*rho*h**2 0.396232 + alpha*h 0.909559 + L2reg 0.48 + L1reg 0.04 ; SHD = 12 ; DAG True
||w||^2 790877108.0735632
exp ma of ||w||^2 315893385642.0327
||w|| 28122.537369049103
exp ma of ||w|| 279833.0032054637
||w||^2 0.8788511128038845
exp ma of ||w||^2 6623.236651321622
||w|| 0.9374705930341946
exp ma of ||w|| 1.7044322045520939
||w||^2 0.19535131198899136
exp ma of ||w||^2 8.84757299151041
||w|| 0.44198564681332286
exp ma of ||w|| 0.5463869726580155
||w||^2 0.261564122690757
exp ma of ||w||^2 2.356435151144089
||w|| 0.5114334000539631
exp ma of ||w|| 0.5462046351088835
||w||^2 0.1743437303082802
exp ma of ||w||^2 0.18345293482715921
||w|| 0.41754488418406016
exp ma of ||w|| 0.42542376281786287
||w||^2 0.21349695248533856
exp ma of ||w||^2 0.19339562691084627
||w|| 0.46205730433068426
exp ma of ||w|| 0.4356933720743079
||w||^2 0.12061796703711807
exp ma of ||w||^2 0.1804048101657141
||w|| 0.34730097471374605
exp ma of ||w|| 0.4208743952821381
||w||^2 0.22513237579201684
exp ma of ||w||^2 0.16972510495243465
||w|| 0.47448116484431374
exp ma of ||w|| 0.40810763660407506
||w||^2 0.1768970714850603
exp ma of ||w||^2 0.16571634859308726
||w|| 0.4205913354850053
exp ma of ||w|| 0.4033745521598906
||w||^2 0.14248870165673047
exp ma of ||w||^2 0.1789164869677866
||w|| 0.37747675644565254
exp ma of ||w|| 0.41890062516649285
||w||^2 0.10737956742748497
exp ma of ||w||^2 0.16210254027398255
||w|| 0.327688216796828
exp ma of ||w|| 0.39952262284335105
||w||^2 0.17918092390081194
exp ma of ||w||^2 0.1561613282076446
||w|| 0.4232976776463721
exp ma of ||w|| 0.39176863694862585
||w||^2 0.10732828302685225
exp ma of ||w||^2 0.13945268869299526
||w|| 0.32760995562841533
exp ma of ||w|| 0.3701090942031673
||w||^2 0.1654017838703102
exp ma of ||w||^2 0.16068154202245744
||w|| 0.4066961812831665
exp ma of ||w|| 0.397561456161079
cpu
[0.004 0.008 0.005 0.035 0.014 0.147 0.007 0.016 0.058 0.869 0.369 0.056
 0.139 0.199 0.349 0.096 0.03  0.388 0.664 0.021 0.058 0.084 0.144 0.836
 0.046 0.02  0.182 1.146 0.116 0.142 0.165 0.037 0.29  0.04  0.135 0.257
 0.148 0.045 0.063 0.045 0.056 0.197 0.005 0.025 0.502 0.323 0.031 0.04
 0.12  0.102 0.317 0.026 0.018 0.261 0.028 0.011 0.007 0.015 0.032 0.023
 0.009 0.004 0.042 0.662 0.081 0.153 0.149 0.655 0.228 0.602 0.07  1.32
 0.249 0.188 0.265 0.077 0.262 0.295 1.041 0.068 0.97  0.159 0.023 0.036
 0.021 0.008 0.02  0.148 0.005 0.004]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7695473251028807, 0.7166556488893965)
Objective function 8.67 = squared loss an data 7.48 + 0.5*rho*h**2 0.149299 + alpha*h 0.558322 + L2reg 0.44 + L1reg 0.04 ; SHD = 11 ; DAG True
Proportion of microbatches that were clipped  0.1603449652639144
iteration 1 in inner loop, alpha 323.1033864419507 rho 100000.0 h 0.0017279994310630542
iteration 5 in outer loop, alpha = 2051.1028175050046, rho = 1000000.0, h = 0.0017279994310630542
Threshold 0.3
[[0.006 0.004 0.008 0.005 0.035 0.014 0.147 0.007 0.016 0.058]
 [0.869 0.005 0.369 0.056 0.139 0.199 0.349 0.096 0.03  0.388]
 [0.664 0.021 0.004 0.058 0.084 0.144 0.836 0.046 0.02  0.182]
 [1.146 0.116 0.142 0.003 0.165 0.037 0.29  0.04  0.135 0.257]
 [0.148 0.045 0.063 0.045 0.003 0.056 0.197 0.005 0.025 0.502]
 [0.323 0.031 0.04  0.12  0.102 0.003 0.317 0.026 0.018 0.261]
 [0.028 0.011 0.007 0.015 0.032 0.023 0.005 0.009 0.004 0.042]
 [0.662 0.081 0.153 0.149 0.655 0.228 0.602 0.004 0.07  1.32 ]
 [0.249 0.188 0.265 0.077 0.262 0.295 1.041 0.068 0.003 0.97 ]
 [0.159 0.023 0.036 0.021 0.008 0.02  0.148 0.005 0.004 0.005]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.869 0.    0.369 0.    0.    0.    0.349 0.    0.    0.388]
 [0.664 0.    0.    0.    0.    0.    0.836 0.    0.    0.   ]
 [1.146 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.502]
 [0.323 0.    0.    0.    0.    0.    0.317 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.662 0.    0.    0.    0.655 0.    0.602 0.    0.    1.32 ]
 [0.    0.    0.    0.    0.    0.    1.041 0.    0.    0.97 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.625, 'tpr': 0.6666666666666666, 'fpr': 0.2777777777777778, 'f1': 0.4800000000000001, 'shd': 11, 'npred': 16, 'ntrue': 9}
[0.004 0.008 0.005 0.035 0.014 0.147 0.007 0.016 0.058 0.869 0.369 0.056
 0.139 0.199 0.349 0.096 0.03  0.388 0.664 0.021 0.058 0.084 0.144 0.836
 0.046 0.02  0.182 1.146 0.116 0.142 0.165 0.037 0.29  0.04  0.135 0.257
 0.148 0.045 0.063 0.045 0.056 0.197 0.005 0.025 0.502 0.323 0.031 0.04
 0.12  0.102 0.317 0.026 0.018 0.261 0.028 0.011 0.007 0.015 0.032 0.023
 0.009 0.004 0.042 0.662 0.081 0.153 0.149 0.655 0.228 0.602 0.07  1.32
 0.249 0.188 0.265 0.077 0.262 0.295 1.041 0.068 0.97  0.159 0.023 0.036
 0.021 0.008 0.02  0.148 0.005 0.004]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.7695473251028807, 0.7166556488893965)
Iterations 2500
Achieves (13.610683049707138, 1e-05)-DP
