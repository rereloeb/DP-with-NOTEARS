samples  5000  graph  10 20 ER mlp  minibatch size  100  noise  0.4  minibatches per NN training  250 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.0585857326205694
iteration 1 in outer loop, alpha = 1.0585857326205694, rho = 1.0, h = 1.0585857326205694
cuda
iteration 1 in inner loop,alpha 1.0585857326205694 rho 1.0 h 0.7232408055071513
iteration 2 in inner loop,alpha 1.0585857326205694 rho 10.0 h 0.3051534967249747
iteration 3 in inner loop,alpha 1.0585857326205694 rho 100.0 h 0.08672487772646242
iteration 2 in outer loop, alpha = 9.731073505266812, rho = 100.0, h = 0.08672487772646242
cuda
iteration 1 in inner loop,alpha 9.731073505266812 rho 100.0 h 0.04510523856360216
iteration 2 in inner loop,alpha 9.731073505266812 rho 1000.0 h 0.017561405929019003
iteration 3 in outer loop, alpha = 27.292479434285816, rho = 1000.0, h = 0.017561405929019003
cuda
iteration 1 in inner loop,alpha 27.292479434285816 rho 1000.0 h 0.010626243977112537
iteration 2 in inner loop,alpha 27.292479434285816 rho 10000.0 h 0.0036017106085548534
iteration 4 in outer loop, alpha = 63.30958551983435, rho = 10000.0, h = 0.0036017106085548534
cuda
iteration 1 in inner loop,alpha 63.30958551983435 rho 10000.0 h 0.0005418722661545416
iteration 5 in outer loop, alpha = 68.72830818137976, rho = 10000.0, h = 0.0005418722661545416
cuda
iteration 1 in inner loop,alpha 68.72830818137976 rho 10000.0 h 0.00030253647801359307
iteration 2 in inner loop,alpha 68.72830818137976 rho 100000.0 h 0.00018029817435305517
iteration 6 in outer loop, alpha = 249.02648253443493, rho = 1000000.0, h = 0.00018029817435305517
Threshold 0.3
[[0.002 0.002 0.393 0.    0.914 0.061 0.258 0.009 0.    3.03 ]
 [1.072 0.009 2.845 0.001 0.106 0.336 1.345 0.028 0.    3.139]
 [0.    0.001 0.003 0.    0.167 0.    1.791 0.    0.    0.002]
 [0.277 4.154 0.501 0.003 0.13  1.687 0.302 1.495 0.015 0.171]
 [0.    0.    0.    0.    0.002 0.    0.002 0.    0.    0.   ]
 [0.006 0.003 2.558 0.    0.381 0.002 2.222 0.    0.    0.271]
 [0.    0.    0.    0.    0.544 0.    0.005 0.    0.    0.001]
 [0.071 0.061 0.438 0.001 0.145 2.009 0.201 0.003 0.004 0.244]
 [0.262 3.472 0.269 0.012 1.006 0.116 1.798 0.042 0.001 0.195]
 [0.    0.    0.285 0.    1.11  0.    0.288 0.    0.    0.003]]
[[0.    0.    0.393 0.    0.914 0.    0.    0.    0.    3.03 ]
 [1.072 0.    2.845 0.    0.    0.336 1.345 0.    0.    3.139]
 [0.    0.    0.    0.    0.    0.    1.791 0.    0.    0.   ]
 [0.    4.154 0.501 0.    0.    1.687 0.302 1.495 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    2.558 0.    0.381 0.    2.222 0.    0.    0.   ]
 [0.    0.    0.    0.    0.544 0.    0.    0.    0.    0.   ]
 [0.    0.    0.438 0.    0.    2.009 0.    0.    0.    0.   ]
 [0.    3.472 0.    0.    1.006 0.    1.798 0.    0.    0.   ]
 [0.    0.    0.    0.    1.11  0.    0.    0.    0.    0.   ]]
{'fdr': 0.20833333333333334, 'tpr': 0.95, 'fpr': 0.2, 'f1': 0.8636363636363635, 'shd': 6, 'npred': 24, 'ntrue': 20}
[2.381e-03 3.926e-01 4.576e-04 9.137e-01 6.145e-02 2.581e-01 9.416e-03
 6.831e-05 3.030e+00 1.072e+00 2.845e+00 7.564e-04 1.065e-01 3.364e-01
 1.345e+00 2.800e-02 1.056e-04 3.139e+00 2.401e-04 8.752e-04 1.623e-04
 1.671e-01 1.873e-04 1.791e+00 1.146e-04 1.866e-05 2.327e-03 2.770e-01
 4.154e+00 5.009e-01 1.296e-01 1.687e+00 3.024e-01 1.495e+00 1.480e-02
 1.708e-01 9.476e-05 3.665e-05 4.639e-04 7.429e-06 2.257e-04 1.857e-03
 1.650e-04 1.309e-05 1.730e-04 6.037e-03 2.958e-03 2.558e+00 2.954e-04
 3.811e-01 2.222e+00 2.864e-04 5.123e-05 2.709e-01 2.930e-04 4.851e-04
 1.364e-04 1.145e-04 5.436e-01 7.286e-05 2.961e-05 2.414e-05 1.138e-03
 7.086e-02 6.058e-02 4.375e-01 1.189e-03 1.454e-01 2.009e+00 2.007e-01
 4.149e-03 2.437e-01 2.618e-01 3.472e+00 2.687e-01 1.229e-02 1.006e+00
 1.163e-01 1.798e+00 4.205e-02 1.949e-01 2.881e-04 1.904e-04 2.845e-01
 4.303e-05 1.110e+00 4.636e-04 2.876e-01 4.518e-04 8.520e-06]
[[0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.995, 0.9870370370370372)
cuda
1210
cuda
Objective function 493.46 = squared loss an data 316.40 + 0.5*rho*h**2 176.710652 + alpha*h 0.000000 + L2reg 0.19 + L1reg 0.16 ; SHD = 55 ; DAG False
||w||^2 1.2167364578505384
exp ma of ||w||^2 515.1632943253546
||w|| 1.1030577762975693
exp ma of ||w|| 0.8900499965586701
||w||^2 0.3933100113778599
exp ma of ||w||^2 0.6389015620953158
||w|| 0.627144330579381
exp ma of ||w|| 0.7787161349265398
||w||^2 0.31479830574022
exp ma of ||w||^2 0.6288905742623546
||w|| 0.5610688957162213
exp ma of ||w|| 0.7621014252564197
||w||^2 0.5663076475814866
exp ma of ||w||^2 0.6531106220438059
||w|| 0.7525341504420159
exp ma of ||w|| 0.7555879482591766
||w||^2 0.6122892747132018
exp ma of ||w||^2 0.7167833326213884
||w|| 0.7824891530961958
exp ma of ||w|| 0.7711128486674035
||w||^2 0.9500853752849073
exp ma of ||w||^2 0.8857748949081812
||w|| 0.9747232300940136
exp ma of ||w|| 0.8870312063595445
||w||^2 0.7183470441472698
exp ma of ||w||^2 0.8198413613958735
||w|| 0.847553564175899
exp ma of ||w|| 0.8541291444640321
||w||^2 0.2921176330412691
exp ma of ||w||^2 0.8532482202376279
||w|| 0.5404790773390484
exp ma of ||w|| 0.8708355637636358
||w||^2 1.0441832219067064
exp ma of ||w||^2 0.9510281327214771
||w|| 1.0218528376956764
exp ma of ||w|| 0.8833279544628551
||w||^2 0.32667202840922865
exp ma of ||w||^2 0.7944520927733453
||w|| 0.5715522971778074
exp ma of ||w|| 0.8243497497961719
||w||^2 0.32311898394468064
exp ma of ||w||^2 1.1513947479226516
||w|| 0.5684355583042643
exp ma of ||w|| 0.9536347329892627
||w||^2 1.118098964232016
exp ma of ||w||^2 0.9130287323954402
||w|| 1.0574019880026782
exp ma of ||w|| 0.8890469079747124
||w||^2 3.305737223721846
exp ma of ||w||^2 1.0320352874684335
||w|| 1.8181686455666994
exp ma of ||w|| 0.9415331261831184
||w||^2 0.6369390919836683
exp ma of ||w||^2 0.9348984686005299
||w|| 0.7980846396114063
exp ma of ||w|| 0.9143889410947896
||w||^2 1.2639277491195742
exp ma of ||w||^2 1.0092647347667365
||w|| 1.1242454132081545
exp ma of ||w|| 0.9467297694809004
||w||^2 1.3949410974609673
exp ma of ||w||^2 0.9923517126330608
||w|| 1.1810762454054213
exp ma of ||w|| 0.9389787834185723
||w||^2 0.370428672668474
exp ma of ||w||^2 1.0643103690235307
||w|| 0.6086285177910036
exp ma of ||w|| 0.957863160080207
||w||^2 0.7345192256390309
exp ma of ||w||^2 0.8135917410901338
||w|| 0.8570409708053816
exp ma of ||w|| 0.8501325977341038
||w||^2 1.4688957992827805
exp ma of ||w||^2 0.9185934722652346
||w|| 1.211980115052545
exp ma of ||w|| 0.9079750516634145
||w||^2 0.6876968525376147
exp ma of ||w||^2 0.9660324464875263
||w|| 0.8292748956393258
exp ma of ||w|| 0.9330638274471771
||w||^2 2.9736420255468357
exp ma of ||w||^2 0.9156030462356658
||w|| 1.724425129005848
exp ma of ||w|| 0.9026834249299277
||w||^2 0.4178551263899327
exp ma of ||w||^2 0.7570997831409577
||w|| 0.64641714580442
exp ma of ||w|| 0.8260817968693284
||w||^2 0.6061211954648789
exp ma of ||w||^2 0.774123480972477
||w|| 0.778537857438467
exp ma of ||w|| 0.8284739495183155
||w||^2 1.727270818357841
exp ma of ||w||^2 0.7648084065947827
||w|| 1.3142567551121207
exp ma of ||w|| 0.8311564475533351
||w||^2 2.022490153597362
exp ma of ||w||^2 0.812979182860606
||w|| 1.4221428035177628
exp ma of ||w|| 0.8387129473334208
||w||^2 0.20889765238289998
exp ma of ||w||^2 0.8410308182092354
||w|| 0.45705322707853185
exp ma of ||w|| 0.8713981787968228
cuda
Objective function 9.29 = squared loss an data 7.88 + 0.5*rho*h**2 0.595449 + alpha*h 0.000000 + L2reg 0.73 + L1reg 0.08 ; SHD = 21 ; DAG False
Proportion of microbatches that were clipped  0.7712138774531356
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.091282393061725
iteration 1 in outer loop, alpha = 1.091282393061725, rho = 1.0, h = 1.091282393061725
cuda
1210
cuda
Objective function 10.48 = squared loss an data 7.88 + 0.5*rho*h**2 0.595449 + alpha*h 1.190897 + L2reg 0.73 + L1reg 0.08 ; SHD = 21 ; DAG False
||w||^2 61888093327.50892
exp ma of ||w||^2 81776935117.15138
||w|| 248773.17646303613
exp ma of ||w|| 207529.68158109652
||w||^2 149311124.8082582
exp ma of ||w||^2 2511586107.7038054
||w|| 12219.29313864997
exp ma of ||w|| 41015.55987563356
||w||^2 24458167.4664642
exp ma of ||w||^2 233943384.71480975
||w|| 4945.519938941122
exp ma of ||w|| 11449.415306307266
||w||^2 39765058.27284602
exp ma of ||w||^2 155885232.8543062
||w|| 6305.954192098609
exp ma of ||w|| 9216.828798011698
||w||^2 769189.4057628268
exp ma of ||w||^2 14510880.61505584
||w|| 877.0344381851985
exp ma of ||w|| 2338.9828412626575
||w||^2 5.326130875005636
exp ma of ||w||^2 253.27358768674335
||w|| 2.3078411719625844
exp ma of ||w|| 2.2025852325453896
||w||^2 0.44561009733172807
exp ma of ||w||^2 1.137620533353858
||w|| 0.667540333861354
exp ma of ||w|| 0.8561893392797036
||w||^2 13.069031611153001
exp ma of ||w||^2 0.8874080160110381
||w|| 3.6151115627533543
exp ma of ||w|| 0.8611532133402938
||w||^2 1.1499577783319561
exp ma of ||w||^2 0.9838893110888194
||w|| 1.0723608433414362
exp ma of ||w|| 0.9435012410345733
||w||^2 1.4241209582243122
exp ma of ||w||^2 1.000125989246526
||w|| 1.1933653917490286
exp ma of ||w|| 0.9538498180669015
||w||^2 0.7575868242957751
exp ma of ||w||^2 1.0497617705536357
||w|| 0.8703946371019154
exp ma of ||w|| 0.9752709429602293
||w||^2 1.612413426589531
exp ma of ||w||^2 1.0804084686568285
||w|| 1.2698084212153937
exp ma of ||w|| 0.9976275704338015
||w||^2 0.9580601540505842
exp ma of ||w||^2 0.9342108474748952
||w|| 0.9788054730387362
exp ma of ||w|| 0.922004094451404
||w||^2 1.0197057498275681
exp ma of ||w||^2 1.0717744892360732
||w|| 1.0098048077859245
exp ma of ||w|| 0.977471592071805
||w||^2 0.37553823553972726
exp ma of ||w||^2 1.2150509969614738
||w|| 0.6128117455954375
exp ma of ||w|| 1.0482384523595096
||w||^2 0.6760159238990632
exp ma of ||w||^2 1.1648409412206235
||w|| 0.8222018753925724
exp ma of ||w|| 1.02906781543432
||w||^2 0.961329017913361
exp ma of ||w||^2 1.0621823418978493
||w|| 0.9804738741615511
exp ma of ||w|| 0.9888127429210041
||w||^2 0.533568964242514
exp ma of ||w||^2 1.2152546006119773
||w|| 0.7304580509806939
exp ma of ||w|| 1.0492274094085141
||w||^2 0.43834118672991373
exp ma of ||w||^2 1.1163658285418805
||w|| 0.6620733998054247
exp ma of ||w|| 0.997863991596961
||w||^2 0.6143208334818187
exp ma of ||w||^2 1.1000669537738745
||w|| 0.783786216695483
exp ma of ||w|| 1.003131594885437
||w||^2 1.483160460810872
exp ma of ||w||^2 0.9853713386863459
||w|| 1.217850754735929
exp ma of ||w|| 0.9411353279313724
||w||^2 0.2388734269625369
exp ma of ||w||^2 1.0022860759498298
||w|| 0.488746792278514
exp ma of ||w|| 0.9546161659776339
||w||^2 0.5900538142372986
exp ma of ||w||^2 1.051710841994049
||w|| 0.7681496040728646
exp ma of ||w|| 0.9768643377903511
||w||^2 0.8677014166554655
exp ma of ||w||^2 1.1138018477166558
||w|| 0.931504920360309
exp ma of ||w|| 0.999539015826225
||w||^2 1.4664458294874458
exp ma of ||w||^2 1.1222450132454567
||w|| 1.2109689630570413
exp ma of ||w|| 1.0108572981317232
||w||^2 0.5354840856502147
exp ma of ||w||^2 1.029619565927235
||w|| 0.7317677812326904
exp ma of ||w|| 0.9605859043468086
||w||^2 0.8778553429021388
exp ma of ||w||^2 1.0750022525934069
||w|| 0.9369393485717946
exp ma of ||w|| 0.9684527539153245
cuda
Objective function 7.15 = squared loss an data 5.08 + 0.5*rho*h**2 0.202174 + alpha*h 0.693928 + L2reg 1.09 + L1reg 0.08 ; SHD = 17 ; DAG False
Proportion of microbatches that were clipped  0.7680995016813191
iteration 1 in inner loop, alpha 1.091282393061725 rho 1.0 h 0.635883079486792
1210
cuda
Objective function 8.97 = squared loss an data 5.08 + 0.5*rho*h**2 2.021736 + alpha*h 0.693928 + L2reg 1.09 + L1reg 0.08 ; SHD = 17 ; DAG False
||w||^2 127.31422674908576
exp ma of ||w||^2 128827.25367632517
||w|| 11.283360614155951
exp ma of ||w|| 75.64445375787261
||w||^2 1.808311494924155
exp ma of ||w||^2 29.8840302258315
||w|| 1.3447347303182717
exp ma of ||w|| 1.2626380750340604
||w||^2 3.5905389886448202
exp ma of ||w||^2 2.672861184587928
||w|| 1.8948717604747873
exp ma of ||w|| 1.0486713336405689
||w||^2 0.5992473712124871
exp ma of ||w||^2 1.2817032134498003
||w|| 0.7741106970017189
exp ma of ||w|| 1.0832237397545363
||w||^2 0.6993445105985011
exp ma of ||w||^2 1.458861151725182
||w|| 0.8362682049429484
exp ma of ||w|| 1.1518979413430501
||w||^2 0.4379928084134959
exp ma of ||w||^2 1.2977943830209606
||w|| 0.6618102510640764
exp ma of ||w|| 1.0871069485663656
||w||^2 1.972009189124737
exp ma of ||w||^2 1.4641721476733323
||w|| 1.4042824463492867
exp ma of ||w|| 1.1483538160506206
||w||^2 1.4241256044839623
exp ma of ||w||^2 1.4550510321031773
||w|| 1.1933673384519798
exp ma of ||w|| 1.1434346909892188
||w||^2 1.2419029654538334
exp ma of ||w||^2 1.3478362019465864
||w|| 1.1144070017071113
exp ma of ||w|| 1.0829486151300922
||w||^2 0.6035210148767353
exp ma of ||w||^2 1.395734830673002
||w|| 0.7768661499104819
exp ma of ||w|| 1.1221406025374623
||w||^2 0.9950040823482228
exp ma of ||w||^2 1.3739961974151305
||w|| 0.997498913457164
exp ma of ||w|| 1.1193650879904815
||w||^2 2.1572010998100795
exp ma of ||w||^2 1.3684194237570089
||w|| 1.4687413318246612
exp ma of ||w|| 1.1080708903862426
||w||^2 0.637913610077694
exp ma of ||w||^2 1.1364576629778655
||w|| 0.7986949418130141
exp ma of ||w|| 1.019814820167218
||w||^2 0.3230859869702625
exp ma of ||w||^2 1.1781801642226404
||w|| 0.5684065331875263
exp ma of ||w|| 1.0421258848728985
||w||^2 0.746020896469715
exp ma of ||w||^2 1.1847944945202336
||w|| 0.8637250120667544
exp ma of ||w|| 1.0473743096374653
||w||^2 5.564903408601551
exp ma of ||w||^2 1.426069278153848
||w|| 2.359004749592834
exp ma of ||w|| 1.1176703903228191
||w||^2 0.4221976134186446
exp ma of ||w||^2 1.379287619308346
||w|| 0.6497673533031992
exp ma of ||w|| 1.117951485332351
||w||^2 1.503687226254547
exp ma of ||w||^2 1.2425907726360297
||w|| 1.2262492512758354
exp ma of ||w|| 1.062760402532225
||w||^2 1.9621664031179997
exp ma of ||w||^2 1.351659909649472
||w|| 1.4007735017189609
exp ma of ||w|| 1.1136857091600594
||w||^2 2.821717565348164
exp ma of ||w||^2 1.3504145208833513
||w|| 1.679796882170033
exp ma of ||w|| 1.1053189992987864
||w||^2 1.1768959415378515
exp ma of ||w||^2 1.1715434348838014
||w|| 1.0848483495576013
exp ma of ||w|| 1.0357208769925406
||w||^2 1.0270123652028365
exp ma of ||w||^2 1.264912376280271
||w|| 1.0134161855836112
exp ma of ||w|| 1.074437344253614
||w||^2 1.3050353553653549
exp ma of ||w||^2 1.2435678695902224
||w|| 1.1423814403978012
exp ma of ||w|| 1.0619241301127416
||w||^2 2.3032316705612113
exp ma of ||w||^2 1.2242137503531878
||w|| 1.5176401650461189
exp ma of ||w|| 1.0569106646232411
||w||^2 1.491805564881651
exp ma of ||w||^2 1.3254583386310994
||w|| 1.2213949258457115
exp ma of ||w|| 1.1003877039363328
||w||^2 0.9012358263907403
exp ma of ||w||^2 1.3025026977031084
||w|| 0.949334412307244
exp ma of ||w|| 1.0924180380129578
cuda
Objective function 7.28 = squared loss an data 5.27 + 0.5*rho*h**2 0.363035 + alpha*h 0.294054 + L2reg 1.27 + L1reg 0.08 ; SHD = 15 ; DAG True
Proportion of microbatches that were clipped  0.7702347759324363
iteration 2 in inner loop, alpha 1.091282393061725 rho 10.0 h 0.26945696136325026
iteration 2 in outer loop, alpha = 3.7858520066942276, rho = 10.0, h = 0.26945696136325026
cuda
1210
cuda
Objective function 8.01 = squared loss an data 5.27 + 0.5*rho*h**2 0.363035 + alpha*h 1.020124 + L2reg 1.27 + L1reg 0.08 ; SHD = 15 ; DAG True
||w||^2 2524581294.3164
exp ma of ||w||^2 24018098926.49848
||w|| 50245.21165560356
exp ma of ||w|| 105663.89810173828
||w||^2 1555816702.2424185
exp ma of ||w||^2 4465210887.23283
||w|| 39443.84238689759
exp ma of ||w|| 44563.890339190206
||w||^2 61168.80658819919
exp ma of ||w||^2 4158483.8368060216
||w|| 247.3232835545396
exp ma of ||w|| 842.2052458746435
||w||^2 8597.991137324132
exp ma of ||w||^2 470548.2298204513
||w|| 92.7253532607136
exp ma of ||w|| 189.3983777567576
||w||^2 2.1849350555860907
exp ma of ||w||^2 1.5139073785568755
||w|| 1.478152581970512
exp ma of ||w|| 1.1618307036021327
||w||^2 0.7430863073817563
exp ma of ||w||^2 1.3946096568926811
||w|| 0.8620245398953306
exp ma of ||w|| 1.1232559567266625
||w||^2 0.3528680347235812
exp ma of ||w||^2 1.2318193102377948
||w|| 0.5940269646435095
exp ma of ||w|| 1.0600949792815884
||w||^2 0.6932187598316419
exp ma of ||w||^2 1.1725533087589999
||w|| 0.8325975977815706
exp ma of ||w|| 1.0271794953118318
||w||^2 1.0417060948848023
exp ma of ||w||^2 1.2462627652881026
||w|| 1.0206400417800598
exp ma of ||w|| 1.061815925999621
||w||^2 0.7059663062400543
exp ma of ||w||^2 1.3426649197086262
||w|| 0.840218011137618
exp ma of ||w|| 1.1044907505506654
||w||^2 1.316723256301158
exp ma of ||w||^2 1.3510989578107058
||w|| 1.147485623570578
exp ma of ||w|| 1.1158483835720696
||w||^2 0.5525055534946504
exp ma of ||w||^2 1.2369715892554938
||w|| 0.7433071730413009
exp ma of ||w|| 1.0632222624304208
||w||^2 0.9349823364845591
exp ma of ||w||^2 1.3485080251696913
||w|| 0.9669448466611522
exp ma of ||w|| 1.1107729676508191
||w||^2 4.802896334229395
exp ma of ||w||^2 1.616930086365852
||w|| 2.191551125168974
exp ma of ||w|| 1.218745912192062
||w||^2 1.4820111797320912
exp ma of ||w||^2 1.5782249084064728
||w|| 1.2173788152141023
exp ma of ||w|| 1.198914640458144
||w||^2 0.6199041236850898
exp ma of ||w||^2 1.5808130604044353
||w|| 0.7873399035264819
exp ma of ||w|| 1.1953266082569631
||w||^2 1.7734138980719594
exp ma of ||w||^2 1.4993792208669876
||w|| 1.3316958729649797
exp ma of ||w|| 1.167859058498722
||w||^2 3.513202386120262
exp ma of ||w||^2 1.6344455162998834
||w|| 1.8743538582989772
exp ma of ||w|| 1.1914778268639594
||w||^2 0.2801515890333279
exp ma of ||w||^2 1.4751694022463153
||w|| 0.5292934810039964
exp ma of ||w|| 1.1433555524026155
||w||^2 7.00114480768485
exp ma of ||w||^2 1.4555013047952676
||w|| 2.6459676505363494
exp ma of ||w|| 1.154562291639455
||w||^2 0.44526062673623973
exp ma of ||w||^2 1.4223885078935035
||w|| 0.6672785226097419
exp ma of ||w|| 1.140384068901746
||w||^2 1.771284070819654
exp ma of ||w||^2 1.400886038943075
||w|| 1.3308959654381909
exp ma of ||w|| 1.1357612379163247
||w||^2 2.3194063443938533
exp ma of ||w||^2 1.3673360370396324
||w|| 1.5229597317046348
exp ma of ||w|| 1.110728840861467
||w||^2 1.8182982616832024
exp ma of ||w||^2 1.5297078853410757
||w|| 1.3484429026411175
exp ma of ||w|| 1.176301846870562
||w||^2 4.056775881829146
exp ma of ||w||^2 1.4545131965334246
||w|| 2.0141439575733275
exp ma of ||w|| 1.1303903122857804
cuda
Objective function 7.84 = squared loss an data 5.60 + 0.5*rho*h**2 0.138556 + alpha*h 0.630219 + L2reg 1.39 + L1reg 0.08 ; SHD = 17 ; DAG True
Proportion of microbatches that were clipped  0.7699721740533129
iteration 1 in inner loop, alpha 3.7858520066942276 rho 10.0 h 0.16646690864656577
1210
cuda
Objective function 9.09 = squared loss an data 5.60 + 0.5*rho*h**2 1.385562 + alpha*h 0.630219 + L2reg 1.39 + L1reg 0.08 ; SHD = 17 ; DAG True
||w||^2 577589758.3898478
exp ma of ||w||^2 874546354.0446832
||w|| 24033.097145183925
exp ma of ||w|| 22003.87479580069
||w||^2 0.8409034356778116
exp ma of ||w||^2 1.410882418253349
||w|| 0.9170078711100639
exp ma of ||w|| 1.1369416653741236
||w||^2 1.8011467094269706
exp ma of ||w||^2 1.4223091529967886
||w|| 1.3420680718305502
exp ma of ||w|| 1.1405613103018264
||w||^2 2.290189742778846
exp ma of ||w||^2 1.448675431613774
||w|| 1.5133372865223556
exp ma of ||w|| 1.1452639334955412
||w||^2 3.093812731226068
exp ma of ||w||^2 1.591299822904613
||w|| 1.7589237422998383
exp ma of ||w|| 1.195664059095238
||w||^2 1.5751613420728057
exp ma of ||w||^2 1.5098449660528959
||w|| 1.2550543183754261
exp ma of ||w|| 1.172809870459494
||w||^2 2.150983723530196
exp ma of ||w||^2 1.4167730160145346
||w|| 1.4666232384393054
exp ma of ||w|| 1.135997690870837
||w||^2 1.8219031223516788
exp ma of ||w||^2 1.4747118369696166
||w|| 1.3497789161013292
exp ma of ||w|| 1.1548963415761302
||w||^2 0.24246792999521147
exp ma of ||w||^2 1.5637806143200297
||w|| 0.4924103268567907
exp ma of ||w|| 1.1820008362875167
||w||^2 0.8179570497145431
exp ma of ||w||^2 1.4806745568174295
||w|| 0.904409779753925
exp ma of ||w|| 1.152740422349497
||w||^2 4.838351875555656
exp ma of ||w||^2 1.5019185327942413
||w|| 2.1996253943696087
exp ma of ||w|| 1.1668342978869262
||w||^2 1.2945846326581507
exp ma of ||w||^2 1.5758862783276595
||w|| 1.1377981511050854
exp ma of ||w|| 1.1876035810644492
||w||^2 1.3150250294221757
exp ma of ||w||^2 1.4408596915080594
||w|| 1.1467454074127246
exp ma of ||w|| 1.1337407688052386
||w||^2 0.9407914937666232
exp ma of ||w||^2 1.432721240945525
||w|| 0.9699440673392581
exp ma of ||w|| 1.132475320091662
||w||^2 0.4067034525986492
exp ma of ||w||^2 1.3921490719612755
||w|| 0.6377330574767544
exp ma of ||w|| 1.123758046822961
||w||^2 1.9317343860377834
exp ma of ||w||^2 1.4479402285807266
||w|| 1.3898684779639343
exp ma of ||w|| 1.144857458388674
||w||^2 0.8234324295294901
exp ma of ||w||^2 1.2823672081641644
||w|| 0.9074317767906798
exp ma of ||w|| 1.0696605359192575
||w||^2 0.6591483706192486
exp ma of ||w||^2 1.4666298270473235
||w|| 0.8118795296220053
exp ma of ||w|| 1.1488731577720064
||w||^2 1.3785067731204
exp ma of ||w||^2 1.4349438414692768
||w|| 1.1740982808608484
exp ma of ||w|| 1.1349308880791233
||w||^2 3.2898454824031824
exp ma of ||w||^2 1.4785800945217518
||w|| 1.8137931200672204
exp ma of ||w|| 1.1584585134380752
||w||^2 1.9160555930879815
exp ma of ||w||^2 1.5120419672061918
||w|| 1.3842165990508788
exp ma of ||w|| 1.1616959046957938
cuda
Objective function 7.86 = squared loss an data 5.71 + 0.5*rho*h**2 0.288467 + alpha*h 0.287559 + L2reg 1.49 + L1reg 0.08 ; SHD = 16 ; DAG True
Proportion of microbatches that were clipped  0.77258830155282
iteration 2 in inner loop, alpha 3.7858520066942276 rho 100.0 h 0.07595615331356953
1210
cuda
Objective function 10.46 = squared loss an data 5.71 + 0.5*rho*h**2 2.884669 + alpha*h 0.287559 + L2reg 1.49 + L1reg 0.08 ; SHD = 16 ; DAG True
||w||^2 11760214811.404171
exp ma of ||w||^2 34207304340.609097
||w|| 108444.52411903595
exp ma of ||w|| 139210.88888165928
||w||^2 253020860.55423227
exp ma of ||w||^2 5026884608.6366825
||w|| 15906.629452974388
exp ma of ||w|| 48326.075639398434
||w||^2 134766.53535675374
exp ma of ||w||^2 5906731.457028304
||w|| 367.1056188030275
exp ma of ||w|| 1009.3158550980394
||w||^2 9.435201195982968
exp ma of ||w||^2 8447.07815620227
||w|| 3.071677261038823
exp ma of ||w|| 9.989335123021341
||w||^2 0.40933685014302357
exp ma of ||w||^2 2.437208063628577
||w|| 0.6397943811436793
exp ma of ||w|| 1.2825042336431236
||w||^2 0.7284530050908992
exp ma of ||w||^2 1.5166661325546908
||w|| 0.8534945841016797
exp ma of ||w|| 1.1697020479072595
v before min max tensor([[-2.705e+00, -2.023e+00, -4.242e+00, -3.535e+00,  4.096e+00, -3.221e+00,
          1.266e+01, -3.624e+00, -2.117e+00,  8.900e+01],
        [-5.390e+00,  2.656e+00,  1.315e+00, -4.344e-01,  6.741e+00, -2.164e+00,
          5.650e+01,  1.144e+01, -5.613e-02,  6.425e+01],
        [-3.859e+00, -3.962e+00,  5.311e+01,  4.663e+00,  2.892e+01,  2.705e+01,
          1.397e+02,  9.725e+00, -8.515e-01,  1.272e+01],
        [-3.890e+00,  4.693e-01, -2.288e+00, -1.781e+00, -3.597e+00, -1.771e+00,
         -3.426e+00, -2.340e+00, -2.689e+00,  1.930e+01],
        [ 1.003e+00, -3.053e-01,  1.944e+01, -2.782e+00,  5.702e+00, -2.352e+00,
         -2.285e+00, -1.623e+00,  5.513e-01, -2.000e+00],
        [ 1.311e+01, -1.514e+00,  6.431e+01, -3.074e+00,  1.891e+01,  1.828e+01,
          1.084e+02,  2.457e-01, -2.584e+00, -4.377e+00],
        [ 1.392e+01, -1.476e+00,  6.701e+01, -6.091e-02,  5.680e+01,  1.632e+01,
          1.424e+02,  7.013e+00,  3.675e-01,  4.537e+01],
        [-1.853e+00, -2.608e+00,  3.133e+01, -8.290e-01, -4.293e+00, -5.280e+00,
          5.118e+01, -4.419e+00, -2.749e+00,  3.826e+01],
        [ 3.085e+01, -4.369e+00,  1.269e+02, -1.070e+00,  7.219e+01, -4.424e+00,
          1.236e+02,  1.492e+00, -1.813e+00,  1.018e+02],
        [ 7.456e+00, -1.760e+00,  9.226e+01,  4.812e+00,  2.012e+01, -5.238e+00,
          1.374e+01,  5.430e+01, -1.493e+00,  1.819e+02],
        [-3.556e+00,  5.763e-02,  2.111e+02, -2.539e+00,  2.367e+02,  5.533e+01,
          2.566e+02,  3.332e+01, -2.796e+00,  1.150e+02],
        [-4.179e+00, -2.508e+00,  3.724e+01,  2.217e+00,  5.932e+01,  5.784e+01,
          1.591e+02,  9.352e+00, -2.165e+00, -1.638e+00],
        [ 2.408e-01,  9.959e+00,  1.141e+02, -1.690e+00,  1.703e+02,  4.298e+01,
          1.648e+02, -9.636e-01,  6.062e+00,  5.995e+01],
        [-8.557e-01,  2.382e+01,  2.152e+02, -6.305e-01,  1.873e+01,  1.391e+02,
          2.023e+02,  6.368e+01, -2.184e-01, -3.780e+00],
        [-1.727e+00, -3.895e+00,  2.308e+02,  1.690e+01,  7.954e+01,  2.287e+01,
          1.570e+02,  1.716e+01, -1.312e+00, -6.601e+00],
        [-2.728e+00, -4.617e+00,  5.409e+02,  9.150e+00,  1.064e+02,  1.177e+02,
          3.022e+02,  1.588e+01, -2.853e+00,  2.324e+01],
        [-3.786e+00,  2.138e-02,  2.671e+02, -1.989e+00,  2.024e+02, -5.532e+00,
          8.303e+01,  9.081e+00,  2.485e+00, -2.093e+00],
        [ 6.573e+00,  2.246e+01,  1.376e+02, -3.132e+00,  2.968e+01,  4.679e+01,
          1.859e+02,  3.025e+01, -9.090e-01,  2.061e+01],
        [-3.970e+00,  4.323e+00,  3.687e+02,  1.798e+00,  3.468e+02,  1.261e+02,
          3.238e+02,  2.396e+02, -1.628e+00,  8.826e+01],
        [ 4.490e+00, -2.163e-01,  5.896e+01, -2.852e+00,  1.760e+02, -6.308e-01,
          2.123e+02,  9.713e+01, -1.874e+00,  6.323e+01],
        [-2.131e+00, -2.615e+00,  1.950e+01, -5.520e-01, -2.623e+00,  3.569e-01,
         -3.025e+00, -3.540e+00, -6.688e-01, -2.386e+00],
        [-3.441e+00,  1.632e+01, -2.251e+00, -3.524e+00,  3.807e-01, -4.411e-01,
          7.890e-01,  4.519e-01,  7.706e-01, -2.212e-01],
        [-2.732e+00, -2.225e+00,  3.176e-01, -1.230e+00,  4.496e-01, -9.278e-01,
         -1.906e+00,  2.335e+00, -2.315e+00, -3.163e+00],
        [ 6.664e+00, -4.097e+00, -4.195e+00, -3.051e+00, -4.331e+00,  2.360e-01,
          3.425e+01,  6.475e+00, -2.997e+00, -3.476e+00],
        [ 4.818e-02, -1.100e+00,  3.776e+00, -1.189e+00, -2.389e+00, -1.081e+00,
         -4.896e+00, -3.619e+00,  4.584e+00,  9.713e+00],
        [-9.400e-01, -1.079e+00,  3.389e+01, -1.645e+00,  1.423e+00, -5.520e-01,
         -4.343e+00,  2.625e+00, -2.150e-01, -2.043e+00],
        [-3.622e+00, -1.488e+00,  3.969e+00, -2.268e+00, -9.216e-01,  2.108e+01,
         -1.446e+00,  2.920e-02,  8.685e+00,  1.608e+01],
        [-6.259e-01, -2.247e+00, -2.755e+00, -6.212e-01,  4.710e+00, -4.098e+00,
         -4.359e+00, -2.056e+00, -1.327e+00, -1.310e+00],
        [-1.566e+00, -2.310e+00,  3.546e+00,  3.064e+01, -4.987e-01, -2.154e+00,
          7.319e+00, -3.499e+00, -1.259e+00, -1.397e+00],
        [-1.902e+00, -2.683e+00, -2.010e-01,  3.883e+00,  5.651e-02,  5.650e-01,
         -4.484e+00, -1.653e+00,  1.130e+00,  6.637e-01],
        [-2.073e+00,  1.883e+01,  1.166e+02, -3.648e+00,  4.969e+00,  5.065e+00,
          1.892e+00, -1.182e+00, -2.909e+00, -4.843e+00],
        [ 2.011e-01,  8.327e+00, -5.610e+00,  1.476e+01, -4.104e+00, -1.117e+00,
          1.145e+02,  9.682e+00, -1.961e+00, -3.562e+00],
        [-2.033e+00,  5.999e+00,  6.416e+01,  9.655e+00, -8.888e-01,  1.469e+01,
          4.786e+01,  6.649e+00, -4.217e+00, -1.912e-01],
        [ 4.734e+00, -3.151e+00,  1.229e+00,  1.544e+00, -4.863e+00,  3.632e+01,
          1.088e+01, -3.101e+00,  8.807e+01, -2.127e+00],
        [-3.199e-02, -2.439e+00,  1.899e+01, -3.385e+00, -4.301e+00,  1.746e+01,
          6.668e+01, -1.359e+00, -3.647e+00, -6.652e+00],
        [-9.813e-01, -4.877e+00,  7.481e+00, -4.237e+00, -1.904e+00, -5.970e-01,
         -1.777e+00, -4.047e+00, -5.270e+00,  4.313e+01],
        [-2.439e+00,  3.841e+01,  9.580e+00,  2.231e+01,  2.050e+00,  8.624e+00,
          5.815e+01, -2.658e+00,  5.360e-01,  4.347e-01],
        [ 1.746e+01,  9.152e+00,  2.111e+00, -3.490e+00,  2.760e+01,  1.496e+01,
          1.760e+02, -6.225e+00,  5.098e+00, -2.258e+00],
        [-2.897e+00,  2.080e+01,  2.182e+01, -6.075e+00, -1.918e+00, -3.928e+00,
          1.081e+01,  3.501e-01, -4.572e+00, -5.367e+00],
        [-3.702e+00,  2.954e+00,  9.149e+01, -1.905e+00,  7.829e-01, -4.320e+00,
          2.537e+01, -4.779e+00,  1.038e+01,  6.329e+00],
        [ 6.161e+00, -1.673e+00,  7.052e+00, -1.452e+00,  4.606e+00,  8.603e-01,
         -1.612e+00, -2.068e+00, -1.871e+00, -2.714e+00],
        [-1.659e+00,  2.344e+00, -4.305e+00,  6.059e-01,  6.242e+00,  4.159e+00,
          1.935e+01, -2.190e+00, -1.933e+00, -2.459e+00],
        [-9.942e-01,  1.820e+00, -8.455e-01, -2.306e+00,  2.795e+01, -2.518e+00,
         -5.039e-01, -2.630e+00, -2.282e-01,  1.357e+01],
        [-2.675e+00, -1.037e+00,  2.168e+00, -1.579e+00, -1.324e+00,  1.976e+01,
          2.915e+01,  3.875e-01, -3.586e-01, -1.509e+00],
        [ 7.549e+00, -2.325e+00, -7.932e-01, -2.252e+00, -3.575e+00, -2.480e+00,
         -4.902e-01,  3.906e+00, -1.650e+00, -1.710e+00],
        [-5.031e-01,  1.920e+01, -3.041e+00,  3.584e+00,  1.207e+01, -8.434e-01,
          9.618e+00, -1.327e+00, -1.934e+00, -3.033e+00],
        [-1.983e+00,  2.358e-01, -4.513e+00, -7.139e-01, -2.459e+00, -2.977e+00,
          5.678e+00,  6.999e-03, -1.481e+00, -2.029e+00],
        [-1.092e+00,  2.011e+00,  8.620e-01, -8.703e-01, -2.647e+00, -3.071e+00,
         -2.832e+00, -4.593e+00, -2.937e+00, -1.348e+00],
        [-1.759e+00, -2.061e+00,  6.807e-01,  7.996e+00, -5.778e+00,  9.203e+00,
          2.035e+00, -3.294e+00, -2.752e+00, -1.318e+00],
        [ 4.060e+00, -1.852e+00, -3.490e-01, -5.048e-01, -4.142e+00, -1.971e+00,
         -4.664e+00, -1.342e+00,  9.085e+00, -2.785e-01],
        [-3.658e-01, -1.626e-01,  3.029e+01,  3.192e-01,  2.661e+00, -2.865e+00,
         -2.182e+00,  2.610e+00,  1.159e+00, -3.012e+00],
        [-1.267e+00, -1.181e+00,  6.347e+00, -2.756e+00,  1.710e+01, -3.082e+00,
          1.775e+00,  1.009e+00,  8.108e+00,  1.483e-01],
        [-2.118e+00,  7.642e-01, -1.858e+00, -3.591e+00,  4.054e+01, -2.441e+00,
          1.824e+00, -1.250e+00, -1.617e+00, -1.594e+00],
        [ 9.858e-01,  2.146e+01,  4.473e+01,  5.881e+00,  8.834e+00,  1.093e+01,
          3.394e+00, -3.072e+00, -1.271e+00, -7.146e-01],
        [-7.491e-01,  5.521e+00,  4.939e+01, -5.022e-01, -2.211e+00,  1.822e+01,
         -2.618e+00,  5.630e-01, -3.572e+00, -3.809e+00],
        [-7.666e-01, -2.813e+00,  4.039e+01, -3.160e-01, -1.440e+00,  1.208e+00,
          1.238e+00, -2.557e+00,  3.410e+00,  1.323e+01],
        [ 1.343e+01, -3.406e-01,  9.595e+01, -3.222e+00, -4.181e+00, -1.996e+00,
          1.471e+00,  2.202e+00, -1.942e+00, -1.975e+00],
        [-2.682e-01, -1.996e+00,  2.484e+00, -6.434e-01,  7.817e+00,  1.153e-01,
         -3.336e+00, -2.039e+00, -2.834e+00,  3.624e-01],
        [-8.447e-01, -1.317e+00,  3.240e+01,  6.071e+00,  1.455e+01, -4.225e+00,
         -4.499e+00,  4.751e+00,  1.826e+01, -5.516e-01],
        [ 6.083e+00, -9.649e-01,  2.829e+00, -2.332e+00,  2.036e+00, -2.131e+00,
         -3.258e+00, -2.543e+00, -1.264e+00, -3.591e-01],
        [ 6.481e+00,  1.343e+01,  1.689e+01, -2.771e+00,  2.536e+00, -3.776e+00,
          5.655e-01, -2.046e+00, -1.841e+00, -3.549e+00],
        [-1.571e+00, -3.803e+00,  1.666e+01, -1.397e+00,  3.473e+01,  1.235e+01,
          1.003e+01,  2.349e+01, -2.634e+00, -1.363e+00],
        [-8.805e-01,  3.890e+00, -1.139e+00, -1.525e+00, -5.199e+00, -1.840e+00,
         -5.090e+00, -3.549e+00, -1.870e+00,  2.529e+00],
        [-3.328e+00,  3.698e+01, -5.490e+00,  3.581e+00, -6.529e+00, -5.915e+00,
          8.891e+00,  2.510e+01, -1.334e+00, -3.890e+00],
        [-2.417e+00,  5.114e-01, -7.396e+00, -2.905e+00, -5.685e+00, -6.035e+00,
          8.171e+00,  7.691e-01, -1.248e+00, -2.500e+00],
        [-2.857e+00,  9.493e-01,  7.466e+00,  3.800e+00, -7.054e+00, -6.933e+00,
         -7.366e+00, -6.019e+00, -1.748e+00, -4.171e+00],
        [-2.619e+00, -3.421e+00,  1.940e+01, -2.152e+00, -5.353e+00,  3.135e+00,
          6.964e-02, -5.264e+00,  1.268e+00,  1.040e+01],
        [-4.498e-01, -4.503e+00, -1.421e+00, -6.512e-01, -3.266e+00,  2.682e+00,
         -6.520e+00, -3.141e-01, -1.618e+00, -4.806e+00],
        [-3.324e+00, -3.712e+00, -4.059e+00, -2.906e+00, -7.667e+00, -7.180e+00,
          9.536e-01, -2.018e+00,  6.542e+00, -3.230e+00],
        [ 2.901e+00,  3.381e+00,  3.073e+01, -2.549e-01,  3.465e+01,  1.417e+01,
          2.819e+01,  2.003e+01, -1.399e+00, -2.670e+00],
        [-2.559e+00, -1.439e+00,  1.230e+00, -1.163e+00, -2.175e+00, -3.488e+00,
         -1.816e+00,  1.204e+01,  3.855e+00,  4.933e-01],
        [-2.197e+00,  2.196e+00, -1.560e+00, -2.556e+00, -2.925e+00, -4.400e+00,
          4.910e-02, -1.472e+00, -2.115e+00,  3.700e+00],
        [-2.218e+00, -2.683e+00, -6.637e+00, -1.308e+00,  4.524e+00, -5.188e+00,
         -4.160e+00, -4.027e+00,  8.806e-01, -3.687e+00],
        [-1.982e+00, -2.218e+00, -5.160e+00, -2.287e+00, -4.096e+00, -1.820e+00,
         -5.776e+00,  9.042e+00,  3.825e-01, -3.583e-01],
        [-2.198e+00,  4.306e+00,  5.771e+00, -2.318e+00,  1.373e+01,  1.796e+01,
         -2.308e+00,  1.212e+01, -2.920e+00,  2.264e+00],
        [-1.782e+00, -1.894e+00, -5.506e+00, -1.399e+00, -3.153e+00,  1.362e+01,
         -5.290e+00, -1.287e+00, -1.546e+00, -1.524e+00],
        [-1.775e+00, -3.515e+00,  5.449e+00, -2.276e+00, -1.196e+00,  6.257e+00,
         -4.176e+00,  2.997e+00, -2.027e+00, -2.480e+00],
        [ 2.613e-01, -2.885e+00, -7.021e-01,  1.501e+00, -2.619e+00, -1.172e+00,
          1.367e+01,  5.599e+00, -3.573e+00,  2.424e-01],
        [-1.328e+00,  2.102e+00, -4.687e+00, -3.609e+00,  7.085e+00, -2.302e+00,
         -2.106e+00, -5.027e+00,  4.773e-01,  3.707e+00],
        [-3.200e+00, -4.277e+00, -3.625e+00,  5.004e+00, -5.401e+00,  6.069e+00,
         -1.296e+00,  1.164e+01, -1.954e+00,  5.082e+00],
        [ 1.876e+00,  3.602e-01, -3.579e+00, -2.463e+00,  1.294e+00, -4.446e+00,
          1.074e+01, -3.372e+00, -3.072e+00, -4.142e+00],
        [ 9.801e-01, -3.786e+00,  1.688e+01, -2.344e+00, -2.241e+00, -4.018e+00,
         -1.647e+00, -3.858e+00, -4.522e+00, -1.382e+00],
        [-1.649e+00,  1.343e+00, -2.483e+00,  4.933e-02, -3.294e+00,  2.099e+01,
         -4.548e+00,  1.471e+00,  5.409e+00, -2.358e+00],
        [-2.526e+00,  8.992e-01, -3.214e+00,  1.717e+00, -5.342e+00, -2.938e-01,
          6.512e+00, -1.802e+00, -1.580e+00, -3.642e+00],
        [-4.419e+00, -2.375e+00, -3.356e+00,  6.121e+00,  8.396e+00,  5.171e+00,
         -5.387e+00, -2.759e+00, -5.230e+00, -5.226e+00],
        [-7.030e-01, -2.914e+00,  2.127e+00, -1.828e+00, -2.831e+00, -1.455e+00,
         -1.220e+00, -2.299e+00,  5.732e+00,  1.339e+00],
        [ 7.120e+00,  7.348e+00, -3.563e+00,  3.193e+00, -2.913e+00, -1.160e+00,
         -1.895e+00, -1.314e+00,  1.432e+01, -3.714e+00],
        [ 4.814e+00,  2.889e+00, -3.004e+00, -3.375e+00,  1.488e+01, -3.231e+00,
         -5.278e+00, -4.315e+00, -2.929e+00, -3.506e+00],
        [ 1.391e+00, -1.480e+00,  2.465e+01, -5.567e-01, -1.079e+00, -2.511e+00,
          1.362e+01, -1.227e+00, -3.394e+00, -4.286e+00],
        [-3.051e+00, -2.100e+00, -5.236e+00, -1.243e+00, -4.863e+00, -5.278e+00,
          1.070e+01,  2.477e+00, -1.176e+00, -2.513e+00],
        [-8.012e-01,  4.807e-01,  1.832e+00,  2.664e+00, -3.092e+00,  1.479e+00,
          1.603e+00,  1.322e+00,  7.306e+00, -3.279e+00],
        [-1.846e+00, -5.767e+00,  2.855e+01, -1.728e+00,  6.212e+00, -4.336e+00,
          7.760e+01,  8.587e+00,  4.256e+00, -4.399e+00],
        [-3.723e+00, -3.064e+00,  3.933e+01, -1.116e+00,  5.261e+00,  6.524e+00,
          3.668e+00, -4.364e+00, -2.098e+00, -1.462e+00],
        [ 1.719e+01, -2.636e+00,  3.281e+01,  4.520e+00, -4.137e+00, -5.068e+00,
          3.772e+01, -8.430e-01, -2.412e+00, -5.632e+00],
        [-2.532e+00, -2.070e+00, -4.061e+00,  4.056e-02, -4.583e+00,  5.255e+00,
         -4.721e+00, -4.100e-02, -1.067e+00, -9.777e-01],
        [-4.130e+00,  1.662e+00,  5.904e+00, -2.032e+00,  2.043e+01, -5.729e+00,
          1.252e+02,  3.229e-01, -9.488e-01,  6.753e+00],
        [ 2.849e+00,  3.286e+00,  5.558e+01,  3.209e-01,  2.719e+01, -4.371e+00,
          5.807e+01,  8.007e+00,  1.349e+00, -3.476e-01],
        [-2.230e+00, -2.795e+00, -1.855e+00, -5.935e-01,  1.842e+00, -1.889e+00,
          1.228e+00, -2.050e+00,  1.976e+00,  1.321e+01],
        [ 2.479e-01,  2.750e+00,  6.001e+01, -6.083e-01,  2.924e+01,  7.640e+00,
          1.081e+02, -5.266e+00,  5.113e+00, -4.680e+00],
        [ 6.160e+00,  3.653e+00,  1.560e+01, -4.178e+00,  6.422e+01,  5.691e+01,
          7.140e+01,  9.406e+00, -2.681e+00, -6.229e+00]], device='cuda:0')
v tensor([[1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.096e+00, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 2.656e+00, 1.315e+00, 1.000e-12, 6.741e+00, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 4.663e+00, 1.000e+01, 1.000e+01,
         1.000e+01, 9.725e+00, 1.000e-12, 1.000e+01],
        [1.000e-12, 4.693e-01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.003e+00, 1.000e-12, 1.000e+01, 1.000e-12, 5.702e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 5.513e-01, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 2.457e-01, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 7.013e+00, 3.675e-01, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 1.492e+00, 1.000e-12, 1.000e+01],
        [7.456e+00, 1.000e-12, 1.000e+01, 4.812e+00, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 5.763e-02, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 2.217e+00, 1.000e+01, 1.000e+01,
         1.000e+01, 9.352e+00, 1.000e-12, 1.000e-12],
        [2.408e-01, 9.959e+00, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e-12, 6.062e+00, 1.000e+01],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 9.150e+00, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 2.138e-02, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 9.081e+00, 2.485e+00, 1.000e-12],
        [6.573e+00, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 4.323e+00, 1.000e+01, 1.798e+00, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [4.490e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 3.569e-01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 3.807e-01, 1.000e-12,
         7.890e-01, 4.519e-01, 7.706e-01, 1.000e-12],
        [1.000e-12, 1.000e-12, 3.176e-01, 1.000e-12, 4.496e-01, 1.000e-12,
         1.000e-12, 2.335e+00, 1.000e-12, 1.000e-12],
        [6.664e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.360e-01,
         1.000e+01, 6.475e+00, 1.000e-12, 1.000e-12],
        [4.818e-02, 1.000e-12, 3.776e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 4.584e+00, 9.713e+00],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.423e+00, 1.000e-12,
         1.000e-12, 2.625e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 3.969e+00, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 2.920e-02, 8.685e+00, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.710e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 3.546e+00, 1.000e+01, 1.000e-12, 1.000e-12,
         7.319e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 3.883e+00, 5.651e-02, 5.650e-01,
         1.000e-12, 1.000e-12, 1.130e+00, 6.637e-01],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 4.969e+00, 5.065e+00,
         1.892e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [2.011e-01, 8.327e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e+01, 9.682e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 5.999e+00, 1.000e+01, 9.655e+00, 1.000e-12, 1.000e+01,
         1.000e+01, 6.649e+00, 1.000e-12, 1.000e-12],
        [4.734e+00, 1.000e-12, 1.229e+00, 1.544e+00, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 7.481e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e+01, 9.580e+00, 1.000e+01, 2.050e+00, 8.624e+00,
         1.000e+01, 1.000e-12, 5.360e-01, 4.347e-01],
        [1.000e+01, 9.152e+00, 2.111e+00, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e-12, 5.098e+00, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 3.501e-01, 1.000e-12, 1.000e-12],
        [1.000e-12, 2.954e+00, 1.000e+01, 1.000e-12, 7.829e-01, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e+01, 6.329e+00],
        [6.161e+00, 1.000e-12, 7.052e+00, 1.000e-12, 4.606e+00, 8.603e-01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 2.344e+00, 1.000e-12, 6.059e-01, 6.242e+00, 4.159e+00,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.820e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 2.168e+00, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 3.875e-01, 1.000e-12, 1.000e-12],
        [7.549e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 3.906e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 3.584e+00, 1.000e+01, 1.000e-12,
         9.618e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 2.358e-01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         5.678e+00, 6.999e-03, 1.000e-12, 1.000e-12],
        [1.000e-12, 2.011e+00, 8.620e-01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 6.807e-01, 7.996e+00, 1.000e-12, 9.203e+00,
         2.035e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [4.060e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 9.085e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 3.192e-01, 2.661e+00, 1.000e-12,
         1.000e-12, 2.610e+00, 1.159e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 6.347e+00, 1.000e-12, 1.000e+01, 1.000e-12,
         1.775e+00, 1.009e+00, 8.108e+00, 1.483e-01],
        [1.000e-12, 7.642e-01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.824e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [9.858e-01, 1.000e+01, 1.000e+01, 5.881e+00, 8.834e+00, 1.000e+01,
         3.394e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 5.521e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 5.630e-01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.208e+00,
         1.238e+00, 1.000e-12, 3.410e+00, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.471e+00, 2.202e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 2.484e+00, 1.000e-12, 7.817e+00, 1.153e-01,
         1.000e-12, 1.000e-12, 1.000e-12, 3.624e-01],
        [1.000e-12, 1.000e-12, 1.000e+01, 6.071e+00, 1.000e+01, 1.000e-12,
         1.000e-12, 4.751e+00, 1.000e+01, 1.000e-12],
        [6.083e+00, 1.000e-12, 2.829e+00, 1.000e-12, 2.036e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [6.481e+00, 1.000e+01, 1.000e+01, 1.000e-12, 2.536e+00, 1.000e-12,
         5.655e-01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 3.890e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 2.529e+00],
        [1.000e-12, 1.000e+01, 1.000e-12, 3.581e+00, 1.000e-12, 1.000e-12,
         8.891e+00, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 5.114e-01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         8.171e+00, 7.691e-01, 1.000e-12, 1.000e-12],
        [1.000e-12, 9.493e-01, 7.466e+00, 3.800e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 3.135e+00,
         6.964e-02, 1.000e-12, 1.268e+00, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.682e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         9.536e-01, 1.000e-12, 6.542e+00, 1.000e-12],
        [2.901e+00, 3.381e+00, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.230e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 3.855e+00, 4.933e-01],
        [1.000e-12, 2.196e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         4.910e-02, 1.000e-12, 1.000e-12, 3.700e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.524e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 8.806e-01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 9.042e+00, 3.825e-01, 1.000e-12],
        [1.000e-12, 4.306e+00, 5.771e+00, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 2.264e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 5.449e+00, 1.000e-12, 1.000e-12, 6.257e+00,
         1.000e-12, 2.997e+00, 1.000e-12, 1.000e-12],
        [2.613e-01, 1.000e-12, 1.000e-12, 1.501e+00, 1.000e-12, 1.000e-12,
         1.000e+01, 5.599e+00, 1.000e-12, 2.424e-01],
        [1.000e-12, 2.102e+00, 1.000e-12, 1.000e-12, 7.085e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 4.773e-01, 3.707e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 5.004e+00, 1.000e-12, 6.069e+00,
         1.000e-12, 1.000e+01, 1.000e-12, 5.082e+00],
        [1.876e+00, 3.602e-01, 1.000e-12, 1.000e-12, 1.294e+00, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [9.801e-01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.343e+00, 1.000e-12, 4.933e-02, 1.000e-12, 1.000e+01,
         1.000e-12, 1.471e+00, 5.409e+00, 1.000e-12],
        [1.000e-12, 8.992e-01, 1.000e-12, 1.717e+00, 1.000e-12, 1.000e-12,
         6.512e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 6.121e+00, 8.396e+00, 5.171e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 2.127e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 5.732e+00, 1.339e+00],
        [7.120e+00, 7.348e+00, 1.000e-12, 3.193e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [4.814e+00, 2.889e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.391e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 2.477e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 4.807e-01, 1.832e+00, 2.664e+00, 1.000e-12, 1.479e+00,
         1.603e+00, 1.322e+00, 7.306e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 6.212e+00, 1.000e-12,
         1.000e+01, 8.587e+00, 4.256e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 5.261e+00, 6.524e+00,
         3.668e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01, 4.520e+00, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 4.056e-02, 1.000e-12, 5.255e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.662e+00, 5.904e+00, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 3.229e-01, 1.000e-12, 6.753e+00],
        [2.849e+00, 3.286e+00, 1.000e+01, 3.209e-01, 1.000e+01, 1.000e-12,
         1.000e+01, 8.007e+00, 1.349e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.842e+00, 1.000e-12,
         1.228e+00, 1.000e-12, 1.976e+00, 1.000e+01],
        [2.479e-01, 2.750e+00, 1.000e+01, 1.000e-12, 1.000e+01, 7.640e+00,
         1.000e+01, 1.000e-12, 5.113e+00, 1.000e-12],
        [6.160e+00, 3.653e+00, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e+01, 9.406e+00, 1.000e-12, 1.000e-12]], device='cuda:0')
v before min max tensor([ 0.870, -1.675, -1.325,  5.158, -1.425,  1.540, -1.682, -0.352,  2.647,
         1.290,  8.686,  0.604, -0.427, -2.765, -2.883, -3.233,  6.148, -3.495,
         1.670,  5.872,  4.683,  0.410, -0.134, -1.992, -1.306, -2.961, -1.148,
        -0.237, -3.793, -0.356, -0.982, -0.507,  3.395, -1.980, -1.902,  6.798,
        -1.237, -1.517,  1.472,  1.472, -2.490, -1.170, -1.846, -2.222,  4.425,
        -1.675,  2.509, -2.405,  3.336, -1.029, -1.854,  0.467,  4.267,  8.808,
        -2.866,  1.200,  2.263,  3.551, -1.756, -1.854, -3.779, -3.526, -1.679,
        -2.814, -3.746,  0.453, -3.723, -2.227, -2.772, -2.384, -1.201,  2.137,
        -1.892, -2.371, -0.106, -0.091,  0.089, -1.388, -1.402, -1.341, -2.699,
        -1.630,  3.190, -0.994,  1.064, -2.067,  4.219, -2.825, -0.321, -1.238,
         6.689, -2.410, -0.944,  5.208,  7.412, -1.130, -2.953, -2.191, -2.407,
        -1.961], device='cuda:0')
v tensor([8.700e-01, 1.000e-12, 1.000e-12, 5.158e+00, 1.000e-12, 1.540e+00,
        1.000e-12, 1.000e-12, 2.647e+00, 1.290e+00, 8.686e+00, 6.037e-01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 6.148e+00, 1.000e-12,
        1.670e+00, 5.872e+00, 4.683e+00, 4.096e-01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 3.395e+00, 1.000e-12, 1.000e-12, 6.798e+00,
        1.000e-12, 1.000e-12, 1.472e+00, 1.472e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 4.425e+00, 1.000e-12, 2.509e+00, 1.000e-12,
        3.336e+00, 1.000e-12, 1.000e-12, 4.675e-01, 4.267e+00, 8.808e+00,
        1.000e-12, 1.200e+00, 2.263e+00, 3.551e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.530e-01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.137e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 8.898e-02, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.190e+00, 1.000e-12,
        1.064e+00, 1.000e-12, 4.219e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        6.689e+00, 1.000e-12, 1.000e-12, 5.208e+00, 7.412e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[-1.381],
         [-2.020],
         [ 0.984],
         [-2.274],
         [-1.438],
         [ 1.581],
         [-3.521],
         [-2.539],
         [ 5.972],
         [ 4.132]],

        [[-4.385],
         [-2.684],
         [ 7.893],
         [-0.577],
         [-2.889],
         [-3.238],
         [-1.078],
         [ 0.050],
         [-1.845],
         [-2.630]],

        [[-1.684],
         [ 2.236],
         [ 9.856],
         [-2.124],
         [-2.560],
         [-1.262],
         [-0.370],
         [-1.272],
         [-3.340],
         [-1.300]],

        [[-0.902],
         [-0.471],
         [-2.611],
         [-1.270],
         [11.886],
         [-1.658],
         [-2.712],
         [ 3.367],
         [-2.855],
         [-3.195]],

        [[-0.525],
         [-1.497],
         [-3.793],
         [-1.756],
         [ 2.607],
         [-1.706],
         [-2.832],
         [-1.872],
         [-2.499],
         [ 8.448]],

        [[-1.190],
         [ 1.888],
         [-0.233],
         [ 1.091],
         [ 1.393],
         [ 2.329],
         [-2.771],
         [-2.626],
         [-1.884],
         [-4.477]],

        [[-1.119],
         [-3.342],
         [ 8.696],
         [-3.758],
         [-1.948],
         [-1.008],
         [ 0.288],
         [-4.633],
         [-1.932],
         [ 1.479]],

        [[ 1.170],
         [-1.688],
         [-2.640],
         [-2.850],
         [-1.791],
         [-2.261],
         [ 0.987],
         [-2.197],
         [-2.723],
         [-1.598]],

        [[-1.877],
         [-4.737],
         [-3.168],
         [-3.034],
         [ 3.557],
         [-3.231],
         [ 9.387],
         [-0.657],
         [-0.975],
         [-2.670]],

        [[ 0.154],
         [-3.288],
         [-2.947],
         [ 2.146],
         [-4.346],
         [-3.020],
         [ 3.545],
         [-2.862],
         [-2.914],
         [-0.048]]], device='cuda:0')
v tensor([[[1.000e-12],
         [1.000e-12],
         [9.835e-01],
         [1.000e-12],
         [1.000e-12],
         [1.581e+00],
         [1.000e-12],
         [1.000e-12],
         [5.972e+00],
         [4.132e+00]],

        [[1.000e-12],
         [1.000e-12],
         [7.893e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.010e-02],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [2.236e+00],
         [9.856e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [3.367e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.607e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.448e+00]],

        [[1.000e-12],
         [1.888e+00],
         [1.000e-12],
         [1.091e+00],
         [1.393e+00],
         [2.329e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [8.696e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.879e-01],
         [1.000e-12],
         [1.000e-12],
         [1.479e+00]],

        [[1.170e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [9.871e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.557e+00],
         [1.000e-12],
         [9.387e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.535e-01],
         [1.000e-12],
         [1.000e-12],
         [2.146e+00],
         [1.000e-12],
         [1.000e-12],
         [3.545e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[ 0.725],
        [-3.849],
        [ 0.658],
        [ 2.518],
        [ 6.309],
        [-0.508],
        [-1.990],
        [-3.428],
        [-1.793],
        [-2.981]], device='cuda:0')
v tensor([[7.254e-01],
        [1.000e-12],
        [6.584e-01],
        [2.518e+00],
        [6.309e+00],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[ 5.593e-03,  8.177e-03,  1.367e-02, -5.643e-03, -7.677e-03,  1.583e-02,
          9.565e-03, -1.142e-02,  1.813e-03,  6.682e-03],
        [-1.271e-02,  2.680e-05, -9.317e-04, -3.343e-04,  4.095e-03,  1.851e-03,
          9.407e-03, -2.074e-03,  2.541e-03,  6.354e-04],
        [-3.152e-03,  3.944e-03,  5.260e-03,  9.113e-03, -8.562e-04, -8.816e-03,
          7.755e-03, -1.956e-02, -6.448e-04,  2.143e-03],
        [-4.791e-03,  7.365e-03, -6.043e-03, -1.402e-02, -6.543e-03,  3.069e-03,
          3.083e-03,  1.822e-03, -1.592e-03, -3.391e-03],
        [ 4.946e-03,  3.432e-03, -2.175e-03, -8.094e-03, -4.731e-03,  2.882e-03,
          7.175e-03, -1.057e-02,  3.719e-03, -7.266e-05],
        [ 9.953e-03, -4.946e-04,  4.556e-03,  5.915e-03, -3.405e-03,  1.512e-02,
         -6.061e-03,  2.363e-03, -1.484e-03,  2.626e-03],
        [ 8.172e-04,  7.514e-03,  5.635e-03, -8.090e-03, -1.762e-03, -2.542e-03,
         -1.538e-02,  6.983e-04, -1.642e-03,  6.519e-04],
        [-2.889e-03, -1.178e-02,  6.376e-03, -6.199e-03, -2.296e-05, -2.531e-03,
         -5.470e-03, -3.690e-03, -1.010e-02, -1.549e-02],
        [ 5.384e-03,  1.032e-02,  3.407e-04, -5.210e-03,  2.698e-03, -9.059e-03,
          4.400e-03, -8.753e-04,  4.405e-03,  6.911e-03],
        [-6.773e-03, -8.961e-05,  1.931e-03,  7.291e-06, -6.916e-04,  1.816e-03,
         -6.905e-03, -1.593e-03, -4.796e-03, -1.384e-02],
        [-7.456e-03, -1.388e-03,  1.325e-02, -3.970e-03, -5.227e-03,  5.251e-03,
         -2.632e-03,  8.938e-03,  2.260e-03, -4.535e-04],
        [ 7.090e-03, -1.437e-02,  3.042e-05, -3.197e-03,  1.220e-02, -1.154e-02,
         -9.635e-03, -1.280e-02,  8.002e-03, -4.990e-03],
        [ 3.490e-03, -1.302e-02, -1.346e-02,  8.007e-03, -2.332e-03, -1.418e-02,
         -2.360e-02, -1.923e-03, -3.128e-03, -3.282e-04],
        [-6.062e-04, -1.838e-03, -1.395e-02,  2.869e-04,  1.230e-02, -1.114e-02,
         -1.552e-02,  1.587e-02,  1.858e-03,  4.124e-03],
        [ 1.482e-03,  1.405e-03, -7.432e-03, -6.484e-04, -9.747e-03,  4.590e-03,
         -5.293e-03,  3.411e-04,  2.444e-04,  5.460e-03],
        [-3.997e-03,  1.569e-03, -3.063e-03,  2.489e-03, -7.771e-04, -1.019e-02,
         -9.908e-03, -1.789e-02,  2.986e-03,  3.619e-03],
        [ 1.125e-02,  2.047e-03,  3.336e-03,  4.562e-03,  7.125e-03,  2.874e-03,
          1.356e-02,  2.403e-02, -2.192e-02,  1.204e-03],
        [ 4.165e-03,  5.181e-03, -9.861e-03,  3.086e-03,  5.858e-03,  2.073e-03,
         -1.199e-02,  2.323e-02,  1.076e-02,  2.944e-03],
        [-4.750e-03, -1.739e-02,  2.763e-03, -1.134e-03, -5.145e-03,  2.265e-02,
          1.489e-02,  1.740e-03,  7.178e-04,  4.664e-04],
        [ 4.643e-03, -9.848e-03,  6.905e-03, -1.094e-03,  1.117e-02, -1.324e-02,
          3.522e-03, -7.909e-03,  4.925e-03, -3.556e-03],
        [-5.428e-05,  1.149e-02,  5.025e-03, -1.265e-02,  7.557e-03,  1.365e-02,
          2.353e-03, -5.050e-03,  2.164e-03, -2.455e-03],
        [ 9.988e-03,  3.763e-03,  6.112e-03,  8.969e-03,  1.855e-02, -1.503e-02,
          1.717e-02,  2.491e-03,  6.449e-03,  4.198e-03],
        [ 9.280e-03,  3.926e-03, -1.211e-04,  8.689e-03,  6.651e-03, -9.759e-03,
         -3.999e-03,  2.699e-03,  4.509e-04, -4.058e-03],
        [ 1.265e-02,  2.166e-02,  1.565e-03,  2.499e-03,  1.338e-02,  6.063e-04,
         -5.495e-03, -2.065e-02,  4.997e-03,  6.252e-03],
        [-2.965e-03, -2.146e-02,  9.257e-04,  9.318e-04,  3.696e-03,  2.869e-03,
         -2.725e-03,  1.805e-02,  1.286e-02, -1.298e-02],
        [ 1.778e-02, -4.691e-03,  1.633e-02, -5.423e-03,  8.362e-03,  1.739e-02,
         -2.910e-03, -9.712e-04, -8.908e-03,  1.888e-03],
        [-7.567e-03,  1.873e-03,  3.281e-03, -2.447e-03, -9.596e-03,  1.187e-02,
         -1.814e-02, -8.282e-03,  1.022e-03,  3.584e-03],
        [ 5.961e-03,  1.598e-03,  5.383e-03, -1.198e-03, -1.257e-02,  3.068e-02,
         -1.008e-02, -6.252e-03, -8.472e-03,  2.701e-04],
        [ 4.880e-03, -1.165e-02,  3.051e-03,  3.596e-04, -1.283e-02, -1.608e-03,
          1.892e-02,  1.691e-03,  2.520e-03, -2.185e-03],
        [ 1.306e-02, -2.492e-03, -1.260e-04,  2.944e-03, -6.721e-03, -5.582e-04,
          8.757e-03,  1.129e-02,  3.352e-03, -5.684e-03],
        [-1.817e-03,  8.039e-03,  1.142e-02,  1.890e-03, -7.354e-03,  2.186e-03,
          6.431e-03, -8.512e-03,  1.463e-03,  4.831e-03],
        [-1.365e-03, -8.481e-04, -2.203e-03, -5.554e-03, -2.206e-02,  2.592e-03,
         -1.290e-02,  6.732e-03, -3.601e-03, -9.304e-03],
        [-5.984e-04, -3.183e-03, -3.311e-03, -9.316e-03, -1.541e-03,  2.165e-03,
          3.287e-03,  2.561e-03,  4.915e-03, -3.243e-03],
        [ 3.694e-03,  4.640e-03,  5.173e-03, -3.639e-03, -9.873e-04,  4.703e-03,
         -1.160e-03, -4.335e-03, -1.052e-03,  1.892e-04],
        [-3.335e-03,  2.903e-03, -6.520e-03,  5.219e-04,  7.899e-03,  3.480e-03,
          6.279e-04, -3.123e-03,  1.777e-03, -2.519e-03],
        [ 2.139e-03, -1.856e-04, -8.062e-03, -9.101e-04, -3.603e-03,  7.144e-03,
          5.995e-03, -7.695e-04,  9.205e-03, -3.708e-03],
        [-4.960e-03, -1.289e-03,  2.736e-03,  1.239e-03,  3.102e-03,  5.942e-03,
         -5.329e-04, -5.023e-03, -8.560e-04, -1.997e-03],
        [-1.970e-03,  4.730e-03,  3.796e-03,  4.439e-04,  3.246e-04,  1.568e-02,
          1.098e-02,  3.082e-03, -3.839e-03, -8.659e-03],
        [-2.238e-04, -9.613e-03,  9.090e-03,  3.609e-03, -7.196e-03,  3.098e-03,
         -7.162e-06,  2.052e-03, -6.278e-03, -5.827e-03],
        [ 4.015e-03,  1.203e-03,  3.032e-03,  1.824e-03, -1.854e-03, -1.246e-02,
          1.469e-02,  1.171e-02,  4.033e-03, -3.600e-04],
        [ 2.361e-03,  9.200e-03,  8.331e-03,  1.478e-03,  8.593e-03,  1.470e-02,
          2.070e-03,  2.452e-02,  1.293e-03, -6.462e-03],
        [ 1.118e-02, -8.394e-03,  2.307e-03, -5.722e-03,  7.454e-03, -3.907e-03,
          5.999e-03,  7.973e-04,  1.261e-02, -5.572e-06],
        [ 8.427e-03, -1.069e-02, -3.719e-03, -1.369e-02,  1.373e-03,  5.068e-03,
          3.411e-03,  1.681e-02,  1.765e-02, -7.268e-03],
        [ 1.246e-02,  8.598e-03,  9.084e-03, -2.944e-03,  7.685e-03,  1.832e-02,
         -4.427e-03,  1.371e-02, -1.322e-02, -8.299e-03],
        [-1.269e-02,  3.016e-03, -1.262e-03, -1.607e-02,  6.305e-03, -6.378e-03,
          1.456e-02,  5.387e-03, -1.455e-02, -9.938e-04],
        [-1.076e-03,  5.126e-03,  2.788e-03, -6.405e-03,  1.463e-03,  4.437e-03,
          7.909e-03,  1.624e-02,  1.152e-03, -6.443e-03],
        [ 7.312e-03,  2.996e-03,  7.261e-04,  5.882e-03, -1.283e-03,  6.642e-04,
         -2.893e-02, -2.143e-02,  4.345e-03,  5.227e-03],
        [-2.794e-03,  4.682e-03,  4.197e-03, -6.662e-03,  6.278e-04,  2.128e-03,
         -5.197e-03,  1.219e-02,  2.767e-03, -3.623e-03],
        [-6.667e-03,  1.204e-02,  2.794e-03,  5.668e-03, -7.549e-04,  1.443e-02,
         -1.959e-02, -1.536e-02, -1.017e-02,  5.119e-03],
        [-2.690e-02, -2.543e-03, -1.084e-03, -1.990e-03, -2.149e-03,  1.507e-04,
          2.291e-02, -3.174e-03, -1.673e-03, -6.124e-03],
        [-8.575e-03,  1.703e-02,  4.105e-03,  5.765e-03,  4.005e-03, -1.724e-03,
         -8.740e-03,  2.306e-03,  1.028e-02,  3.584e-03],
        [-6.786e-04, -1.175e-03, -3.781e-03, -7.480e-04,  2.911e-03, -2.575e-03,
         -8.748e-04, -1.315e-02,  1.299e-03,  5.383e-03],
        [ 4.346e-03,  9.571e-05,  5.107e-03, -8.995e-03,  3.494e-03, -9.095e-04,
          1.994e-03, -1.289e-02,  1.093e-02,  4.099e-03],
        [ 3.742e-03,  4.492e-03, -7.255e-03, -1.787e-02, -3.852e-03, -5.355e-03,
          1.057e-03,  2.905e-03, -4.004e-03,  1.555e-03],
        [-2.700e-03, -1.544e-02, -1.479e-03, -1.347e-02,  4.669e-03, -4.094e-03,
          4.159e-03,  1.882e-02,  3.732e-03, -8.239e-03],
        [-3.276e-03,  5.032e-03, -1.045e-02,  7.524e-03, -1.208e-02, -6.400e-03,
          2.863e-03,  1.584e-02, -1.761e-02,  4.079e-03],
        [-1.040e-02, -1.670e-03, -1.716e-03,  3.946e-03,  1.083e-02,  1.261e-03,
         -1.010e-02, -6.671e-03,  1.013e-02,  9.324e-04],
        [-4.024e-04, -4.692e-03, -5.545e-03, -4.988e-03,  2.520e-03, -2.878e-03,
          5.705e-03, -5.527e-03,  5.684e-03,  1.458e-03],
        [ 7.081e-03,  7.662e-03,  6.492e-03, -3.696e-03, -1.688e-03, -6.057e-03,
          4.251e-04,  1.186e-04, -6.369e-03,  3.391e-04],
        [-2.573e-03,  1.587e-03, -2.148e-03,  1.481e-02,  9.395e-04,  9.101e-04,
          9.136e-04, -9.923e-03,  1.329e-02,  1.778e-03],
        [-4.790e-03, -1.050e-02,  1.813e-02,  3.819e-03,  1.341e-02,  3.356e-02,
          2.303e-02, -1.916e-02,  4.004e-03, -9.558e-03],
        [ 1.087e-02, -7.593e-03,  2.520e-02,  2.027e-03,  3.022e-02, -7.747e-03,
          4.277e-02, -1.368e-02,  4.660e-03, -6.921e-03],
        [ 7.751e-03, -2.087e-02, -3.256e-04,  1.362e-03, -9.328e-04,  2.471e-02,
          5.442e-03, -1.906e-03, -3.517e-03, -8.515e-03],
        [ 4.478e-03, -1.344e-03,  3.576e-02, -1.131e-02,  4.140e-02,  1.479e-02,
          1.723e-02, -2.250e-02, -4.141e-03,  2.041e-03],
        [ 1.253e-02, -1.506e-02,  1.223e-03,  1.609e-03,  4.694e-02,  5.422e-03,
          3.137e-02, -9.276e-04, -7.927e-03, -6.911e-03],
        [ 5.714e-03, -5.420e-03,  1.021e-02, -1.440e-02,  3.109e-02,  1.487e-02,
          2.876e-02, -2.524e-02,  8.650e-03,  3.015e-03],
        [-5.583e-03, -1.393e-02,  1.575e-02,  9.573e-04,  1.293e-02, -1.567e-02,
          1.739e-02,  5.189e-03, -9.900e-03,  1.274e-03],
        [-5.467e-03,  2.745e-03,  1.219e-02,  5.404e-03,  2.685e-02,  2.981e-02,
          3.410e-02, -3.040e-03, -6.586e-03, -9.680e-03],
        [ 1.512e-02, -1.543e-02,  3.099e-02,  2.519e-02,  1.199e-02, -7.289e-03,
          2.005e-02, -3.055e-02, -3.003e-04, -6.211e-04],
        [-2.018e-02,  3.995e-03,  2.333e-02,  1.096e-02,  3.397e-02,  1.688e-02,
          3.618e-02, -1.515e-02, -4.388e-03, -6.074e-03],
        [ 7.192e-04, -1.622e-02, -1.743e-03,  6.980e-04, -2.458e-03, -7.871e-03,
         -3.514e-03,  1.808e-03,  1.348e-03,  5.951e-03],
        [ 1.360e-02,  1.897e-02, -9.111e-03,  9.762e-03, -1.264e-02,  3.943e-05,
         -7.645e-03,  5.303e-03, -4.201e-03,  9.453e-03],
        [-1.380e-02,  5.134e-03,  7.790e-03,  8.362e-03,  2.666e-05, -1.580e-02,
          1.779e-03, -2.861e-03,  4.873e-04,  2.654e-03],
        [-1.418e-02, -2.459e-02,  4.911e-04, -2.018e-03,  5.204e-03, -4.993e-03,
          9.360e-03, -2.963e-03,  3.090e-03,  1.566e-03],
        [ 6.174e-03, -6.656e-03,  5.095e-03,  3.482e-03,  8.341e-03,  2.856e-03,
         -7.014e-03, -3.470e-03, -1.369e-02, -3.979e-03],
        [ 8.303e-03, -4.572e-03,  7.847e-04, -5.285e-03,  1.826e-03, -1.640e-03,
          5.470e-03, -5.543e-04, -2.712e-03,  1.054e-03],
        [ 3.382e-03, -4.823e-03,  4.374e-03,  8.130e-03,  1.185e-03, -1.737e-03,
         -2.306e-03,  1.402e-04, -1.950e-02,  2.512e-03],
        [ 6.513e-03,  3.286e-03, -7.071e-04,  2.825e-03, -8.845e-04,  5.930e-03,
         -4.571e-03,  5.308e-03, -6.483e-03, -3.498e-03],
        [-1.662e-03,  1.795e-03,  7.199e-04,  1.540e-02, -1.617e-03, -1.985e-03,
         -8.968e-03, -5.176e-03,  4.426e-03, -5.293e-03],
        [-8.732e-03,  1.943e-03,  9.855e-03,  1.849e-03, -5.415e-03,  4.338e-03,
         -1.172e-02,  5.847e-03,  9.382e-03, -6.009e-04],
        [-1.445e-02,  3.784e-03,  6.358e-04, -1.216e-02,  8.948e-03,  6.807e-03,
          8.959e-03, -6.409e-03, -2.370e-03,  4.939e-03],
        [ 7.260e-03, -4.304e-03,  2.990e-04, -2.677e-03, -3.570e-03,  2.097e-03,
         -8.191e-03,  6.883e-03,  1.189e-02, -2.460e-04],
        [-5.866e-03, -8.607e-03,  7.018e-03, -9.825e-03, -8.576e-03,  9.043e-03,
          5.312e-03, -3.367e-03, -9.013e-04, -4.933e-04],
        [ 2.517e-03, -1.292e-02, -1.425e-03,  5.149e-03,  4.553e-03, -2.253e-03,
         -9.853e-03, -9.532e-03,  6.157e-03,  5.009e-03],
        [-1.133e-02, -2.226e-03, -7.386e-03, -4.229e-03, -5.681e-03,  1.793e-03,
         -9.491e-03,  4.906e-04,  2.742e-03, -6.953e-03],
        [-3.672e-04,  3.814e-03, -4.273e-03,  1.167e-02,  4.979e-03,  3.319e-03,
          8.920e-04,  2.494e-03, -1.425e-03,  8.168e-03],
        [ 9.254e-04, -1.725e-03, -2.913e-03, -5.258e-03, -4.136e-03,  2.390e-03,
          5.855e-03,  9.201e-04, -1.209e-02,  2.560e-03],
        [ 1.108e-02,  3.481e-03,  3.972e-03,  4.941e-03,  1.579e-03, -7.369e-04,
         -4.494e-03, -1.255e-03,  1.275e-05,  3.154e-04],
        [-1.611e-04, -1.737e-04, -4.427e-03, -4.359e-03, -4.196e-04,  6.484e-03,
          8.811e-04, -6.502e-03, -9.305e-03,  1.296e-02],
        [-4.400e-04,  8.587e-04,  5.233e-03, -1.228e-03,  3.722e-03,  1.127e-02,
          5.901e-03, -8.199e-03, -3.274e-03,  5.313e-03],
        [-2.131e-03,  3.891e-03, -1.342e-02,  4.147e-03, -9.742e-03,  9.756e-04,
          9.355e-03, -1.649e-03,  4.470e-03,  1.575e-03],
        [ 1.116e-02,  2.122e-02, -3.681e-02,  6.586e-03,  4.359e-03,  1.783e-02,
         -2.353e-02, -5.559e-03, -1.326e-02,  1.088e-02],
        [-1.203e-02, -6.480e-03,  9.218e-03,  6.731e-03, -1.956e-02,  2.152e-02,
         -2.643e-03, -1.550e-02, -2.271e-03,  6.414e-03],
        [-1.349e-02,  2.435e-03,  6.516e-04, -1.492e-02, -1.015e-02,  1.359e-02,
          1.528e-02, -2.931e-02, -7.722e-03, -3.557e-05],
        [ 3.311e-02,  8.066e-03, -3.425e-03,  7.843e-03, -1.071e-02,  1.096e-02,
          1.266e-02,  6.841e-03,  1.170e-04, -1.510e-03],
        [ 1.846e-02, -3.661e-04,  5.850e-03, -9.853e-03, -9.449e-03, -1.877e-02,
         -4.285e-04, -2.615e-02, -1.629e-03, -2.955e-04],
        [-1.047e-02,  1.788e-02, -6.590e-03,  1.016e-02,  1.428e-02, -1.524e-02,
          1.610e-02,  5.243e-03, -8.981e-03,  2.183e-03],
        [ 9.080e-03, -9.072e-03, -9.254e-03,  6.535e-03,  1.121e-02, -7.792e-03,
         -4.306e-03, -1.271e-02, -3.924e-03,  9.240e-03],
        [ 1.944e-02, -2.257e-02,  3.350e-02,  4.056e-03, -1.607e-03, -1.008e-02,
          3.890e-02,  2.593e-02, -1.201e-03,  9.185e-03],
        [-3.253e-02,  1.139e-02,  2.045e-02, -2.369e-03,  1.560e-02, -2.723e-03,
          4.261e-04,  2.359e-02,  7.614e-03, -4.203e-03]], device='cuda:0')
s after update for 1 param tensor([[1.581, 1.316, 2.116, 1.346, 2.095, 1.814, 2.242, 1.521, 0.803, 2.249],
        [2.068, 1.081, 2.132, 1.007, 2.324, 1.938, 2.573, 1.915, 1.154, 2.420],
        [1.497, 1.537, 2.831, 1.365, 2.668, 1.796, 2.771, 2.126, 0.974, 2.077],
        [1.964, 1.323, 1.557, 0.901, 1.626, 0.895, 1.461, 1.131, 1.005, 2.580],
        [1.225, 0.944, 1.868, 1.103, 1.821, 0.931, 1.668, 1.459, 1.477, 2.343],
        [2.244, 0.751, 2.472, 1.155, 2.274, 1.915, 2.603, 2.011, 1.157, 2.227],
        [1.713, 1.249, 2.415, 0.898, 2.315, 2.487, 2.634, 2.003, 0.883, 2.622],
        [1.175, 1.071, 2.029, 0.576, 1.688, 1.982, 2.550, 1.985, 1.416, 2.867],
        [1.739, 1.705, 2.597, 0.529, 2.522, 2.025, 2.660, 1.858, 0.901, 2.656],
        [1.476, 0.712, 2.064, 1.103, 2.540, 2.048, 2.427, 1.777, 1.142, 2.387],
        [1.438, 1.595, 2.913, 0.970, 2.821, 2.744, 3.045, 2.724, 1.146, 2.354],
        [1.666, 2.050, 2.975, 0.881, 2.697, 2.873, 3.058, 2.569, 0.809, 1.791],
        [1.728, 2.327, 2.793, 0.801, 2.811, 2.779, 2.924, 2.309, 1.413, 2.203],
        [0.897, 1.715, 3.092, 0.332, 2.824, 2.719, 2.886, 2.052, 0.909, 2.574],
        [1.140, 2.290, 2.808, 1.609, 2.709, 2.345, 2.772, 2.435, 1.272, 2.521],
        [1.046, 2.034, 2.912, 1.593, 2.774, 2.566, 2.900, 2.510, 1.277, 2.595],
        [1.634, 1.603, 2.942, 1.176, 2.707, 2.406, 2.987, 2.795, 1.240, 2.287],
        [1.572, 2.414, 3.108, 1.444, 2.947, 2.521, 2.832, 2.646, 0.978, 2.224],
        [1.517, 2.452, 2.871, 1.890, 2.810, 2.809, 2.993, 2.698, 0.789, 2.227],
        [1.705, 1.971, 2.861, 1.230, 2.917, 2.332, 2.884, 2.683, 0.749, 2.160],
        [0.850, 1.052, 2.193, 0.890, 1.676, 1.619, 1.827, 1.322, 0.403, 0.974],
        [1.917, 1.699, 1.695, 1.427, 1.560, 1.164, 2.139, 1.703, 0.698, 0.678],
        [1.120, 1.309, 1.122, 0.783, 1.508, 1.787, 1.432, 1.078, 1.315, 1.330],
        [1.817, 1.568, 2.378, 1.167, 2.235, 1.792, 2.897, 2.296, 1.136, 1.401],
        [0.608, 1.079, 1.709, 1.092, 1.516, 1.182, 1.833, 1.418, 0.894, 1.559],
        [0.966, 1.333, 2.236, 0.708, 1.628, 1.570, 1.851, 0.845, 0.877, 0.782],
        [1.403, 1.025, 1.909, 0.877, 1.726, 1.851, 1.759, 1.683, 1.242, 1.801],
        [0.607, 1.310, 1.731, 0.790, 1.495, 1.706, 1.686, 1.264, 0.990, 1.245],
        [0.672, 1.014, 2.504, 1.665, 1.790, 0.939, 2.107, 1.364, 0.524, 0.608],
        [0.978, 1.142, 1.111, 1.093, 1.139, 1.136, 1.753, 0.739, 0.627, 0.978],
        [0.864, 1.998, 2.409, 1.379, 1.735, 1.473, 1.660, 1.153, 1.224, 2.037],
        [0.888, 2.279, 2.731, 2.120, 2.111, 1.618, 2.409, 2.171, 1.465, 2.330],
        [1.054, 1.896, 3.084, 1.956, 1.153, 2.251, 2.314, 1.416, 1.630, 1.624],
        [1.427, 2.224, 2.197, 1.720, 1.839, 2.211, 2.249, 1.549, 2.254, 1.949],
        [1.205, 1.810, 2.757, 1.975, 2.120, 2.115, 2.383, 1.912, 1.869, 2.483],
        [0.766, 1.830, 2.511, 1.840, 1.547, 1.886, 1.856, 1.531, 1.979, 2.925],
        [1.387, 2.272, 2.799, 2.342, 2.066, 2.015, 2.881, 1.611, 1.061, 2.338],
        [1.448, 1.904, 2.429, 1.332, 2.356, 2.195, 2.645, 2.337, 1.933, 2.213],
        [1.568, 2.694, 2.812, 2.270, 1.901, 1.589, 2.374, 1.195, 2.100, 2.328],
        [1.432, 2.490, 2.838, 0.987, 2.289, 2.213, 2.291, 1.806, 2.086, 2.297],
        [0.915, 0.889, 1.369, 0.751, 2.131, 1.377, 0.732, 0.776, 1.036, 1.016],
        [0.917, 1.167, 1.800, 0.931, 2.425, 1.524, 2.413, 1.489, 0.724, 1.037],
        [1.030, 1.216, 0.820, 0.920, 2.250, 1.131, 0.990, 0.990, 1.147, 1.319],
        [1.559, 1.323, 1.717, 0.623, 2.124, 1.813, 2.546, 1.633, 0.815, 1.420],
        [1.653, 0.959, 1.308, 1.012, 2.168, 1.462, 1.569, 1.702, 1.118, 1.004],
        [0.374, 1.251, 1.197, 1.040, 1.619, 0.459, 1.704, 1.445, 0.844, 1.243],
        [0.758, 0.933, 1.685, 1.565, 1.814, 1.656, 2.052, 1.040, 0.554, 0.816],
        [0.562, 1.062, 2.042, 0.694, 1.816, 1.186, 1.283, 1.910, 1.103, 1.146],
        [1.460, 1.174, 1.835, 1.015, 2.161, 1.837, 2.018, 1.304, 1.206, 0.618],
        [1.736, 1.281, 1.366, 0.381, 1.637, 1.032, 1.753, 1.405, 1.508, 0.563],
        [0.573, 1.118, 2.376, 0.770, 1.401, 1.129, 1.897, 1.401, 0.679, 1.303],
        [0.535, 0.981, 2.397, 1.420, 1.964, 1.552, 1.878, 1.281, 1.123, 1.262],
        [0.791, 0.744, 2.137, 1.385, 1.571, 1.540, 1.405, 0.487, 0.677, 1.794],
        [0.817, 1.800, 2.626, 0.943, 1.556, 2.470, 1.497, 1.239, 0.540, 0.772],
        [0.745, 1.359, 2.319, 0.458, 0.831, 2.118, 1.361, 0.678, 1.338, 1.437],
        [0.720, 1.187, 2.675, 0.955, 1.488, 1.656, 1.234, 1.065, 1.437, 1.279],
        [1.596, 1.072, 2.430, 1.204, 1.627, 1.296, 1.684, 0.966, 0.830, 0.879],
        [0.383, 1.287, 2.294, 0.541, 1.670, 1.307, 1.250, 0.944, 1.074, 0.821],
        [1.282, 1.364, 2.721, 1.133, 1.922, 1.725, 1.711, 1.053, 1.480, 1.048],
        [1.063, 0.366, 1.862, 0.976, 1.179, 0.843, 1.489, 0.962, 0.531, 0.509],
        [0.955, 1.821, 2.900, 1.567, 2.811, 2.725, 2.874, 2.614, 0.800, 1.714],
        [1.279, 1.462, 2.905, 1.416, 2.811, 2.869, 3.089, 2.683, 1.143, 1.152],
        [0.699, 1.588, 1.568, 0.655, 1.964, 1.002, 1.903, 1.378, 0.699, 1.748],
        [1.426, 2.368, 2.937, 1.406, 2.828, 2.732, 3.048, 2.805, 1.070, 1.698],
        [1.474, 2.054, 2.762, 1.103, 2.862, 2.726, 3.015, 2.763, 0.475, 1.777],
        [1.387, 1.515, 2.723, 1.015, 2.721, 2.620, 2.840, 2.426, 0.689, 1.613],
        [1.064, 1.854, 2.785, 0.950, 2.535, 2.411, 2.634, 2.447, 1.351, 1.485],
        [0.825, 1.682, 2.662, 1.166, 2.427, 2.074, 2.826, 2.459, 0.710, 1.794],
        [1.565, 2.284, 2.528, 1.115, 2.882, 2.769, 2.871, 2.644, 1.327, 1.706],
        [1.575, 1.579, 2.863, 1.055, 2.998, 2.927, 2.984, 2.473, 0.608, 1.727],
        [1.076, 0.978, 1.977, 0.577, 1.800, 1.709, 2.000, 1.947, 0.762, 1.559],
        [1.241, 1.676, 2.253, 1.103, 2.124, 1.979, 2.211, 1.978, 0.812, 1.467],
        [1.312, 1.004, 2.514, 0.688, 1.836, 2.142, 1.969, 1.688, 0.424, 1.402],
        [1.414, 1.128, 2.115, 0.929, 1.839, 1.551, 2.479, 2.304, 0.531, 1.676],
        [0.841, 1.592, 2.360, 0.908, 2.404, 2.403, 2.073, 2.284, 1.315, 1.589],
        [0.758, 0.987, 2.135, 0.630, 1.607, 1.609, 2.076, 1.428, 0.799, 0.598],
        [0.800, 1.318, 2.342, 0.851, 1.425, 1.548, 1.656, 1.960, 1.483, 0.990],
        [1.084, 1.576, 1.963, 1.008, 1.908, 1.838, 2.469, 1.997, 1.393, 1.204],
        [0.818, 1.090, 1.764, 1.358, 1.472, 1.374, 1.790, 1.877, 0.503, 1.110],
        [1.257, 1.613, 2.458, 1.356, 2.114, 2.299, 2.361, 2.101, 1.086, 1.197],
        [1.109, 1.380, 2.228, 0.943, 2.236, 1.974, 2.489, 1.552, 1.152, 1.813],
        [1.046, 1.468, 1.967, 0.880, 1.510, 1.520, 1.613, 1.481, 1.953, 0.870],
        [0.742, 1.834, 1.990, 0.394, 1.621, 1.930, 1.924, 1.392, 1.194, 1.480],
        [0.963, 1.768, 2.239, 1.409, 2.520, 2.178, 2.616, 1.770, 1.344, 1.649],
        [1.659, 0.997, 1.961, 1.374, 1.888, 1.688, 2.237, 1.730, 2.025, 1.971],
        [0.462, 1.383, 1.519, 0.766, 1.250, 0.544, 1.311, 0.929, 1.415, 1.498],
        [1.142, 1.538, 1.535, 0.658, 1.094, 0.583, 0.845, 1.060, 2.344, 1.401],
        [1.074, 1.291, 1.778, 1.349, 2.332, 1.591, 2.021, 1.618, 1.103, 1.448],
        [1.131, 1.154, 1.960, 0.239, 1.536, 0.938, 2.256, 1.431, 1.715, 1.670],
        [1.175, 0.977, 2.148, 0.636, 1.946, 1.978, 2.606, 1.505, 2.159, 1.218],
        [0.690, 1.071, 1.938, 1.131, 1.566, 1.351, 1.979, 1.281, 1.513, 1.249],
        [2.031, 2.217, 2.979, 1.423, 2.552, 2.569, 2.799, 2.492, 0.830, 1.927],
        [1.453, 1.824, 2.591, 0.963, 2.290, 2.473, 2.630, 1.962, 0.784, 1.760],
        [2.099, 1.906, 2.887, 1.427, 2.172, 2.294, 2.916, 2.413, 0.906, 2.114],
        [0.962, 0.793, 1.785, 0.691, 1.782, 1.850, 1.891, 1.182, 0.712, 1.575],
        [1.679, 1.247, 2.613, 0.867, 2.550, 2.142, 2.748, 2.081, 0.838, 2.106],
        [0.918, 1.461, 2.744, 0.699, 2.420, 1.789, 2.764, 1.893, 0.790, 1.378],
        [0.849, 1.122, 1.125, 0.612, 1.559, 1.034, 0.933, 0.836, 0.862, 1.983],
        [0.517, 1.186, 2.495, 0.558, 2.318, 1.617, 2.831, 2.110, 1.306, 2.255],
        [1.610, 2.279, 2.834, 1.576, 2.686, 2.500, 3.013, 2.547, 1.238, 2.450]],
       device='cuda:0')
b after update for 1 param tensor([[54.955, 50.125, 63.569, 50.710, 63.258, 58.861, 65.442, 53.902, 39.159,
         65.539],
        [62.840, 45.429, 63.807, 43.855, 66.615, 60.839, 70.102, 60.474, 46.939,
         67.989],
        [53.467, 54.178, 73.531, 51.066, 71.388, 58.574, 72.741, 63.726, 43.129,
         62.975],
        [61.239, 50.273, 54.536, 41.472, 55.734, 41.344, 52.827, 46.485, 43.821,
         70.200],
        [48.371, 42.465, 59.733, 45.902, 58.972, 42.163, 56.448, 52.780, 53.108,
         66.889],
        [65.463, 37.880, 68.711, 46.966, 65.897, 60.480, 70.510, 61.974, 47.008,
         65.212],
        [57.191, 48.839, 67.917, 41.420, 66.492, 68.916, 70.920, 61.849, 41.066,
         70.758],
        [47.381, 45.225, 62.254, 33.158, 56.782, 61.530, 69.786, 61.567, 51.996,
         73.998],
        [57.627, 57.068, 70.428, 31.800, 69.401, 62.191, 71.274, 59.568, 41.475,
         71.223],
        [53.098, 36.873, 62.786, 45.887, 69.649, 62.541, 68.088, 58.258, 46.696,
         67.514],
        [52.411, 55.189, 74.587, 43.036, 73.403, 72.386, 76.253, 72.134, 46.790,
         67.057],
        [56.410, 62.578, 75.381, 41.009, 71.763, 74.078, 76.420, 70.044, 39.312,
         58.491],
        [57.455, 66.661, 73.034, 39.114, 73.274, 72.856, 74.727, 66.413, 51.956,
         64.866],
        [41.393, 57.238, 76.841, 25.184, 73.436, 72.055, 74.246, 62.597, 41.667,
         70.115],
        [46.657, 66.137, 73.237, 55.429, 71.928, 66.929, 72.756, 68.192, 49.284,
         69.390],
        [44.704, 62.321, 74.575, 55.157, 72.791, 70.002, 74.415, 69.241, 49.384,
         70.398],
        [55.857, 55.331, 74.952, 47.402, 71.902, 67.792, 75.535, 73.066, 48.668,
         66.084],
        [54.788, 67.899, 77.042, 52.521, 75.019, 69.382, 73.545, 71.087, 43.211,
         65.169],
        [53.830, 68.425, 74.053, 60.079, 73.262, 73.243, 75.604, 71.778, 38.815,
         65.216],
        [57.065, 61.352, 73.921, 48.474, 74.642, 66.734, 74.218, 71.580, 37.821,
         64.224],
        [40.299, 44.823, 64.719, 41.226, 56.569, 55.599, 59.067, 50.245, 27.745,
         43.129],
        [60.511, 56.967, 56.900, 52.197, 54.580, 47.145, 63.916, 57.035, 36.504,
         35.989],
        [46.257, 49.998, 46.301, 38.670, 53.660, 58.428, 52.292, 45.379, 50.107,
         50.393],
        [58.904, 54.723, 67.393, 47.206, 65.339, 58.499, 74.386, 66.212, 46.572,
         51.720],
        [34.063, 45.401, 57.127, 45.658, 53.802, 47.518, 59.161, 52.045, 41.310,
         54.566],
        [42.959, 50.452, 65.352, 36.781, 55.752, 54.757, 59.454, 40.174, 40.929,
         38.643],
        [51.759, 44.234, 60.383, 40.916, 57.414, 59.460, 57.955, 56.689, 48.702,
         58.649],
        [34.062, 50.022, 57.500, 38.840, 53.443, 57.075, 56.743, 49.137, 43.490,
         48.765],
        [35.825, 44.003, 69.148, 56.384, 58.468, 42.339, 63.428, 51.045, 31.646,
         34.069],
        [43.208, 46.693, 46.062, 45.681, 46.650, 46.581, 57.863, 37.577, 34.614,
         43.225],
        [40.617, 61.776, 67.823, 51.325, 57.565, 53.035, 56.310, 46.931, 48.356,
         62.372],
        [41.187, 65.971, 72.220, 63.626, 63.488, 55.586, 67.836, 64.394, 52.900,
         66.711],
        [44.869, 60.180, 76.745, 61.119, 46.934, 65.562, 66.478, 52.009, 55.791,
         55.695],
        [52.211, 65.174, 64.776, 57.321, 59.267, 64.980, 65.532, 54.396, 65.604,
         61.005],
        [47.977, 58.799, 72.567, 61.423, 63.624, 63.563, 67.469, 60.422, 59.741,
         68.867],
        [38.255, 59.116, 69.246, 59.284, 54.361, 60.009, 59.538, 54.068, 61.476,
         74.741],
        [51.468, 65.866, 73.114, 66.879, 62.816, 62.033, 74.173, 55.471, 45.005,
         66.817],
        [52.594, 60.304, 68.116, 50.446, 67.079, 64.744, 71.079, 66.814, 60.760,
         65.011],
        [54.722, 71.729, 73.290, 65.844, 60.248, 55.084, 67.331, 47.768, 63.336,
         66.685],
        [52.295, 68.957, 73.615, 43.418, 66.115, 65.014, 66.144, 58.735, 63.123,
         66.235],
        [41.793, 41.195, 51.137, 37.868, 63.789, 51.282, 37.400, 38.509, 44.473,
         44.049],
        [41.856, 47.217, 58.633, 42.173, 68.060, 53.942, 67.886, 53.326, 37.182,
         44.498],
        [44.350, 48.197, 39.574, 41.922, 65.555, 46.486, 43.483, 43.484, 46.796,
         50.186],
        [54.563, 50.273, 57.258, 34.481, 63.694, 58.841, 69.728, 55.845, 39.450,
         52.080],
        [56.182, 42.792, 49.980, 43.961, 64.344, 52.841, 54.741, 57.010, 46.200,
         43.799],
        [26.739, 48.872, 47.812, 44.576, 55.598, 29.605, 57.048, 52.534, 40.159,
         48.730],
        [38.039, 42.211, 56.731, 54.675, 58.853, 56.240, 62.596, 44.576, 32.518,
         39.484],
        [32.773, 45.037, 62.452, 36.412, 58.899, 47.592, 49.501, 60.401, 45.907,
         46.792],
        [52.800, 47.344, 59.201, 44.036, 64.238, 59.230, 62.074, 49.901, 47.988,
         34.346],
        [57.580, 49.466, 51.072, 26.981, 55.914, 44.402, 57.859, 51.804, 53.658,
         32.777],
        [33.080, 46.199, 67.363, 38.358, 51.735, 46.428, 60.190, 51.719, 36.007,
         49.887],
        [31.956, 43.288, 67.661, 52.082, 61.245, 54.446, 59.881, 49.456, 46.308,
         49.099],
        [38.856, 37.684, 63.879, 51.426, 54.771, 54.240, 51.793, 30.499, 35.945,
         58.531],
        [39.499, 58.634, 70.813, 42.430, 54.507, 68.682, 53.463, 48.647, 32.109,
         38.390],
        [37.716, 50.951, 66.551, 29.569, 39.843, 63.604, 50.986, 35.983, 50.559,
         52.393],
        [37.086, 47.608, 71.475, 42.702, 53.305, 56.240, 48.545, 45.109, 52.382,
         49.420],
        [55.204, 45.239, 68.123, 47.957, 55.740, 49.760, 56.718, 42.944, 39.810,
         40.967],
        [27.056, 49.571, 66.190, 32.152, 56.477, 49.958, 48.860, 42.450, 45.298,
         39.607],
        [49.483, 51.034, 72.093, 46.514, 60.582, 57.404, 57.161, 44.847, 53.169,
         44.741],
        [45.067, 26.425, 59.632, 43.170, 47.443, 40.118, 53.324, 42.855, 31.836,
         31.173],
        [42.706, 58.979, 74.420, 54.705, 73.274, 72.140, 74.088, 70.662, 39.081,
         57.207],
        [49.418, 52.835, 74.484, 51.998, 73.276, 74.022, 76.813, 71.578, 46.720,
         46.898],
        [36.526, 55.075, 54.715, 35.377, 61.251, 43.756, 60.282, 51.294, 36.535,
         57.785],
        [52.192, 67.249, 74.898, 51.819, 73.497, 72.227, 76.302, 73.195, 45.212,
         56.940],
        [53.066, 62.633, 72.631, 45.904, 73.933, 72.149, 75.877, 72.638, 30.120,
         58.259],
        [51.461, 53.786, 72.119, 44.026, 72.091, 70.742, 73.646, 68.063, 36.272,
         55.497],
        [45.089, 59.504, 72.934, 42.598, 69.574, 67.855, 70.930, 68.366, 50.804,
         53.257],
        [39.692, 56.676, 71.296, 47.192, 68.080, 62.942, 73.466, 68.532, 36.826,
         58.539],
        [54.663, 66.043, 69.484, 46.151, 74.194, 72.727, 74.049, 71.055, 50.339,
         57.078],
        [54.845, 54.910, 73.944, 44.888, 75.666, 74.762, 75.492, 68.728, 34.067,
         57.429],
        [45.337, 43.222, 61.454, 33.204, 58.640, 57.128, 61.800, 60.973, 38.157,
         54.558],
        [48.674, 56.583, 65.599, 45.894, 63.685, 61.471, 64.979, 61.465, 39.372,
         52.933],
        [50.065, 43.785, 69.296, 36.247, 59.217, 63.953, 61.323, 56.782, 28.459,
         51.751],
        [51.958, 46.405, 63.554, 42.120, 59.257, 54.427, 68.809, 66.330, 31.858,
         56.569],
        [40.067, 55.138, 67.143, 41.632, 67.756, 67.747, 62.924, 66.046, 50.115,
         55.094],
        [38.056, 43.417, 63.850, 34.692, 55.406, 55.442, 62.961, 52.230, 39.075,
         33.802],
        [39.088, 50.177, 66.878, 40.306, 52.175, 54.365, 56.237, 61.190, 53.210,
         43.472],
        [45.508, 54.859, 61.235, 43.887, 60.369, 59.250, 68.672, 61.764, 51.585,
         47.951],
        [39.532, 45.630, 58.041, 50.936, 53.021, 51.221, 58.471, 59.880, 30.981,
         46.037],
        [48.997, 55.509, 68.516, 50.881, 63.537, 66.256, 67.150, 63.348, 45.544,
         47.814],
        [46.024, 51.334, 65.225, 42.438, 65.352, 61.397, 68.943, 54.451, 46.914,
         58.842],
        [44.697, 52.947, 61.295, 41.004, 53.710, 53.877, 55.499, 53.191, 61.066,
         40.765],
        [37.638, 59.187, 61.650, 27.435, 55.635, 60.708, 60.622, 51.556, 47.750,
         53.165],
        [42.886, 58.116, 65.397, 51.877, 69.376, 64.493, 70.687, 58.146, 50.655,
         56.126],
        [56.296, 43.634, 61.205, 51.229, 60.056, 56.774, 65.359, 57.483, 62.189,
         61.352],
        [29.715, 51.399, 53.866, 38.246, 48.852, 32.247, 50.030, 42.113, 51.976,
         53.484],
        [46.705, 54.198, 54.136, 35.441, 45.710, 33.381, 40.172, 44.990, 66.912,
         51.722],
        [45.279, 49.657, 58.264, 50.752, 66.731, 55.123, 62.120, 55.596, 45.892,
         52.585],
        [46.482, 46.953, 61.181, 21.356, 54.163, 42.330, 65.643, 52.284, 57.230,
         56.480],
        [47.365, 43.204, 64.054, 34.840, 60.960, 61.458, 70.548, 53.608, 64.217,
         48.235],
        [36.305, 45.231, 60.834, 46.467, 54.694, 50.797, 61.473, 49.466, 53.764,
         48.841],
        [62.275, 65.073, 75.429, 52.124, 69.816, 70.039, 73.111, 68.986, 39.824,
         60.664],
        [52.679, 59.026, 70.344, 42.877, 66.136, 68.722, 70.868, 61.210, 38.683,
         57.982],
        [63.318, 60.328, 74.251, 52.201, 64.410, 66.196, 74.623, 67.883, 41.586,
         63.543],
        [42.856, 38.921, 58.385, 36.323, 58.345, 59.448, 60.101, 47.510, 36.871,
         54.838],
        [56.631, 48.796, 70.645, 40.693, 69.788, 63.959, 72.440, 63.049, 40.007,
         63.426],
        [41.862, 52.825, 72.396, 36.541, 67.978, 58.453, 72.649, 60.121, 38.840,
         51.294],
        [40.271, 46.282, 46.360, 34.183, 54.564, 44.440, 42.215, 39.968, 40.573,
         61.542],
        [31.410, 47.588, 69.026, 32.636, 66.542, 55.570, 73.531, 63.478, 49.948,
         65.628],
        [55.455, 65.971, 73.576, 54.859, 71.616, 69.102, 75.859, 69.749, 48.635,
         68.403]], device='cuda:0')
clipping threshold 0.9078155417080745
a after update for 1 param tensor([-2.177e-05,  6.323e-03,  9.150e-03,  1.869e-02,  1.087e-03, -8.471e-03,
         8.101e-03,  4.812e-04, -1.505e-02, -6.144e-04,  1.360e-02,  6.909e-03,
        -1.994e-03, -1.944e-02, -1.341e-03, -2.356e-03,  1.608e-02,  2.917e-03,
         1.967e-02,  2.608e-03, -2.192e-03, -8.187e-03, -4.679e-03,  3.518e-03,
        -1.259e-04,  1.357e-03,  3.428e-03, -9.714e-03, -9.512e-03,  9.695e-03,
        -1.588e-03, -8.927e-03,  1.739e-03, -5.851e-03,  3.303e-03,  3.504e-03,
         4.232e-04,  8.613e-03,  5.374e-03, -1.167e-02, -1.293e-02,  4.999e-03,
         1.714e-02,  2.653e-04,  5.311e-03,  2.999e-03,  1.098e-02,  2.041e-03,
        -8.476e-04, -7.003e-04,  1.078e-02,  1.557e-02, -1.360e-03, -7.087e-03,
         6.757e-03,  1.384e-03, -8.059e-03,  6.365e-04,  2.906e-03,  5.845e-03,
         2.844e-02, -4.461e-03,  4.738e-03,  9.116e-03, -2.932e-02,  1.672e-02,
        -2.120e-02, -2.240e-02,  5.983e-03,  4.395e-03, -5.850e-03, -2.805e-03,
        -1.241e-02,  5.241e-03, -8.967e-04,  4.855e-03, -8.268e-03,  7.783e-03,
         3.564e-03,  9.088e-03,  5.797e-03, -5.645e-03, -4.121e-03,  2.773e-03,
        -3.311e-03,  5.045e-03, -1.297e-02, -1.090e-02, -1.749e-03, -7.741e-03,
        -1.112e-02,  1.696e-03, -6.201e-03,  1.600e-02, -1.354e-02, -1.139e-03,
         7.570e-03,  1.472e-03,  4.812e-03,  4.917e-03], device='cuda:0')
s after update for 1 param tensor([0.399, 0.628, 0.829, 1.218, 0.558, 1.555, 0.755, 0.890, 1.015, 0.626,
        1.545, 1.317, 0.810, 1.039, 1.102, 1.207, 1.309, 1.568, 1.530, 1.183,
        1.264, 0.755, 1.135, 0.744, 1.019, 1.125, 0.925, 0.854, 1.431, 0.790,
        0.729, 1.178, 0.946, 0.979, 1.030, 1.113, 0.594, 0.931, 0.770, 0.779,
        0.930, 0.443, 0.710, 0.876, 1.233, 0.710, 1.149, 0.933, 1.118, 0.558,
        0.897, 1.059, 1.144, 1.103, 1.082, 0.480, 1.068, 0.926, 0.822, 0.887,
        1.703, 1.394, 0.787, 1.360, 1.542, 1.172, 1.393, 1.277, 1.286, 1.293,
        0.939, 1.616, 0.794, 1.138, 0.656, 1.018, 0.573, 1.154, 0.532, 0.925,
        1.243, 0.610, 1.656, 0.930, 0.768, 0.773, 1.033, 1.139, 1.317, 1.243,
        1.354, 0.993, 0.827, 1.443, 1.155, 1.581, 1.115, 0.890, 1.152, 1.069],
       device='cuda:0')
b after update for 1 param tensor([27.590, 34.632, 39.799, 48.221, 32.635, 54.499, 37.963, 41.232, 44.032,
        34.572, 54.320, 50.152, 39.333, 44.555, 45.871, 48.022, 50.007, 54.717,
        54.060, 47.533, 49.137, 37.973, 46.550, 37.687, 44.118, 46.345, 42.021,
        40.375, 52.285, 38.833, 37.308, 47.434, 42.495, 43.249, 44.362, 46.100,
        33.671, 42.176, 38.353, 38.560, 42.137, 29.084, 36.830, 40.898, 48.521,
        36.813, 46.838, 42.208, 46.212, 32.648, 41.382, 44.970, 46.738, 45.899,
        45.453, 30.275, 45.171, 42.061, 39.616, 41.166, 57.035, 51.589, 38.777,
        50.958, 54.265, 47.319, 51.578, 49.380, 49.562, 49.686, 42.346, 55.546,
        38.942, 46.610, 35.400, 44.093, 33.067, 46.950, 31.878, 42.040, 48.724,
        34.136, 56.241, 42.135, 38.301, 38.417, 44.419, 46.641, 50.155, 48.715,
        50.849, 43.548, 39.740, 52.487, 46.974, 54.943, 46.149, 41.232, 46.908,
        45.176], device='cuda:0')
clipping threshold 0.9078155417080745
a after update for 1 param tensor([[[-5.723e-03],
         [ 7.618e-03],
         [ 1.882e-02],
         [-8.310e-03],
         [-3.472e-03],
         [ 1.003e-03],
         [-9.039e-03],
         [-9.043e-03],
         [ 7.907e-03],
         [-1.047e-02]],

        [[-2.246e-02],
         [ 4.843e-04],
         [ 1.807e-02],
         [ 1.145e-02],
         [-3.510e-03],
         [ 1.902e-02],
         [-8.625e-03],
         [ 1.111e-03],
         [-1.045e-02],
         [-1.070e-02]],

        [[-2.738e-03],
         [ 2.158e-04],
         [ 1.242e-02],
         [ 1.329e-02],
         [-1.034e-02],
         [ 2.518e-02],
         [-1.311e-02],
         [-4.248e-03],
         [ 1.317e-04],
         [-1.008e-02]],

        [[-5.163e-03],
         [ 4.157e-03],
         [ 1.159e-02],
         [-1.908e-03],
         [ 7.450e-03],
         [-1.641e-03],
         [-2.315e-02],
         [ 6.926e-03],
         [ 1.735e-02],
         [ 5.824e-04]],

        [[-1.372e-02],
         [ 1.760e-02],
         [-2.127e-02],
         [-8.803e-03],
         [ 4.152e-03],
         [-8.018e-04],
         [ 5.897e-03],
         [-1.576e-02],
         [-9.657e-03],
         [-1.487e-03]],

        [[ 6.947e-03],
         [ 6.007e-03],
         [ 1.614e-02],
         [ 2.497e-02],
         [-1.362e-03],
         [-8.644e-03],
         [-6.811e-04],
         [-2.765e-02],
         [ 8.582e-03],
         [ 3.924e-03]],

        [[ 1.235e-02],
         [-1.194e-02],
         [ 8.916e-04],
         [-1.317e-02],
         [-6.506e-03],
         [-2.756e-03],
         [ 5.008e-03],
         [ 1.863e-02],
         [ 1.920e-02],
         [-2.139e-02]],

        [[-2.364e-02],
         [-1.065e-02],
         [-4.285e-03],
         [ 1.383e-02],
         [-2.115e-02],
         [-5.718e-03],
         [ 1.967e-02],
         [-4.389e-03],
         [-1.294e-02],
         [-9.694e-03]],

        [[ 5.081e-03],
         [-1.768e-02],
         [ 5.373e-03],
         [ 7.218e-03],
         [-2.810e-02],
         [-1.962e-02],
         [-2.469e-05],
         [ 1.310e-02],
         [ 5.833e-03],
         [ 1.651e-03]],

        [[ 1.612e-02],
         [-6.767e-03],
         [-8.050e-03],
         [ 3.202e-03],
         [ 2.445e-02],
         [ 1.178e-02],
         [-7.325e-03],
         [-2.097e-02],
         [-4.575e-03],
         [-1.776e-04]]], device='cuda:0')
s after update for 1 param tensor([[[1.082],
         [1.225],
         [1.279],
         [0.915],
         [0.596],
         [0.774],
         [1.877],
         [1.259],
         [1.355],
         [1.717]],

        [[1.816],
         [1.218],
         [1.538],
         [0.994],
         [1.103],
         [1.327],
         [0.519],
         [0.826],
         [1.142],
         [1.170]],

        [[1.499],
         [1.426],
         [1.446],
         [1.017],
         [0.957],
         [1.019],
         [1.210],
         [1.558],
         [1.487],
         [1.202]],

        [[0.352],
         [0.395],
         [0.977],
         [0.701],
         [1.833],
         [0.621],
         [1.043],
         [0.778],
         [1.066],
         [1.557]],

        [[1.060],
         [1.425],
         [1.502],
         [1.113],
         [1.266],
         [0.637],
         [1.298],
         [1.237],
         [1.376],
         [1.427]],

        [[1.238],
         [1.154],
         [0.889],
         [1.547],
         [1.116],
         [1.291],
         [1.060],
         [1.013],
         [0.705],
         [1.683]],

        [[1.491],
         [1.574],
         [2.087],
         [1.479],
         [1.604],
         [1.655],
         [2.246],
         [2.054],
         [1.839],
         [1.491]],

        [[1.487],
         [1.018],
         [1.029],
         [1.065],
         [1.326],
         [0.858],
         [1.081],
         [0.927],
         [1.021],
         [0.893]],

        [[1.567],
         [1.801],
         [1.342],
         [1.324],
         [1.119],
         [1.207],
         [1.639],
         [1.449],
         [1.547],
         [1.246]],

        [[1.396],
         [1.230],
         [1.113],
         [1.436],
         [1.751],
         [1.492],
         [1.026],
         [1.534],
         [1.449],
         [0.865]]], device='cuda:0')
b after update for 1 param tensor([[[45.461],
         [48.371],
         [49.423],
         [41.814],
         [33.732],
         [38.458],
         [59.866],
         [49.027],
         [50.863],
         [57.271]],

        [[58.887],
         [48.230],
         [54.202],
         [43.571],
         [45.904],
         [50.346],
         [31.486],
         [39.713],
         [46.710],
         [47.276]],

        [[53.499],
         [52.179],
         [52.553],
         [44.078],
         [42.757],
         [44.110],
         [48.078],
         [54.542],
         [53.299],
         [47.920]],

        [[25.937],
         [27.468],
         [43.197],
         [36.588],
         [59.169],
         [34.439],
         [44.632],
         [38.536],
         [45.120],
         [54.532]],

        [[44.983],
         [52.168],
         [53.554],
         [46.098],
         [49.177],
         [34.891],
         [49.798],
         [48.613],
         [51.269],
         [52.201]],

        [[48.618],
         [46.954],
         [41.207],
         [54.359],
         [46.171],
         [49.661],
         [44.984],
         [43.992],
         [36.693],
         [56.689]],

        [[53.358],
         [54.820],
         [63.135],
         [53.142],
         [55.344],
         [56.227],
         [65.500],
         [62.632],
         [59.267],
         [53.371]],

        [[53.286],
         [44.098],
         [44.333],
         [45.104],
         [50.317],
         [40.478],
         [45.440],
         [42.085],
         [44.152],
         [41.302]],

        [[54.710],
         [58.649],
         [50.635],
         [50.294],
         [46.235],
         [48.019],
         [55.940],
         [52.602],
         [54.357],
         [48.778]],

        [[51.629],
         [48.464],
         [46.101],
         [52.364],
         [57.825],
         [53.379],
         [44.260],
         [54.125],
         [52.599],
         [40.650]]], device='cuda:0')
clipping threshold 0.9078155417080745
a after update for 1 param tensor([[-0.015],
        [-0.004],
        [ 0.012],
        [ 0.010],
        [-0.014],
        [ 0.007],
        [-0.008],
        [ 0.009],
        [ 0.010],
        [-0.001]], device='cuda:0')
s after update for 1 param tensor([[1.583],
        [1.531],
        [1.110],
        [1.209],
        [1.209],
        [0.492],
        [1.670],
        [1.443],
        [0.756],
        [1.443]], device='cuda:0')
b after update for 1 param tensor([[54.978],
        [54.075],
        [46.052],
        [48.042],
        [48.045],
        [30.667],
        [56.483],
        [52.497],
        [38.008],
        [52.495]], device='cuda:0')
clipping threshold 0.9078155417080745
||w||^2 1.308477774848725
exp ma of ||w||^2 1.5783408181633904
||w|| 1.143887133789311
exp ma of ||w|| 1.2050569198054928
||w||^2 1.4179264022546847
exp ma of ||w||^2 1.510892516835477
||w|| 1.1907671486292712
exp ma of ||w|| 1.1667104272484057
||w||^2 0.5526788893685535
exp ma of ||w||^2 1.572057784520982
||w|| 0.7434237616383764
exp ma of ||w|| 1.1952032773389396
||w||^2 1.2989062843996613
exp ma of ||w||^2 1.8323577301055678
||w|| 1.1396956981579167
exp ma of ||w|| 1.2702915107226724
||w||^2 0.6651859426574915
exp ma of ||w||^2 1.4401223988161107
||w|| 0.8155893223047317
exp ma of ||w|| 1.1343889633093809
||w||^2 1.710060902448508
exp ma of ||w||^2 1.4892459425789366
||w|| 1.307692969488063
exp ma of ||w|| 1.1596668839928792
||w||^2 0.6201910030417055
exp ma of ||w||^2 1.5228468055833784
||w|| 0.7875220651141818
exp ma of ||w|| 1.1608438533504069
||w||^2 0.43582375440660215
exp ma of ||w||^2 1.6311219785548503
||w|| 0.6601694891515376
exp ma of ||w|| 1.2087903181604363
||w||^2 5.137172280599465
exp ma of ||w||^2 1.6348416234886185
||w|| 2.2665330971771547
exp ma of ||w|| 1.2113259422019613
||w||^2 1.2240789456318573
exp ma of ||w||^2 1.473210123732922
||w|| 1.1063810128666605
exp ma of ||w|| 1.1517706090256474
||w||^2 1.116694706650506
exp ma of ||w||^2 1.4222104496656938
||w|| 1.0567377662648885
exp ma of ||w|| 1.126870479892698
||w||^2 0.6256466189871498
exp ma of ||w||^2 1.8101055377898028
||w|| 0.7909782670763779
exp ma of ||w|| 1.2728140154560892
||w||^2 0.5831797115093706
exp ma of ||w||^2 1.4547480429325907
||w|| 0.7636620401128831
exp ma of ||w|| 1.1400956918693985
||w||^2 0.7167031118543008
exp ma of ||w||^2 1.481995127364836
||w|| 0.84658319842429
exp ma of ||w|| 1.1472978621009728
||w||^2 1.4118157945358396
exp ma of ||w||^2 1.4515292321765343
||w|| 1.1881985501320222
exp ma of ||w|| 1.1543351860054558
||w||^2 0.8049428093327542
exp ma of ||w||^2 1.5913504637321094
||w|| 0.897186050567414
exp ma of ||w|| 1.193512424376655
cuda
Objective function 8.58 = squared loss an data 6.59 + 0.5*rho*h**2 0.288053 + alpha*h 0.090869 + L2reg 1.52 + L1reg 0.09 ; SHD = 16 ; DAG True
Proportion of microbatches that were clipped  0.7759341052882893
iteration 3 in inner loop, alpha 3.7858520066942276 rho 1000.0 h 0.02400222558995324
iteration 3 in outer loop, alpha = 27.78807759664747, rho = 1000.0, h = 0.02400222558995324
cuda
1210
cuda
Objective function 9.16 = squared loss an data 6.59 + 0.5*rho*h**2 0.288053 + alpha*h 0.666976 + L2reg 1.52 + L1reg 0.09 ; SHD = 16 ; DAG True
||w||^2 267029.54839718726
exp ma of ||w||^2 11218710.961702565
||w|| 516.7490187675128
exp ma of ||w|| 1293.5856745942253
||w||^2 1.2074217497242463
exp ma of ||w||^2 1.4076742311315804
||w|| 1.0988274431066265
exp ma of ||w|| 1.1374365492603022
||w||^2 1.4269020350138306
exp ma of ||w||^2 1.4469425376328497
||w|| 1.194530047765158
exp ma of ||w|| 1.1439159168872963
||w||^2 1.2895491963854586
exp ma of ||w||^2 1.495055825326557
||w|| 1.1355831965934766
exp ma of ||w|| 1.1643245942807714
||w||^2 2.8265133532326416
exp ma of ||w||^2 1.4224712499261047
||w|| 1.6812237665559697
exp ma of ||w|| 1.13383911880775
||w||^2 3.0668095886094497
exp ma of ||w||^2 1.549695002633466
||w|| 1.7512308781566894
exp ma of ||w|| 1.1805216141778128
||w||^2 1.1744344108733886
exp ma of ||w||^2 1.4709220717162
||w|| 1.0837132512216452
exp ma of ||w|| 1.1576706546139997
||w||^2 1.0955895725274394
exp ma of ||w||^2 1.4276001567534782
||w|| 1.0467041475638852
exp ma of ||w|| 1.1360578632476113
||w||^2 0.598542202982688
exp ma of ||w||^2 1.40123598324214
||w|| 0.773655093037387
exp ma of ||w|| 1.1327073123681237
||w||^2 2.431249882094361
exp ma of ||w||^2 1.4661815685109671
||w|| 1.559246575142739
exp ma of ||w|| 1.1553662245009992
||w||^2 3.4277518899420163
exp ma of ||w||^2 1.5731161614096547
||w|| 1.8514188855961302
exp ma of ||w|| 1.1916264251233672
||w||^2 0.9972856631145276
exp ma of ||w||^2 1.6289737185402116
||w|| 0.9986419093521599
exp ma of ||w|| 1.2031842851299084
||w||^2 0.6584178530506009
exp ma of ||w||^2 1.507996386407141
||w|| 0.8114295120653432
exp ma of ||w|| 1.159480685585086
||w||^2 2.242843054893571
exp ma of ||w||^2 1.7211861168442535
||w|| 1.4976124515019136
exp ma of ||w|| 1.2379862393094434
||w||^2 1.1081335557174707
exp ma of ||w||^2 1.5078627639043491
||w|| 1.0526792273610563
exp ma of ||w|| 1.1716638874205032
||w||^2 1.0338046711485833
exp ma of ||w||^2 1.5826588663009744
||w|| 1.0167618556715152
exp ma of ||w|| 1.1890480341266823
cuda
Objective function 8.45 = squared loss an data 6.44 + 0.5*rho*h**2 0.058822 + alpha*h 0.301399 + L2reg 1.55 + L1reg 0.09 ; SHD = 14 ; DAG True
Proportion of microbatches that were clipped  0.7765087143816939
iteration 1 in inner loop, alpha 27.78807759664747 rho 1000.0 h 0.010846338951056467
1210
cuda
Objective function 8.98 = squared loss an data 6.44 + 0.5*rho*h**2 0.588215 + alpha*h 0.301399 + L2reg 1.55 + L1reg 0.09 ; SHD = 14 ; DAG True
||w||^2 1183761338.084171
exp ma of ||w||^2 642069880454.8586
||w|| 34405.83290786856
exp ma of ||w|| 157680.96858244663
||w||^2 65651020.36420758
exp ma of ||w||^2 115449063840.92442
||w|| 8102.531725590933
exp ma of ||w|| 42651.09736897471
||w||^2 14561.92096839985
exp ma of ||w||^2 61313752.865965605
||w|| 120.67278470475375
exp ma of ||w|| 326.910936798581
||w||^2 141.28334142590924
exp ma of ||w||^2 4107028.909486032
||w|| 11.886266925570418
exp ma of ||w|| 46.04850653003581
||w||^2 7.4118428299559955
exp ma of ||w||^2 590404.6366756556
||w|| 2.7224699869706543
exp ma of ||w|| 11.774439647616306
||w||^2 3.4745197631670193
exp ma of ||w||^2 3582.773020923316
||w|| 1.8640063742291815
exp ma of ||w|| 1.7858185546236451
||w||^2 1.63288919881789
exp ma of ||w||^2 4.910369320027831
||w|| 1.27784553010835
exp ma of ||w|| 1.2848799792057726
||w||^2 1.4843059829104421
exp ma of ||w||^2 1.6301571789590052
||w|| 1.2183209687559524
exp ma of ||w|| 1.177725223717828
||w||^2 0.5567811971775019
exp ma of ||w||^2 1.4499208291742003
||w|| 0.7461777249271797
exp ma of ||w|| 1.1370172582741105
||w||^2 1.0459800112196607
exp ma of ||w||^2 1.4232841851714007
||w|| 1.0227316418394714
exp ma of ||w|| 1.1425290881694312
||w||^2 0.8641094640365302
exp ma of ||w||^2 1.5572856686514027
||w|| 0.9295748835013402
exp ma of ||w|| 1.17152290923048
||w||^2 1.64413831100892
exp ma of ||w||^2 1.4323720021807247
||w|| 1.282239568492924
exp ma of ||w|| 1.1318435779440719
||w||^2 0.5115996690325423
exp ma of ||w||^2 1.4076902942202894
||w|| 0.7152619583289344
exp ma of ||w|| 1.1361656248730874
||w||^2 2.03713101104455
exp ma of ||w||^2 1.3985855637614948
||w|| 1.4272809853159785
exp ma of ||w|| 1.1271444638208628
||w||^2 0.35861530903906325
exp ma of ||w||^2 1.4186226846839585
||w|| 0.5988449791382268
exp ma of ||w|| 1.1256936484010613
||w||^2 0.388210469608064
exp ma of ||w||^2 1.3083833360084962
||w|| 0.6230653814874199
exp ma of ||w|| 1.082102988141454
||w||^2 1.6428771571449325
exp ma of ||w||^2 1.3979425575271254
||w|| 1.2817476963681005
exp ma of ||w|| 1.1213161620171355
||w||^2 0.40713640464708106
exp ma of ||w||^2 1.3470619533799517
||w|| 0.6380724133255418
exp ma of ||w|| 1.0964917070084996
||w||^2 1.7869669243025796
exp ma of ||w||^2 1.6442495481627983
||w|| 1.3367748218389586
exp ma of ||w|| 1.2109913578262606
||w||^2 1.8715687461178874
exp ma of ||w||^2 1.356289151040111
||w|| 1.368052903259917
exp ma of ||w|| 1.1141031322292458
||w||^2 1.2266111522090783
exp ma of ||w||^2 1.5383786101364274
||w|| 1.1075247862730109
exp ma of ||w|| 1.1748696079439993
||w||^2 1.532471966887731
exp ma of ||w||^2 1.4211317983134055
||w|| 1.237930517794812
exp ma of ||w|| 1.1397703377075212
||w||^2 0.9432220033740331
exp ma of ||w||^2 1.4508346211234884
||w|| 0.9711961714164822
exp ma of ||w|| 1.1550157835449502
||w||^2 0.40179906224780154
exp ma of ||w||^2 1.4834039209183254
||w|| 0.6338762199734279
exp ma of ||w|| 1.16436858693801
||w||^2 1.6318776698976054
exp ma of ||w||^2 1.2982058761647803
||w|| 1.2774496741154249
exp ma of ||w|| 1.0878646759355772
||w||^2 0.6852362693914279
exp ma of ||w||^2 1.2082170053522343
||w|| 0.8277899911157587
exp ma of ||w|| 1.0596329450748934
||w||^2 1.7440114468705687
exp ma of ||w||^2 1.2511499517901605
||w|| 1.3206102554768264
exp ma of ||w|| 1.07227152345983
||w||^2 2.0488262763789558
exp ma of ||w||^2 1.2740125892462837
||w|| 1.4313721655736344
exp ma of ||w|| 1.0793518303401843
||w||^2 0.19545767571901806
exp ma of ||w||^2 1.2711968962418363
||w|| 0.4421059553082474
exp ma of ||w|| 1.064970319119292
||w||^2 3.6515454515226424
exp ma of ||w||^2 1.2579180615456984
||w|| 1.9109017377988442
exp ma of ||w|| 1.0781687805648936
cuda
Objective function 8.25 = squared loss an data 6.40 + 0.5*rho*h**2 0.085806 + alpha*h 0.115115 + L2reg 1.55 + L1reg 0.10 ; SHD = 13 ; DAG True
Proportion of microbatches that were clipped  0.7748768081386107
iteration 2 in inner loop, alpha 27.78807759664747 rho 10000.0 h 0.00414261324552001
iteration 4 in outer loop, alpha = 69.21421005184757, rho = 10000.0, h = 0.00414261324552001
cuda
1210
cuda
Objective function 8.42 = squared loss an data 6.40 + 0.5*rho*h**2 0.085806 + alpha*h 0.286728 + L2reg 1.55 + L1reg 0.10 ; SHD = 13 ; DAG True
||w||^2 399949748564.2607
exp ma of ||w||^2 143133758835.96368
||w|| 632415.8035377205
exp ma of ||w|| 193719.88202222355
||w||^2 43258.54138867164
exp ma of ||w||^2 4620126038.818623
||w|| 207.98687792423743
exp ma of ||w|| 828.6480697345443
||w||^2 1.6868180179182715
exp ma of ||w||^2 86615.06083947199
||w|| 1.2987755841246291
exp ma of ||w|| 1.886050408569523
||w||^2 4.040708408523702
exp ma of ||w||^2 1.3032165313229824
||w|| 2.0101513397064665
exp ma of ||w|| 1.0957044407887055
||w||^2 1.8143341280888143
exp ma of ||w||^2 1.2739024097370282
||w|| 1.3469722076155894
exp ma of ||w|| 1.079045727034961
||w||^2 2.5067817435230046
exp ma of ||w||^2 1.4673402717618456
||w|| 1.5832819532613276
exp ma of ||w|| 1.1442262105758607
||w||^2 0.608674795285573
exp ma of ||w||^2 1.192074457454768
||w|| 0.780176130938119
exp ma of ||w|| 1.0391585221225217
||w||^2 1.997416894825056
exp ma of ||w||^2 1.2301345948555784
||w|| 1.4133000017070176
exp ma of ||w|| 1.0642185191033016
||w||^2 1.4125312649777657
exp ma of ||w||^2 1.3043144127654072
||w|| 1.1884995856026899
exp ma of ||w|| 1.094718768641737
||w||^2 0.5650239090359817
exp ma of ||w||^2 1.2322481408375956
||w|| 0.7516807228045572
exp ma of ||w|| 1.0610516364841491
||w||^2 0.5098511640632999
exp ma of ||w||^2 1.138091145597595
||w|| 0.7140386292514571
exp ma of ||w|| 1.020742642064668
||w||^2 1.2951309814995895
exp ma of ||w||^2 1.2325937476297752
||w|| 1.138038216185902
exp ma of ||w|| 1.0615004502750103
||w||^2 1.1282908461242152
exp ma of ||w||^2 1.1099451981491983
||w|| 1.0622103586974734
exp ma of ||w|| 1.0130520149183542
||w||^2 2.6880387737241542
exp ma of ||w||^2 1.1573104890919206
||w|| 1.6395239472859657
exp ma of ||w|| 1.0303979758712336
||w||^2 1.2555809583779869
exp ma of ||w||^2 1.07275275918555
||w|| 1.1205270895333084
exp ma of ||w|| 0.9914509336338864
||w||^2 0.6147263597078005
exp ma of ||w||^2 1.0798628502089809
||w|| 0.7840448709785688
exp ma of ||w|| 0.9990854163472106
||w||^2 1.1227187395021097
exp ma of ||w||^2 1.14043620191596
||w|| 1.0595842295457731
exp ma of ||w|| 1.0272932560249428
||w||^2 1.4695146828339403
exp ma of ||w||^2 1.0623679455943564
||w|| 1.2122354073503794
exp ma of ||w|| 1.0018330326805849
cuda
Objective function 8.12 = squared loss an data 6.26 + 0.5*rho*h**2 0.040126 + alpha*h 0.196075 + L2reg 1.52 + L1reg 0.10 ; SHD = 13 ; DAG True
Proportion of microbatches that were clipped  0.7736428542938056
iteration 1 in inner loop, alpha 69.21421005184757 rho 10000.0 h 0.0028328736062945836
1210
cuda
Objective function 8.48 = squared loss an data 6.26 + 0.5*rho*h**2 0.401259 + alpha*h 0.196075 + L2reg 1.52 + L1reg 0.10 ; SHD = 13 ; DAG True
||w||^2 243595015.79752603
exp ma of ||w||^2 272836769097910.25
||w|| 15607.530739919304
exp ma of ||w|| 1165267.8249899761
||w||^2 430.55369664908835
exp ma of ||w||^2 128801731252.72586
||w|| 20.749787869978054
exp ma of ||w|| 607.3024786898803
||w||^2 7.527284616309284
exp ma of ||w||^2 120433379.46055429
||w|| 2.74358973177647
exp ma of ||w|| 5.866068999216491
||w||^2 2.8945523153897903
exp ma of ||w||^2 20201.05800859516
||w|| 1.7013383894422034
exp ma of ||w|| 2.868698190189127
||w||^2 3.9957182990971813
exp ma of ||w||^2 7.05559278185836
||w|| 1.9989292881683387
exp ma of ||w|| 1.9057296404615738
||w||^2 2.6672911769952328
exp ma of ||w||^2 1.5473258901954658
||w|| 1.6331843671169624
exp ma of ||w|| 1.1969324390540559
||w||^2 1.193332957025577
exp ma of ||w||^2 1.506520525317783
||w|| 1.0923978016389346
exp ma of ||w|| 1.189647357868606
||w||^2 0.5285520604401847
exp ma of ||w||^2 1.089148197697706
||w|| 0.7270158598271325
exp ma of ||w|| 1.0033795399610803
||w||^2 1.2936153252564961
exp ma of ||w||^2 1.1033865291003375
||w|| 1.1373721138029085
exp ma of ||w|| 1.00952031411662
||w||^2 0.6256496457419675
exp ma of ||w||^2 1.1213954070597176
||w|| 0.7909801803724082
exp ma of ||w|| 1.016097556012866
||w||^2 0.7416685420438364
exp ma of ||w||^2 1.154645300314371
||w|| 0.8612018009989507
exp ma of ||w|| 1.0262972756042352
||w||^2 0.4463083830510712
exp ma of ||w||^2 1.2903621472280984
||w|| 0.6680631579806442
exp ma of ||w|| 1.0878703288137386
||w||^2 0.5444026090171249
exp ma of ||w||^2 1.2077731563523926
||w|| 0.7378364378486094
exp ma of ||w|| 1.057708959146253
||w||^2 0.9908138440661313
exp ma of ||w||^2 1.199280364604811
||w|| 0.9953963251218739
exp ma of ||w|| 1.0562520018170063
||w||^2 1.9415691086572826
exp ma of ||w||^2 1.2310056354948309
||w|| 1.3934019910482698
exp ma of ||w|| 1.0702043193444333
||w||^2 1.7326617212069757
exp ma of ||w||^2 1.2878301876632452
||w|| 1.3163060894818408
exp ma of ||w|| 1.0783479821077695
||w||^2 1.6064934926275274
exp ma of ||w||^2 1.212590967163056
||w|| 1.2674752433982792
exp ma of ||w|| 1.0561527285546648
||w||^2 0.7897278514544076
exp ma of ||w||^2 1.1994461402289167
||w|| 0.8886663330262983
exp ma of ||w|| 1.0455294368589354
||w||^2 1.1210371165966075
exp ma of ||w||^2 1.1340654868604716
||w|| 1.0587904025805144
exp ma of ||w|| 1.0143045920164553
||w||^2 0.5679035235517097
exp ma of ||w||^2 1.2795649716909516
||w|| 0.7535937390608481
exp ma of ||w|| 1.0696643159001693
||w||^2 0.8596464386929805
exp ma of ||w||^2 1.2472088067932665
||w|| 0.9271712024717875
exp ma of ||w|| 1.0601051168244753
||w||^2 0.4604610370821698
exp ma of ||w||^2 1.2288400885319612
||w|| 0.678572794239623
exp ma of ||w|| 1.0642608587086022
||w||^2 0.4475864737897109
exp ma of ||w||^2 1.1675394709517584
||w|| 0.6690190384359109
exp ma of ||w|| 1.031983080105524
||w||^2 0.26147189429880413
exp ma of ||w||^2 1.0905551318210729
||w|| 0.5113432255333047
exp ma of ||w|| 0.996000126608977
||w||^2 1.2027305233215388
exp ma of ||w||^2 1.15133078868456
||w|| 1.09669071452326
exp ma of ||w|| 1.0239505862537182
cuda
Objective function 8.12 = squared loss an data 6.39 + 0.5*rho*h**2 0.055905 + alpha*h 0.073187 + L2reg 1.50 + L1reg 0.10 ; SHD = 12 ; DAG True
Proportion of microbatches that were clipped  0.7724257166981625
iteration 2 in inner loop, alpha 69.21421005184757 rho 100000.0 h 0.0010574025895309092
iteration 5 in outer loop, alpha = 1126.6167995827568, rho = 1000000.0, h = 0.0010574025895309092
Threshold 0.3
[[0.006 0.011 0.185 0.003 0.839 0.048 0.208 0.035 0.005 2.708]
 [0.616 0.011 2.467 0.002 0.281 0.252 0.896 0.047 0.001 2.82 ]
 [0.02  0.003 0.007 0.    0.089 0.001 0.671 0.002 0.001 0.12 ]
 [0.631 3.423 0.727 0.004 0.876 1.494 0.639 1.272 0.115 0.885]
 [0.006 0.004 0.039 0.003 0.003 0.007 0.02  0.01  0.003 0.011]
 [0.082 0.013 2.074 0.002 0.51  0.006 1.47  0.003 0.009 0.213]
 [0.015 0.005 0.01  0.002 0.233 0.003 0.006 0.004 0.002 0.217]
 [0.136 0.068 0.493 0.001 0.264 0.929 0.334 0.004 0.016 0.359]
 [0.395 2.551 0.712 0.023 0.71  0.325 1.315 0.176 0.003 0.641]
 [0.001 0.001 0.033 0.    0.331 0.009 0.011 0.007 0.    0.003]]
[[0.    0.    0.    0.    0.839 0.    0.    0.    0.    2.708]
 [0.616 0.    2.467 0.    0.    0.    0.896 0.    0.    2.82 ]
 [0.    0.    0.    0.    0.    0.    0.671 0.    0.    0.   ]
 [0.631 3.423 0.727 0.    0.876 1.494 0.639 1.272 0.    0.885]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    2.074 0.    0.51  0.    1.47  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.493 0.    0.    0.929 0.334 0.    0.    0.359]
 [0.395 2.551 0.712 0.    0.71  0.325 1.315 0.    0.    0.641]
 [0.    0.    0.    0.    0.331 0.    0.    0.    0.    0.   ]]
{'fdr': 0.36666666666666664, 'tpr': 0.95, 'fpr': 0.44, 'f1': 0.7599999999999999, 'shd': 12, 'npred': 30, 'ntrue': 20}
[1.111e-02 1.850e-01 2.723e-03 8.394e-01 4.838e-02 2.077e-01 3.529e-02
 4.823e-03 2.708e+00 6.159e-01 2.467e+00 1.540e-03 2.806e-01 2.516e-01
 8.962e-01 4.705e-02 9.866e-04 2.820e+00 1.976e-02 2.855e-03 2.415e-04
 8.946e-02 1.314e-03 6.709e-01 2.020e-03 7.461e-04 1.204e-01 6.310e-01
 3.423e+00 7.273e-01 8.760e-01 1.494e+00 6.388e-01 1.272e+00 1.153e-01
 8.845e-01 6.198e-03 3.512e-03 3.930e-02 2.990e-03 6.953e-03 1.976e-02
 1.031e-02 2.512e-03 1.148e-02 8.233e-02 1.276e-02 2.074e+00 2.137e-03
 5.097e-01 1.470e+00 2.843e-03 9.173e-03 2.126e-01 1.535e-02 4.749e-03
 1.026e-02 2.080e-03 2.329e-01 2.672e-03 3.537e-03 2.188e-03 2.172e-01
 1.362e-01 6.776e-02 4.928e-01 1.357e-03 2.643e-01 9.295e-01 3.337e-01
 1.582e-02 3.595e-01 3.951e-01 2.551e+00 7.125e-01 2.258e-02 7.098e-01
 3.247e-01 1.315e+00 1.762e-01 6.410e-01 7.458e-04 8.172e-04 3.252e-02
 2.781e-04 3.310e-01 9.398e-03 1.116e-02 7.500e-03 3.001e-04]
[[0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9685714285714286, 0.9107924469724047)
Iterations 2500
Achieves (92.9799135797352, 1e-05)-DP
