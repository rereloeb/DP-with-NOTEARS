samples  5000  graph  15 30 ER mlp  minibatch size  100  noise  0.5  minibatches per NN training  32 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006583550827196305
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0011537970975332712
iteration 4 in outer loop, alpha = 156.00431709949223, rho = 100000.0, h = 0.0011537970975332712
cuda
iteration 1 in inner loop,alpha 156.00431709949223 rho 100000.0 h 0.00033575511004180214
iteration 5 in outer loop, alpha = 491.7594271412944, rho = 1000000.0, h = 0.00033575511004180214
Threshold 0.3
[[0.002 0.    0.    0.005 0.194 0.    0.    0.    0.    2.734 0.179 0.001
  0.001 0.    0.006]
 [2.52  0.001 0.002 0.262 0.215 0.01  0.01  0.001 0.002 2.189 0.199 1.342
  0.001 0.001 0.098]
 [0.027 0.068 0.001 0.147 0.166 0.035 0.137 0.001 0.067 0.125 0.135 0.162
  0.001 0.001 0.006]
 [0.153 0.    0.001 0.001 0.204 0.    0.    0.    0.    0.214 0.335 0.001
  0.    0.    1.022]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    0.001]
 [2.646 0.03  0.001 0.968 0.376 0.001 1.377 0.    0.055 1.746 0.956 1.472
  0.012 0.002 1.923]
 [0.414 0.044 0.    0.336 0.408 0.    0.001 0.    0.059 0.139 1.162 0.096
  0.001 0.    0.163]
 [2.69  0.002 0.    2.092 0.137 0.612 0.046 0.    0.003 0.313 0.139 0.061
  0.014 0.022 0.118]
 [0.103 0.383 0.002 0.638 0.415 0.002 0.001 0.001 0.003 2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.001 0.    0.    0.002 0.402 0.001
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.057 0.    0.001 0.    0.001 0.001 0.015 0.
  0.    0.    0.005]
 [0.37  0.    0.    0.18  0.153 0.    0.004 0.    0.002 0.427 0.58  0.001
  0.    0.001 0.032]
 [0.041 0.078 0.    0.112 3.047 0.017 0.008 0.    1.212 0.107 1.585 0.031
  0.001 0.016 0.021]
 [0.126 1.398 0.    1.127 1.231 0.003 0.006 0.    3.368 0.386 0.434 0.158
  0.011 0.002 0.562]
 [0.089 0.    0.    0.    0.111 0.    0.    0.    0.    0.106 0.081 0.002
  0.004 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.734 0.    0.
  0.    0.    0.   ]
 [2.52  0.    0.    0.    0.    0.    0.    0.    0.    2.189 0.    1.342
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.335 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.968 0.376 0.    1.377 0.    0.    1.746 0.956 1.472
  0.    0.    1.923]
 [0.414 0.    0.    0.336 0.408 0.    0.    0.    0.    0.    1.162 0.
  0.    0.    0.   ]
 [2.69  0.    0.    2.092 0.    0.612 0.    0.    0.    0.313 0.    0.
  0.    0.    0.   ]
 [0.    0.383 0.    0.638 0.415 0.    0.    0.    0.    2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.    0.    0.    0.    0.402 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.057 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.427 0.58  0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.047 0.    0.    0.    1.212 0.    1.585 0.
  0.    0.    0.   ]
 [0.    1.398 0.    1.127 1.231 0.    0.    0.    3.368 0.386 0.434 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[7.166e-05 3.031e-04 4.737e-03 1.942e-01 6.655e-05 2.207e-04 4.993e-05
 2.937e-04 2.734e+00 1.793e-01 5.105e-04 1.050e-03 1.242e-04 5.740e-03
 2.520e+00 1.954e-03 2.620e-01 2.147e-01 9.724e-03 9.801e-03 9.328e-04
 1.620e-03 2.189e+00 1.994e-01 1.342e+00 5.488e-04 9.886e-04 9.781e-02
 2.679e-02 6.849e-02 1.472e-01 1.660e-01 3.508e-02 1.368e-01 5.716e-04
 6.687e-02 1.254e-01 1.352e-01 1.619e-01 8.477e-04 1.272e-03 6.092e-03
 1.532e-01 1.509e-04 9.882e-04 2.036e-01 4.044e-05 3.897e-04 2.489e-04
 6.858e-05 2.135e-01 3.353e-01 1.155e-03 4.106e-04 1.293e-04 1.022e+00
 6.064e-05 4.791e-06 5.172e-05 2.471e-04 1.136e-05 1.928e-04 4.191e-06
 2.163e-04 3.383e-04 1.833e-03 1.007e-04 2.631e-05 7.614e-05 1.086e-03
 2.646e+00 3.009e-02 1.478e-03 9.684e-01 3.765e-01 1.377e+00 1.229e-04
 5.513e-02 1.746e+00 9.560e-01 1.472e+00 1.180e-02 1.838e-03 1.923e+00
 4.141e-01 4.437e-02 3.959e-04 3.360e-01 4.079e-01 4.254e-04 5.935e-05
 5.872e-02 1.392e-01 1.162e+00 9.588e-02 6.731e-04 4.063e-04 1.632e-01
 2.690e+00 2.126e-03 4.852e-04 2.092e+00 1.369e-01 6.121e-01 4.646e-02
 3.330e-03 3.125e-01 1.385e-01 6.062e-02 1.448e-02 2.181e-02 1.178e-01
 1.034e-01 3.829e-01 2.017e-03 6.383e-01 4.147e-01 2.074e-03 1.394e-03
 1.328e-03 2.308e+00 1.722e+00 1.030e+00 5.492e-05 3.972e-04 3.674e-01
 1.834e-04 4.607e-06 8.949e-05 4.394e-04 2.722e+00 6.113e-06 8.992e-04
 1.147e-05 4.538e-04 4.020e-01 1.020e-03 1.265e-04 1.194e-04 2.179e-03
 6.634e-04 5.288e-05 3.430e-04 1.176e-03 3.057e+00 1.252e-04 7.885e-04
 1.415e-04 9.900e-04 8.589e-04 1.728e-04 2.153e-04 3.103e-04 5.194e-03
 3.696e-01 4.210e-04 8.175e-05 1.805e-01 1.529e-01 2.371e-04 3.593e-03
 6.136e-05 1.729e-03 4.267e-01 5.799e-01 2.072e-04 6.430e-04 3.169e-02
 4.077e-02 7.764e-02 2.020e-04 1.125e-01 3.047e+00 1.651e-02 8.033e-03
 3.454e-04 1.212e+00 1.069e-01 1.585e+00 3.144e-02 1.628e-02 2.119e-02
 1.264e-01 1.398e+00 2.136e-04 1.127e+00 1.231e+00 3.196e-03 6.293e-03
 3.254e-04 3.368e+00 3.864e-01 4.339e-01 1.576e-01 1.130e-02 5.624e-01
 8.923e-02 3.157e-04 3.306e-04 7.760e-05 1.109e-01 7.500e-05 1.536e-04
 2.248e-04 2.530e-04 1.060e-01 8.126e-02 2.152e-03 4.216e-03 1.418e-04]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9805555555555556, 0.9709424613206593)
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 58.55091749524359 clip 2.301154480791764
total norm for a microbatch 38.44375536607758 clip 3.6335650159454
cuda
Objective function 93.55 = squared loss an data 91.87 + 0.5*rho*h**2 1.367543 + alpha*h 0.000000 + L2reg 0.16 + L1reg 0.15 ; SHD = 53 ; DAG False
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.6538094244854733
iteration 1 in outer loop, alpha = 1.6538094244854733, rho = 1.0, h = 1.6538094244854733
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 96.29 = squared loss an data 91.87 + 0.5*rho*h**2 1.367543 + alpha*h 2.735086 + L2reg 0.16 + L1reg 0.15 ; SHD = 53 ; DAG False
total norm for a microbatch 59.81718866975312 clip 1.593718315822677
total norm for a microbatch 25.32092355489039 clip 1.9191560728619166
total norm for a microbatch 38.790146334004085 clip 2.2790083290056993
total norm for a microbatch 30.412777954911082 clip 8.698658116302799
cuda
Objective function 40.80 = squared loss an data 37.77 + 0.5*rho*h**2 0.669431 + alpha*h 1.913610 + L2reg 0.31 + L1reg 0.14 ; SHD = 49 ; DAG False
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 1.6538094244854733 rho 1.0 h 1.1570921928223505
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 46.82 = squared loss an data 37.77 + 0.5*rho*h**2 6.694312 + alpha*h 1.913610 + L2reg 0.31 + L1reg 0.14 ; SHD = 49 ; DAG False
total norm for a microbatch 29.94378612264629 clip 4.256923352742274
total norm for a microbatch 32.760993934348086 clip 5.488667591376027
total norm for a microbatch 23.74446129859849 clip 5.488667591376027
total norm for a microbatch 37.44624911881925 clip 7.084047691239481
cuda
Objective function 25.36 = squared loss an data 23.05 + 0.5*rho*h**2 0.974505 + alpha*h 0.730117 + L2reg 0.48 + L1reg 0.12 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 1.6538094244854733 rho 10.0 h 0.4414759676669959
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 34.13 = squared loss an data 23.05 + 0.5*rho*h**2 9.745052 + alpha*h 0.730117 + L2reg 0.48 + L1reg 0.12 ; SHD = 36 ; DAG True
total norm for a microbatch 56.44033192594901 clip 1.0913572274517724
total norm for a microbatch 37.93818196240266 clip 3.651564962747892
total norm for a microbatch 37.489288603613524 clip 3.651564962747892
cuda
Objective function 21.12 = squared loss an data 19.15 + 0.5*rho*h**2 0.996475 + alpha*h 0.233471 + L2reg 0.62 + L1reg 0.11 ; SHD = 32 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 3 in inner loop, alpha 1.6538094244854733 rho 100.0 h 0.14117189281984643
iteration 2 in outer loop, alpha = 15.770998706470117, rho = 100.0, h = 0.14117189281984643
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 23.11 = squared loss an data 19.15 + 0.5*rho*h**2 0.996475 + alpha*h 2.226422 + L2reg 0.62 + L1reg 0.11 ; SHD = 32 ; DAG True
total norm for a microbatch 43.35474681582809 clip 1.4233584334316403
total norm for a microbatch 27.456204991187782 clip 3.544239167058602
total norm for a microbatch 89.38183263850195 clip 14.501695135508603
cuda
Objective function 18.72 = squared loss an data 16.39 + 0.5*rho*h**2 0.279981 + alpha*h 1.180154 + L2reg 0.76 + L1reg 0.11 ; SHD = 30 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 15.770998706470117 rho 100.0 h 0.07483064855898114
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 21.24 = squared loss an data 16.39 + 0.5*rho*h**2 2.799813 + alpha*h 1.180154 + L2reg 0.76 + L1reg 0.11 ; SHD = 30 ; DAG True
total norm for a microbatch 42.93661136893733 clip 1.3112287510071101
total norm for a microbatch 54.15214549539891 clip 10.179545700513199
cuda
Objective function 17.67 = squared loss an data 15.72 + 0.5*rho*h**2 0.489710 + alpha*h 0.493564 + L2reg 0.86 + L1reg 0.11 ; SHD = 28 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 15.770998706470117 rho 1000.0 h 0.03129567609669337
iteration 3 in outer loop, alpha = 47.06667480316349, rho = 1000.0, h = 0.03129567609669337
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 18.65 = squared loss an data 15.72 + 0.5*rho*h**2 0.489710 + alpha*h 1.472983 + L2reg 0.86 + L1reg 0.11 ; SHD = 28 ; DAG True
total norm for a microbatch 112.33647647956535 clip 11.451200597801304
cuda
Objective function 17.76 = squared loss an data 15.53 + 0.5*rho*h**2 0.215032 + alpha*h 0.976068 + L2reg 0.92 + L1reg 0.12 ; SHD = 27 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 47.06667480316349 rho 1000.0 h 0.020737983466222687
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 19.70 = squared loss an data 15.53 + 0.5*rho*h**2 2.150320 + alpha*h 0.976068 + L2reg 0.92 + L1reg 0.12 ; SHD = 27 ; DAG True
total norm for a microbatch 104.47254070793525 clip 2.978931273104799
total norm for a microbatch 67.56313935466427 clip 9.632578855249987
cuda
Objective function 17.21 = squared loss an data 15.27 + 0.5*rho*h**2 0.407769 + alpha*h 0.425046 + L2reg 0.99 + L1reg 0.12 ; SHD = 32 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 47.06667480316349 rho 10000.0 h 0.009030711756700782
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 20.88 = squared loss an data 15.27 + 0.5*rho*h**2 4.077688 + alpha*h 0.425046 + L2reg 0.99 + L1reg 0.12 ; SHD = 32 ; DAG True
total norm for a microbatch 113.94269249356874 clip 1.8726514021038707
total norm for a microbatch 71.91285032409176 clip 3.2472923091326544
total norm for a microbatch 115.74973644794399 clip 7.007716660748898
total norm for a microbatch 64.79841063700907 clip 10.014306006103203
total norm for a microbatch 192.88601976966115 clip 14.60004568093299
cuda
Objective function 17.40 = squared loss an data 15.46 + 0.5*rho*h**2 0.634401 + alpha*h 0.167653 + L2reg 1.03 + L1reg 0.11 ; SHD = 30 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 3 in inner loop, alpha 47.06667480316349 rho 100000.0 h 0.003562024213550785
iteration 4 in outer loop, alpha = 403.269096158242, rho = 100000.0, h = 0.003562024213550785
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 18.67 = squared loss an data 15.46 + 0.5*rho*h**2 0.634401 + alpha*h 1.436454 + L2reg 1.03 + L1reg 0.11 ; SHD = 30 ; DAG True
total norm for a microbatch 436.61011128141735 clip 1.3010985412580667
total norm for a microbatch 68.01636456187094 clip 11.677020691592418
cuda
Objective function 17.99 = squared loss an data 15.55 + 0.5*rho*h**2 0.291970 + alpha*h 0.974494 + L2reg 1.06 + L1reg 0.11 ; SHD = 29 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 403.269096158242 rho 100000.0 h 0.0024164851270427334
iteration 5 in outer loop, alpha = 2819.7542232009755, rho = 1000000.0, h = 0.0024164851270427334
Threshold 0.3
[[0.003 0.012 0.097 0.321 0.294 0.035 0.019 0.004 0.03  0.979 0.133 0.02
  0.062 0.013 0.187]
 [0.142 0.004 0.194 0.358 0.197 0.114 0.036 0.04  0.251 0.553 0.179 0.857
  0.087 0.003 0.253]
 [0.064 0.019 0.004 0.432 0.265 0.037 0.029 0.028 0.04  0.108 0.103 0.032
  0.061 0.024 0.121]
 [0.011 0.007 0.007 0.004 0.092 0.015 0.006 0.006 0.013 0.024 0.016 0.009
  0.008 0.009 0.05 ]
 [0.004 0.006 0.014 0.054 0.006 0.012 0.004 0.004 0.003 0.003 0.002 0.012
  0.004 0.003 0.023]
 [0.158 0.03  0.15  0.306 0.273 0.004 0.007 0.01  0.117 0.203 0.099 0.426
  0.067 0.017 0.234]
 [0.196 0.101 0.157 0.801 0.409 0.511 0.005 0.025 0.139 0.325 0.98  0.4
  0.149 0.042 0.646]
 [1.197 0.13  0.129 0.641 0.324 0.351 0.183 0.005 0.205 0.401 0.279 0.2
  0.385 0.024 0.366]
 [0.088 0.015 0.106 0.386 0.326 0.029 0.027 0.023 0.005 1.003 0.776 0.013
  0.022 0.003 0.116]
 [0.005 0.006 0.043 0.12  1.829 0.017 0.014 0.007 0.004 0.004 0.134 0.012
  0.013 0.003 0.124]
 [0.02  0.018 0.042 0.231 1.883 0.05  0.004 0.009 0.005 0.047 0.004 0.018
  0.004 0.002 0.148]
 [0.141 0.004 0.122 0.334 0.243 0.01  0.008 0.017 0.425 0.184 0.213 0.005
  0.035 0.003 0.259]
 [0.064 0.039 0.065 0.464 0.522 0.081 0.03  0.011 0.161 0.401 1.138 0.145
  0.003 0.023 0.273]
 [0.295 1.245 0.14  0.369 0.457 0.2   0.122 0.162 1.842 0.601 0.38  1.345
  0.174 0.003 0.36 ]
 [0.022 0.013 0.036 0.092 0.193 0.02  0.006 0.013 0.022 0.031 0.023 0.024
  0.007 0.006 0.005]]
[[0.    0.    0.    0.321 0.    0.    0.    0.    0.    0.979 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.358 0.    0.    0.    0.    0.    0.553 0.    0.857
  0.    0.    0.   ]
 [0.    0.    0.    0.432 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.306 0.    0.    0.    0.    0.    0.    0.    0.426
  0.    0.    0.   ]
 [0.    0.    0.    0.801 0.409 0.511 0.    0.    0.    0.325 0.98  0.4
  0.    0.    0.646]
 [1.197 0.    0.    0.641 0.324 0.351 0.    0.    0.    0.401 0.    0.
  0.385 0.    0.366]
 [0.    0.    0.    0.386 0.326 0.    0.    0.    0.    1.003 0.776 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.829 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.883 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.334 0.    0.    0.    0.    0.425 0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.464 0.522 0.    0.    0.    0.    0.401 1.138 0.
  0.    0.    0.   ]
 [0.    1.245 0.    0.369 0.457 0.    0.    0.    1.842 0.601 0.38  1.345
  0.    0.    0.36 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.5238095238095238, 'tpr': 0.6666666666666666, 'fpr': 0.29333333333333333, 'f1': 0.5555555555555556, 'shd': 29, 'npred': 42, 'ntrue': 30}
[0.012 0.097 0.321 0.294 0.035 0.019 0.004 0.03  0.979 0.133 0.02  0.062
 0.013 0.187 0.142 0.194 0.358 0.197 0.114 0.036 0.04  0.251 0.553 0.179
 0.857 0.087 0.003 0.253 0.064 0.019 0.432 0.265 0.037 0.029 0.028 0.04
 0.108 0.103 0.032 0.061 0.024 0.121 0.011 0.007 0.007 0.092 0.015 0.006
 0.006 0.013 0.024 0.016 0.009 0.008 0.009 0.05  0.004 0.006 0.014 0.054
 0.012 0.004 0.004 0.003 0.003 0.002 0.012 0.004 0.003 0.023 0.158 0.03
 0.15  0.306 0.273 0.007 0.01  0.117 0.203 0.099 0.426 0.067 0.017 0.234
 0.196 0.101 0.157 0.801 0.409 0.511 0.025 0.139 0.325 0.98  0.4   0.149
 0.042 0.646 1.197 0.13  0.129 0.641 0.324 0.351 0.183 0.205 0.401 0.279
 0.2   0.385 0.024 0.366 0.088 0.015 0.106 0.386 0.326 0.029 0.027 0.023
 1.003 0.776 0.013 0.022 0.003 0.116 0.005 0.006 0.043 0.12  1.829 0.017
 0.014 0.007 0.004 0.134 0.012 0.013 0.003 0.124 0.02  0.018 0.042 0.231
 1.883 0.05  0.004 0.009 0.005 0.047 0.018 0.004 0.002 0.148 0.141 0.004
 0.122 0.334 0.243 0.01  0.008 0.017 0.425 0.184 0.213 0.035 0.003 0.259
 0.064 0.039 0.065 0.464 0.522 0.081 0.03  0.011 0.161 0.401 1.138 0.145
 0.023 0.273 0.295 1.245 0.14  0.369 0.457 0.2   0.122 0.162 1.842 0.601
 0.38  1.345 0.174 0.36  0.022 0.013 0.036 0.092 0.193 0.02  0.006 0.013
 0.022 0.031 0.023 0.024 0.007 0.006]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8322222222222222, 0.6170727380849645)
Iterations 320
Achieves (18.273120775215382, 1e-05)-DP
