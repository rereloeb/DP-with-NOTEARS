samples  5000  graph  30 90 ER mlp  minibatch size  100  noise  1.0  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.115034010050131
iteration 1 in outer loop, alpha = 2.115034010050131, rho = 1.0, h = 2.115034010050131
cuda
iteration 1 in inner loop,alpha 2.115034010050131 rho 1.0 h 1.3374026753201562
iteration 2 in inner loop,alpha 2.115034010050131 rho 10.0 h 0.5764720350935555
iteration 3 in inner loop,alpha 2.115034010050131 rho 100.0 h 0.17913910773663844
iteration 2 in outer loop, alpha = 20.028944783713975, rho = 100.0, h = 0.17913910773663844
cuda
iteration 1 in inner loop,alpha 20.028944783713975 rho 100.0 h 0.09527302396207205
iteration 2 in inner loop,alpha 20.028944783713975 rho 1000.0 h 0.03353337960312075
iteration 3 in outer loop, alpha = 53.562324386834725, rho = 1000.0, h = 0.03353337960312075
cuda
iteration 1 in inner loop,alpha 53.562324386834725 rho 1000.0 h 0.017592909973625837
iteration 2 in inner loop,alpha 53.562324386834725 rho 10000.0 h 0.005608353474073624
iteration 4 in outer loop, alpha = 109.64585912757096, rho = 10000.0, h = 0.005608353474073624
cuda
iteration 1 in inner loop,alpha 109.64585912757096 rho 10000.0 h 0.0030572143360423354
iteration 2 in inner loop,alpha 109.64585912757096 rho 100000.0 h 0.0008668962783389134
iteration 5 in outer loop, alpha = 196.3354869614623, rho = 100000.0, h = 0.0008668962783389134
cuda
iteration 1 in inner loop,alpha 196.3354869614623 rho 100000.0 h 0.00044613044736507845
iteration 6 in outer loop, alpha = 642.4659343265407, rho = 1000000.0, h = 0.00044613044736507845
Threshold 0.3
[[0.006 0.001 0.001 1.939 0.    0.001 0.    0.001 0.001 0.    0.    0.002
  0.    0.    0.    0.186 0.    0.    0.971 0.297 0.    0.    1.679 0.
  0.    0.    0.084 0.    0.109 0.   ]
 [1.038 0.002 0.366 0.476 0.    0.139 0.001 0.418 0.001 0.    0.    1.06
  0.    0.    0.    1.598 0.    0.001 0.988 0.389 0.001 0.001 0.103 0.
  0.    0.001 0.57  0.    0.204 0.001]
 [0.364 0.001 0.003 0.279 0.001 0.111 0.    0.27  0.    0.    0.    0.29
  0.    0.    0.    0.227 0.    0.001 0.146 0.227 0.    0.    0.129 0.
  0.    0.    0.285 0.    0.203 0.   ]
 [0.    0.    0.    0.007 0.    0.    0.002 0.    0.    0.    0.    0.
  0.    0.    0.    1.717 0.    0.    0.862 1.212 0.    0.    0.236 0.
  0.    0.    0.055 0.    0.141 0.   ]
 [0.199 0.165 0.232 0.131 0.002 0.129 0.009 0.144 0.001 0.    0.    0.255
  0.    0.    0.    0.253 0.    0.    0.087 0.269 0.001 0.    0.079 0.
  0.    0.    0.101 0.    0.194 0.   ]
 [0.417 0.001 0.001 0.226 0.001 0.002 0.001 0.001 0.    0.    0.    0.001
  0.    0.    0.    0.146 0.    0.    0.88  1.238 0.    0.    0.093 0.
  0.    0.    0.037 0.    1.198 0.   ]
 [0.078 0.21  0.152 0.134 0.055 0.135 0.001 0.083 0.59  0.006 0.031 0.096
  0.    0.005 0.    0.147 0.    0.001 0.655 0.119 0.031 0.063 0.076 0.
  0.    0.002 0.111 0.    0.05  0.001]
 [1.013 0.    0.003 0.526 0.001 2.022 0.003 0.002 0.    0.    0.    0.003
  0.    0.    0.    0.211 0.    0.    0.145 1.245 0.    0.001 0.089 0.
  0.    0.    0.032 0.    0.203 0.   ]
 [0.153 0.245 0.111 0.188 0.136 0.084 0.001 0.175 0.001 0.005 0.093 0.116
  0.    0.017 0.    0.254 0.    0.001 0.113 1.14  0.106 0.167 0.118 0.011
  0.    0.004 0.482 0.    0.109 0.001]
 [0.158 0.124 0.164 1.463 0.054 0.314 0.023 2.613 0.016 0.003 0.428 0.575
  0.001 3.392 0.018 0.322 0.002 0.467 0.119 0.216 0.272 0.102 0.37  0.111
  0.    0.455 0.236 0.    0.274 0.286]
 [1.031 0.109 0.119 1.815 0.147 0.176 0.033 0.208 0.004 0.001 0.001 0.246
  0.006 0.086 0.001 1.139 0.001 0.079 0.256 0.143 2.817 1.134 1.46  0.076
  0.001 0.282 0.141 0.001 0.096 0.079]
 [0.211 0.    0.002 0.26  0.001 0.107 0.002 0.255 0.001 0.    0.    0.003
  0.    0.    0.    0.197 0.    0.    0.145 0.326 0.    0.    1.441 0.001
  0.    0.    0.011 0.    0.14  0.   ]
 [1.075 0.864 0.069 0.207 0.007 1.792 0.636 0.125 0.174 0.028 0.058 0.248
  0.001 0.001 0.001 0.107 0.009 0.164 0.595 0.077 0.029 0.025 0.174 0.046
  0.003 3.168 0.138 0.004 1.053 0.464]
 [0.97  2.184 1.878 0.677 0.028 0.268 0.011 1.893 0.002 0.    0.005 1.895
  0.004 0.001 0.001 0.296 0.001 0.196 0.337 0.871 0.125 0.011 0.201 0.004
  0.    0.026 0.113 0.001 0.265 0.072]
 [0.149 0.074 1.626 0.15  0.09  0.152 0.414 0.098 0.001 0.001 0.004 0.076
  0.008 0.037 0.    1.41  0.016 0.064 0.075 0.827 0.026 0.017 0.137 0.011
  0.005 0.081 0.015 0.028 0.049 0.06 ]
 [0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.003 0.    0.    0.05  0.295 0.    0.    0.004 0.
  0.    0.    0.001 0.    0.069 0.   ]
 [0.158 0.043 1.426 0.207 0.019 0.086 0.693 0.147 0.043 0.023 0.004 0.156
  0.005 0.002 0.001 0.105 0.001 0.04  0.089 0.838 0.054 0.022 0.078 0.028
  0.012 0.015 0.465 0.005 0.174 1.98 ]
 [0.143 0.133 0.161 1.494 1.365 0.101 0.189 0.294 0.154 0.    0.001 0.319
  0.    0.    0.    1.171 0.    0.001 0.154 0.25  0.022 0.003 0.178 0.
  0.    0.    0.115 0.    0.995 0.   ]
 [0.002 0.    0.    0.006 0.001 0.    0.001 0.    0.    0.    0.    0.001
  0.    0.    0.    0.035 0.    0.    0.008 0.006 0.    0.    0.011 0.
  0.    0.    0.021 0.    0.005 0.   ]
 [0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.047 0.003 0.    0.    0.003 0.
  0.    0.    0.002 0.    1.202 0.   ]
 [0.929 1.262 1.835 0.34  2.128 0.086 0.005 0.117 0.001 0.    0.    1.3
  0.001 0.001 0.001 0.636 0.002 0.011 0.142 1.051 0.002 0.001 0.203 0.001
  0.    0.005 0.245 0.001 0.13  0.004]
 [0.785 0.243 0.195 0.168 0.129 0.036 0.009 0.149 0.001 0.001 0.    0.258
  0.004 0.03  0.002 0.321 0.001 0.048 0.129 0.147 0.46  0.001 0.079 0.036
  0.    0.11  0.091 0.    0.101 0.023]
 [0.    0.001 0.    0.002 0.001 0.    0.001 0.    0.    0.    0.    0.001
  0.    0.    0.    0.154 0.    0.    0.108 0.125 0.    0.    0.004 0.
  0.    0.    0.003 0.    0.238 0.   ]
 [1.203 0.081 0.083 0.203 0.108 0.038 0.071 0.283 0.006 0.002 0.001 0.115
  0.    0.039 0.001 0.09  0.005 0.194 0.103 0.112 0.082 0.002 0.107 0.001
  0.    0.001 0.013 0.    0.175 0.093]
 [0.051 0.251 0.022 0.271 0.106 0.156 0.078 0.443 0.004 4.11  1.061 0.226
  0.013 0.329 0.001 0.039 0.013 0.176 0.082 0.09  0.114 0.019 0.196 0.001
  0.001 1.678 0.034 0.001 0.739 0.157]
 [0.483 0.39  0.202 0.274 0.093 1.991 0.087 0.135 0.05  0.001 0.    0.066
  0.    0.013 0.    0.154 0.001 0.391 0.215 0.2   0.104 0.003 0.066 0.251
  0.    0.001 0.642 0.014 1.232 1.916]
 [0.005 0.003 0.    0.019 0.004 0.014 0.002 0.002 0.001 0.    0.    0.075
  0.    0.    0.001 0.264 0.    0.002 0.257 0.232 0.001 0.    1.613 0.001
  0.    0.    0.005 0.    1.177 0.001]
 [0.223 0.025 0.015 0.117 2.204 0.198 0.034 1.753 0.023 0.001 0.012 2.04
  0.002 0.037 0.001 0.141 0.022 1.036 0.126 0.332 0.008 0.038 0.089 1.496
  0.006 0.001 0.045 0.001 0.121 0.076]
 [0.    0.001 0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    0.001 0.    0.    0.105 0.    0.    0.    0.001 0.
  0.    0.    0.002 0.    0.003 0.   ]
 [0.217 0.078 1.818 0.21  0.111 0.092 0.059 0.09  0.055 0.    0.    0.119
  0.    0.001 0.    0.177 0.    1.122 0.125 0.196 0.029 0.002 0.13  0.003
  0.    0.    0.716 0.    0.379 0.001]]
[[0.    0.    0.    1.939 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.971 0.    0.    0.    1.679 0.
  0.    0.    0.    0.    0.    0.   ]
 [1.038 0.    0.366 0.476 0.    0.    0.    0.418 0.    0.    0.    1.06
  0.    0.    0.    1.598 0.    0.    0.988 0.389 0.    0.    0.    0.
  0.    0.    0.57  0.    0.    0.   ]
 [0.364 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.717 0.    0.    0.862 1.212 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.417 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.88  1.238 0.    0.    0.    0.
  0.    0.    0.    0.    1.198 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.59  0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.655 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [1.013 0.    0.    0.526 0.    2.022 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    1.245 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    1.14  0.    0.    0.    0.
  0.    0.    0.482 0.    0.    0.   ]
 [0.    0.    0.    1.463 0.    0.314 0.    2.613 0.    0.    0.428 0.575
  0.    3.392 0.    0.322 0.    0.467 0.    0.    0.    0.    0.37  0.
  0.    0.455 0.    0.    0.    0.   ]
 [1.031 0.    0.    1.815 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.139 0.    0.    0.    0.    2.817 1.134 1.46  0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.326 0.    0.    1.441 0.
  0.    0.    0.    0.    0.    0.   ]
 [1.075 0.864 0.    0.    0.    1.792 0.636 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.595 0.    0.    0.    0.    0.
  0.    3.168 0.    0.    1.053 0.464]
 [0.97  2.184 1.878 0.677 0.    0.    0.    1.893 0.    0.    0.    1.895
  0.    0.    0.    0.    0.    0.    0.337 0.871 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.626 0.    0.    0.    0.414 0.    0.    0.    0.    0.
  0.    0.    0.    1.41  0.    0.    0.    0.827 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.426 0.    0.    0.    0.693 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.838 0.    0.    0.    0.
  0.    0.    0.465 0.    0.    1.98 ]
 [0.    0.    0.    1.494 1.365 0.    0.    0.    0.    0.    0.    0.319
  0.    0.    0.    1.171 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.995 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.202 0.   ]
 [0.929 1.262 1.835 0.34  2.128 0.    0.    0.    0.    0.    0.    1.3
  0.    0.    0.    0.636 0.    0.    0.    1.051 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.785 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.321 0.    0.    0.    0.    0.46  0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [1.203 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.443 0.    4.11  1.061 0.
  0.    0.329 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    1.678 0.    0.    0.739 0.   ]
 [0.483 0.39  0.    0.    0.    1.991 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.391 0.    0.    0.    0.    0.    0.
  0.    0.    0.642 0.    1.232 1.916]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.613 0.
  0.    0.    0.    0.    1.177 0.   ]
 [0.    0.    0.    0.    2.204 0.    0.    1.753 0.    0.    0.    2.04
  0.    0.    0.    0.    0.    1.036 0.    0.332 0.    0.    0.    1.496
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.818 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    1.122 0.    0.    0.    0.    0.    0.
  0.    0.    0.716 0.    0.379 0.   ]]
{'fdr': 0.2807017543859649, 'tpr': 0.9111111111111111, 'fpr': 0.0927536231884058, 'f1': 0.803921568627451, 'shd': 39, 'npred': 114, 'ntrue': 90}
[8.440e-04 7.219e-04 1.939e+00 4.627e-04 6.072e-04 4.282e-04 5.407e-04
 5.941e-04 2.652e-05 3.725e-05 1.775e-03 3.306e-05 2.513e-05 7.095e-05
 1.858e-01 8.165e-05 2.742e-04 9.710e-01 2.966e-01 2.798e-04 1.258e-04
 1.679e+00 3.558e-05 1.861e-05 3.209e-05 8.412e-02 1.985e-05 1.085e-01
 8.496e-05 1.038e+00 3.657e-01 4.765e-01 3.151e-04 1.385e-01 1.352e-03
 4.175e-01 9.958e-04 8.211e-06 3.574e-05 1.060e+00 3.192e-04 1.886e-05
 3.311e-04 1.598e+00 6.678e-05 1.060e-03 9.881e-01 3.893e-01 7.631e-04
 6.463e-04 1.029e-01 2.982e-04 1.031e-06 7.397e-04 5.701e-01 4.000e-05
 2.036e-01 5.088e-04 3.640e-01 1.481e-03 2.793e-01 6.570e-04 1.109e-01
 2.830e-04 2.696e-01 3.262e-04 7.838e-05 1.385e-05 2.898e-01 4.557e-05
 2.104e-04 1.627e-05 2.274e-01 9.188e-05 1.155e-03 1.459e-01 2.275e-01
 2.586e-04 1.064e-04 1.291e-01 3.565e-04 5.149e-05 5.560e-05 2.853e-01
 1.030e-05 2.029e-01 1.170e-04 1.424e-04 5.799e-05 1.497e-04 3.406e-04
 1.300e-04 1.805e-03 8.684e-05 3.730e-04 4.676e-05 1.244e-04 4.663e-04
 1.192e-05 3.216e-05 2.650e-05 1.717e+00 2.107e-05 1.173e-04 8.618e-01
 1.212e+00 9.523e-05 2.089e-05 2.363e-01 5.635e-05 1.934e-06 3.511e-05
 5.525e-02 6.933e-05 1.409e-01 3.600e-05 1.986e-01 1.647e-01 2.322e-01
 1.309e-01 1.287e-01 9.175e-03 1.439e-01 8.159e-04 9.942e-05 1.051e-05
 2.554e-01 5.474e-06 3.503e-04 3.428e-04 2.525e-01 1.770e-05 2.710e-04
 8.658e-02 2.694e-01 5.059e-04 2.129e-04 7.943e-02 2.742e-04 2.504e-05
 1.176e-04 1.009e-01 4.059e-05 1.939e-01 1.394e-04 4.169e-01 5.305e-04
 9.138e-04 2.261e-01 9.880e-04 1.261e-03 5.152e-04 1.766e-04 7.901e-06
 2.858e-05 1.062e-03 1.659e-05 6.194e-05 7.817e-05 1.462e-01 8.216e-05
 2.035e-04 8.803e-01 1.238e+00 3.819e-05 4.024e-04 9.287e-02 9.369e-05
 1.081e-05 8.386e-05 3.709e-02 8.138e-05 1.198e+00 1.689e-04 7.818e-02
 2.105e-01 1.520e-01 1.340e-01 5.455e-02 1.350e-01 8.268e-02 5.904e-01
 5.751e-03 3.073e-02 9.578e-02 4.078e-04 4.575e-03 2.456e-04 1.468e-01
 4.523e-04 5.355e-04 6.546e-01 1.188e-01 3.127e-02 6.250e-02 7.556e-02
 4.631e-04 1.741e-04 1.730e-03 1.109e-01 1.814e-04 5.023e-02 6.573e-04
 1.013e+00 4.245e-04 3.170e-03 5.263e-01 9.551e-04 2.022e+00 2.530e-03
 3.991e-04 7.637e-06 2.681e-05 3.202e-03 1.714e-04 1.103e-04 8.537e-05
 2.115e-01 7.759e-05 1.986e-04 1.454e-01 1.245e+00 1.010e-04 6.071e-04
 8.892e-02 2.892e-04 7.238e-06 2.039e-04 3.206e-02 8.374e-05 2.027e-01
 4.523e-04 1.530e-01 2.452e-01 1.115e-01 1.877e-01 1.358e-01 8.448e-02
 8.603e-04 1.746e-01 4.589e-03 9.340e-02 1.162e-01 3.348e-04 1.702e-02
 1.397e-04 2.539e-01 7.858e-05 1.090e-03 1.129e-01 1.140e+00 1.058e-01
 1.668e-01 1.182e-01 1.111e-02 1.140e-04 4.128e-03 4.819e-01 4.234e-04
 1.085e-01 6.206e-04 1.581e-01 1.236e-01 1.640e-01 1.463e+00 5.355e-02
 3.137e-01 2.284e-02 2.613e+00 1.619e-02 4.276e-01 5.746e-01 7.964e-04
 3.392e+00 1.839e-02 3.219e-01 1.515e-03 4.669e-01 1.185e-01 2.160e-01
 2.716e-01 1.016e-01 3.705e-01 1.105e-01 4.709e-04 4.554e-01 2.362e-01
 4.338e-04 2.741e-01 2.858e-01 1.031e+00 1.086e-01 1.193e-01 1.815e+00
 1.471e-01 1.764e-01 3.252e-02 2.085e-01 4.393e-03 9.773e-04 2.460e-01
 6.127e-03 8.583e-02 9.144e-04 1.139e+00 1.106e-03 7.925e-02 2.564e-01
 1.426e-01 2.817e+00 1.134e+00 1.460e+00 7.646e-02 1.177e-03 2.824e-01
 1.407e-01 1.075e-03 9.629e-02 7.923e-02 2.109e-01 9.916e-05 2.089e-03
 2.601e-01 5.691e-04 1.068e-01 1.805e-03 2.552e-01 5.575e-04 2.751e-05
 2.158e-05 2.237e-05 9.668e-05 7.973e-05 1.973e-01 6.914e-05 2.339e-04
 1.446e-01 3.257e-01 8.902e-05 5.136e-05 1.441e+00 1.353e-03 2.065e-05
 2.186e-04 1.083e-02 1.710e-04 1.397e-01 1.197e-04 1.075e+00 8.645e-01
 6.871e-02 2.069e-01 6.769e-03 1.792e+00 6.361e-01 1.247e-01 1.744e-01
 2.787e-02 5.812e-02 2.485e-01 7.722e-04 7.581e-04 1.072e-01 8.682e-03
 1.644e-01 5.946e-01 7.657e-02 2.935e-02 2.512e-02 1.739e-01 4.623e-02
 2.583e-03 3.168e+00 1.381e-01 4.464e-03 1.053e+00 4.637e-01 9.704e-01
 2.184e+00 1.878e+00 6.772e-01 2.815e-02 2.685e-01 1.125e-02 1.893e+00
 1.654e-03 2.260e-04 5.042e-03 1.895e+00 4.361e-03 8.689e-04 2.960e-01
 5.357e-04 1.960e-01 3.371e-01 8.708e-01 1.253e-01 1.130e-02 2.014e-01
 3.696e-03 7.182e-05 2.622e-02 1.135e-01 8.683e-04 2.651e-01 7.236e-02
 1.487e-01 7.401e-02 1.626e+00 1.496e-01 8.965e-02 1.519e-01 4.145e-01
 9.805e-02 5.763e-04 6.067e-04 3.540e-03 7.594e-02 8.038e-03 3.675e-02
 1.410e+00 1.600e-02 6.398e-02 7.459e-02 8.267e-01 2.585e-02 1.660e-02
 1.371e-01 1.133e-02 4.685e-03 8.134e-02 1.480e-02 2.812e-02 4.869e-02
 5.989e-02 4.244e-05 1.062e-04 1.127e-04 7.886e-04 1.426e-04 1.168e-04
 2.673e-04 5.192e-05 1.927e-05 2.073e-05 5.446e-05 3.131e-04 1.254e-05
 1.661e-05 2.441e-05 6.740e-06 3.999e-05 4.973e-02 2.950e-01 7.316e-05
 5.252e-05 3.726e-03 1.689e-05 1.652e-05 2.108e-05 1.414e-03 5.297e-06
 6.903e-02 3.826e-05 1.584e-01 4.319e-02 1.426e+00 2.068e-01 1.857e-02
 8.580e-02 6.928e-01 1.472e-01 4.287e-02 2.261e-02 3.814e-03 1.561e-01
 5.411e-03 2.423e-03 7.759e-04 1.045e-01 3.995e-02 8.883e-02 8.381e-01
 5.405e-02 2.174e-02 7.818e-02 2.751e-02 1.200e-02 1.541e-02 4.653e-01
 5.427e-03 1.736e-01 1.980e+00 1.429e-01 1.330e-01 1.611e-01 1.494e+00
 1.365e+00 1.014e-01 1.889e-01 2.945e-01 1.540e-01 2.094e-04 1.332e-03
 3.188e-01 4.740e-05 2.391e-04 3.886e-04 1.171e+00 1.888e-05 1.536e-01
 2.502e-01 2.212e-02 3.070e-03 1.779e-01 1.087e-04 3.497e-05 8.802e-05
 1.148e-01 2.076e-05 9.952e-01 1.765e-04 1.926e-03 1.747e-04 1.866e-04
 6.451e-03 1.022e-03 1.533e-04 7.694e-04 2.091e-04 3.302e-04 1.875e-05
 1.510e-04 1.114e-03 4.888e-05 2.036e-05 9.998e-05 3.521e-02 9.067e-05
 1.150e-04 6.252e-03 1.142e-04 1.329e-04 1.121e-02 9.334e-05 1.204e-05
 2.532e-05 2.129e-02 2.216e-05 4.534e-03 1.063e-04 4.910e-05 5.796e-05
 1.385e-04 8.023e-04 8.626e-05 1.312e-04 5.310e-05 2.162e-05 2.082e-05
 5.308e-06 2.318e-05 2.803e-04 2.039e-05 1.286e-05 3.928e-05 3.494e-04
 4.682e-05 6.709e-05 4.710e-02 6.213e-05 2.182e-05 3.060e-03 3.353e-05
 1.244e-05 9.999e-06 1.711e-03 3.396e-06 1.202e+00 6.575e-06 9.287e-01
 1.262e+00 1.835e+00 3.396e-01 2.128e+00 8.609e-02 5.186e-03 1.166e-01
 8.942e-04 1.061e-04 1.387e-05 1.300e+00 6.467e-04 1.238e-03 1.034e-03
 6.357e-01 1.754e-03 1.136e-02 1.422e-01 1.051e+00 5.856e-04 2.027e-01
 8.122e-04 2.640e-05 5.100e-03 2.454e-01 7.562e-04 1.295e-01 4.114e-03
 7.854e-01 2.431e-01 1.947e-01 1.685e-01 1.291e-01 3.581e-02 9.467e-03
 1.489e-01 1.242e-03 5.282e-04 8.151e-05 2.579e-01 3.548e-03 2.964e-02
 2.038e-03 3.205e-01 8.220e-04 4.802e-02 1.287e-01 1.471e-01 4.596e-01
 7.909e-02 3.626e-02 3.161e-04 1.096e-01 9.069e-02 2.044e-04 1.007e-01
 2.344e-02 5.666e-05 5.631e-04 3.268e-04 1.782e-03 7.571e-04 2.903e-04
 8.054e-04 3.701e-05 4.756e-04 1.724e-05 9.297e-05 9.575e-04 5.753e-05
 1.226e-05 6.870e-05 1.542e-01 2.322e-04 2.247e-04 1.080e-01 1.251e-01
 3.053e-04 7.710e-06 5.597e-05 3.008e-05 1.233e-04 2.929e-03 1.241e-04
 2.381e-01 3.262e-04 1.203e+00 8.080e-02 8.321e-02 2.032e-01 1.085e-01
 3.831e-02 7.099e-02 2.832e-01 6.003e-03 1.757e-03 1.123e-03 1.146e-01
 6.422e-05 3.925e-02 1.035e-03 9.047e-02 5.412e-03 1.941e-01 1.026e-01
 1.119e-01 8.247e-02 1.705e-03 1.066e-01 9.293e-05 5.063e-04 1.342e-02
 1.468e-04 1.751e-01 9.320e-02 5.064e-02 2.508e-01 2.194e-02 2.708e-01
 1.058e-01 1.565e-01 7.817e-02 4.425e-01 3.514e-03 4.110e+00 1.061e+00
 2.257e-01 1.252e-02 3.293e-01 6.810e-04 3.850e-02 1.292e-02 1.757e-01
 8.156e-02 8.974e-02 1.137e-01 1.908e-02 1.960e-01 7.679e-04 1.678e+00
 3.357e-02 6.533e-04 7.388e-01 1.568e-01 4.833e-01 3.896e-01 2.022e-01
 2.743e-01 9.350e-02 1.991e+00 8.722e-02 1.354e-01 4.974e-02 1.099e-03
 1.219e-04 6.562e-02 1.961e-05 1.285e-02 3.503e-04 1.545e-01 6.333e-04
 3.907e-01 2.152e-01 1.996e-01 1.037e-01 3.108e-03 6.625e-02 2.509e-01
 4.933e-04 6.419e-01 1.398e-02 1.232e+00 1.916e+00 4.745e-03 3.244e-03
 4.824e-04 1.859e-02 3.892e-03 1.393e-02 1.904e-03 2.441e-03 6.527e-04
 4.155e-06 2.235e-05 7.479e-02 7.288e-05 4.818e-05 5.703e-04 2.637e-01
 1.713e-04 2.467e-03 2.575e-01 2.317e-01 8.290e-04 2.998e-04 1.613e+00
 1.378e-03 1.675e-05 1.991e-04 3.919e-04 1.177e+00 7.240e-04 2.234e-01
 2.468e-02 1.502e-02 1.169e-01 2.204e+00 1.978e-01 3.419e-02 1.753e+00
 2.255e-02 1.055e-03 1.246e-02 2.040e+00 2.258e-03 3.741e-02 1.428e-03
 1.408e-01 2.201e-02 1.036e+00 1.263e-01 3.324e-01 7.647e-03 3.835e-02
 8.859e-02 1.496e+00 5.654e-03 5.441e-04 4.512e-02 1.208e-01 7.645e-02
 1.147e-04 5.664e-04 2.471e-04 2.152e-04 3.416e-04 6.539e-05 5.821e-04
 1.910e-05 8.189e-05 6.293e-06 1.107e-05 2.651e-04 1.099e-05 1.854e-05
 1.898e-05 5.435e-04 6.834e-05 4.379e-05 1.050e-01 1.667e-04 4.621e-05
 8.079e-05 1.500e-03 9.276e-05 2.103e-05 9.231e-05 1.935e-03 1.077e-05
 3.077e-05 2.174e-01 7.793e-02 1.818e+00 2.101e-01 1.112e-01 9.215e-02
 5.927e-02 9.011e-02 5.474e-02 1.117e-04 4.135e-04 1.190e-01 4.552e-05
 7.961e-04 3.317e-04 1.766e-01 9.057e-05 1.122e+00 1.246e-01 1.964e-01
 2.944e-02 1.579e-03 1.303e-01 2.611e-03 5.363e-05 1.175e-04 7.164e-01
 1.361e-04 3.795e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 1. 0. 0. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.9796723646723646, 0.9385900027007384)
cuda
9630
cuda
Objective function 982.29 = squared loss an data 726.28 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 415 ; DAG False
||w||^2 0.2179142395583623
exp ma of ||w||^2 6.347017557386879
||w|| 0.4668128528204449
exp ma of ||w|| 0.413586600843409
||w||^2 0.11366944266292897
exp ma of ||w||^2 0.10992278667184736
||w|| 0.3371489917869086
exp ma of ||w|| 0.32844054675795464
||w||^2 0.09986970813809738
exp ma of ||w||^2 0.1062942823175965
||w|| 0.31602168934757846
exp ma of ||w|| 0.3229698036195576
||w||^2 0.09783493657942788
exp ma of ||w||^2 0.1004873055134709
||w|| 0.31278576786584755
exp ma of ||w|| 0.3129062900895176
||w||^2 0.11015918109155794
exp ma of ||w||^2 0.10082814566434588
||w|| 0.3319023668062009
exp ma of ||w|| 0.3140638778314192
||w||^2 0.08193146195238327
exp ma of ||w||^2 0.08988953889466511
||w|| 0.2862367236264125
exp ma of ||w|| 0.294648324458811
||w||^2 0.03417993092197281
exp ma of ||w||^2 0.09077544483722189
||w|| 0.1848781515538621
exp ma of ||w|| 0.2944120007960911
cuda
Objective function 230.86 = squared loss an data 227.17 + 0.5*rho*h**2 2.621895 + alpha*h 0.000000 + L2reg 0.56 + L1reg 0.51 ; SHD = 152 ; DAG False
Proportion of microbatches that were clipped  0.7172118132740553
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.2899321879511234
iteration 1 in outer loop, alpha = 2.2899321879511234, rho = 1.0, h = 2.2899321879511234
cuda
9630
cuda
Objective function 236.11 = squared loss an data 227.17 + 0.5*rho*h**2 2.621895 + alpha*h 5.243789 + L2reg 0.56 + L1reg 0.51 ; SHD = 152 ; DAG False
||w||^2 1116.523852459951
exp ma of ||w||^2 499197.0356086488
||w|| 33.414425813710324
exp ma of ||w|| 191.9769857589883
||w||^2 493.5047259751619
exp ma of ||w||^2 287969.5330112695
||w|| 22.214966260950337
exp ma of ||w|| 126.87695714320628
||w||^2 37.27891975029413
exp ma of ||w||^2 11946.73068884956
||w|| 6.1056465464596075
exp ma of ||w|| 15.362929368644568
||w||^2 0.18171773404388683
exp ma of ||w||^2 0.14163029489164977
||w|| 0.426283630982808
exp ma of ||w|| 0.36350570998292
||w||^2 0.0945549349833969
exp ma of ||w||^2 0.14847801676420477
||w|| 0.3074978617541867
exp ma of ||w|| 0.3679280067651543
cuda
Objective function 103.83 = squared loss an data 96.07 + 0.5*rho*h**2 1.882241 + alpha*h 4.442989 + L2reg 0.94 + L1reg 0.50 ; SHD = 135 ; DAG False
Proportion of microbatches that were clipped  0.7257828979393153
iteration 1 in inner loop, alpha 2.2899321879511234 rho 1.0 h 1.9402271659070394
9630
cuda
Objective function 120.77 = squared loss an data 96.07 + 0.5*rho*h**2 18.822407 + alpha*h 4.442989 + L2reg 0.94 + L1reg 0.50 ; SHD = 135 ; DAG False
||w||^2 0.5522672127670519
exp ma of ||w||^2 0.36598533113695525
||w|| 0.743146831229907
exp ma of ||w|| 0.5830296093215802
||w||^2 0.25170135995746384
exp ma of ||w||^2 0.3741930632969214
||w|| 0.5016984751396638
exp ma of ||w|| 0.5928296217584533
||w||^2 0.17518954244777224
exp ma of ||w||^2 0.3307039941546878
||w|| 0.41855649851336946
exp ma of ||w|| 0.5531962098743893
cuda
Objective function 75.59 = squared loss an data 67.71 + 0.5*rho*h**2 4.204917 + alpha*h 2.099986 + L2reg 1.15 + L1reg 0.43 ; SHD = 114 ; DAG False
Proportion of microbatches that were clipped  0.7447405329593267
iteration 2 in inner loop, alpha 2.2899321879511234 rho 10.0 h 0.9170514599669488
9630
cuda
Objective function 113.44 = squared loss an data 67.71 + 0.5*rho*h**2 42.049169 + alpha*h 2.099986 + L2reg 1.15 + L1reg 0.43 ; SHD = 114 ; DAG False
||w||^2 119183117220.00694
exp ma of ||w||^2 67232520824.96606
||w|| 345229.0793371945
exp ma of ||w|| 150240.99865931444
||w||^2 189409085550.45703
exp ma of ||w||^2 85784100198.5374
||w|| 435211.5411503434
exp ma of ||w|| 189275.367515641
||w||^2 1302531408.225828
exp ma of ||w||^2 10341993757.519753
||w|| 36090.59999814118
exp ma of ||w|| 83130.06867952643
||w||^2 123395.06061886555
exp ma of ||w||^2 6967423.074605984
||w|| 351.27633085487776
exp ma of ||w|| 1034.006984386928
||w||^2 420.12020075961505
exp ma of ||w||^2 169653.53531867597
||w|| 20.496833920379387
exp ma of ||w|| 80.18514934070234
||w||^2 1.625753981781942
exp ma of ||w||^2 59.11734446526702
||w|| 1.275050580087685
exp ma of ||w|| 1.587323218709038
||w||^2 0.8624074679542505
exp ma of ||w||^2 10.165033502543203
||w|| 0.9286589621353204
exp ma of ||w|| 1.117432243091962
||w||^2 0.5346434515052608
exp ma of ||w||^2 2.791506314297002
||w|| 0.7311931697610836
exp ma of ||w|| 0.9514460811008214
||w||^2 1.3619713791655235
exp ma of ||w||^2 0.7995144670354901
||w|| 1.1670352947385625
exp ma of ||w|| 0.8621342265448352
cuda
Objective function 70.89 = squared loss an data 61.29 + 0.5*rho*h**2 7.114903 + alpha*h 0.863818 + L2reg 1.26 + L1reg 0.36 ; SHD = 102 ; DAG False
Proportion of microbatches that were clipped  0.7577941406861977
iteration 3 in inner loop, alpha 2.2899321879511234 rho 100.0 h 0.3772241585176488
iteration 2 in outer loop, alpha = 40.012348039716, rho = 100.0, h = 0.3772241585176488
cuda
9630
cuda
Objective function 85.12 = squared loss an data 61.29 + 0.5*rho*h**2 7.114903 + alpha*h 15.093624 + L2reg 1.26 + L1reg 0.36 ; SHD = 102 ; DAG False
||w||^2 933355623.3682946
exp ma of ||w||^2 7361969900.163707
||w|| 30550.869437191057
exp ma of ||w|| 67017.65223258012
||w||^2 4730.972782613952
exp ma of ||w||^2 2088842.1979034736
||w|| 68.78206730401429
exp ma of ||w|| 369.3992128352789
||w||^2 2.087359855713363
exp ma of ||w||^2 146.96258169407506
||w|| 1.4447698279357037
exp ma of ||w|| 2.3965085282022383
||w||^2 1.975058049436906
exp ma of ||w||^2 110.28787482450369
||w|| 1.4053675851665663
exp ma of ||w|| 2.1345514133683445
||w||^2 1.6449100279644568
exp ma of ||w||^2 39.270422214974225
||w|| 1.282540458607235
exp ma of ||w|| 1.6235058384316576
||w||^2 1.2092189400189488
exp ma of ||w||^2 4.78186259053652
||w|| 1.099644915424497
exp ma of ||w|| 1.1755398360395868
||w||^2 3.7386448148339606
exp ma of ||w||^2 1.3435365209931944
||w|| 1.9335575540526226
exp ma of ||w|| 1.101406896901752
||w||^2 2.0411445380532722
exp ma of ||w||^2 1.126867396220004
||w|| 1.4286862979861157
exp ma of ||w|| 1.0302874661623334
||w||^2 1.4170445522630275
exp ma of ||w||^2 1.0726302805643495
||w|| 1.1903968045416735
exp ma of ||w|| 1.0025744465198019
cuda
Objective function 77.05 = squared loss an data 60.58 + 0.5*rho*h**2 3.769285 + alpha*h 10.985972 + L2reg 1.37 + L1reg 0.34 ; SHD = 105 ; DAG True
Proportion of microbatches that were clipped  0.7584187408491947
iteration 1 in inner loop, alpha 40.012348039716 rho 100.0 h 0.2745645539189674
9630
cuda
Objective function 110.97 = squared loss an data 60.58 + 0.5*rho*h**2 37.692847 + alpha*h 10.985972 + L2reg 1.37 + L1reg 0.34 ; SHD = 105 ; DAG True
||w||^2 30900.747072700062
exp ma of ||w||^2 7937533.440492469
||w|| 175.78608327367687
exp ma of ||w|| 884.1152092351572
||w||^2 110.43754115031228
exp ma of ||w||^2 13698.07649590902
||w|| 10.508926736366197
exp ma of ||w|| 15.34834198457717
||w||^2 3.431843397749545
exp ma of ||w||^2 3.5007208694058813
||w|| 1.8525235215104678
exp ma of ||w|| 1.4909044591308818
v before min max tensor([[ -136.239,  -410.555,  -213.198,  ...,   -75.139,  -290.279,
          -381.256],
        [ 2956.832,  -280.465,  -451.654,  ...,   608.728,  -176.110,
          5021.257],
        [ 4268.606,  -426.545,   -31.356,  ...,  -326.573,   361.744,
          1191.757],
        ...,
        [  123.913,  -328.316,  -378.415,  ...,   490.330,   221.550,
         17981.761],
        [ -509.386,  -357.600,   999.382,  ...,   186.075,  -454.613,
           982.659],
        [ -544.788,  -440.266,  -284.633,  ...,  1224.468,   331.594,
           727.547]], device='cuda:0')
v tensor([[1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        ...,
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01]], device='cuda:0')
v before min max tensor([-2.135e+02, -1.392e+02,  1.764e+03, -1.382e+02, -4.009e+02, -2.887e+02,
         4.673e+02, -5.530e+01,  8.087e+02, -2.696e+02,  2.480e+03, -4.979e+02,
        -4.422e+02, -2.589e+00,  3.676e+01,  2.713e+03, -3.541e+02, -4.413e+02,
         1.347e+02, -4.555e+02, -1.573e+02, -2.538e+02, -3.645e+02, -7.710e+01,
        -4.391e+02, -2.514e+02,  1.547e+02,  5.751e+01, -1.960e+02, -2.275e+02,
         6.647e+01,  6.057e+01, -4.318e+02, -1.354e+02, -4.513e+02,  9.230e+02,
         1.737e+03,  3.657e+02, -3.027e+02,  4.511e+02,  4.274e+01, -4.325e+02,
         6.087e+01, -1.175e+01,  1.989e+00, -3.482e+02,  8.044e+01, -4.316e+02,
         6.302e+03, -1.716e+02,  1.488e+03, -4.739e+02, -3.701e+02, -2.870e+02,
         1.210e+03, -9.442e+01,  5.343e+02, -1.615e+02, -3.365e+02, -3.889e+02,
        -3.093e+02, -2.660e+02,  3.166e+03,  5.639e+02, -2.493e+02, -3.329e+01,
         2.353e+02,  6.522e+01,  6.720e+02,  6.699e+02, -2.918e+02, -3.526e+02,
        -1.475e+02,  4.161e+02, -2.074e+02,  6.021e+02,  4.899e+01, -2.853e+02,
        -4.224e+02, -2.727e+02,  6.982e+01, -3.531e+02, -1.898e+02, -3.365e+02,
        -8.146e+01, -3.650e+02, -3.441e+02, -3.748e+02,  4.222e+02, -1.238e+02,
        -4.913e+02,  1.868e+03,  2.376e+02, -3.725e+02,  1.448e+02, -5.194e+02,
        -5.175e+02, -9.080e+01,  5.569e+02, -3.749e+02,  2.274e+03,  1.982e+03,
         7.039e+02, -2.234e+02,  2.389e+02,  1.859e+03, -1.598e+02, -3.585e+02,
         1.350e+02,  4.072e+02,  5.123e+02, -3.471e+02, -4.827e+02, -4.446e+02,
         1.224e+03, -4.621e+02,  1.063e+03,  2.374e+02,  3.334e+02,  8.613e+02,
        -2.137e+02, -5.004e+02,  6.616e+01,  1.479e+02, -3.219e+02, -4.273e+02,
         8.669e+01, -3.348e+02,  7.398e+01,  1.695e+02, -2.541e+02, -3.888e+02,
        -4.390e+02,  6.881e+02, -4.219e+02, -4.168e+02,  1.640e+03, -5.219e+02,
         1.506e+03, -6.851e+01, -2.037e+02, -4.723e+02,  4.711e+02,  4.897e+02,
        -2.742e+02, -2.102e+02, -4.382e+02, -4.831e+01,  1.188e+03, -1.432e+02,
        -4.640e+02,  2.719e+02,  1.501e+02,  3.247e+03, -2.959e+01,  2.618e+02,
        -2.515e+02, -3.967e+02, -3.845e+02,  1.798e+02,  2.179e+02, -1.348e+01,
        -3.174e+02, -4.497e+02, -1.687e+02,  4.311e+02, -1.021e+02,  1.618e+02,
        -2.882e+02,  3.952e+02, -4.250e+02,  1.127e+03, -4.029e+02, -1.919e+02,
         1.929e+03,  8.590e+02, -3.231e+02, -2.362e+02,  2.466e+02, -3.221e+02,
        -3.169e+02, -3.957e+02,  3.875e+02,  1.774e+03,  2.008e+03, -3.845e+02,
        -5.361e+02,  6.388e+02, -1.496e+02,  3.097e+02, -4.159e+02, -2.813e+02,
        -2.075e+02, -4.086e+02,  6.190e+02,  1.945e+02,  2.276e+03, -1.970e+02,
         5.189e+01, -5.066e+02, -3.184e+02, -1.253e+02, -3.502e+02, -4.508e+02,
        -2.601e+02,  8.700e+02, -4.120e+02, -4.699e+02, -1.971e+02, -2.812e+02,
         2.424e+02, -4.728e+02, -4.391e+02,  8.256e+02, -1.025e+02,  3.340e+03,
        -3.107e+02,  4.563e+02, -3.273e+02, -2.100e+02,  8.211e+01, -3.502e+02,
        -3.484e+02, -5.768e+02, -3.830e+02, -2.944e+02,  1.904e+03,  1.793e+03,
        -2.051e+02, -3.966e+02,  8.327e+02, -2.557e+02, -1.996e+01, -2.526e+02,
        -4.536e+02,  3.278e+02, -1.034e+02,  1.524e+02, -3.729e+02, -3.928e+02,
        -7.159e+01, -3.865e+02,  1.550e+03, -3.941e+02, -3.827e+02, -3.198e+02,
         8.633e+02, -2.567e+02, -2.237e+02, -2.512e+02, -1.820e+02, -1.817e+02,
         3.157e+02,  3.686e+02, -2.277e+02, -8.086e+01,  1.554e+02, -3.519e+02,
        -3.284e+02,  5.741e+02, -3.154e+02,  6.476e+01, -4.622e+02, -3.914e+02,
        -3.671e+02, -9.717e+01, -5.182e+02, -4.045e+02, -3.018e+02, -3.933e+02,
         5.149e+03, -5.349e+01, -5.239e+02,  6.160e+02, -1.577e+02, -5.782e+01,
        -4.425e+02, -1.080e+02,  1.180e+02,  5.244e+01, -9.144e+01, -1.460e+02,
        -2.019e+02, -2.167e+02, -1.712e+02,  4.924e+02,  5.423e+02,  1.909e+03,
        -4.521e+02, -1.984e+02, -5.269e+02, -3.732e+02, -2.964e+02, -4.713e+02,
        -3.430e+02,  4.030e+02,  6.532e+02, -1.468e+02,  1.410e+03, -1.951e+02],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.989e+00, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 1.644e+03],
         [-3.359e+02],
         [ 1.208e+03],
         [-2.744e+02],
         [ 6.446e+01],
         [ 8.744e+02],
         [-5.412e+02],
         [ 5.808e+02],
         [-3.945e+02],
         [ 3.231e+02]],

        [[-2.733e+02],
         [-5.326e+02],
         [-3.165e+02],
         [-4.632e+02],
         [ 1.506e+01],
         [ 1.652e+02],
         [ 3.965e+02],
         [-2.331e+02],
         [ 4.906e+02],
         [ 8.724e+02]],

        [[-3.351e+02],
         [-2.073e+02],
         [-2.190e+02],
         [-4.345e+02],
         [-3.369e+02],
         [-2.787e+02],
         [-1.506e+01],
         [-3.061e+02],
         [ 4.556e+02],
         [-2.543e+02]],

        [[-5.469e+02],
         [ 7.269e+02],
         [ 4.642e+02],
         [ 1.557e+03],
         [ 1.566e+02],
         [ 3.503e+03],
         [-2.168e+02],
         [-3.174e+02],
         [ 8.450e+02],
         [ 8.427e+02]],

        [[ 1.038e+02],
         [ 8.749e+01],
         [ 9.784e+01],
         [ 5.136e+02],
         [ 4.637e+02],
         [-5.761e+02],
         [-4.167e+02],
         [-5.206e+02],
         [-1.539e+02],
         [-3.738e+02]],

        [[ 4.455e+02],
         [-4.567e+02],
         [-2.202e+02],
         [-3.152e+02],
         [-3.983e+02],
         [-2.078e+02],
         [ 2.028e+02],
         [-4.150e+02],
         [-4.513e+02],
         [ 2.580e+03]],

        [[ 2.017e+03],
         [-3.132e+02],
         [-5.243e+02],
         [ 5.203e+02],
         [-3.454e+01],
         [-1.908e+02],
         [-1.198e+02],
         [-3.756e+02],
         [-4.405e+02],
         [ 4.827e+02]],

        [[-2.985e+02],
         [-4.060e+02],
         [ 1.094e+03],
         [-2.893e+02],
         [ 2.785e+02],
         [ 1.317e+03],
         [-5.921e+01],
         [-2.426e+02],
         [-2.000e+02],
         [ 7.502e+02]],

        [[-7.539e+00],
         [-3.835e+02],
         [ 2.848e+02],
         [-3.266e+02],
         [-1.982e+02],
         [-1.720e+02],
         [ 9.903e+01],
         [-1.617e+02],
         [ 3.179e+03],
         [-3.523e+01]],

        [[-5.414e+02],
         [-1.252e+02],
         [ 6.795e+01],
         [-4.019e+02],
         [-2.448e+02],
         [ 4.557e+01],
         [ 3.019e+02],
         [-3.689e+02],
         [-1.106e+02],
         [ 2.078e+03]],

        [[-3.687e+02],
         [-3.456e+02],
         [-2.958e+02],
         [-3.329e+02],
         [ 4.642e+01],
         [ 1.705e+02],
         [-4.778e+01],
         [ 2.202e+02],
         [-2.074e+02],
         [ 2.879e+02]],

        [[-1.040e+02],
         [-4.087e+02],
         [-2.750e+02],
         [ 3.541e+02],
         [ 3.164e+01],
         [-2.714e+02],
         [ 4.087e+02],
         [ 1.466e+02],
         [-2.382e+02],
         [-4.420e+02]],

        [[ 3.177e+02],
         [-1.825e+02],
         [ 2.219e+02],
         [-9.625e+01],
         [-3.480e+02],
         [ 1.476e+03],
         [-1.003e+01],
         [ 4.404e+02],
         [-3.839e+02],
         [ 5.255e+02]],

        [[-3.206e+02],
         [ 2.291e+03],
         [-2.819e+02],
         [-2.195e+02],
         [-2.948e+02],
         [-4.203e+02],
         [-4.711e+02],
         [-3.104e+02],
         [-4.313e+02],
         [-3.763e+02]],

        [[-4.725e+02],
         [-1.635e+02],
         [ 1.067e+03],
         [-1.207e+02],
         [-2.115e+02],
         [-2.319e+02],
         [-2.801e+01],
         [-2.737e+02],
         [ 1.890e+02],
         [ 1.672e+02]],

        [[-3.771e+02],
         [-4.426e+02],
         [-5.526e+02],
         [ 9.018e+02],
         [-3.785e+02],
         [-1.987e+02],
         [-7.551e+01],
         [ 5.511e+02],
         [-3.492e+02],
         [-3.936e+02]],

        [[-3.313e+02],
         [-4.422e+02],
         [-1.727e+02],
         [ 1.530e+02],
         [-4.532e+02],
         [-4.858e+02],
         [ 2.589e+02],
         [-2.739e+02],
         [ 1.366e+02],
         [-3.131e+02]],

        [[-4.363e+02],
         [-2.477e+02],
         [ 5.068e+02],
         [-2.516e+02],
         [ 2.082e+03],
         [-3.461e+02],
         [-3.309e+02],
         [-4.557e+02],
         [ 8.859e+01],
         [-4.706e+02]],

        [[ 6.304e+02],
         [-4.363e+02],
         [ 2.565e+02],
         [ 5.145e+02],
         [-2.455e+02],
         [-1.586e+02],
         [ 1.582e+02],
         [-3.177e+02],
         [ 1.167e+03],
         [ 1.375e+03]],

        [[ 9.105e+02],
         [-3.891e+02],
         [-4.244e+02],
         [-3.586e+02],
         [ 1.335e+03],
         [-5.314e+02],
         [ 1.772e+02],
         [-2.689e+02],
         [ 3.048e+02],
         [-2.025e+02]],

        [[-3.749e+02],
         [-3.386e+02],
         [ 2.827e+02],
         [-1.544e+02],
         [ 2.372e+02],
         [-4.275e+02],
         [ 6.227e+02],
         [-5.335e+02],
         [-4.210e+02],
         [ 1.546e+02]],

        [[-1.116e+02],
         [-1.201e+02],
         [ 6.562e+02],
         [ 6.569e+02],
         [-4.259e+02],
         [ 8.217e-01],
         [-2.878e+02],
         [-1.685e+01],
         [-4.940e+02],
         [-3.566e+02]],

        [[-3.267e+02],
         [-1.993e+02],
         [ 1.374e+02],
         [-3.925e+02],
         [-2.382e+02],
         [-1.413e+02],
         [ 6.452e+01],
         [-3.166e+02],
         [ 4.219e+01],
         [-4.978e+02]],

        [[-2.838e+02],
         [-4.166e+02],
         [-3.503e+02],
         [-9.138e+01],
         [-3.360e+02],
         [-4.029e+02],
         [-4.510e+02],
         [-3.211e+02],
         [-2.962e+02],
         [ 1.365e+03]],

        [[-3.166e+02],
         [-3.991e+02],
         [-2.346e+02],
         [ 8.953e+02],
         [-4.684e+02],
         [-8.236e+01],
         [-3.334e+02],
         [-3.902e+02],
         [ 7.694e+02],
         [ 6.668e+02]],

        [[-4.468e+02],
         [-3.533e+02],
         [ 4.932e+03],
         [-5.043e+02],
         [-2.616e+02],
         [-3.454e+01],
         [-5.334e+02],
         [-4.622e+02],
         [ 1.512e+02],
         [-2.535e+02]],

        [[ 8.355e+02],
         [-2.904e+01],
         [ 7.670e+02],
         [-2.680e+02],
         [ 5.183e+02],
         [-4.302e+02],
         [ 3.480e+02],
         [-2.768e+02],
         [-4.043e+02],
         [-5.903e+00]],

        [[-2.180e+02],
         [ 3.358e+02],
         [-2.994e+02],
         [-4.662e+01],
         [ 1.778e+03],
         [-2.136e+02],
         [-1.763e+02],
         [-2.052e+02],
         [-3.326e+02],
         [ 1.173e+03]],

        [[-2.191e+02],
         [-7.007e+01],
         [-2.934e+02],
         [ 1.249e+02],
         [-3.639e+02],
         [ 5.191e+01],
         [-3.256e+01],
         [-1.773e+02],
         [-3.617e+02],
         [ 8.527e+01]],

        [[ 2.983e+02],
         [-2.712e+02],
         [ 1.494e+02],
         [-3.999e+02],
         [-3.868e+02],
         [-3.971e+02],
         [-2.500e+02],
         [-2.255e+02],
         [-1.712e+02],
         [-4.693e+02]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [8.217e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-219.887],
        [-409.647],
        [-408.585],
        [-509.077],
        [-459.218],
        [-515.106],
        [ -32.833],
        [-171.764],
        [-306.467],
        [  59.315],
        [-138.224],
        [-182.917],
        [-321.411],
        [ -57.413],
        [-366.251],
        [-163.647],
        [ 236.846],
        [-358.810],
        [ -76.435],
        [-163.748],
        [ 183.436],
        [-388.922],
        [-466.371],
        [-434.360],
        [ 822.596],
        [-404.297],
        [1444.959],
        [-196.097],
        [-185.922],
        [-427.708]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.157,  0.006, -0.032,  ...,  0.002, -0.097,  0.081],
        [ 0.026, -0.111,  0.010,  ...,  0.197, -0.045,  0.025],
        [ 0.123,  0.068,  0.026,  ..., -0.049, -0.149, -0.013],
        ...,
        [ 0.097,  0.072, -0.026,  ..., -0.166, -0.010,  0.183],
        [-0.035, -0.177, -0.098,  ..., -0.081,  0.134, -0.029],
        [-0.068, -0.205, -0.027,  ...,  0.009,  0.016, -0.149]],
       device='cuda:0')
s after update for 1 param tensor([[2.368, 1.553, 1.681,  ..., 1.994, 1.710, 2.014],
        [2.241, 1.638, 1.753,  ..., 1.934, 1.469, 2.342],
        [2.266, 1.668, 2.063,  ..., 1.253, 2.175, 1.956],
        ...,
        [1.933, 1.356, 1.999,  ..., 1.999, 1.789, 2.598],
        [1.939, 1.660, 2.129,  ..., 2.103, 1.812, 2.269],
        [2.112, 1.715, 2.143,  ..., 2.121, 1.627, 2.370]], device='cuda:0')
b after update for 1 param tensor([[198.499, 160.752, 167.256,  ..., 182.126, 168.669, 183.048],
        [193.083, 165.093, 170.764,  ..., 179.375, 156.357, 197.386],
        [194.188, 166.568, 185.279,  ..., 144.403, 190.237, 180.396],
        ...,
        [179.351, 150.218, 182.388,  ..., 182.364, 172.527, 207.916],
        [179.613, 166.196, 188.206,  ..., 187.046, 173.627, 194.317],
        [187.471, 168.939, 188.818,  ..., 187.858, 164.544, 198.571]],
       device='cuda:0')
clipping threshold 1.2354840129457598
a after update for 1 param tensor([ 5.457e-02, -2.833e-01, -6.094e-02,  1.129e-01, -8.376e-02,  5.289e-02,
         1.292e-01,  1.030e-01,  9.653e-03,  2.245e-02, -1.090e-01, -1.158e-01,
         1.073e-01,  1.002e-01,  1.365e-02,  1.493e-02, -9.781e-02, -2.011e-01,
         2.666e-01,  1.516e-01,  1.194e-01,  1.360e-02, -3.077e-02, -6.947e-02,
        -1.572e-01, -5.702e-03,  8.278e-02, -1.821e-01, -3.144e-03, -1.544e-01,
        -4.523e-02, -1.286e-01, -7.595e-02, -1.427e-01,  1.516e-01, -3.442e-02,
         2.285e-01,  4.648e-02,  7.799e-02,  1.199e-01, -1.126e-01,  6.663e-02,
        -6.943e-02, -1.613e-01,  4.110e-03,  1.058e-01, -5.664e-02,  1.337e-01,
        -9.161e-03, -2.961e-02,  1.270e-01, -6.940e-02, -9.135e-02, -1.191e-01,
         1.554e-02, -5.837e-02, -1.471e-01, -2.988e-02,  8.560e-02,  1.299e-01,
        -6.906e-02, -1.922e-01, -3.550e-01,  1.569e-01,  5.735e-02, -5.959e-02,
         2.666e-01,  3.787e-01,  1.048e-02, -2.160e-01,  5.553e-02,  2.076e-02,
         4.677e-02, -3.298e-01, -2.773e-01, -1.886e-02,  3.570e-03, -3.075e-02,
        -3.900e-01, -1.421e-01,  2.920e-02,  1.673e-02, -3.477e-03,  9.799e-02,
         7.421e-02,  1.082e-01, -3.582e-01, -1.938e-02,  1.641e-01,  1.135e-01,
        -2.656e-04,  1.807e-01, -1.139e-02, -7.370e-02,  2.957e-02, -3.804e-02,
         2.246e-01, -1.031e-01, -3.586e-01,  8.299e-02,  7.889e-02, -3.416e-01,
         8.021e-03,  6.340e-02,  8.580e-02,  2.531e-01,  2.163e-02, -6.193e-02,
        -1.021e-01, -2.451e-02,  1.872e-01,  1.331e-01,  1.382e-01, -2.850e-02,
        -1.174e-01, -1.529e-01,  3.238e-02, -5.468e-02, -4.117e-01,  4.854e-02,
        -1.118e-01, -3.112e-01,  5.474e-02, -6.479e-02, -1.552e-01,  1.695e-01,
        -3.752e-02,  2.530e-01,  2.567e-01,  1.292e-01,  7.148e-02,  6.419e-02,
         1.385e-01, -2.526e-01,  5.686e-02,  6.653e-02, -1.261e-01,  3.982e-02,
         4.590e-02,  2.686e-01,  5.876e-02, -8.443e-03, -6.230e-02,  1.833e-01,
        -2.389e-01, -5.031e-02, -4.930e-02,  1.211e-02,  5.713e-02,  1.431e-01,
         9.451e-03,  9.013e-02, -1.209e-01, -9.595e-02,  7.370e-02,  8.304e-03,
         1.436e-01, -2.883e-02, -9.306e-02, -3.715e-02,  4.570e-02, -5.996e-02,
         2.016e-01,  4.639e-01, -2.018e-02, -1.209e-02,  5.234e-02, -4.456e-02,
        -1.187e-01,  4.892e-02,  2.199e-02,  1.571e-01, -1.492e-01,  4.291e-01,
        -1.853e-01,  4.418e-02, -9.996e-02,  1.957e-01, -2.101e-01,  4.613e-02,
        -2.324e-01,  1.311e-02,  2.178e-02,  2.024e-03,  1.079e-01, -2.835e-01,
        -2.255e-01, -1.841e-01,  3.079e-02,  4.812e-02,  5.983e-02, -1.683e-01,
        -5.988e-02,  8.527e-03,  3.889e-02, -9.825e-02, -2.449e-01,  3.405e-02,
        -1.569e-03, -7.852e-02, -1.023e-03,  4.892e-02, -3.272e-02, -1.152e-01,
        -1.258e-01, -4.582e-02, -1.539e-01,  7.558e-02,  2.562e-01,  1.010e-01,
        -3.091e-02, -2.746e-01,  1.000e-01, -7.196e-02,  1.971e-02, -7.546e-02,
         1.731e-01, -5.086e-02, -6.077e-02,  2.985e-02,  1.012e-01,  3.742e-02,
        -3.458e-02, -1.165e-01, -1.485e-01,  1.296e-01, -7.727e-02, -4.846e-01,
         2.039e-01,  1.383e-02, -1.651e-01,  1.432e-02,  3.091e-02, -6.846e-02,
         3.105e-02, -9.169e-02, -7.549e-02,  3.278e-01, -1.716e-01, -2.186e-01,
        -1.799e-02, -5.486e-02,  1.033e-01,  1.293e-02,  4.626e-02, -8.485e-02,
        -2.092e-01, -3.858e-01, -1.810e-01, -2.792e-02,  3.416e-03, -2.773e-01,
        -7.781e-02,  9.796e-02, -1.460e-01,  1.032e-01, -6.788e-02, -1.362e-01,
         5.353e-03, -8.861e-02, -7.455e-02, -2.904e-01,  7.255e-03, -7.985e-03,
        -2.637e-01, -1.840e-02,  4.172e-02,  7.526e-02,  9.230e-02, -1.784e-01,
         2.116e-02, -1.428e-01,  1.006e-01,  1.919e-01,  4.902e-02,  3.850e-03,
         1.697e-01, -5.456e-02,  4.168e-02,  2.278e-01,  1.084e-01, -3.958e-01,
         8.729e-04, -1.566e-01, -1.816e-01, -1.128e-01, -2.159e-02,  7.778e-02,
        -5.338e-02, -2.057e-02,  2.221e-01,  1.380e-01, -1.755e-01, -4.487e-01,
        -1.580e-01, -1.575e-01, -5.579e-02, -1.770e-01, -3.396e-01, -7.151e-03],
       device='cuda:0')
s after update for 1 param tensor([1.717, 2.033, 2.102, 1.404, 1.583, 1.592, 1.675, 1.846, 2.084, 1.792,
        1.957, 1.884, 2.057, 1.313, 1.844, 1.648, 1.495, 1.756, 1.782, 2.126,
        1.820, 1.032, 1.378, 1.407, 1.703, 0.950, 2.205, 1.930, 1.753, 1.511,
        2.250, 1.620, 1.693, 1.942, 1.697, 2.057, 1.675, 1.218, 1.411, 2.198,
        1.557, 1.806, 1.938, 1.969, 1.148, 1.433, 2.116, 1.826, 2.113, 1.227,
        2.347, 1.782, 1.716, 1.346, 2.062, 1.939, 2.025, 1.757, 2.062, 1.526,
        1.532, 1.860, 2.136, 1.979, 1.573, 1.528, 1.906, 2.074, 2.207, 1.506,
        1.224, 1.390, 1.059, 1.738, 1.464, 1.890, 2.261, 1.562, 1.631, 1.214,
        1.831, 1.360, 0.755, 1.265, 1.538, 1.418, 1.562, 1.539, 1.844, 1.677,
        1.847, 1.982, 2.371, 1.474, 1.842, 1.982, 2.000, 1.725, 2.264, 1.775,
        1.841, 1.983, 1.894, 0.845, 1.724, 1.984, 1.451, 1.350, 1.703, 1.242,
        1.423, 1.638, 1.973, 1.673, 2.213, 1.779, 1.777, 1.737, 2.083, 1.673,
        1.940, 2.042, 1.585, 1.498, 1.309, 1.964, 1.716, 1.998, 2.066, 2.236,
        1.163, 1.470, 1.837, 1.721, 1.587, 1.567, 1.864, 1.974, 1.623, 1.762,
        0.954, 1.776, 2.237, 1.915, 1.852, 1.842, 1.658, 0.516, 1.753, 1.137,
        1.744, 1.795, 1.558, 2.282, 1.551, 2.004, 1.284, 1.492, 1.625, 1.689,
        1.381, 1.641, 1.563, 1.691, 1.659, 1.673, 1.093, 1.405, 1.492, 2.072,
        1.655, 1.941, 1.657, 1.349, 1.667, 1.964, 1.446, 1.421, 1.805, 1.213,
        1.301, 1.891, 1.795, 1.762, 1.630, 1.626, 2.049, 1.774, 1.136, 2.021,
        1.948, 1.825, 1.725, 1.707, 2.053, 1.655, 1.783, 1.753, 1.320, 1.922,
        1.593, 1.429, 1.532, 1.793, 1.280, 1.870, 1.586, 1.768, 2.044, 1.711,
        1.436, 1.875, 1.838, 1.653, 0.621, 1.923, 1.169, 2.130, 1.530, 1.691,
        1.828, 2.049, 1.311, 2.227, 1.669, 1.280, 2.017, 2.276, 1.884, 1.491,
        1.938, 1.099, 1.398, 1.486, 1.709, 2.018, 1.516, 1.929, 1.442, 1.769,
        1.326, 1.487, 1.978, 1.493, 1.487, 1.259, 1.933, 1.823, 1.788, 1.011,
        1.089, 2.092, 2.188, 1.690, 1.907, 1.529, 1.669, 1.923, 1.348, 2.028,
        1.230, 2.286, 2.122, 1.887, 1.466, 0.626, 1.964, 1.522, 1.957, 1.535,
        1.676, 1.775, 1.997, 2.143, 1.512, 0.226, 1.666, 1.165, 1.397, 2.067,
        1.458, 2.309, 1.365, 1.681, 1.560, 1.689, 1.759, 1.404, 2.006, 1.123,
        1.999, 1.613, 1.201, 1.822, 1.290, 2.071, 1.887, 1.705, 1.787, 1.791],
       device='cuda:0')
b after update for 1 param tensor([169.015, 183.916, 187.009, 152.839, 162.302, 162.738, 166.928, 175.278,
        186.223, 172.679, 180.464, 177.060, 184.982, 147.832, 175.182, 165.568,
        157.713, 170.944, 172.209, 188.085, 174.042, 131.013, 151.418, 152.982,
        168.323, 125.736, 191.540, 179.193, 170.769, 158.534, 193.492, 164.158,
        167.844, 179.746, 168.022, 184.989, 166.952, 142.358, 153.217, 191.230,
        160.932, 173.362, 179.566, 180.994, 138.179, 154.426, 187.632, 174.309,
        187.490, 142.883, 197.626, 172.204, 168.997, 149.641, 185.226, 179.595,
        183.535, 171.000, 185.217, 159.330, 159.678, 175.918, 188.521, 181.475,
        161.761, 159.437, 178.078, 185.745, 191.622, 158.288, 142.734, 152.087,
        132.755, 170.063, 156.079, 177.348, 193.952, 161.211, 164.757, 142.136,
        174.520, 150.436, 112.068, 145.083, 159.985, 153.599, 161.206, 160.013,
        175.181, 167.050, 175.301, 181.604, 198.625, 156.595, 175.069, 181.585,
        182.405, 169.427, 194.074, 171.873, 175.000, 181.644, 177.531, 118.543,
        169.346, 181.693, 155.380, 149.882, 168.342, 143.734, 153.883, 165.094,
        181.191, 166.856, 191.868, 172.054, 171.936, 170.019, 186.157, 166.826,
        179.683, 184.304, 162.402, 157.896, 147.597, 180.755, 168.971, 182.330,
        185.390, 192.895, 139.109, 156.389, 174.806, 169.206, 162.497, 161.480,
        176.093, 181.214, 164.341, 171.237, 125.957, 171.897, 192.915, 178.494,
        175.555, 175.080, 166.101,  92.641, 170.781, 137.522, 170.365, 172.827,
        161.031, 194.852, 160.662, 182.609, 146.161, 157.569, 164.443, 167.640,
        151.584, 165.252, 161.280, 167.744, 166.125, 166.841, 134.843, 152.888,
        157.584, 185.688, 165.949, 179.707, 166.019, 149.821, 166.533, 180.759,
        155.133, 153.743, 173.307, 142.048, 147.104, 177.383, 172.834, 171.218,
        164.677, 164.464, 184.629, 171.819, 137.470, 183.395, 180.054, 174.234,
        169.417, 168.532, 184.802, 165.920, 172.245, 170.791, 148.179, 178.850,
        162.780, 154.180, 159.671, 172.740, 145.923, 176.376, 162.424, 171.513,
        184.427, 168.722, 154.564, 176.619, 174.886, 165.850, 101.658, 178.880,
        139.493, 188.242, 159.572, 167.748, 174.389, 184.620, 147.671, 192.482,
        166.637, 145.949, 183.209, 194.622, 177.052, 157.524, 179.558, 135.197,
        152.522, 157.264, 168.651, 183.261, 158.845, 179.174, 154.908, 171.561,
        148.512, 157.274, 181.436, 157.623, 157.279, 144.748, 179.350, 174.169,
        172.462, 129.682, 134.581, 186.590, 190.781, 167.709, 178.146, 159.477,
        166.631, 178.860, 149.779, 183.689, 143.053, 195.028, 187.909, 177.197,
        156.162, 102.048, 180.754, 159.130, 180.436, 159.803, 166.994, 171.868,
        182.273, 188.826, 158.609,  61.349, 166.475, 139.227, 152.444, 185.429,
        155.737, 196.011, 150.708, 167.243, 161.107, 167.646, 171.054, 152.853,
        182.680, 136.676, 182.373, 163.829, 141.375, 174.128, 146.527, 185.640,
        177.186, 168.445, 172.412, 172.648], device='cuda:0')
clipping threshold 1.2354840129457598
a after update for 1 param tensor([[[ 1.400e-02],
         [ 1.031e-01],
         [-2.797e-01],
         [ 1.398e-01],
         [ 7.647e-02],
         [ 2.074e-02],
         [-7.928e-02],
         [ 2.871e-01],
         [-1.568e-01],
         [-2.312e-01]],

        [[-1.606e-02],
         [ 3.355e-01],
         [ 1.284e-01],
         [ 7.695e-02],
         [ 4.396e-01],
         [-6.898e-02],
         [ 2.232e-02],
         [-1.112e-01],
         [ 4.446e-02],
         [-5.524e-02]],

        [[ 1.383e-01],
         [ 8.748e-02],
         [-1.332e-02],
         [-1.544e-02],
         [ 2.935e-01],
         [-1.214e-01],
         [-5.605e-02],
         [-1.974e-01],
         [ 1.434e-01],
         [-5.904e-02]],

        [[ 8.857e-03],
         [ 1.221e-01],
         [ 1.425e-01],
         [ 1.892e-01],
         [-6.930e-02],
         [-1.263e-01],
         [-6.985e-02],
         [-1.001e-01],
         [ 1.016e-02],
         [ 1.993e-01]],

        [[ 1.503e-01],
         [ 1.318e-03],
         [ 8.063e-02],
         [ 9.192e-02],
         [-1.106e-01],
         [ 6.566e-03],
         [-2.024e-02],
         [-1.071e-01],
         [-6.736e-02],
         [-4.936e-02]],

        [[-1.058e-01],
         [-1.312e-01],
         [-1.222e-01],
         [-9.506e-02],
         [ 1.093e-01],
         [ 1.087e-01],
         [ 1.804e-01],
         [-1.062e-01],
         [-2.555e-01],
         [ 1.121e-01]],

        [[-1.708e-01],
         [ 1.246e-01],
         [ 2.675e-01],
         [-6.886e-02],
         [-7.319e-02],
         [-3.544e-01],
         [ 1.190e-01],
         [-8.684e-03],
         [-3.861e-02],
         [ 7.887e-02]],

        [[ 2.305e-01],
         [-1.052e-01],
         [-1.462e-03],
         [-1.402e-01],
         [ 2.215e-01],
         [-2.687e-02],
         [ 7.611e-03],
         [-2.342e-01],
         [-1.175e-01],
         [-1.335e-01]],

        [[-6.866e-02],
         [ 6.890e-03],
         [ 2.457e-01],
         [ 5.335e-02],
         [-1.780e-01],
         [ 2.272e-01],
         [ 2.468e-02],
         [ 8.021e-02],
         [ 8.733e-02],
         [ 8.101e-02]],

        [[ 1.260e-01],
         [-1.247e-01],
         [ 1.773e-02],
         [-1.504e-01],
         [-1.087e-01],
         [ 1.724e-01],
         [ 2.480e-01],
         [-2.231e-01],
         [ 1.130e-02],
         [-7.235e-02]],

        [[ 2.161e-01],
         [-2.097e-01],
         [ 1.287e-01],
         [ 6.020e-02],
         [-9.558e-02],
         [ 1.908e-01],
         [-1.926e-01],
         [-1.389e-01],
         [-7.442e-02],
         [-7.593e-02]],

        [[-3.517e-02],
         [-1.151e-01],
         [-1.348e-01],
         [-1.024e-01],
         [-7.397e-02],
         [-2.766e-02],
         [ 1.249e-01],
         [-6.678e-02],
         [ 5.069e-02],
         [-1.178e-01]],

        [[-1.896e-01],
         [ 1.452e-01],
         [ 3.042e-01],
         [ 2.149e-02],
         [ 1.950e-01],
         [ 2.064e-01],
         [ 3.012e-02],
         [ 4.542e-02],
         [ 2.414e-02],
         [-2.646e-01]],

        [[-1.541e-01],
         [-1.230e-01],
         [ 1.739e-02],
         [ 2.010e-01],
         [-7.384e-02],
         [-1.250e-01],
         [ 9.967e-02],
         [ 3.176e-02],
         [-5.399e-02],
         [-1.054e-01]],

        [[-2.746e-02],
         [-6.114e-02],
         [-1.006e-01],
         [-1.414e-01],
         [-7.126e-02],
         [-1.816e-02],
         [ 6.309e-02],
         [ 9.375e-02],
         [-8.698e-02],
         [-3.601e-02]],

        [[-1.967e-01],
         [-3.339e-02],
         [ 1.662e-02],
         [ 6.148e-02],
         [ 1.226e-01],
         [-2.378e-01],
         [-1.146e-01],
         [-5.509e-02],
         [-2.383e-01],
         [-4.311e-02]],

        [[-5.346e-02],
         [-3.180e-02],
         [-6.702e-02],
         [-1.427e-02],
         [-4.093e-02],
         [ 1.459e-01],
         [-1.110e-01],
         [ 4.318e-02],
         [ 1.937e-01],
         [ 1.237e-01]],

        [[ 6.838e-02],
         [ 5.691e-02],
         [ 1.239e-01],
         [ 1.815e-01],
         [ 1.282e-02],
         [ 1.018e-01],
         [ 5.430e-02],
         [ 3.541e-02],
         [-2.339e-01],
         [ 9.636e-02]],

        [[ 2.322e-01],
         [ 2.282e-01],
         [ 3.665e-02],
         [ 2.079e-01],
         [-2.928e-02],
         [-2.864e-02],
         [-5.612e-02],
         [-6.505e-02],
         [ 2.199e-02],
         [ 2.757e-04]],

        [[ 3.473e-01],
         [ 5.079e-02],
         [ 2.653e-01],
         [-3.350e-02],
         [ 1.070e-02],
         [-3.920e-01],
         [ 2.744e-01],
         [-1.146e-01],
         [-8.492e-02],
         [-9.398e-02]],

        [[ 1.301e-01],
         [ 2.037e-01],
         [ 1.882e-01],
         [ 7.630e-02],
         [ 3.833e-02],
         [-9.117e-02],
         [ 2.483e-02],
         [ 3.272e-02],
         [ 8.200e-02],
         [ 1.990e-01]],

        [[ 3.288e-03],
         [-6.565e-02],
         [-1.856e-02],
         [ 2.594e-01],
         [-5.551e-02],
         [ 6.812e-03],
         [-2.004e-03],
         [ 9.706e-02],
         [ 1.691e-01],
         [-3.607e-01]],

        [[-7.704e-02],
         [ 1.266e-01],
         [ 2.631e-02],
         [ 1.491e-01],
         [ 3.737e-02],
         [ 2.246e-01],
         [-1.923e-02],
         [ 9.140e-02],
         [ 6.321e-02],
         [-1.143e-01]],

        [[ 9.551e-02],
         [-1.007e-01],
         [-9.685e-03],
         [-2.127e-02],
         [ 6.778e-02],
         [ 1.031e-01],
         [ 1.079e-01],
         [-1.576e-01],
         [-1.110e-01],
         [-6.860e-03]],

        [[-1.005e-01],
         [-3.336e-03],
         [-7.414e-02],
         [ 8.390e-02],
         [-3.071e-02],
         [-4.782e-02],
         [-3.661e-01],
         [-1.390e-01],
         [ 1.199e-01],
         [ 7.676e-02]],

        [[-6.276e-02],
         [ 6.161e-03],
         [-3.687e-01],
         [ 2.376e-02],
         [ 1.149e-02],
         [-1.358e-01],
         [ 5.037e-02],
         [-1.837e-02],
         [ 1.359e-02],
         [ 9.239e-03]],

        [[ 7.675e-02],
         [ 1.725e-01],
         [-9.615e-02],
         [-5.204e-02],
         [ 9.205e-02],
         [-9.688e-02],
         [ 4.730e-02],
         [-3.268e-02],
         [ 1.227e-01],
         [ 9.052e-02]],

        [[-5.569e-02],
         [-1.553e-01],
         [ 9.726e-02],
         [ 6.233e-02],
         [-8.298e-02],
         [-1.783e-02],
         [ 1.460e-01],
         [-5.411e-02],
         [ 1.720e-01],
         [ 2.653e-02]],

        [[ 1.413e-01],
         [ 1.499e-01],
         [-8.067e-02],
         [-1.038e-01],
         [-5.165e-02],
         [-1.309e-01],
         [ 6.619e-02],
         [ 1.221e-01],
         [ 2.267e-01],
         [-4.364e-02]],

        [[-1.742e-02],
         [-1.874e-01],
         [ 1.101e-01],
         [ 1.462e-01],
         [-3.070e-02],
         [-9.651e-02],
         [ 9.338e-02],
         [ 3.839e-02],
         [-2.258e-01],
         [ 3.568e-01]]], device='cuda:0')
s after update for 1 param tensor([[[1.982],
         [1.480],
         [2.276],
         [1.743],
         [1.546],
         [1.516],
         [2.035],
         [2.554],
         [1.826],
         [2.079]],

        [[1.378],
         [2.037],
         [1.628],
         [1.832],
         [1.841],
         [2.010],
         [1.745],
         [1.337],
         [1.765],
         [1.555]],

        [[2.069],
         [1.907],
         [1.471],
         [2.013],
         [1.355],
         [1.942],
         [1.285],
         [1.864],
         [1.776],
         [1.395]],

        [[2.056],
         [1.993],
         [1.953],
         [1.428],
         [1.404],
         [2.018],
         [0.968],
         [1.520],
         [1.812],
         [2.345]],

        [[1.797],
         [2.082],
         [1.574],
         [1.771],
         [1.873],
         [2.167],
         [1.568],
         [2.038],
         [2.058],
         [1.595]],

        [[2.079],
         [1.738],
         [1.413],
         [1.324],
         [1.610],
         [1.369],
         [2.228],
         [1.647],
         [2.076],
         [1.971]],

        [[2.091],
         [1.324],
         [2.187],
         [1.805],
         [1.011],
         [1.893],
         [1.473],
         [1.484],
         [1.687],
         [1.569]],

        [[1.263],
         [1.589],
         [1.931],
         [1.551],
         [1.854],
         [2.186],
         [1.295],
         [1.946],
         [1.404],
         [2.045]],

        [[0.657],
         [1.582],
         [1.580],
         [1.536],
         [1.620],
         [1.744],
         [1.576],
         [1.506],
         [2.271],
         [1.295]],

        [[2.050],
         [1.182],
         [1.811],
         [1.927],
         [1.579],
         [2.039],
         [1.309],
         [1.583],
         [1.350],
         [1.497]],

        [[1.522],
         [1.395],
         [1.708],
         [1.867],
         [2.093],
         [1.433],
         [1.947],
         [2.121],
         [1.357],
         [1.746]],

        [[1.465],
         [1.602],
         [1.315],
         [1.664],
         [1.754],
         [1.139],
         [2.037],
         [1.599],
         [1.081],
         [1.694]],

        [[2.220],
         [1.819],
         [1.808],
         [0.771],
         [1.323],
         [1.794],
         [1.013],
         [1.972],
         [1.443],
         [2.381]],

        [[1.714],
         [1.918],
         [1.272],
         [1.862],
         [1.510],
         [1.581],
         [1.890],
         [1.172],
         [1.714],
         [1.561]],

        [[1.832],
         [1.550],
         [1.507],
         [1.657],
         [1.088],
         [1.412],
         [0.731],
         [1.370],
         [1.504],
         [2.004]],

        [[1.576],
         [1.821],
         [2.119],
         [1.771],
         [1.580],
         [2.020],
         [0.955],
         [2.149],
         [1.564],
         [1.582]],

        [[1.420],
         [1.869],
         [1.303],
         [1.170],
         [1.780],
         [1.965],
         [2.216],
         [1.287],
         [2.389],
         [1.356]],

        [[1.844],
         [1.244],
         [2.079],
         [1.686],
         [2.028],
         [1.313],
         [1.574],
         [1.761],
         [1.988],
         [1.817]],

        [[1.758],
         [1.865],
         [1.735],
         [1.919],
         [0.935],
         [1.743],
         [1.962],
         [1.578],
         [1.794],
         [1.460]],

        [[2.069],
         [1.710],
         [1.597],
         [1.661],
         [2.198],
         [2.002],
         [1.819],
         [1.050],
         [1.659],
         [1.244]],

        [[1.661],
         [1.768],
         [2.020],
         [1.542],
         [1.660],
         [1.699],
         [1.430],
         [2.224],
         [1.809],
         [1.869]],

        [[0.623],
         [1.762],
         [1.636],
         [1.984],
         [1.722],
         [0.287],
         [1.688],
         [1.420],
         [2.079],
         [1.755]],

        [[1.343],
         [1.084],
         [2.157],
         [1.503],
         [1.263],
         [1.163],
         [2.172],
         [1.322],
         [1.462],
         [2.040]],

        [[1.837],
         [1.586],
         [1.331],
         [1.680],
         [1.265],
         [1.794],
         [2.014],
         [1.611],
         [1.122],
         [2.010]],

        [[1.260],
         [1.871],
         [1.164],
         [1.888],
         [1.800],
         [1.537],
         [1.864],
         [1.544],
         [1.854],
         [1.750]],

        [[1.779],
         [1.459],
         [1.697],
         [1.896],
         [1.263],
         [1.364],
         [2.179],
         [1.857],
         [2.254],
         [0.981]],

        [[2.008],
         [0.919],
         [1.636],
         [1.025],
         [1.876],
         [1.656],
         [1.699],
         [1.198],
         [1.539],
         [1.317]],

        [[1.629],
         [1.600],
         [2.070],
         [1.500],
         [1.599],
         [1.155],
         [1.731],
         [1.322],
         [1.954],
         [1.503]],

        [[1.655],
         [2.026],
         [1.332],
         [2.239],
         [1.562],
         [1.819],
         [1.648],
         [1.142],
         [1.367],
         [1.755]],

        [[1.451],
         [1.495],
         [1.936],
         [1.615],
         [1.821],
         [1.641],
         [1.405],
         [1.382],
         [1.214],
         [1.791]]], device='cuda:0')
b after update for 1 param tensor([[[181.598],
         [156.933],
         [194.618],
         [170.314],
         [160.403],
         [158.811],
         [183.992],
         [206.123],
         [174.317],
         [185.977]],

        [[151.445],
         [184.080],
         [164.575],
         [174.590],
         [175.026],
         [182.877],
         [170.371],
         [149.154],
         [171.387],
         [160.854]],

        [[185.524],
         [178.120],
         [156.457],
         [182.998],
         [150.137],
         [179.776],
         [146.194],
         [176.094],
         [171.896],
         [152.372]],

        [[184.954],
         [182.117],
         [180.282],
         [154.119],
         [152.863],
         [183.225],
         [126.942],
         [159.010],
         [173.613],
         [197.515]],

        [[172.902],
         [186.103],
         [161.816],
         [171.670],
         [176.510],
         [189.879],
         [161.511],
         [184.129],
         [185.032],
         [162.910]],

        [[186.007],
         [170.029],
         [153.314],
         [148.444],
         [163.667],
         [150.899],
         [192.549],
         [165.544],
         [185.856],
         [181.114]],

        [[186.521],
         [148.448],
         [190.774],
         [173.303],
         [129.707],
         [177.483],
         [156.540],
         [157.129],
         [167.521],
         [161.566]],

        [[144.939],
         [162.599],
         [179.257],
         [160.629],
         [175.645],
         [190.717],
         [146.790],
         [179.926],
         [152.867],
         [184.445]],

        [[104.583],
         [162.260],
         [162.145],
         [159.864],
         [164.161],
         [170.338],
         [161.923],
         [158.308],
         [194.367],
         [146.792]],

        [[184.704],
         [140.213],
         [173.589],
         [179.077],
         [162.111],
         [184.170],
         [147.566],
         [162.269],
         [149.883],
         [157.817]],

        [[159.113],
         [152.332],
         [168.578],
         [176.236],
         [186.617],
         [154.424],
         [179.969],
         [187.844],
         [150.258],
         [170.461]],

        [[156.134],
         [163.264],
         [147.922],
         [166.384],
         [170.840],
         [137.686],
         [184.081],
         [163.112],
         [134.129],
         [167.881]],

        [[192.212],
         [173.990],
         [173.450],
         [113.243],
         [148.370],
         [172.771],
         [129.803],
         [181.141],
         [154.971],
         [199.059]],

        [[168.858],
         [178.638],
         [145.480],
         [176.036],
         [158.509],
         [162.168],
         [177.318],
         [139.628],
         [168.894],
         [161.150]],

        [[174.568],
         [160.573],
         [158.345],
         [166.060],
         [134.547],
         [153.262],
         [110.308],
         [150.998],
         [158.184],
         [182.614]],

        [[161.927],
         [174.049],
         [187.757],
         [171.682],
         [162.121],
         [183.321],
         [126.055],
         [189.076],
         [161.301],
         [162.247]],

        [[153.730],
         [176.339],
         [147.262],
         [139.528],
         [172.080],
         [180.806],
         [192.021],
         [146.309],
         [199.373],
         [150.200]],

        [[175.171],
         [143.891],
         [185.968],
         [167.469],
         [183.697],
         [147.780],
         [161.849],
         [171.194],
         [181.872],
         [173.893]],

        [[171.015],
         [176.179],
         [169.893],
         [178.704],
         [124.759],
         [170.288],
         [180.662],
         [162.036],
         [172.782],
         [155.873]],

        [[185.519],
         [168.693],
         [163.014],
         [166.232],
         [191.239],
         [182.489],
         [173.985],
         [132.193],
         [166.151],
         [143.859]],

        [[166.250],
         [171.516],
         [183.345],
         [160.172],
         [166.176],
         [168.138],
         [154.235],
         [192.357],
         [173.472],
         [176.361]],

        [[101.776],
         [171.202],
         [164.963],
         [181.700],
         [169.244],
         [ 69.063],
         [167.581],
         [153.685],
         [185.983],
         [170.867]],

        [[149.494],
         [134.326],
         [189.467],
         [158.147],
         [144.963],
         [139.136],
         [190.115],
         [148.302],
         [155.947],
         [184.247]],

        [[174.823],
         [162.430],
         [148.835],
         [167.207],
         [145.082],
         [172.775],
         [183.071],
         [163.717],
         [136.632],
         [182.861]],

        [[144.811],
         [176.436],
         [139.161],
         [177.234],
         [173.037],
         [159.923],
         [176.123],
         [160.306],
         [175.641],
         [170.641]],

        [[172.058],
         [155.792],
         [168.039],
         [177.631],
         [144.980],
         [150.622],
         [190.413],
         [175.781],
         [193.673],
         [127.744]],

        [[182.788],
         [123.664],
         [165.009],
         [130.572],
         [176.663],
         [166.008],
         [168.140],
         [141.206],
         [160.006],
         [148.044]],

        [[164.645],
         [163.157],
         [185.571],
         [157.974],
         [163.122],
         [138.633],
         [169.728],
         [148.335],
         [180.304],
         [158.164]],

        [[165.948],
         [183.589],
         [148.854],
         [193.021],
         [161.238],
         [173.979],
         [165.588],
         [137.845],
         [150.829],
         [170.866]],

        [[155.357],
         [157.740],
         [179.462],
         [163.920],
         [174.059],
         [165.223],
         [152.900],
         [151.619],
         [142.129],
         [172.646]]], device='cuda:0')
clipping threshold 1.2354840129457598
a after update for 1 param tensor([[-0.002],
        [ 0.048],
        [ 0.134],
        [ 0.098],
        [-0.050],
        [ 0.156],
        [ 0.120],
        [ 0.082],
        [-0.166],
        [-0.017],
        [-0.081],
        [ 0.013],
        [-0.046],
        [ 0.089],
        [-0.023],
        [-0.011],
        [-0.076],
        [ 0.117],
        [ 0.055],
        [ 0.029],
        [-0.133],
        [ 0.211],
        [ 0.121],
        [-0.009],
        [ 0.221],
        [ 0.246],
        [-0.252],
        [ 0.061],
        [ 0.118],
        [ 0.073]], device='cuda:0')
s after update for 1 param tensor([[1.179],
        [1.576],
        [1.711],
        [1.919],
        [1.956],
        [1.936],
        [1.893],
        [1.403],
        [1.755],
        [1.741],
        [1.633],
        [2.033],
        [1.258],
        [1.369],
        [1.884],
        [1.815],
        [1.493],
        [1.658],
        [0.661],
        [0.995],
        [1.953],
        [1.467],
        [1.769],
        [1.668],
        [2.019],
        [1.756],
        [2.303],
        [1.322],
        [1.462],
        [1.825]], device='cuda:0')
b after update for 1 param tensor([[140.068],
        [161.913],
        [168.712],
        [178.687],
        [180.386],
        [179.501],
        [177.464],
        [152.810],
        [170.868],
        [170.219],
        [164.817],
        [183.902],
        [144.662],
        [150.906],
        [177.039],
        [173.763],
        [157.606],
        [166.109],
        [104.879],
        [128.647],
        [180.286],
        [156.229],
        [171.540],
        [166.592],
        [183.287],
        [170.915],
        [195.770],
        [148.297],
        [155.971],
        [174.270]], device='cuda:0')
clipping threshold 1.2354840129457598
||w||^2 1.2581515950314213
exp ma of ||w||^2 2.1422192175182846
||w|| 1.1216735688387336
exp ma of ||w|| 1.3754127613702722
||w||^2 0.8387191794417758
exp ma of ||w||^2 1.474722235571581
||w|| 0.915816127528761
exp ma of ||w|| 1.1759362624488081
cuda
Objective function 74.88 = squared loss an data 59.63 + 0.5*rho*h**2 8.342823 + alpha*h 5.168512 + L2reg 1.45 + L1reg 0.30 ; SHD = 98 ; DAG True
Proportion of microbatches that were clipped  0.771000965561635
iteration 2 in inner loop, alpha 40.012348039716 rho 1000.0 h 0.12917292696030813
9630
cuda
Objective function 149.97 = squared loss an data 59.63 + 0.5*rho*h**2 83.428225 + alpha*h 5.168512 + L2reg 1.45 + L1reg 0.30 ; SHD = 98 ; DAG True
||w||^2 45.18651653078313
exp ma of ||w||^2 7788.935806705148
||w|| 6.722091678248901
exp ma of ||w|| 11.86279833384378
||w||^2 1.6541872409479552
exp ma of ||w||^2 2.321900108140957
||w|| 1.2861521064586239
exp ma of ||w|| 1.4912466737741725
||w||^2 2.4019752412717725
exp ma of ||w||^2 1.8637915815702026
||w|| 1.5498307137464313
exp ma of ||w|| 1.3341295241149467
||w||^2 1.1140567288147951
exp ma of ||w||^2 1.7994915967111942
||w|| 1.055488857740713
exp ma of ||w|| 1.3093860149849148
||w||^2 2.030034571283255
exp ma of ||w||^2 1.7467318445227356
||w|| 1.4247928169678759
exp ma of ||w|| 1.2909158442616733
cuda
Objective function 77.59 = squared loss an data 62.61 + 0.5*rho*h**2 11.321592 + alpha*h 1.903982 + L2reg 1.51 + L1reg 0.25 ; SHD = 100 ; DAG True
Proportion of microbatches that were clipped  0.7752464049119405
iteration 3 in inner loop, alpha 40.012348039716 rho 10000.0 h 0.047584854448899705
iteration 3 in outer loop, alpha = 515.8608925287131, rho = 10000.0, h = 0.047584854448899705
cuda
9630
cuda
Objective function 100.24 = squared loss an data 62.61 + 0.5*rho*h**2 11.321592 + alpha*h 24.547165 + L2reg 1.51 + L1reg 0.25 ; SHD = 100 ; DAG True
||w||^2 12.242950963609946
exp ma of ||w||^2 1621.0591689316523
||w|| 3.498992849894087
exp ma of ||w|| 6.222503786810161
||w||^2 11.990736570123026
exp ma of ||w||^2 396.37873847819503
||w|| 3.462764296067959
exp ma of ||w|| 3.8278323793654585
||w||^2 3.720358170904286
exp ma of ||w||^2 41.3952692929213
||w|| 1.9288230014452559
exp ma of ||w|| 2.484928315611074
cuda
Objective function 84.66 = squared loss an data 62.06 + 0.5*rho*h**2 4.799499 + alpha*h 15.982531 + L2reg 1.59 + L1reg 0.24 ; SHD = 97 ; DAG True
Proportion of microbatches that were clipped  0.782532612153993
iteration 1 in inner loop, alpha 515.8608925287131 rho 10000.0 h 0.030982249659782468
9630
cuda
Objective function 127.86 = squared loss an data 62.06 + 0.5*rho*h**2 47.994990 + alpha*h 15.982531 + L2reg 1.59 + L1reg 0.24 ; SHD = 97 ; DAG True
||w||^2 6839105150833.084
exp ma of ||w||^2 4012269225291.303
||w|| 2615168.283463434
exp ma of ||w|| 1719163.4015634109
||w||^2 496.23777480470085
exp ma of ||w||^2 791173.0505354838
||w|| 22.2763950136619
exp ma of ||w|| 47.16357227123048
||w||^2 3.0875629882955304
exp ma of ||w||^2 4.419739221296265
||w|| 1.7571462626359624
exp ma of ||w|| 2.06634701324828
cuda
Objective function 84.77 = squared loss an data 64.06 + 0.5*rho*h**2 11.173100 + alpha*h 7.711421 + L2reg 1.62 + L1reg 0.21 ; SHD = 94 ; DAG True
Proportion of microbatches that were clipped  0.7813915857605178
iteration 2 in inner loop, alpha 515.8608925287131 rho 100000.0 h 0.014948645104201574
iteration 4 in outer loop, alpha = 15464.505996730288, rho = 1000000.0, h = 0.014948645104201574
Threshold 0.3
[[0.004 0.032 0.058 0.053 0.062 0.038 0.031 0.038 0.039 0.057 0.026 0.04
  0.058 0.124 0.043 0.017 0.104 0.04  0.166 0.029 0.114 0.033 0.055 0.031
  0.031 0.028 0.048 0.024 0.014 0.054]
 [0.194 0.003 0.077 0.057 0.081 0.035 0.072 0.03  0.048 0.081 0.038 0.078
  0.079 0.083 0.079 0.07  0.098 0.041 0.154 0.059 0.196 0.147 0.112 0.069
  0.055 0.074 0.103 0.127 0.048 0.044]
 [0.087 0.052 0.004 0.049 0.075 0.03  0.06  0.025 0.028 0.075 0.017 0.036
  0.057 0.014 0.056 0.06  0.087 0.062 0.117 0.031 0.077 0.086 0.054 0.031
  0.042 0.077 0.034 0.043 0.015 0.067]
 [0.095 0.083 0.09  0.004 0.133 0.082 0.096 0.062 0.164 0.085 0.043 0.15
  0.081 0.251 0.089 0.123 0.119 0.051 0.569 0.045 0.096 0.176 0.171 0.168
  0.134 0.066 0.128 0.129 0.028 0.055]
 [0.094 0.051 0.084 0.032 0.005 0.055 0.028 0.051 0.017 0.117 0.026 0.048
  0.089 0.053 0.027 0.044 0.048 0.032 0.105 0.031 0.213 0.039 0.054 0.078
  0.074 0.034 0.077 0.084 0.028 0.037]
 [0.164 0.156 0.252 0.096 0.119 0.005 0.086 0.111 0.135 0.12  0.037 0.166
  0.159 0.088 0.152 0.095 0.163 0.087 0.127 0.094 0.153 0.105 0.045 0.091
  0.054 0.074 0.06  0.092 0.067 0.105]
 [0.131 0.08  0.086 0.054 0.169 0.074 0.005 0.117 0.192 0.226 0.062 0.14
  0.068 0.241 0.067 0.205 0.057 0.066 0.182 0.097 0.267 0.061 0.156 0.106
  0.214 0.018 0.042 0.145 0.059 0.117]
 [0.189 0.158 0.171 0.079 0.165 0.064 0.054 0.002 0.104 0.104 0.057 0.069
  0.105 0.189 0.073 0.08  0.171 0.072 0.383 0.097 0.104 0.119 0.11  0.053
  0.069 0.045 0.083 0.05  0.154 0.069]
 [0.171 0.1   0.128 0.032 0.254 0.04  0.029 0.038 0.004 0.123 0.047 0.07
  0.062 0.073 0.09  0.139 0.329 0.082 0.409 0.115 0.262 0.192 0.084 0.05
  0.112 0.101 0.147 0.083 0.047 0.157]
 [0.109 0.077 0.074 0.041 0.039 0.046 0.023 0.031 0.026 0.004 0.009 0.041
  0.057 0.015 0.041 0.03  0.049 0.037 0.036 0.039 0.162 0.051 0.044 0.03
  0.018 0.009 0.036 0.026 0.035 0.064]
 [0.216 0.209 0.36  0.174 0.241 0.112 0.088 0.091 0.09  0.568 0.003 0.466
  0.231 0.422 0.08  0.212 0.097 0.11  0.422 0.248 0.23  0.146 0.382 0.069
  0.054 0.129 0.086 0.103 0.123 0.282]
 [0.137 0.051 0.204 0.046 0.097 0.034 0.028 0.067 0.053 0.162 0.015 0.005
  0.084 0.423 0.045 0.21  0.087 0.04  0.11  0.071 0.13  0.072 0.456 0.055
  0.069 0.075 0.061 0.039 0.048 0.029]
 [0.095 0.08  0.117 0.049 0.056 0.039 0.079 0.058 0.054 0.087 0.027 0.053
  0.006 0.068 0.114 0.062 0.03  0.05  0.122 0.081 0.092 0.116 0.081 0.033
  0.048 0.077 0.069 0.046 0.042 0.112]
 [0.038 0.064 0.512 0.031 0.117 0.067 0.013 0.029 0.059 0.402 0.014 0.016
  0.091 0.004 0.059 0.024 0.046 0.029 0.166 0.017 0.141 0.073 0.021 0.047
  0.029 0.077 0.034 0.068 0.023 0.051]
 [0.155 0.077 0.093 0.109 0.122 0.03  0.056 0.039 0.055 0.159 0.065 0.1
  0.04  0.091 0.003 0.055 0.13  0.07  0.234 0.099 0.18  0.089 0.125 0.061
  0.054 0.038 0.074 0.217 0.036 0.047]
 [0.194 0.055 0.124 0.066 0.117 0.044 0.027 0.059 0.041 0.198 0.024 0.032
  0.069 0.309 0.099 0.004 0.08  0.032 0.199 0.034 0.078 0.05  0.113 0.073
  0.029 0.034 0.044 0.112 0.077 0.09 ]
 [0.052 0.041 0.036 0.074 0.138 0.047 0.045 0.033 0.016 0.121 0.045 0.051
  0.146 0.086 0.03  0.049 0.004 0.04  0.155 0.063 0.137 0.057 0.065 0.041
  0.062 0.04  0.043 0.057 0.034 0.026]
 [0.175 0.174 0.076 0.129 0.166 0.054 0.118 0.083 0.085 0.103 0.056 0.118
  0.075 0.105 0.089 0.122 0.134 0.003 0.119 0.072 0.14  0.083 0.145 0.172
  0.092 0.049 0.136 0.16  0.061 0.088]
 [0.041 0.047 0.048 0.007 0.052 0.038 0.018 0.02  0.013 0.137 0.014 0.034
  0.042 0.038 0.026 0.034 0.028 0.037 0.004 0.029 0.087 0.04  0.024 0.017
  0.028 0.059 0.034 0.044 0.032 0.04 ]
 [0.207 0.072 0.197 0.087 0.18  0.073 0.064 0.037 0.059 0.11  0.034 0.113
  0.065 0.378 0.055 0.165 0.057 0.084 0.186 0.003 0.383 0.117 0.214 0.086
  0.054 0.073 0.088 0.097 0.054 0.08 ]
 [0.041 0.038 0.056 0.037 0.03  0.014 0.015 0.051 0.025 0.045 0.013 0.031
  0.057 0.041 0.033 0.055 0.043 0.043 0.055 0.017 0.004 0.049 0.04  0.019
  0.027 0.021 0.035 0.048 0.02  0.042]
 [0.202 0.031 0.058 0.05  0.143 0.041 0.067 0.043 0.034 0.071 0.031 0.033
  0.052 0.062 0.073 0.067 0.091 0.059 0.099 0.037 0.094 0.004 0.083 0.067
  0.04  0.06  0.04  0.06  0.028 0.06 ]
 [0.056 0.038 0.111 0.034 0.091 0.061 0.035 0.049 0.038 0.12  0.013 0.011
  0.061 0.209 0.03  0.078 0.075 0.047 0.207 0.032 0.13  0.101 0.006 0.042
  0.034 0.045 0.006 0.042 0.033 0.026]
 [0.168 0.073 0.139 0.047 0.055 0.063 0.035 0.083 0.095 0.17  0.064 0.085
  0.121 0.103 0.066 0.058 0.106 0.023 0.212 0.095 0.293 0.089 0.126 0.003
  0.119 0.064 0.078 0.07  0.028 0.092]
 [0.172 0.099 0.089 0.042 0.088 0.073 0.034 0.078 0.051 0.33  0.073 0.068
  0.135 0.249 0.113 0.14  0.071 0.046 0.123 0.069 0.147 0.087 0.099 0.054
  0.005 0.09  0.116 0.161 0.099 0.062]
 [0.21  0.066 0.086 0.09  0.09  0.089 0.182 0.128 0.048 0.443 0.05  0.086
  0.065 0.109 0.135 0.184 0.13  0.097 0.079 0.05  0.24  0.123 0.108 0.095
  0.084 0.004 0.069 0.169 0.053 0.046]
 [0.103 0.081 0.1   0.052 0.049 0.059 0.097 0.061 0.041 0.153 0.075 0.074
  0.056 0.127 0.081 0.103 0.126 0.05  0.161 0.052 0.14  0.124 0.702 0.098
  0.039 0.039 0.005 0.131 0.058 0.045]
 [0.154 0.045 0.107 0.051 0.03  0.075 0.038 0.087 0.071 0.208 0.042 0.144
  0.093 0.091 0.026 0.044 0.102 0.025 0.114 0.062 0.149 0.136 0.133 0.079
  0.045 0.03  0.032 0.003 0.028 0.047]
 [0.218 0.144 0.202 0.173 0.183 0.108 0.078 0.054 0.056 0.182 0.068 0.14
  0.091 0.273 0.177 0.069 0.114 0.113 0.131 0.101 0.188 0.166 0.08  0.228
  0.055 0.079 0.098 0.188 0.005 0.206]
 [0.096 0.09  0.086 0.089 0.166 0.054 0.055 0.069 0.036 0.095 0.021 0.175
  0.036 0.075 0.136 0.058 0.141 0.063 0.087 0.069 0.151 0.109 0.131 0.05
  0.075 0.117 0.103 0.097 0.027 0.005]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.569 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.383 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.329 0.    0.409 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.36  0.    0.    0.    0.    0.    0.    0.568 0.    0.466
  0.    0.422 0.    0.    0.    0.    0.422 0.    0.    0.    0.382 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.423 0.    0.    0.    0.    0.    0.    0.    0.    0.456 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.512 0.    0.    0.    0.    0.    0.    0.402 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.309 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.378 0.    0.    0.    0.    0.    0.    0.383 0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.33  0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.443 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.702 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.7, 'tpr': 0.06666666666666667, 'fpr': 0.04057971014492753, 'f1': 0.1090909090909091, 'shd': 94, 'npred': 20, 'ntrue': 90}
[0.032 0.058 0.053 0.062 0.038 0.031 0.038 0.039 0.057 0.026 0.04  0.058
 0.124 0.043 0.017 0.104 0.04  0.166 0.029 0.114 0.033 0.055 0.031 0.031
 0.028 0.048 0.024 0.014 0.054 0.194 0.077 0.057 0.081 0.035 0.072 0.03
 0.048 0.081 0.038 0.078 0.079 0.083 0.079 0.07  0.098 0.041 0.154 0.059
 0.196 0.147 0.112 0.069 0.055 0.074 0.103 0.127 0.048 0.044 0.087 0.052
 0.049 0.075 0.03  0.06  0.025 0.028 0.075 0.017 0.036 0.057 0.014 0.056
 0.06  0.087 0.062 0.117 0.031 0.077 0.086 0.054 0.031 0.042 0.077 0.034
 0.043 0.015 0.067 0.095 0.083 0.09  0.133 0.082 0.096 0.062 0.164 0.085
 0.043 0.15  0.081 0.251 0.089 0.123 0.119 0.051 0.569 0.045 0.096 0.176
 0.171 0.168 0.134 0.066 0.128 0.129 0.028 0.055 0.094 0.051 0.084 0.032
 0.055 0.028 0.051 0.017 0.117 0.026 0.048 0.089 0.053 0.027 0.044 0.048
 0.032 0.105 0.031 0.213 0.039 0.054 0.078 0.074 0.034 0.077 0.084 0.028
 0.037 0.164 0.156 0.252 0.096 0.119 0.086 0.111 0.135 0.12  0.037 0.166
 0.159 0.088 0.152 0.095 0.163 0.087 0.127 0.094 0.153 0.105 0.045 0.091
 0.054 0.074 0.06  0.092 0.067 0.105 0.131 0.08  0.086 0.054 0.169 0.074
 0.117 0.192 0.226 0.062 0.14  0.068 0.241 0.067 0.205 0.057 0.066 0.182
 0.097 0.267 0.061 0.156 0.106 0.214 0.018 0.042 0.145 0.059 0.117 0.189
 0.158 0.171 0.079 0.165 0.064 0.054 0.104 0.104 0.057 0.069 0.105 0.189
 0.073 0.08  0.171 0.072 0.383 0.097 0.104 0.119 0.11  0.053 0.069 0.045
 0.083 0.05  0.154 0.069 0.171 0.1   0.128 0.032 0.254 0.04  0.029 0.038
 0.123 0.047 0.07  0.062 0.073 0.09  0.139 0.329 0.082 0.409 0.115 0.262
 0.192 0.084 0.05  0.112 0.101 0.147 0.083 0.047 0.157 0.109 0.077 0.074
 0.041 0.039 0.046 0.023 0.031 0.026 0.009 0.041 0.057 0.015 0.041 0.03
 0.049 0.037 0.036 0.039 0.162 0.051 0.044 0.03  0.018 0.009 0.036 0.026
 0.035 0.064 0.216 0.209 0.36  0.174 0.241 0.112 0.088 0.091 0.09  0.568
 0.466 0.231 0.422 0.08  0.212 0.097 0.11  0.422 0.248 0.23  0.146 0.382
 0.069 0.054 0.129 0.086 0.103 0.123 0.282 0.137 0.051 0.204 0.046 0.097
 0.034 0.028 0.067 0.053 0.162 0.015 0.084 0.423 0.045 0.21  0.087 0.04
 0.11  0.071 0.13  0.072 0.456 0.055 0.069 0.075 0.061 0.039 0.048 0.029
 0.095 0.08  0.117 0.049 0.056 0.039 0.079 0.058 0.054 0.087 0.027 0.053
 0.068 0.114 0.062 0.03  0.05  0.122 0.081 0.092 0.116 0.081 0.033 0.048
 0.077 0.069 0.046 0.042 0.112 0.038 0.064 0.512 0.031 0.117 0.067 0.013
 0.029 0.059 0.402 0.014 0.016 0.091 0.059 0.024 0.046 0.029 0.166 0.017
 0.141 0.073 0.021 0.047 0.029 0.077 0.034 0.068 0.023 0.051 0.155 0.077
 0.093 0.109 0.122 0.03  0.056 0.039 0.055 0.159 0.065 0.1   0.04  0.091
 0.055 0.13  0.07  0.234 0.099 0.18  0.089 0.125 0.061 0.054 0.038 0.074
 0.217 0.036 0.047 0.194 0.055 0.124 0.066 0.117 0.044 0.027 0.059 0.041
 0.198 0.024 0.032 0.069 0.309 0.099 0.08  0.032 0.199 0.034 0.078 0.05
 0.113 0.073 0.029 0.034 0.044 0.112 0.077 0.09  0.052 0.041 0.036 0.074
 0.138 0.047 0.045 0.033 0.016 0.121 0.045 0.051 0.146 0.086 0.03  0.049
 0.04  0.155 0.063 0.137 0.057 0.065 0.041 0.062 0.04  0.043 0.057 0.034
 0.026 0.175 0.174 0.076 0.129 0.166 0.054 0.118 0.083 0.085 0.103 0.056
 0.118 0.075 0.105 0.089 0.122 0.134 0.119 0.072 0.14  0.083 0.145 0.172
 0.092 0.049 0.136 0.16  0.061 0.088 0.041 0.047 0.048 0.007 0.052 0.038
 0.018 0.02  0.013 0.137 0.014 0.034 0.042 0.038 0.026 0.034 0.028 0.037
 0.029 0.087 0.04  0.024 0.017 0.028 0.059 0.034 0.044 0.032 0.04  0.207
 0.072 0.197 0.087 0.18  0.073 0.064 0.037 0.059 0.11  0.034 0.113 0.065
 0.378 0.055 0.165 0.057 0.084 0.186 0.383 0.117 0.214 0.086 0.054 0.073
 0.088 0.097 0.054 0.08  0.041 0.038 0.056 0.037 0.03  0.014 0.015 0.051
 0.025 0.045 0.013 0.031 0.057 0.041 0.033 0.055 0.043 0.043 0.055 0.017
 0.049 0.04  0.019 0.027 0.021 0.035 0.048 0.02  0.042 0.202 0.031 0.058
 0.05  0.143 0.041 0.067 0.043 0.034 0.071 0.031 0.033 0.052 0.062 0.073
 0.067 0.091 0.059 0.099 0.037 0.094 0.083 0.067 0.04  0.06  0.04  0.06
 0.028 0.06  0.056 0.038 0.111 0.034 0.091 0.061 0.035 0.049 0.038 0.12
 0.013 0.011 0.061 0.209 0.03  0.078 0.075 0.047 0.207 0.032 0.13  0.101
 0.042 0.034 0.045 0.006 0.042 0.033 0.026 0.168 0.073 0.139 0.047 0.055
 0.063 0.035 0.083 0.095 0.17  0.064 0.085 0.121 0.103 0.066 0.058 0.106
 0.023 0.212 0.095 0.293 0.089 0.126 0.119 0.064 0.078 0.07  0.028 0.092
 0.172 0.099 0.089 0.042 0.088 0.073 0.034 0.078 0.051 0.33  0.073 0.068
 0.135 0.249 0.113 0.14  0.071 0.046 0.123 0.069 0.147 0.087 0.099 0.054
 0.09  0.116 0.161 0.099 0.062 0.21  0.066 0.086 0.09  0.09  0.089 0.182
 0.128 0.048 0.443 0.05  0.086 0.065 0.109 0.135 0.184 0.13  0.097 0.079
 0.05  0.24  0.123 0.108 0.095 0.084 0.069 0.169 0.053 0.046 0.103 0.081
 0.1   0.052 0.049 0.059 0.097 0.061 0.041 0.153 0.075 0.074 0.056 0.127
 0.081 0.103 0.126 0.05  0.161 0.052 0.14  0.124 0.702 0.098 0.039 0.039
 0.131 0.058 0.045 0.154 0.045 0.107 0.051 0.03  0.075 0.038 0.087 0.071
 0.208 0.042 0.144 0.093 0.091 0.026 0.044 0.102 0.025 0.114 0.062 0.149
 0.136 0.133 0.079 0.045 0.03  0.032 0.028 0.047 0.218 0.144 0.202 0.173
 0.183 0.108 0.078 0.054 0.056 0.182 0.068 0.14  0.091 0.273 0.177 0.069
 0.114 0.113 0.131 0.101 0.188 0.166 0.08  0.228 0.055 0.079 0.098 0.188
 0.206 0.096 0.09  0.086 0.089 0.166 0.054 0.055 0.069 0.036 0.095 0.021
 0.175 0.036 0.075 0.136 0.058 0.141 0.063 0.087 0.069 0.151 0.109 0.131
 0.05  0.075 0.117 0.103 0.097 0.027]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 1. 0. 0. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.5477065527065527, 0.16346109656594396)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
