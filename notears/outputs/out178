samples  5000  graph  20 60 ER mim  minibatch size  100  noise  1.0  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.09213537717594988
iteration 3 in outer loop, alpha = 15.699588471753614, rho = 100.0, h = 0.09213537717594988
cuda
iteration 1 in inner loop,alpha 15.699588471753614 rho 100.0 h 0.05367211647120129
iteration 2 in inner loop,alpha 15.699588471753614 rho 1000.0 h 0.020112090838999563
iteration 4 in outer loop, alpha = 35.81167931075318, rho = 1000.0, h = 0.020112090838999563
cuda
iteration 1 in inner loop,alpha 35.81167931075318 rho 1000.0 h 0.01121992358910262
iteration 2 in inner loop,alpha 35.81167931075318 rho 10000.0 h 0.0037097766179101654
iteration 5 in outer loop, alpha = 72.90944548985483, rho = 10000.0, h = 0.0037097766179101654
cuda
iteration 1 in inner loop,alpha 72.90944548985483 rho 10000.0 h 0.001669049753473928
iteration 2 in inner loop,alpha 72.90944548985483 rho 100000.0 h 0.0005919139467991386
iteration 6 in outer loop, alpha = 132.10084016976867, rho = 100000.0, h = 0.0005919139467991386
cuda
iteration 1 in inner loop,alpha 132.10084016976867 rho 100000.0 h 0.00035712315776237347
iteration 7 in outer loop, alpha = 489.22399793214214, rho = 1000000.0, h = 0.00035712315776237347
Threshold 0.3
[[0.    0.038 0.01  0.044 0.155 0.175 0.206 0.003 0.035 0.045 0.449 0.853
  0.971 0.    0.103 0.005 0.19  2.558 0.512 0.022]
 [0.    0.001 0.    0.038 0.084 0.459 0.121 0.    0.005 0.058 0.11  0.
  0.038 0.    0.073 0.136 0.006 0.    0.084 0.803]
 [0.007 0.288 0.002 0.123 0.547 0.087 0.064 0.    1.436 0.707 0.113 0.011
  0.028 0.    0.034 0.213 0.103 0.032 0.135 0.115]
 [0.    0.007 0.    0.002 0.001 0.005 0.    0.    0.    0.011 0.129 0.
  0.    0.    0.051 0.    0.    0.    0.002 0.031]
 [0.    0.003 0.    0.167 0.002 0.008 0.005 0.    0.    0.049 0.143 0.
  0.022 0.    0.048 0.121 0.004 0.    0.001 0.012]
 [0.    0.002 0.006 0.097 0.032 0.005 0.009 0.    0.026 0.02  0.054 0.
  0.022 0.    1.182 0.068 0.007 0.001 0.29  0.048]
 [0.    0.005 0.005 0.213 0.282 0.24  0.003 0.    0.015 0.171 0.399 0.001
  0.388 0.    0.09  0.079 0.001 0.    0.01  0.148]
 [0.007 0.101 0.003 1.097 0.044 0.05  0.068 0.    0.343 0.075 0.063 2.265
  1.239 0.048 0.109 0.018 0.011 2.156 0.046 0.034]
 [0.    0.027 0.    1.438 1.398 0.02  0.093 0.    0.003 0.348 1.136 0.
  0.059 0.    0.412 0.136 0.027 0.    0.006 0.018]
 [0.    0.002 0.001 0.031 0.004 0.212 0.005 0.    0.005 0.004 1.071 0.
  0.003 0.    0.106 0.008 0.003 0.    0.008 0.664]
 [0.    0.002 0.    0.001 0.    0.012 0.003 0.    0.001 0.001 0.002 0.
  0.002 0.    0.013 0.005 0.003 0.    0.003 0.749]
 [0.    2.089 0.003 1.586 1.303 0.062 0.602 0.    1.024 0.074 0.94  0.001
  0.229 0.    0.007 0.898 0.459 0.    0.068 0.142]
 [0.    0.004 0.002 0.175 0.036 0.091 0.004 0.    0.    0.498 0.048 0.005
  0.003 0.    1.583 0.146 0.002 0.    0.002 0.037]
 [0.001 1.036 4.607 0.094 0.05  0.161 0.078 0.    0.818 0.17  0.04  0.017
  0.04  0.    0.062 0.656 0.012 0.073 0.702 1.197]
 [0.    0.001 0.    0.013 0.011 0.001 0.001 0.    0.002 0.004 0.011 0.
  0.    0.    0.002 0.013 0.001 0.    0.001 0.003]
 [0.    0.001 0.002 0.072 0.004 0.015 0.002 0.    0.004 0.279 0.047 0.
  0.008 0.    0.034 0.003 0.003 0.    0.001 0.103]
 [0.    0.016 0.006 0.982 0.092 0.007 0.926 0.    0.022 0.015 0.099 0.001
  0.051 0.001 0.552 0.009 0.002 0.    0.01  0.055]
 [0.    0.041 0.003 0.296 0.045 0.05  0.017 0.    0.108 0.047 0.116 0.721
  1.645 0.    0.003 0.065 0.006 0.002 0.023 1.247]
 [0.    0.001 0.01  0.092 0.775 0.016 0.027 0.    0.407 0.182 0.153 0.001
  0.014 0.    0.69  1.018 0.011 0.013 0.004 0.067]
 [0.    0.    0.    0.003 0.001 0.016 0.001 0.    0.001 0.001 0.001 0.
  0.002 0.    0.05  0.001 0.002 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.449 0.853
  0.971 0.    0.    0.    0.    2.558 0.512 0.   ]
 [0.    0.    0.    0.    0.    0.459 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.803]
 [0.    0.    0.    0.    0.547 0.    0.    0.    1.436 0.707 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.182 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.399 0.
  0.388 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.097 0.    0.    0.    0.    0.343 0.    0.    2.265
  1.239 0.    0.    0.    0.    2.156 0.    0.   ]
 [0.    0.    0.    1.438 1.398 0.    0.    0.    0.    0.348 1.136 0.
  0.    0.    0.412 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.071 0.
  0.    0.    0.    0.    0.    0.    0.    0.664]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.749]
 [0.    2.089 0.    1.586 1.303 0.    0.602 0.    1.024 0.    0.94  0.
  0.    0.    0.    0.898 0.459 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.498 0.    0.
  0.    0.    1.583 0.    0.    0.    0.    0.   ]
 [0.    1.036 4.607 0.    0.    0.    0.    0.    0.818 0.    0.    0.
  0.    0.    0.    0.656 0.    0.    0.702 1.197]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.982 0.    0.    0.926 0.    0.    0.    0.    0.
  0.    0.    0.552 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.721
  1.645 0.    0.    0.    0.    0.    0.    1.247]
 [0.    0.    0.    0.    0.775 0.    0.    0.    0.407 0.    0.    0.
  0.    0.    0.69  1.018 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.11538461538461539, 'tpr': 0.7666666666666667, 'fpr': 0.046153846153846156, 'f1': 0.8214285714285715, 'shd': 16, 'npred': 52, 'ntrue': 60}
[3.779e-02 1.033e-02 4.415e-02 1.552e-01 1.747e-01 2.060e-01 3.120e-03
 3.487e-02 4.462e-02 4.488e-01 8.530e-01 9.714e-01 1.543e-04 1.032e-01
 4.637e-03 1.902e-01 2.558e+00 5.120e-01 2.212e-02 1.453e-05 3.336e-04
 3.772e-02 8.443e-02 4.590e-01 1.215e-01 2.710e-06 5.179e-03 5.755e-02
 1.101e-01 1.001e-04 3.849e-02 2.063e-05 7.347e-02 1.365e-01 5.939e-03
 9.156e-05 8.369e-02 8.031e-01 7.232e-03 2.875e-01 1.229e-01 5.467e-01
 8.721e-02 6.439e-02 1.583e-04 1.436e+00 7.068e-01 1.126e-01 1.144e-02
 2.787e-02 2.951e-05 3.367e-02 2.133e-01 1.031e-01 3.214e-02 1.345e-01
 1.149e-01 5.094e-05 7.118e-03 4.921e-05 1.371e-03 5.036e-03 3.221e-04
 3.328e-06 3.674e-04 1.073e-02 1.292e-01 1.372e-04 3.628e-04 3.390e-06
 5.109e-02 1.777e-04 4.111e-04 6.278e-05 2.103e-03 3.137e-02 3.458e-05
 3.357e-03 1.617e-04 1.667e-01 8.476e-03 4.584e-03 4.443e-06 4.882e-04
 4.913e-02 1.432e-01 1.679e-04 2.151e-02 6.268e-06 4.831e-02 1.213e-01
 4.197e-03 1.888e-04 6.591e-04 1.226e-02 6.854e-05 1.769e-03 5.629e-03
 9.655e-02 3.221e-02 9.316e-03 4.171e-05 2.622e-02 1.973e-02 5.370e-02
 3.638e-04 2.199e-02 2.664e-04 1.182e+00 6.762e-02 6.916e-03 8.374e-04
 2.900e-01 4.794e-02 8.733e-05 5.304e-03 5.445e-03 2.128e-01 2.824e-01
 2.396e-01 2.021e-05 1.514e-02 1.705e-01 3.985e-01 1.196e-03 3.879e-01
 1.732e-04 9.020e-02 7.936e-02 1.233e-03 4.247e-04 1.000e-02 1.477e-01
 6.786e-03 1.010e-01 3.261e-03 1.097e+00 4.408e-02 4.960e-02 6.807e-02
 3.435e-01 7.542e-02 6.327e-02 2.265e+00 1.239e+00 4.802e-02 1.093e-01
 1.841e-02 1.119e-02 2.156e+00 4.639e-02 3.351e-02 1.831e-05 2.656e-02
 2.184e-04 1.438e+00 1.398e+00 1.950e-02 9.326e-02 2.861e-05 3.482e-01
 1.136e+00 3.254e-04 5.891e-02 7.938e-06 4.123e-01 1.362e-01 2.670e-02
 3.014e-04 6.123e-03 1.834e-02 2.326e-05 2.396e-03 8.391e-04 3.140e-02
 4.408e-03 2.124e-01 4.698e-03 1.706e-05 5.469e-03 1.071e+00 4.788e-04
 2.812e-03 1.100e-05 1.059e-01 8.030e-03 2.972e-03 2.777e-04 7.822e-03
 6.643e-01 2.350e-05 2.119e-03 2.072e-04 1.418e-03 4.901e-04 1.172e-02
 3.242e-03 1.254e-05 5.154e-04 8.448e-04 1.833e-04 1.668e-03 6.010e-06
 1.286e-02 4.857e-03 2.952e-03 2.604e-04 3.157e-03 7.494e-01 7.198e-05
 2.089e+00 3.054e-03 1.586e+00 1.303e+00 6.206e-02 6.020e-01 1.386e-05
 1.024e+00 7.356e-02 9.398e-01 2.286e-01 2.583e-04 6.656e-03 8.977e-01
 4.595e-01 4.449e-04 6.842e-02 1.415e-01 6.051e-06 4.236e-03 1.956e-03
 1.748e-01 3.586e-02 9.088e-02 3.900e-03 1.027e-05 3.457e-04 4.977e-01
 4.784e-02 4.524e-03 1.300e-04 1.583e+00 1.460e-01 2.329e-03 1.273e-04
 2.482e-03 3.703e-02 1.452e-03 1.036e+00 4.607e+00 9.416e-02 5.002e-02
 1.608e-01 7.770e-02 3.219e-04 8.177e-01 1.702e-01 3.955e-02 1.739e-02
 3.974e-02 6.250e-02 6.559e-01 1.192e-02 7.339e-02 7.020e-01 1.197e+00
 2.359e-06 8.411e-04 2.467e-04 1.257e-02 1.059e-02 8.153e-04 5.815e-04
 4.341e-06 1.687e-03 3.896e-03 1.107e-02 9.860e-05 4.683e-04 1.490e-05
 1.287e-02 5.782e-04 5.525e-05 1.249e-03 3.113e-03 2.730e-05 7.555e-04
 1.972e-03 7.237e-02 4.443e-03 1.469e-02 2.279e-03 9.528e-05 4.055e-03
 2.794e-01 4.732e-02 4.986e-04 7.654e-03 1.391e-05 3.372e-02 3.429e-03
 1.858e-04 1.492e-03 1.026e-01 4.508e-05 1.557e-02 5.663e-03 9.816e-01
 9.155e-02 7.447e-03 9.258e-01 2.384e-04 2.222e-02 1.541e-02 9.888e-02
 1.156e-03 5.085e-02 5.937e-04 5.523e-01 9.344e-03 1.554e-04 1.043e-02
 5.460e-02 3.423e-06 4.091e-02 3.403e-03 2.958e-01 4.501e-02 4.986e-02
 1.719e-02 2.458e-05 1.079e-01 4.686e-02 1.156e-01 7.211e-01 1.645e+00
 2.030e-04 3.151e-03 6.528e-02 5.804e-03 2.334e-02 1.247e+00 1.385e-04
 1.032e-03 9.885e-03 9.195e-02 7.747e-01 1.571e-02 2.670e-02 1.046e-04
 4.065e-01 1.816e-01 1.529e-01 9.820e-04 1.377e-02 1.720e-04 6.903e-01
 1.018e+00 1.123e-02 1.291e-02 6.698e-02 3.553e-06 3.209e-04 1.356e-04
 2.861e-03 1.033e-03 1.590e-02 9.623e-04 5.338e-06 7.573e-04 6.040e-04
 7.490e-04 2.633e-05 1.542e-03 1.791e-05 4.992e-02 7.014e-04 1.539e-03
 3.732e-04 1.994e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.91046875, 0.8411162813434908)
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
total norm for a microbatch 18.602761338283063 clip 9.658064220834715
total norm for a microbatch 12.874034881341663 clip 12.105367776663085
total norm for a microbatch 19.272882591684905 clip 11.850477109767564
total norm for a microbatch 13.72159082221802 clip 12.339523516052312
total norm for a microbatch 13.936929723254243 clip 11.895945606449347
cuda
Objective function 20.06 = squared loss an data 18.61 + 0.5*rho*h**2 0.980576 + alpha*h 0.000000 + L2reg 0.23 + L1reg 0.24 ; SHD = 59 ; DAG False
Proportion of microbatches that were clipped  0.8784213876511776
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.400411380457907
iteration 1 in outer loop, alpha = 1.400411380457907, rho = 1.0, h = 1.400411380457907
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 22.02 = squared loss an data 18.61 + 0.5*rho*h**2 0.980576 + alpha*h 1.961152 + L2reg 0.23 + L1reg 0.24 ; SHD = 59 ; DAG False
total norm for a microbatch 14.528378205515882 clip 5.796128526196119
total norm for a microbatch 14.148031642017814 clip 7.00071832024005
total norm for a microbatch 14.015598243200916 clip 14.100033310238105
total norm for a microbatch 22.75380344885635 clip 14.360934063481892
total norm for a microbatch 12.34940412031461 clip 14.589071338432978
cuda
Objective function 19.36 = squared loss an data 16.83 + 0.5*rho*h**2 0.511812 + alpha*h 1.416856 + L2reg 0.38 + L1reg 0.22 ; SHD = 49 ; DAG False
Proportion of microbatches that were clipped  0.8816088225754135
iteration 1 in inner loop, alpha 1.400411380457907 rho 1.0 h 1.0117429635042647
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 23.97 = squared loss an data 16.83 + 0.5*rho*h**2 5.118119 + alpha*h 1.416856 + L2reg 0.38 + L1reg 0.22 ; SHD = 49 ; DAG False
total norm for a microbatch 22.608722272920126 clip 3.856639643488005
total norm for a microbatch 13.947566503851355 clip 15.869339571148664
total norm for a microbatch 30.642259607398387 clip 16.813934650014453
total norm for a microbatch 16.100347931627706 clip 17.527636002690013
total norm for a microbatch 21.522199930131844 clip 17.90380674256187
cuda
Objective function 19.57 = squared loss an data 17.41 + 0.5*rho*h**2 0.903644 + alpha*h 0.595346 + L2reg 0.47 + L1reg 0.19 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.8892728129417308
iteration 2 in inner loop, alpha 1.400411380457907 rho 10.0 h 0.42512201213887835
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 27.71 = squared loss an data 17.41 + 0.5*rho*h**2 9.036436 + alpha*h 0.595346 + L2reg 0.47 + L1reg 0.19 ; SHD = 46 ; DAG True
total norm for a microbatch 25.568770196526405 clip 1.7191142432582052
total norm for a microbatch 19.750834851903516 clip 18.55111337524481
total norm for a microbatch 28.810730208022303 clip 19.40846607827097
total norm for a microbatch 24.781903323911532 clip 19.17629752648987
cuda
Objective function 20.23 = squared loss an data 18.44 + 0.5*rho*h**2 0.897157 + alpha*h 0.187588 + L2reg 0.54 + L1reg 0.17 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.8965786595844399
iteration 3 in inner loop, alpha 1.400411380457907 rho 100.0 h 0.1339520369794016
iteration 2 in outer loop, alpha = 14.795615078398068, rho = 100.0, h = 0.1339520369794016
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 22.03 = squared loss an data 18.44 + 0.5*rho*h**2 0.897157 + alpha*h 1.981903 + L2reg 0.54 + L1reg 0.17 ; SHD = 40 ; DAG True
total norm for a microbatch 32.25683894749199 clip 1.7234391714497566
total norm for a microbatch 29.879318714012143 clip 1.7234391714497566
total norm for a microbatch 18.170253091187 clip 2.4961687087424425
total norm for a microbatch 24.71439024514244 clip 6.133355975213887
total norm for a microbatch 34.74311333811497 clip 7.901057120076323
total norm for a microbatch 44.41145523054676 clip 13.550583191110325
total norm for a microbatch 28.537973413306048 clip 16.08353194574484
total norm for a microbatch 18.91681218429805 clip 20.7474378705459
total norm for a microbatch 28.11174004275671 clip 20.654893955039398
total norm for a microbatch 28.002460442047433 clip 21.0676004180135
cuda
Objective function 21.19 = squared loss an data 18.55 + 0.5*rho*h**2 0.447673 + alpha*h 1.400002 + L2reg 0.62 + L1reg 0.17 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.8939121919373266
iteration 1 in inner loop, alpha 14.795615078398068 rho 100.0 h 0.09462275970060219
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 25.22 = squared loss an data 18.55 + 0.5*rho*h**2 4.476733 + alpha*h 1.400002 + L2reg 0.62 + L1reg 0.17 ; SHD = 51 ; DAG True
total norm for a microbatch 22.004483184040364 clip 2.08949831268089
total norm for a microbatch 20.66198736077972 clip 6.143904674117205
total norm for a microbatch 27.935467557695432 clip 13.714537978693137
cuda
Objective function 21.24 = squared loss an data 19.04 + 0.5*rho*h**2 0.802377 + alpha*h 0.592703 + L2reg 0.66 + L1reg 0.15 ; SHD = 52 ; DAG True
Proportion of microbatches that were clipped  0.8971783835485414
iteration 2 in inner loop, alpha 14.795615078398068 rho 1000.0 h 0.040059380215801355
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 28.47 = squared loss an data 19.04 + 0.5*rho*h**2 8.023770 + alpha*h 0.592703 + L2reg 0.66 + L1reg 0.15 ; SHD = 52 ; DAG True
total norm for a microbatch 30.540933486028987 clip 5.06717997469573
total norm for a microbatch 30.68755915917474 clip 25.741198421640647
total norm for a microbatch 51.17484699017808 clip 26.675891552094107
total norm for a microbatch 25.962266267284082 clip 26.250674777408115
total norm for a microbatch 25.683556411480804 clip 25.8694451620105
cuda
Objective function 21.41 = squared loss an data 19.34 + 0.5*rho*h**2 1.037902 + alpha*h 0.213170 + L2reg 0.67 + L1reg 0.14 ; SHD = 53 ; DAG True
Proportion of microbatches that were clipped  0.9031164217665105
iteration 3 in inner loop, alpha 14.795615078398068 rho 10000.0 h 0.01440765396510102
iteration 3 in outer loop, alpha = 158.87215472940827, rho = 10000.0, h = 0.01440765396510102
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 23.49 = squared loss an data 19.34 + 0.5*rho*h**2 1.037902 + alpha*h 2.288975 + L2reg 0.67 + L1reg 0.14 ; SHD = 53 ; DAG True
total norm for a microbatch 51.19418644277807 clip 1.0
total norm for a microbatch 58.81485724715758 clip 1.401524849263198
total norm for a microbatch 25.871620112675036 clip 24.48492849976962
total norm for a microbatch 35.483529341203436 clip 28.740055573671924
total norm for a microbatch 36.16704043559891 clip 29.769756411199957
cuda
Objective function 22.01 = squared loss an data 19.26 + 0.5*rho*h**2 0.418481 + alpha*h 1.453452 + L2reg 0.74 + L1reg 0.15 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.9113198535265086
iteration 1 in inner loop, alpha 158.87215472940827 rho 10000.0 h 0.009148562059063892
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 25.78 = squared loss an data 19.26 + 0.5*rho*h**2 4.184809 + alpha*h 1.453452 + L2reg 0.74 + L1reg 0.15 ; SHD = 54 ; DAG True
total norm for a microbatch 92.7078785186898 clip 1.0
total norm for a microbatch 142.11650237469746 clip 1.7444983489094892
total norm for a microbatch 41.490947856832626 clip 10.755457944377467
total norm for a microbatch 46.1865936882796 clip 20.31047702811882
total norm for a microbatch 54.650377483019994 clip 47.51615578761535
cuda
Objective function 23.01 = squared loss an data 19.67 + 0.5*rho*h**2 1.548117 + alpha*h 0.884025 + L2reg 0.75 + L1reg 0.15 ; SHD = 59 ; DAG True
Proportion of microbatches that were clipped  0.9273997076498295
iteration 2 in inner loop, alpha 158.87215472940827 rho 100000.0 h 0.005564380962329807
iteration 4 in outer loop, alpha = 5723.253117059215, rho = 1000000.0, h = 0.005564380962329807
Threshold 0.3
[[0.003 0.115 0.441 0.334 0.392 0.244 0.397 0.049 0.304 0.49  0.398 0.169
  0.721 0.108 0.391 0.474 0.408 0.507 0.62  0.38 ]
 [0.031 0.004 0.2   0.121 0.269 0.537 0.041 0.012 0.067 0.337 0.226 0.252
  0.066 0.026 0.189 0.272 0.221 0.055 0.122 0.391]
 [0.01  0.021 0.003 0.053 0.161 0.02  0.027 0.011 0.02  0.222 0.038 0.052
  0.014 0.003 0.052 0.15  0.093 0.085 0.043 0.057]
 [0.015 0.033 0.098 0.005 0.022 0.05  0.033 0.01  0.006 0.053 0.024 0.009
  0.059 0.004 0.099 0.104 0.016 0.062 0.036 0.098]
 [0.012 0.015 0.03  0.11  0.004 0.018 0.017 0.004 0.005 0.061 0.094 0.011
  0.037 0.008 0.078 0.068 0.037 0.025 0.009 0.044]
 [0.015 0.009 0.238 0.082 0.266 0.003 0.011 0.016 0.037 0.239 0.241 0.025
  0.033 0.024 0.508 0.111 0.173 0.05  0.018 0.21 ]
 [0.009 0.127 0.119 0.122 0.283 0.403 0.004 0.033 0.052 0.372 0.308 0.247
  0.028 0.036 0.258 0.282 0.651 0.114 0.114 0.296]
 [0.139 0.413 0.32  0.414 0.608 0.308 0.186 0.003 0.157 0.419 0.387 0.876
  0.533 0.083 0.505 0.246 0.517 1.328 0.38  0.584]
 [0.018 0.062 0.354 0.599 0.756 0.103 0.084 0.039 0.004 0.299 0.415 0.194
  0.124 0.007 0.474 0.416 0.437 0.227 0.494 0.37 ]
 [0.011 0.012 0.033 0.114 0.086 0.024 0.007 0.009 0.011 0.003 0.024 0.033
  0.011 0.013 0.065 0.212 0.03  0.033 0.019 0.055]
 [0.009 0.018 0.153 0.233 0.073 0.027 0.014 0.017 0.011 0.273 0.002 0.019
  0.075 0.011 0.062 0.158 0.153 0.07  0.028 0.278]
 [0.03  0.027 0.107 0.573 0.47  0.241 0.023 0.003 0.021 0.203 0.3   0.005
  0.14  0.026 0.16  0.346 0.092 0.208 0.107 0.171]
 [0.005 0.075 0.211 0.111 0.128 0.124 0.143 0.01  0.051 0.419 0.099 0.038
  0.006 0.019 0.662 0.145 0.161 0.023 0.075 0.194]
 [0.046 0.146 1.445 0.742 0.457 0.268 0.226 0.062 0.666 0.322 0.303 0.197
  0.274 0.004 0.627 0.512 0.381 0.464 0.63  0.725]
 [0.007 0.023 0.081 0.051 0.079 0.01  0.015 0.005 0.01  0.053 0.099 0.023
  0.005 0.007 0.005 0.101 0.045 0.039 0.016 0.157]
 [0.01  0.016 0.036 0.056 0.101 0.049 0.018 0.011 0.009 0.028 0.035 0.014
  0.024 0.006 0.046 0.002 0.067 0.029 0.006 0.043]
 [0.011 0.022 0.046 0.192 0.107 0.035 0.008 0.008 0.015 0.18  0.019 0.06
  0.028 0.005 0.18  0.085 0.003 0.047 0.021 0.14 ]
 [0.008 0.031 0.092 0.099 0.197 0.085 0.044 0.003 0.023 0.154 0.093 0.025
  0.189 0.01  0.182 0.156 0.111 0.002 0.042 0.36 ]
 [0.007 0.057 0.078 0.155 0.528 0.337 0.04  0.012 0.009 0.321 0.201 0.055
  0.061 0.008 0.261 0.583 0.201 0.11  0.005 0.197]
 [0.013 0.014 0.07  0.055 0.133 0.031 0.015 0.005 0.016 0.101 0.025 0.031
  0.02  0.004 0.041 0.145 0.036 0.01  0.027 0.005]]
[[0.    0.    0.441 0.334 0.392 0.    0.397 0.    0.304 0.49  0.398 0.
  0.721 0.    0.391 0.474 0.408 0.507 0.62  0.38 ]
 [0.    0.    0.    0.    0.    0.537 0.    0.    0.    0.337 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.391]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.508 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.403 0.    0.    0.    0.372 0.308 0.
  0.    0.    0.    0.    0.651 0.    0.    0.   ]
 [0.    0.413 0.32  0.414 0.608 0.308 0.    0.    0.    0.419 0.387 0.876
  0.533 0.    0.505 0.    0.517 1.328 0.38  0.584]
 [0.    0.    0.354 0.599 0.756 0.    0.    0.    0.    0.    0.415 0.
  0.    0.    0.474 0.416 0.437 0.    0.494 0.37 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.573 0.47  0.    0.    0.    0.    0.    0.3   0.
  0.    0.    0.    0.346 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.419 0.    0.
  0.    0.    0.662 0.    0.    0.    0.    0.   ]
 [0.    0.    1.445 0.742 0.457 0.    0.    0.    0.666 0.322 0.303 0.
  0.    0.    0.627 0.512 0.381 0.464 0.63  0.725]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.36 ]
 [0.    0.    0.    0.    0.528 0.337 0.    0.    0.    0.321 0.    0.
  0.    0.    0.    0.583 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5147058823529411, 'tpr': 0.55, 'fpr': 0.2692307692307692, 'f1': 0.515625, 'shd': 59, 'npred': 68, 'ntrue': 60}
[0.115 0.441 0.334 0.392 0.244 0.397 0.049 0.304 0.49  0.398 0.169 0.721
 0.108 0.391 0.474 0.408 0.507 0.62  0.38  0.031 0.2   0.121 0.269 0.537
 0.041 0.012 0.067 0.337 0.226 0.252 0.066 0.026 0.189 0.272 0.221 0.055
 0.122 0.391 0.01  0.021 0.053 0.161 0.02  0.027 0.011 0.02  0.222 0.038
 0.052 0.014 0.003 0.052 0.15  0.093 0.085 0.043 0.057 0.015 0.033 0.098
 0.022 0.05  0.033 0.01  0.006 0.053 0.024 0.009 0.059 0.004 0.099 0.104
 0.016 0.062 0.036 0.098 0.012 0.015 0.03  0.11  0.018 0.017 0.004 0.005
 0.061 0.094 0.011 0.037 0.008 0.078 0.068 0.037 0.025 0.009 0.044 0.015
 0.009 0.238 0.082 0.266 0.011 0.016 0.037 0.239 0.241 0.025 0.033 0.024
 0.508 0.111 0.173 0.05  0.018 0.21  0.009 0.127 0.119 0.122 0.283 0.403
 0.033 0.052 0.372 0.308 0.247 0.028 0.036 0.258 0.282 0.651 0.114 0.114
 0.296 0.139 0.413 0.32  0.414 0.608 0.308 0.186 0.157 0.419 0.387 0.876
 0.533 0.083 0.505 0.246 0.517 1.328 0.38  0.584 0.018 0.062 0.354 0.599
 0.756 0.103 0.084 0.039 0.299 0.415 0.194 0.124 0.007 0.474 0.416 0.437
 0.227 0.494 0.37  0.011 0.012 0.033 0.114 0.086 0.024 0.007 0.009 0.011
 0.024 0.033 0.011 0.013 0.065 0.212 0.03  0.033 0.019 0.055 0.009 0.018
 0.153 0.233 0.073 0.027 0.014 0.017 0.011 0.273 0.019 0.075 0.011 0.062
 0.158 0.153 0.07  0.028 0.278 0.03  0.027 0.107 0.573 0.47  0.241 0.023
 0.003 0.021 0.203 0.3   0.14  0.026 0.16  0.346 0.092 0.208 0.107 0.171
 0.005 0.075 0.211 0.111 0.128 0.124 0.143 0.01  0.051 0.419 0.099 0.038
 0.019 0.662 0.145 0.161 0.023 0.075 0.194 0.046 0.146 1.445 0.742 0.457
 0.268 0.226 0.062 0.666 0.322 0.303 0.197 0.274 0.627 0.512 0.381 0.464
 0.63  0.725 0.007 0.023 0.081 0.051 0.079 0.01  0.015 0.005 0.01  0.053
 0.099 0.023 0.005 0.007 0.101 0.045 0.039 0.016 0.157 0.01  0.016 0.036
 0.056 0.101 0.049 0.018 0.011 0.009 0.028 0.035 0.014 0.024 0.006 0.046
 0.067 0.029 0.006 0.043 0.011 0.022 0.046 0.192 0.107 0.035 0.008 0.008
 0.015 0.18  0.019 0.06  0.028 0.005 0.18  0.085 0.047 0.021 0.14  0.008
 0.031 0.092 0.099 0.197 0.085 0.044 0.003 0.023 0.154 0.093 0.025 0.189
 0.01  0.182 0.156 0.111 0.042 0.36  0.007 0.057 0.078 0.155 0.528 0.337
 0.04  0.012 0.009 0.321 0.201 0.055 0.061 0.008 0.261 0.583 0.201 0.11
 0.197 0.013 0.014 0.07  0.055 0.133 0.031 0.015 0.005 0.016 0.101 0.025
 0.031 0.02  0.004 0.041 0.145 0.036 0.01  0.027]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7878125, 0.5560175912582511)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
