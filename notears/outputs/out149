samples  5000  graph  30 120 ER mlp  minibatch size  50  noise  0.8  minibatches per NN training  250 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.407192360868244
iteration 1 in outer loop, alpha = 2.407192360868244, rho = 1.0, h = 2.407192360868244
cuda
iteration 1 in inner loop,alpha 2.407192360868244 rho 1.0 h 1.519847643277938
iteration 2 in inner loop,alpha 2.407192360868244 rho 10.0 h 0.6681085661998409
iteration 3 in inner loop,alpha 2.407192360868244 rho 100.0 h 0.2142382096787543
iteration 2 in outer loop, alpha = 23.831013328743673, rho = 100.0, h = 0.2142382096787543
cuda
iteration 1 in inner loop,alpha 23.831013328743673 rho 100.0 h 0.12078187772351612
iteration 2 in inner loop,alpha 23.831013328743673 rho 1000.0 h 0.04372606397762979
iteration 3 in outer loop, alpha = 67.55707730637346, rho = 1000.0, h = 0.04372606397762979
cuda
iteration 1 in inner loop,alpha 67.55707730637346 rho 1000.0 h 0.023239082362103147
iteration 2 in inner loop,alpha 67.55707730637346 rho 10000.0 h 0.007686345269199535
iteration 4 in outer loop, alpha = 144.4205299983688, rho = 10000.0, h = 0.007686345269199535
cuda
iteration 1 in inner loop,alpha 144.4205299983688 rho 10000.0 h 0.004012326571370295
iteration 2 in inner loop,alpha 144.4205299983688 rho 100000.0 h 0.0013267240677272696
iteration 5 in outer loop, alpha = 277.09293677109576, rho = 100000.0, h = 0.0013267240677272696
cuda
iteration 1 in inner loop,alpha 277.09293677109576 rho 100000.0 h 0.000637695786529946
iteration 6 in outer loop, alpha = 914.7887233010417, rho = 1000000.0, h = 0.000637695786529946
Threshold 0.3
[[0.002 0.    0.001 1.883 0.004 0.161 0.156 0.    0.    0.    0.    0.094
  0.    0.    0.    0.914 0.    0.    0.663 0.16  0.    0.    0.163 0.
  0.    0.    0.165 0.    0.204 0.   ]
 [0.392 0.003 0.02  0.091 0.202 1.539 0.186 0.    1.    0.    0.    0.123
  0.    0.    0.    0.832 0.    1.189 0.336 0.589 0.    0.003 0.996 0.001
  0.    0.    0.166 0.    0.215 0.149]
 [0.167 0.039 0.001 0.452 0.011 0.443 0.215 0.002 0.126 0.    0.    0.154
  0.    0.    0.    0.734 0.    0.253 0.05  0.076 0.    0.    0.196 0.
  0.    0.002 0.177 0.    0.233 0.004]
 [0.    0.    0.    0.003 0.    0.002 0.136 0.    0.    0.    0.    0.
  0.    0.    0.    0.799 0.    0.    0.196 0.547 0.    0.    0.172 0.
  0.    0.    0.    0.    0.171 0.   ]
 [0.079 0.006 0.066 0.212 0.003 0.079 0.332 0.    0.207 0.    0.    0.187
  0.    0.    0.    0.175 0.    0.001 0.665 0.151 0.    0.001 1.418 0.
  0.    0.    0.087 0.    0.153 0.005]
 [0.001 0.    0.    0.181 0.007 0.002 1.615 0.    0.001 0.    0.    0.
  0.    0.    0.    0.18  0.    0.    0.141 0.105 0.    0.    0.279 0.
  0.    0.    0.    0.    0.779 0.   ]
 [0.    0.    0.    0.001 0.    0.    0.004 0.    0.001 0.    0.    0.
  0.    0.    0.    0.957 0.    0.    0.005 0.001 0.    0.    0.    0.
  0.    0.    0.    0.    0.069 0.   ]
 [0.421 0.769 0.108 0.142 2.121 0.176 0.196 0.002 1.375 0.    0.    0.131
  0.    0.    0.    0.509 0.    0.301 0.165 0.687 0.    0.004 0.259 0.001
  0.    0.005 0.146 0.    0.332 0.274]
 [1.206 0.    0.002 0.251 0.002 0.111 0.101 0.    0.002 0.    0.    0.281
  0.    0.    0.    0.072 0.    0.    0.258 0.836 0.    0.    0.116 0.
  0.    0.    0.111 0.    0.114 0.   ]
 [0.313 0.957 0.167 0.23  0.278 0.354 0.213 0.319 1.047 0.001 0.    0.214
  2.239 0.093 0.007 0.29  0.001 0.393 0.23  0.095 0.002 0.478 0.452 0.084
  0.    0.164 0.241 0.002 0.325 1.846]
 [1.65  1.791 0.317 0.166 0.182 0.327 0.304 1.956 0.276 1.27  0.002 0.062
  0.169 0.276 0.004 1.067 0.    0.28  0.227 0.172 0.017 0.09  0.263 0.02
  0.    0.208 0.091 0.015 1.003 0.423]
 [0.012 0.001 0.003 0.133 0.005 0.134 0.887 0.    0.004 0.    0.    0.001
  0.    0.    0.    0.312 0.    0.    0.194 0.122 0.    0.    0.154 0.
  0.    0.    0.904 0.    0.139 0.   ]
 [0.156 1.641 0.103 0.176 2.287 0.36  0.346 1.359 0.855 0.    0.    1.731
  0.001 0.    0.    0.226 0.    0.184 0.637 0.091 0.    2.507 0.539 0.003
  0.    0.03  1.051 0.    0.346 0.264]
 [0.223 0.187 2.753 1.116 0.126 1.683 0.352 0.126 0.409 0.004 0.002 0.49
  0.299 0.001 0.    0.489 0.    1.74  0.4   0.218 0.    0.126 0.313 2.614
  0.    0.522 0.365 0.007 0.218 2.422]
 [0.231 0.836 0.07  0.251 0.252 0.238 0.422 0.282 0.26  0.028 0.019 0.676
  2.495 0.083 0.001 0.253 0.    0.176 0.185 0.167 0.358 0.619 1.023 3.569
  0.006 0.103 0.45  0.003 0.439 0.273]
 [0.    0.    0.    0.    0.001 0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    0.005 0.    0.    0.004 0.001 0.    0.    0.    0.
  0.    0.    0.    0.    0.101 0.   ]
 [0.206 0.228 0.147 0.067 0.349 1.065 0.548 0.276 0.545 0.128 0.079 0.128
  1.091 0.109 3.006 0.455 0.001 0.026 0.2   0.07  0.707 1.326 0.198 0.332
  0.002 0.053 0.137 0.004 0.267 0.142]
 [0.171 0.    0.001 0.225 0.142 0.242 0.19  0.    1.092 0.    0.    0.214
  0.    0.    0.    0.186 0.    0.001 0.688 0.235 0.    0.    0.339 0.
  0.    0.    0.688 0.    0.182 0.001]
 [0.001 0.001 0.002 0.001 0.    0.002 0.25  0.    0.001 0.    0.    0.
  0.    0.    0.    0.165 0.    0.    0.012 0.001 0.    0.    0.009 0.
  0.    0.001 0.001 0.    0.007 0.   ]
 [0.    0.    0.001 0.004 0.001 0.003 0.253 0.001 0.    0.    0.    0.001
  0.    0.    0.    0.233 0.    0.    0.693 0.004 0.    0.    0.208 0.
  0.    0.    0.    0.    0.385 0.   ]
 [0.191 0.535 0.214 0.393 1.823 1.008 0.266 1.103 0.183 0.083 0.027 0.205
  0.223 3.741 0.001 0.134 0.001 0.215 0.222 0.06  0.001 1.695 0.636 0.075
  0.    2.439 0.223 0.002 0.22  0.158]
 [0.084 0.183 2.458 1.061 1.499 0.216 0.588 0.234 0.446 0.    0.    0.363
  0.    0.    0.    0.258 0.    2.282 0.285 0.224 0.    0.001 0.506 0.
  0.    0.003 0.285 0.    0.508 2.097]
 [0.001 0.001 0.001 0.002 0.    0.001 1.053 0.    0.001 0.    0.    0.
  0.    0.    0.    0.191 0.    0.    0.512 0.003 0.    0.    0.005 0.
  0.    0.    0.001 0.    0.083 0.   ]
 [2.093 0.278 0.128 0.19  0.22  0.149 0.988 0.133 0.516 0.003 0.    2.154
  0.249 0.    0.    0.754 0.    0.087 0.773 0.764 0.    0.259 0.449 0.002
  0.    0.013 1.525 0.    0.12  0.126]
 [0.229 0.447 0.695 0.129 0.167 0.525 0.579 0.282 0.116 1.144 4.368 0.149
  0.191 2.14  0.04  0.147 0.01  0.393 0.164 0.064 0.033 0.101 0.879 0.108
  0.001 2.782 0.113 0.003 0.262 0.902]
 [0.228 1.713 0.18  0.071 0.186 0.245 0.186 0.116 0.19  0.003 0.004 0.103
  0.018 0.    0.    0.12  0.    0.116 0.336 0.163 0.    0.127 1.331 0.037
  0.    0.003 0.065 0.    0.089 0.143]
 [0.002 0.    0.001 1.52  0.009 1.769 0.358 0.    0.001 0.    0.    0.
  0.    0.    0.    0.733 0.    0.    0.8   0.182 0.    0.    1.298 0.
  0.    0.    0.003 0.    0.74  0.   ]
 [2.067 0.355 0.188 0.264 0.124 0.222 0.088 0.1   0.247 0.013 0.007 0.978
  0.005 0.017 0.021 0.188 0.018 0.121 0.166 0.073 0.001 0.051 0.895 0.033
  0.003 2.995 0.137 0.    0.123 0.089]
 [0.002 0.001 0.001 0.012 0.005 0.002 0.033 0.002 0.001 0.    0.    0.001
  0.    0.    0.    0.081 0.    0.    0.122 0.014 0.    0.    0.008 0.
  0.    0.    0.002 0.    0.011 0.   ]
 [0.268 0.004 0.236 0.383 0.143 0.244 0.224 0.002 0.555 0.    0.    1.962
  0.    0.    0.    0.19  0.    0.312 0.603 0.199 0.    0.    0.163 0.
  0.    0.001 0.689 0.    0.109 0.002]]
[[0.    0.    0.    1.883 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.914 0.    0.    0.663 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.392 0.    0.    0.    0.    1.539 0.    0.    1.    0.    0.    0.
  0.    0.    0.    0.832 0.    1.189 0.336 0.589 0.    0.    0.996 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.452 0.    0.443 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.734 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.799 0.    0.    0.    0.547 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.332 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.665 0.    0.    0.    1.418 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.615 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.779 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.957 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.421 0.769 0.    0.    2.121 0.    0.    0.    1.375 0.    0.    0.
  0.    0.    0.    0.509 0.    0.301 0.    0.687 0.    0.    0.    0.
  0.    0.    0.    0.    0.332 0.   ]
 [1.206 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.836 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.313 0.957 0.    0.    0.    0.354 0.    0.319 1.047 0.    0.    0.
  2.239 0.    0.    0.    0.    0.393 0.    0.    0.    0.478 0.452 0.
  0.    0.    0.    0.    0.325 1.846]
 [1.65  1.791 0.317 0.    0.    0.327 0.304 1.956 0.    1.27  0.    0.
  0.    0.    0.    1.067 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.003 0.423]
 [0.    0.    0.    0.    0.    0.    0.887 0.    0.    0.    0.    0.
  0.    0.    0.    0.312 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.904 0.    0.    0.   ]
 [0.    1.641 0.    0.    2.287 0.36  0.346 1.359 0.855 0.    0.    1.731
  0.    0.    0.    0.    0.    0.    0.637 0.    0.    2.507 0.539 0.
  0.    0.    1.051 0.    0.346 0.   ]
 [0.    0.    2.753 1.116 0.    1.683 0.352 0.    0.409 0.    0.    0.49
  0.    0.    0.    0.489 0.    1.74  0.4   0.    0.    0.    0.313 2.614
  0.    0.522 0.365 0.    0.    2.422]
 [0.    0.836 0.    0.    0.    0.    0.422 0.    0.    0.    0.    0.676
  2.495 0.    0.    0.    0.    0.    0.    0.    0.358 0.619 1.023 3.569
  0.    0.    0.45  0.    0.439 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.349 1.065 0.548 0.    0.545 0.    0.    0.
  1.091 0.    3.006 0.455 0.    0.    0.    0.    0.707 1.326 0.    0.332
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.092 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.688 0.    0.    0.    0.339 0.
  0.    0.    0.688 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.693 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.385 0.   ]
 [0.    0.535 0.    0.393 1.823 1.008 0.    1.103 0.    0.    0.    0.
  0.    3.741 0.    0.    0.    0.    0.    0.    0.    1.695 0.636 0.
  0.    2.439 0.    0.    0.    0.   ]
 [0.    0.    2.458 1.061 1.499 0.    0.588 0.    0.446 0.    0.    0.363
  0.    0.    0.    0.    0.    2.282 0.    0.    0.    0.    0.506 0.
  0.    0.    0.    0.    0.508 2.097]
 [0.    0.    0.    0.    0.    0.    1.053 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.512 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [2.093 0.    0.    0.    0.    0.    0.988 0.    0.516 0.    0.    2.154
  0.    0.    0.    0.754 0.    0.    0.773 0.764 0.    0.    0.449 0.
  0.    0.    1.525 0.    0.    0.   ]
 [0.    0.447 0.695 0.    0.    0.525 0.579 0.    0.    1.144 4.368 0.
  0.    2.14  0.    0.    0.    0.393 0.    0.    0.    0.    0.879 0.
  0.    2.782 0.    0.    0.    0.902]
 [0.    1.713 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.336 0.    0.    0.    1.331 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.52  0.    1.769 0.358 0.    0.    0.    0.    0.
  0.    0.    0.    0.733 0.    0.    0.8   0.    0.    0.    1.298 0.
  0.    0.    0.    0.    0.74  0.   ]
 [2.067 0.355 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.978
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.895 0.
  0.    2.995 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.383 0.    0.    0.    0.    0.555 0.    0.    1.962
  0.    0.    0.    0.    0.    0.312 0.603 0.    0.    0.    0.    0.
  0.    0.    0.689 0.    0.    0.   ]]
{'fdr': 0.3235294117647059, 'tpr': 0.9583333333333334, 'fpr': 0.1746031746031746, 'f1': 0.7931034482758621, 'shd': 60, 'npred': 170, 'ntrue': 120}
[2.742e-04 8.636e-04 1.883e+00 4.131e-03 1.612e-01 1.563e-01 2.559e-05
 4.319e-04 2.048e-05 1.846e-05 9.361e-02 1.420e-05 2.572e-05 3.680e-05
 9.139e-01 2.256e-05 6.886e-05 6.635e-01 1.597e-01 7.071e-07 4.064e-05
 1.633e-01 2.023e-04 2.176e-06 1.138e-04 1.647e-01 2.700e-05 2.035e-01
 1.128e-04 3.920e-01 1.958e-02 9.135e-02 2.020e-01 1.539e+00 1.855e-01
 3.002e-04 9.997e-01 7.185e-05 3.445e-05 1.227e-01 2.110e-04 1.359e-04
 1.006e-04 8.325e-01 4.921e-05 1.189e+00 3.364e-01 5.889e-01 4.955e-05
 3.246e-03 9.964e-01 6.399e-04 1.061e-06 2.944e-04 1.657e-01 2.243e-05
 2.145e-01 1.487e-01 1.674e-01 3.938e-02 4.524e-01 1.131e-02 4.434e-01
 2.146e-01 2.433e-03 1.258e-01 2.544e-06 1.240e-05 1.536e-01 2.531e-05
 1.822e-04 4.446e-06 7.344e-01 5.887e-06 2.532e-01 4.952e-02 7.645e-02
 1.742e-05 1.065e-04 1.962e-01 3.398e-04 1.802e-05 2.317e-03 1.775e-01
 1.036e-04 2.328e-01 3.626e-03 1.072e-04 1.617e-04 1.434e-04 3.673e-04
 1.544e-03 1.360e-01 5.817e-05 1.417e-04 5.154e-05 6.069e-05 5.599e-05
 1.059e-04 1.953e-05 2.121e-06 7.987e-01 1.889e-05 4.185e-05 1.960e-01
 5.469e-01 6.406e-06 1.306e-05 1.718e-01 1.857e-05 1.126e-05 2.389e-05
 1.423e-04 1.226e-05 1.712e-01 5.646e-05 7.927e-02 5.863e-03 6.612e-02
 2.121e-01 7.855e-02 3.322e-01 3.968e-04 2.067e-01 2.825e-05 1.748e-05
 1.866e-01 6.280e-05 1.135e-05 1.650e-05 1.749e-01 1.537e-06 1.097e-03
 6.654e-01 1.506e-01 4.110e-05 6.835e-04 1.418e+00 3.232e-04 8.627e-07
 4.899e-04 8.683e-02 7.379e-05 1.527e-01 4.701e-03 7.419e-04 4.758e-04
 7.687e-05 1.809e-01 6.626e-03 1.615e+00 3.663e-04 1.429e-03 2.374e-05
 1.747e-05 7.249e-05 6.494e-05 8.976e-06 6.951e-06 1.803e-01 3.077e-06
 2.173e-04 1.412e-01 1.052e-01 2.769e-06 1.196e-05 2.785e-01 3.116e-05
 1.611e-06 3.793e-05 1.380e-04 5.979e-06 7.793e-01 4.863e-05 2.356e-04
 1.504e-04 1.274e-04 9.708e-04 7.729e-05 3.855e-04 1.169e-04 8.062e-04
 2.002e-06 4.779e-05 8.054e-05 3.270e-05 1.216e-05 1.474e-05 9.574e-01
 1.115e-05 3.400e-05 4.895e-03 8.059e-04 1.990e-06 1.880e-05 4.521e-04
 2.497e-04 3.712e-06 6.501e-05 6.180e-05 3.052e-06 6.892e-02 1.241e-05
 4.208e-01 7.686e-01 1.083e-01 1.424e-01 2.121e+00 1.764e-01 1.960e-01
 1.375e+00 4.229e-05 2.297e-05 1.308e-01 1.956e-04 1.338e-04 1.981e-05
 5.090e-01 1.468e-05 3.015e-01 1.653e-01 6.865e-01 5.433e-06 3.739e-03
 2.595e-01 1.428e-03 5.591e-06 5.037e-03 1.459e-01 1.420e-04 3.322e-01
 2.744e-01 1.206e+00 4.672e-04 1.593e-03 2.509e-01 1.672e-03 1.110e-01
 1.011e-01 8.798e-05 6.905e-05 1.513e-05 2.808e-01 3.155e-05 1.830e-05
 1.211e-05 7.250e-02 1.203e-05 1.740e-04 2.580e-01 8.359e-01 4.150e-06
 6.512e-05 1.164e-01 1.955e-04 6.318e-07 1.030e-04 1.106e-01 1.324e-05
 1.144e-01 1.394e-04 3.127e-01 9.568e-01 1.666e-01 2.299e-01 2.778e-01
 3.536e-01 2.134e-01 3.190e-01 1.047e+00 9.524e-05 2.143e-01 2.239e+00
 9.265e-02 6.614e-03 2.901e-01 5.602e-04 3.926e-01 2.305e-01 9.531e-02
 1.838e-03 4.776e-01 4.520e-01 8.362e-02 5.153e-05 1.637e-01 2.409e-01
 1.733e-03 3.251e-01 1.846e+00 1.650e+00 1.791e+00 3.165e-01 1.663e-01
 1.817e-01 3.275e-01 3.043e-01 1.956e+00 2.756e-01 1.270e+00 6.228e-02
 1.692e-01 2.759e-01 4.441e-03 1.067e+00 1.301e-04 2.804e-01 2.268e-01
 1.725e-01 1.668e-02 8.969e-02 2.628e-01 2.014e-02 2.472e-04 2.076e-01
 9.098e-02 1.531e-02 1.003e+00 4.228e-01 1.171e-02 1.020e-03 2.662e-03
 1.333e-01 5.283e-03 1.341e-01 8.870e-01 3.715e-04 3.926e-03 2.483e-05
 2.430e-05 3.752e-05 1.033e-05 3.596e-06 3.121e-01 5.065e-06 1.380e-04
 1.944e-01 1.221e-01 2.342e-06 4.047e-05 1.540e-01 5.241e-05 4.251e-06
 4.926e-04 9.039e-01 5.330e-05 1.392e-01 2.221e-04 1.562e-01 1.641e+00
 1.032e-01 1.756e-01 2.287e+00 3.604e-01 3.456e-01 1.359e+00 8.551e-01
 1.562e-04 3.392e-05 1.731e+00 1.149e-04 7.237e-05 2.258e-01 2.286e-05
 1.843e-01 6.371e-01 9.115e-02 3.875e-05 2.507e+00 5.389e-01 3.197e-03
 7.566e-06 3.047e-02 1.051e+00 3.484e-04 3.457e-01 2.644e-01 2.228e-01
 1.866e-01 2.753e+00 1.116e+00 1.255e-01 1.683e+00 3.522e-01 1.259e-01
 4.094e-01 4.462e-03 1.766e-03 4.897e-01 2.986e-01 8.546e-05 4.892e-01
 1.493e-04 1.740e+00 4.002e-01 2.182e-01 7.674e-05 1.263e-01 3.135e-01
 2.614e+00 2.586e-04 5.218e-01 3.651e-01 6.657e-03 2.179e-01 2.422e+00
 2.309e-01 8.357e-01 6.997e-02 2.509e-01 2.520e-01 2.384e-01 4.215e-01
 2.817e-01 2.604e-01 2.783e-02 1.940e-02 6.759e-01 2.495e+00 8.339e-02
 2.530e-01 2.554e-04 1.757e-01 1.855e-01 1.672e-01 3.583e-01 6.193e-01
 1.023e+00 3.569e+00 5.857e-03 1.027e-01 4.502e-01 2.588e-03 4.394e-01
 2.725e-01 8.834e-05 4.268e-04 3.401e-04 3.216e-04 5.407e-04 2.418e-04
 1.376e-03 4.520e-05 5.175e-05 2.499e-05 4.052e-05 1.061e-04 5.196e-05
 1.551e-05 5.389e-06 9.212e-06 1.250e-04 4.133e-03 5.947e-04 3.414e-06
 1.753e-05 2.746e-04 3.354e-05 2.920e-06 1.357e-04 2.296e-04 2.487e-05
 1.007e-01 2.955e-05 2.064e-01 2.281e-01 1.466e-01 6.673e-02 3.491e-01
 1.065e+00 5.483e-01 2.758e-01 5.452e-01 1.285e-01 7.864e-02 1.281e-01
 1.091e+00 1.088e-01 3.006e+00 4.554e-01 2.595e-02 2.004e-01 7.001e-02
 7.072e-01 1.326e+00 1.978e-01 3.323e-01 2.240e-03 5.299e-02 1.365e-01
 3.955e-03 2.670e-01 1.421e-01 1.712e-01 4.764e-04 6.666e-04 2.248e-01
 1.420e-01 2.423e-01 1.901e-01 9.561e-05 1.092e+00 5.459e-06 1.829e-05
 2.141e-01 3.911e-05 1.126e-04 4.882e-06 1.858e-01 3.694e-07 6.875e-01
 2.349e-01 1.996e-05 1.236e-04 3.389e-01 9.995e-05 6.817e-06 1.298e-04
 6.879e-01 9.730e-06 1.818e-01 9.526e-04 1.210e-03 5.518e-04 1.819e-03
 7.932e-04 3.810e-04 2.198e-03 2.505e-01 7.468e-05 5.524e-04 2.262e-05
 2.454e-05 4.572e-04 6.260e-05 6.021e-05 7.258e-05 1.646e-01 5.745e-05
 3.011e-04 5.053e-04 5.062e-06 9.968e-05 9.172e-03 3.173e-04 1.149e-06
 5.237e-04 1.129e-03 1.892e-05 7.079e-03 1.503e-04 4.231e-04 3.676e-04
 6.708e-04 4.417e-03 1.215e-03 2.869e-03 2.532e-01 5.634e-04 2.323e-04
 7.971e-06 1.445e-05 6.650e-04 2.801e-05 6.672e-05 6.336e-05 2.330e-01
 3.706e-05 6.759e-05 6.927e-01 1.989e-05 5.136e-05 2.082e-01 2.633e-04
 1.984e-05 1.971e-04 1.015e-04 2.928e-05 3.851e-01 1.722e-04 1.912e-01
 5.353e-01 2.138e-01 3.931e-01 1.823e+00 1.008e+00 2.661e-01 1.103e+00
 1.832e-01 8.259e-02 2.702e-02 2.047e-01 2.230e-01 3.741e+00 7.477e-04
 1.344e-01 6.654e-04 2.154e-01 2.215e-01 6.002e-02 1.695e+00 6.365e-01
 7.477e-02 1.710e-04 2.439e+00 2.231e-01 1.552e-03 2.205e-01 1.583e-01
 8.384e-02 1.833e-01 2.458e+00 1.061e+00 1.499e+00 2.159e-01 5.876e-01
 2.336e-01 4.461e-01 9.699e-06 4.001e-06 3.629e-01 3.466e-05 2.134e-05
 1.965e-05 2.576e-01 6.013e-06 2.282e+00 2.847e-01 2.238e-01 5.713e-05
 5.058e-01 2.539e-04 1.202e-06 3.341e-03 2.850e-01 5.063e-05 5.079e-01
 2.097e+00 6.309e-04 1.141e-03 6.393e-04 1.686e-03 3.079e-04 9.429e-04
 1.053e+00 3.218e-05 1.434e-03 7.099e-06 9.307e-06 2.635e-04 2.716e-05
 2.186e-05 3.998e-06 1.908e-01 2.988e-06 1.284e-04 5.120e-01 2.680e-03
 3.010e-06 6.395e-05 2.408e-05 8.083e-06 1.342e-04 6.505e-04 4.176e-05
 8.274e-02 8.368e-05 2.093e+00 2.778e-01 1.278e-01 1.902e-01 2.205e-01
 1.491e-01 9.882e-01 1.332e-01 5.165e-01 3.011e-03 6.491e-05 2.154e+00
 2.489e-01 9.041e-05 1.858e-04 7.536e-01 6.139e-05 8.730e-02 7.732e-01
 7.638e-01 5.865e-06 2.592e-01 4.494e-01 4.152e-05 1.348e-02 1.525e+00
 1.888e-05 1.197e-01 1.257e-01 2.288e-01 4.472e-01 6.954e-01 1.292e-01
 1.674e-01 5.247e-01 5.795e-01 2.817e-01 1.156e-01 1.144e+00 4.368e+00
 1.492e-01 1.909e-01 2.140e+00 4.046e-02 1.471e-01 1.045e-02 3.927e-01
 1.643e-01 6.402e-02 3.341e-02 1.006e-01 8.786e-01 1.078e-01 2.782e+00
 1.134e-01 2.980e-03 2.617e-01 9.020e-01 2.276e-01 1.713e+00 1.796e-01
 7.086e-02 1.863e-01 2.446e-01 1.862e-01 1.159e-01 1.901e-01 2.946e-03
 4.163e-03 1.028e-01 1.755e-02 3.583e-04 1.353e-04 1.204e-01 8.019e-05
 1.160e-01 3.357e-01 1.628e-01 3.052e-05 1.266e-01 1.331e+00 3.742e-02
 3.801e-04 6.529e-02 7.895e-05 8.906e-02 1.433e-01 1.526e-03 1.951e-04
 7.464e-04 1.520e+00 8.829e-03 1.769e+00 3.575e-01 3.180e-04 8.248e-04
 5.796e-06 1.441e-05 3.377e-04 4.800e-05 7.360e-06 5.405e-06 7.326e-01
 7.053e-06 1.617e-04 8.000e-01 1.823e-01 3.714e-06 8.866e-05 1.298e+00
 3.607e-05 1.152e-06 2.878e-04 1.398e-05 7.405e-01 1.017e-04 2.067e+00
 3.546e-01 1.877e-01 2.643e-01 1.235e-01 2.220e-01 8.795e-02 9.962e-02
 2.473e-01 1.314e-02 6.824e-03 9.781e-01 5.164e-03 1.662e-02 2.150e-02
 1.882e-01 1.843e-02 1.213e-01 1.664e-01 7.311e-02 1.008e-03 5.142e-02
 8.947e-01 3.273e-02 3.313e-03 2.995e+00 1.371e-01 1.227e-01 8.853e-02
 2.256e-03 6.331e-04 5.770e-04 1.245e-02 5.488e-03 1.619e-03 3.338e-02
 2.118e-03 1.258e-03 1.533e-05 1.861e-04 8.047e-04 6.521e-05 2.216e-05
 2.748e-05 8.051e-02 3.154e-05 4.127e-04 1.220e-01 1.417e-02 2.719e-05
 1.630e-04 7.591e-03 1.790e-04 8.717e-05 4.850e-05 1.701e-03 2.295e-05
 2.765e-04 2.683e-01 3.862e-03 2.355e-01 3.832e-01 1.426e-01 2.439e-01
 2.241e-01 1.827e-03 5.553e-01 4.483e-05 3.224e-05 1.962e+00 2.129e-05
 1.077e-04 7.009e-06 1.897e-01 5.386e-06 3.117e-01 6.031e-01 1.987e-01
 1.967e-05 1.378e-04 1.628e-01 3.420e-04 2.305e-05 8.278e-04 6.894e-01
 6.769e-05 1.086e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.9850777777777777, 0.9567360984318124)
cuda
9630
cuda
Objective function 1220.30 = squared loss an data 964.29 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 397 ; DAG False
||w||^2 104212549.67192225
exp ma of ||w||^2 45916256015.57644
||w|| 10208.45481314005
exp ma of ||w|| 82276.23047853533
||w||^2 0.18721884470734168
exp ma of ||w||^2 0.18148808992987966
||w|| 0.43268792993026933
exp ma of ||w|| 0.4151812724243994
||w||^2 0.17835806156976358
exp ma of ||w||^2 0.22302946482947292
||w|| 0.4223245926651248
exp ma of ||w|| 0.45465808678413977
||w||^2 0.6767297887549835
exp ma of ||w||^2 0.2882137379937874
||w|| 0.8226358785969546
exp ma of ||w|| 0.514884676485737
||w||^2 0.18207371771422165
exp ma of ||w||^2 0.3423746344173497
||w|| 0.4267009699007276
exp ma of ||w|| 0.5619916811428601
||w||^2 0.7389977155644608
exp ma of ||w||^2 0.4249290277229798
||w|| 0.8596497633132116
exp ma of ||w|| 0.6265040254485089
||w||^2 1.0527680473841416
exp ma of ||w||^2 0.4181248625121549
||w|| 1.0260448564191245
exp ma of ||w|| 0.6236425869310603
||w||^2 0.45947339061421016
exp ma of ||w||^2 0.5259065590933156
||w|| 0.6778446655497188
exp ma of ||w|| 0.6929316066240938
||w||^2 0.6254041531918237
exp ma of ||w||^2 0.754316038832179
||w|| 0.7908249826553431
exp ma of ||w|| 0.8383867579386824
||w||^2 0.35209831090652666
exp ma of ||w||^2 0.7733069234381181
||w|| 0.5933787246830869
exp ma of ||w|| 0.8424164693820603
||w||^2 1.8746034900343513
exp ma of ||w||^2 0.9254594984523428
||w|| 1.3691616011393073
exp ma of ||w|| 0.9265693104902522
||w||^2 0.7600622040205632
exp ma of ||w||^2 0.9451841818652535
||w|| 0.8718154644307264
exp ma of ||w|| 0.9353845504963211
cuda
Objective function 110.09 = squared loss an data 87.74 + 0.5*rho*h**2 20.070770 + alpha*h 0.000000 + L2reg 1.66 + L1reg 0.62 ; SHD = 259 ; DAG False
Proportion of microbatches that were clipped  0.7717584369449378
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 6.335735174090786
iteration 1 in outer loop, alpha = 6.335735174090786, rho = 1.0, h = 6.335735174090786
cuda
9630
cuda
Objective function 150.23 = squared loss an data 87.74 + 0.5*rho*h**2 20.070770 + alpha*h 40.141540 + L2reg 1.66 + L1reg 0.62 ; SHD = 259 ; DAG False
||w||^2 2166506.554805388
exp ma of ||w||^2 2090343106.4652982
||w|| 1471.9057560881363
exp ma of ||w|| 13419.647902735765
||w||^2 3.4615794910948474
exp ma of ||w||^2 3873618.3734622197
||w|| 1.8605320451674159
exp ma of ||w|| 35.11387073753471
||w||^2 1.070580441363097
exp ma of ||w||^2 1308333.6093374013
||w|| 1.0346885721622217
exp ma of ||w|| 12.945197810465869
||w||^2 1.1475597712898222
exp ma of ||w||^2 2.1705002954083095
||w|| 1.0712421627670479
exp ma of ||w|| 1.4127755734904357
||w||^2 0.7440733156084509
exp ma of ||w||^2 1.7757418509532783
||w|| 0.8625968441911035
exp ma of ||w|| 1.2783915673632618
||w||^2 0.7598472825785203
exp ma of ||w||^2 1.6929628626938829
||w|| 0.8716921948592407
exp ma of ||w|| 1.2457513239170233
||w||^2 2.1682495641022212
exp ma of ||w||^2 1.6475035761652685
||w|| 1.4724977297443351
exp ma of ||w|| 1.2379931530337274
||w||^2 1.4863643072755093
exp ma of ||w||^2 1.834498113947397
||w|| 1.2191654142385722
exp ma of ||w|| 1.3052556884596982
cuda
Objective function 105.49 = squared loss an data 68.93 + 0.5*rho*h**2 8.191209 + alpha*h 25.644015 + L2reg 2.15 + L1reg 0.58 ; SHD = 221 ; DAG False
Proportion of microbatches that were clipped  0.7727383763537171
iteration 1 in inner loop, alpha 6.335735174090786 rho 1.0 h 4.047520054630624
9630
cuda
Objective function 179.21 = squared loss an data 68.93 + 0.5*rho*h**2 81.912093 + alpha*h 25.644015 + L2reg 2.15 + L1reg 0.58 ; SHD = 221 ; DAG False
||w||^2 376042.225020993
exp ma of ||w||^2 2434936135.136353
||w|| 613.2228184118665
exp ma of ||w|| 10674.555617715641
||w||^2 2.0942672598831438
exp ma of ||w||^2 3.6981822924033674
||w|| 1.4471583396032184
exp ma of ||w|| 1.8353863122670495
||w||^2 3.3425793923166096
exp ma of ||w||^2 3.4730980807089646
||w|| 1.8282722423962492
exp ma of ||w|| 1.8069535374230439
||w||^2 2.8184504964379187
exp ma of ||w||^2 3.3153556564866933
||w|| 1.6788241410099864
exp ma of ||w|| 1.7734771387878114
||w||^2 0.9426236627116564
exp ma of ||w||^2 3.4555152955212147
||w|| 0.9708880793951775
exp ma of ||w|| 1.7953287626345693
||w||^2 3.8459068452822613
exp ma of ||w||^2 3.2578262395114037
||w|| 1.9610983772575667
exp ma of ||w|| 1.724754805359111
||w||^2 3.312420076775703
exp ma of ||w||^2 3.2733330670977483
||w|| 1.820005515589363
exp ma of ||w|| 1.7439766839088178
||w||^2 4.4651156758536334
exp ma of ||w||^2 3.5105652971807095
||w|| 2.113082032447778
exp ma of ||w|| 1.814885291872608
||w||^2 1.9313168668824265
exp ma of ||w||^2 3.4646328923315486
||w|| 1.389718268888492
exp ma of ||w|| 1.8053407032208646
||w||^2 13.997916077898045
exp ma of ||w||^2 3.791666360598679
||w|| 3.7413789006057705
exp ma of ||w|| 1.862860466745658
||w||^2 3.8468785488470543
exp ma of ||w||^2 4.028477006701947
||w|| 1.9613461063379545
exp ma of ||w|| 1.944944273447967
||w||^2 2.1989577764078314
exp ma of ||w||^2 3.8851756098190675
||w|| 1.4828883222980183
exp ma of ||w|| 1.9056865582073192
||w||^2 10.812990615060214
exp ma of ||w||^2 3.547009750211917
||w|| 3.2883112101898466
exp ma of ||w|| 1.816807187713727
cuda
Objective function 107.47 = squared loss an data 72.71 + 0.5*rho*h**2 19.385805 + alpha*h 12.475384 + L2reg 2.39 + L1reg 0.51 ; SHD = 170 ; DAG False
Proportion of microbatches that were clipped  0.7781797716674707
iteration 2 in inner loop, alpha 6.335735174090786 rho 10.0 h 1.969050763730273
9630
cuda
Objective function 281.95 = squared loss an data 72.71 + 0.5*rho*h**2 193.858046 + alpha*h 12.475384 + L2reg 2.39 + L1reg 0.51 ; SHD = 170 ; DAG False
||w||^2 1980416369131.558
exp ma of ||w||^2 687992102224.6406
||w|| 1407272.6704983502
exp ma of ||w|| 436877.6260090209
||w||^2 149.54520861043173
exp ma of ||w||^2 82173113.40038215
||w|| 12.228867838456335
exp ma of ||w|| 261.6226461534932
||w||^2 109.18842897338858
exp ma of ||w||^2 80537870.24432297
||w|| 10.449326723449152
exp ma of ||w|| 256.6046125314901
||w||^2 24.402262669495062
exp ma of ||w||^2 313789.8305924687
||w|| 4.939864640806979
exp ma of ||w|| 3.734253417932683
||w||^2 3.8074917113048357
exp ma of ||w||^2 15707.11316411527
||w|| 1.9512795061971095
exp ma of ||w|| 2.538287673183826
||w||^2 6.939946011052941
exp ma of ||w||^2 13.273312593942519
||w|| 2.6343777274819455
exp ma of ||w|| 2.3792871277749206
||w||^2 8.21050896892304
exp ma of ||w||^2 5.444123696421586
||w|| 2.865398570691875
exp ma of ||w|| 2.2501518241229523
||w||^2 4.443395148923952
exp ma of ||w||^2 5.494935030043153
||w|| 2.107936229804866
exp ma of ||w|| 2.2666585475218763
||w||^2 2.0466642951876985
exp ma of ||w||^2 5.19069707237495
||w|| 1.4306167534275902
exp ma of ||w|| 2.2045440975634665
||w||^2 5.436373892913128
exp ma of ||w||^2 5.465637517712212
||w|| 2.3316032880644872
exp ma of ||w|| 2.2629796647032445
||w||^2 4.855043696729471
exp ma of ||w||^2 5.354194015782406
||w|| 2.2034163693522544
exp ma of ||w|| 2.2408717334611588
||w||^2 6.34446693780707
exp ma of ||w||^2 4.846114700265455
||w|| 2.5188225300340377
exp ma of ||w|| 2.1379184361426478
||w||^2 2.576448205492072
exp ma of ||w||^2 4.759624592557005
||w|| 1.6051318343027379
exp ma of ||w|| 2.1107132372916673
||w||^2 11.618196099127085
exp ma of ||w||^2 5.288259086513974
||w|| 3.4085475057753096
exp ma of ||w|| 2.231457553176315
||w||^2 2.2241090652770725
exp ma of ||w||^2 5.019171461513286
||w|| 1.4913447171184375
exp ma of ||w|| 2.15527760424993
||w||^2 3.181357366439112
exp ma of ||w||^2 4.971212488259824
||w|| 1.7836359960594852
exp ma of ||w|| 2.1738085047444966
||w||^2 2.2022011086259705
exp ma of ||w||^2 5.177622967666962
||w|| 1.483981505486497
exp ma of ||w|| 2.1932774531050687
cuda
Objective function 114.24 = squared loss an data 76.72 + 0.5*rho*h**2 29.660960 + alpha*h 4.879829 + L2reg 2.54 + L1reg 0.44 ; SHD = 157 ; DAG True
Proportion of microbatches that were clipped  0.7789422373872776
iteration 3 in inner loop, alpha 6.335735174090786 rho 100.0 h 0.7702072400272186
iteration 2 in outer loop, alpha = 83.35645917681265, rho = 100.0, h = 0.7702072400272186
cuda
9630
cuda
Objective function 173.56 = squared loss an data 76.72 + 0.5*rho*h**2 29.660960 + alpha*h 64.201748 + L2reg 2.54 + L1reg 0.44 ; SHD = 157 ; DAG True
||w||^2 69771196892.28171
exp ma of ||w||^2 462681874086.3329
||w|| 264142.379962553
exp ma of ||w|| 560149.5942901964
||w||^2 445213759.2497293
exp ma of ||w||^2 27194440929.160957
||w|| 21100.089081559094
exp ma of ||w|| 71416.17693079496
||w||^2 1487957.9059780447
exp ma of ||w||^2 5406670818.812489
||w|| 1219.81880046917
exp ma of ||w|| 16822.44792087798
||w||^2 12.764840802901483
exp ma of ||w||^2 9815306.028467953
||w|| 3.572791737969271
exp ma of ||w|| 41.34920263145958
||w||^2 2.891746220476937
exp ma of ||w||^2 1064.305184449916
||w|| 1.7005135166992755
exp ma of ||w|| 2.5138459925671794
v before min max tensor([[-1636.026,   445.728,  1175.877,  ...,  -142.117, -1150.891,
          -792.130],
        [ -928.346,  -896.766,   334.119,  ...,  -835.317, -1705.974,
         -1496.527],
        [ -501.024, -1183.427, -1308.324,  ..., -1497.789,  -958.972,
          -584.079],
        ...,
        [-1126.460,  1105.109, -1818.167,  ...,  -859.804, -1069.397,
         -1849.265],
        [ -426.951, -1425.180,  -914.887,  ..., -1199.273, -1291.088,
           -11.811],
        [ -582.875, -1658.831, -1872.256,  ...,  -555.916,  2894.664,
         -1524.487]], device='cuda:0')
v tensor([[1.000e-12, 1.000e+01, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        ...,
        [1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e-12]], device='cuda:0')
v before min max tensor([ 1.359e+03, -1.759e+03, -7.930e+02, -1.062e+03,  2.604e+02,  9.621e+01,
        -1.549e+03,  4.464e+01,  1.114e+03, -2.119e+03, -1.472e+03, -1.669e+03,
        -1.527e+03,  4.370e+02,  1.545e+03, -1.581e+03,  1.117e+03, -1.175e+03,
        -1.222e+03, -1.463e+03,  3.226e+02, -1.454e+03,  1.790e+03,  1.209e+03,
        -1.227e+03, -4.955e+02,  2.122e+03, -7.956e+02,  3.934e+03, -4.723e+01,
        -9.713e+02,  6.226e+03,  2.108e+03, -1.161e+03, -9.836e+02, -1.772e+03,
         1.667e+03, -8.465e+02, -5.627e+02, -1.616e+03, -8.924e+02,  7.119e+02,
        -3.738e+02, -5.907e+02, -6.200e+02, -1.264e+03, -1.134e+03, -3.543e+02,
        -9.675e+02, -1.097e+03,  1.519e+03,  6.728e+03, -9.934e+02, -1.057e+03,
        -7.663e+02, -1.115e+03, -1.721e+03,  4.098e+02,  5.639e+03, -4.626e+02,
        -1.042e+03,  2.680e+02, -1.283e+03, -1.764e+03, -3.058e+02, -3.836e+02,
        -1.010e+03,  5.102e+03,  5.068e+01, -9.002e+02, -7.582e+02, -1.442e+02,
        -1.167e+03, -5.259e+02,  9.012e+02, -1.158e+03, -9.287e+01, -1.862e+03,
        -2.519e+02,  3.245e+03, -9.182e+02, -1.374e+03, -9.350e+02, -1.115e+03,
        -1.421e+03, -1.430e+03,  6.015e+02,  1.990e+03,  2.367e+03, -1.266e+03,
        -1.317e+03, -1.286e+03, -1.525e+03, -1.354e+03,  1.619e+03, -1.275e+03,
        -1.029e+03, -9.388e+02, -2.822e+01, -1.452e+03, -4.101e+02, -1.491e+03,
         3.240e+03, -1.534e+03, -1.976e+03, -1.452e+03, -1.202e+03, -4.622e+02,
        -1.507e+03, -1.081e+03, -1.581e+03, -3.523e+02,  5.700e+02, -1.488e+03,
        -1.210e+03,  9.269e+02, -1.470e+03,  6.487e+03, -1.245e+03, -1.723e+03,
        -9.525e+02, -7.619e+02, -3.771e+02, -9.102e+01, -1.466e+03, -1.231e+03,
        -8.843e+02, -8.859e+02, -1.317e+03, -2.822e+02,  3.124e+03, -3.436e+02,
         4.883e+02,  8.619e+00,  1.328e+03, -1.221e+03, -1.733e+03,  9.117e+01,
         7.757e+02,  2.796e+03, -1.267e+03, -1.017e+03,  4.781e+03,  4.172e+03,
         7.511e+02, -7.299e+01, -1.440e+03,  1.192e+03, -1.024e+03, -1.439e+03,
        -4.230e+02, -1.035e+03, -1.439e+03, -9.730e+02,  1.851e+02,  4.300e+02,
        -8.801e+02, -1.266e+03,  2.607e+03,  1.267e+03, -9.006e+02, -1.429e+03,
        -5.150e+01, -5.835e+02, -1.152e+03, -1.565e+03, -1.569e+03, -2.020e+03,
        -9.137e+02,  3.571e+03, -1.275e+03,  7.925e+02,  4.796e+03, -1.741e+03,
        -9.685e+02, -1.516e+03,  4.377e+02, -5.395e+02, -1.265e+03, -1.030e+03,
        -1.232e+03,  1.026e+03, -8.292e+02, -5.525e+02, -5.793e+02,  1.503e+03,
        -1.206e+03,  1.291e+03, -1.572e+03, -1.298e+03,  6.962e+02,  1.057e+03,
         4.907e+03, -1.149e+03, -1.245e+03, -6.560e+02, -1.877e+03,  6.746e+02,
        -1.791e+03, -9.505e+02, -1.106e+03,  1.813e+03, -1.966e+03, -1.076e+03,
        -8.423e+02, -1.434e+03, -3.960e+02,  3.419e+02, -1.573e+03, -1.353e+03,
         1.009e+02, -4.731e+02, -1.572e+03, -1.103e+03, -1.219e+03, -1.320e+03,
        -1.028e+03, -3.056e+02, -1.451e+03,  4.089e+02, -1.069e+02, -1.423e+03,
        -1.720e+03,  9.615e+02, -1.711e+03, -1.134e+03,  7.098e+02, -1.285e+03,
        -3.574e+01, -6.132e+02,  2.591e+03, -1.243e+03, -8.069e+02,  1.642e+02,
        -2.888e+02,  5.459e+02, -1.049e+03, -1.377e+03,  4.067e+03, -1.600e+02,
        -1.547e+03,  1.801e+03, -1.532e+03, -7.418e+02, -1.061e+03, -7.837e+02,
        -1.599e+03, -3.742e+01, -7.642e+02, -8.419e+02, -8.120e+02, -1.222e+03,
        -7.465e+02,  5.152e+03,  4.218e+02,  1.936e+02, -1.683e+03, -1.205e+03,
         2.309e+02, -1.340e+03, -1.519e+03, -1.245e+03, -2.001e+03, -1.682e+03,
        -1.607e+03, -1.269e+03,  6.936e+02, -8.799e+02, -1.513e+03, -1.401e+03,
        -7.305e+02, -1.479e+03,  6.258e+03, -1.126e+03, -1.462e+03, -1.169e+03,
        -1.590e+03, -1.310e+03,  8.629e+02,  1.528e+03,  1.675e+03, -1.502e+03,
        -1.422e+03, -6.019e+02, -9.160e+02, -1.684e+03, -7.969e+02,  1.014e+04,
         5.902e+02, -1.019e+03,  1.804e+03, -1.020e+03, -2.003e+02, -9.528e+02,
        -1.594e+03,  9.399e+02, -1.009e+03, -1.511e+03, -4.066e+02, -1.166e+03],
       device='cuda:0')
v tensor([1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 8.619e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[-7.826e+02],
         [-1.590e+03],
         [ 1.823e+03],
         [-1.527e+03],
         [-1.475e+03],
         [-6.835e+02],
         [ 7.976e+02],
         [ 1.717e+03],
         [-1.089e+03],
         [-3.043e+02]],

        [[ 3.521e+03],
         [-1.052e+03],
         [ 3.181e+02],
         [-1.870e+03],
         [ 6.664e+00],
         [-1.162e+03],
         [ 1.816e+03],
         [-1.817e+03],
         [-1.237e+02],
         [ 1.384e+02]],

        [[ 8.608e+02],
         [-1.199e+02],
         [-1.841e+03],
         [-7.528e+02],
         [ 3.272e+03],
         [ 1.593e+03],
         [-1.232e+03],
         [-1.516e+03],
         [-7.287e+02],
         [ 3.604e+03]],

        [[-5.999e+02],
         [-4.667e+02],
         [ 6.839e+02],
         [-1.272e+03],
         [-2.423e+02],
         [-1.289e+03],
         [-7.030e+02],
         [ 2.087e+03],
         [-1.353e+03],
         [-1.124e+03]],

        [[-1.449e+02],
         [-4.221e+02],
         [ 3.292e+03],
         [-1.251e+03],
         [ 7.784e+03],
         [-1.114e+03],
         [-1.597e+03],
         [ 4.780e+03],
         [ 2.932e+02],
         [ 6.952e+03]],

        [[-6.385e+02],
         [ 2.520e+03],
         [-1.346e+03],
         [-7.765e+02],
         [-6.535e+02],
         [ 8.492e+02],
         [-1.553e+03],
         [-1.801e+03],
         [ 3.970e+02],
         [-1.070e+03]],

        [[-1.721e+03],
         [-8.431e+02],
         [-7.305e+02],
         [-1.167e+03],
         [-1.271e+03],
         [ 7.204e+00],
         [-1.771e+03],
         [-1.264e+03],
         [-7.076e+02],
         [-4.183e+02]],

        [[-1.598e+03],
         [-1.420e+03],
         [-1.673e+03],
         [-1.985e+03],
         [ 4.077e+03],
         [-1.841e+03],
         [-1.761e+03],
         [ 4.736e+02],
         [-1.713e+03],
         [-1.850e+03]],

        [[-1.452e+03],
         [ 6.447e+02],
         [ 7.795e+02],
         [-1.772e+03],
         [ 2.730e+03],
         [ 1.628e+03],
         [-1.060e+03],
         [-9.750e+02],
         [-1.477e+03],
         [-6.830e+02]],

        [[-1.323e+03],
         [-9.680e+02],
         [-1.207e+03],
         [-7.307e+02],
         [-1.354e+03],
         [ 8.623e+02],
         [-1.697e+03],
         [-1.716e+02],
         [-8.825e+02],
         [-1.632e+03]],

        [[-1.576e+03],
         [-2.011e+02],
         [-1.165e+03],
         [-9.320e+02],
         [-1.394e+03],
         [ 1.381e+03],
         [-1.611e+03],
         [-1.256e+03],
         [-1.609e+03],
         [ 2.206e+03]],

        [[-1.648e+02],
         [-5.371e+02],
         [-1.662e+03],
         [ 4.476e+02],
         [ 9.009e+03],
         [ 5.703e+03],
         [-1.026e+03],
         [ 1.075e+02],
         [-1.254e+03],
         [ 1.278e+04]],

        [[-1.067e+03],
         [ 2.754e+02],
         [-1.116e+03],
         [ 1.353e+02],
         [-1.243e+03],
         [ 2.556e+03],
         [-1.601e+03],
         [-4.937e+02],
         [ 2.469e+02],
         [-1.564e+03]],

        [[ 3.052e+02],
         [ 6.680e+03],
         [ 1.293e+03],
         [-1.638e+03],
         [-1.558e+03],
         [-7.008e+01],
         [-1.672e+03],
         [-8.857e+02],
         [-1.242e+03],
         [-4.663e+02]],

        [[-9.445e+02],
         [-1.172e+03],
         [-1.760e+03],
         [-7.078e+02],
         [ 6.193e+02],
         [ 5.055e+03],
         [-1.112e+03],
         [-1.373e+03],
         [-1.664e+03],
         [ 2.428e+02]],

        [[-8.187e+02],
         [-9.042e+02],
         [ 6.011e+02],
         [ 3.765e+03],
         [-1.058e+03],
         [-9.764e+02],
         [-1.568e+03],
         [-1.239e+03],
         [-1.705e+03],
         [ 6.121e+02]],

        [[-1.356e+03],
         [-1.422e+03],
         [-9.348e+02],
         [-9.552e+02],
         [-3.007e+02],
         [ 1.249e+03],
         [-1.584e+03],
         [-3.793e+02],
         [-9.445e+02],
         [ 1.489e+03]],

        [[-9.804e+02],
         [ 3.414e+03],
         [ 2.003e+03],
         [-1.677e+03],
         [-1.601e+03],
         [-1.483e+03],
         [ 7.076e+02],
         [-1.321e+03],
         [ 3.139e+03],
         [-1.573e+03]],

        [[-9.853e+02],
         [ 7.290e+02],
         [-1.207e+03],
         [ 1.836e+03],
         [-1.167e+03],
         [-5.088e+02],
         [-9.364e+02],
         [-9.603e+02],
         [-1.396e+02],
         [ 6.646e+02]],

        [[-1.715e+03],
         [-9.400e+02],
         [-1.191e+03],
         [-7.211e+02],
         [-1.075e+03],
         [ 7.031e+02],
         [-1.652e+03],
         [ 3.499e+03],
         [ 4.708e+02],
         [-1.616e+03]],

        [[ 5.458e+02],
         [-1.192e+03],
         [-1.058e+03],
         [-2.881e+02],
         [-5.611e+02],
         [-1.538e+03],
         [-6.179e+02],
         [-1.465e+03],
         [ 1.603e+03],
         [ 1.305e+02]],

        [[-5.668e+01],
         [-9.160e+02],
         [-1.747e+03],
         [-1.361e+03],
         [ 1.862e+03],
         [ 4.671e+02],
         [ 2.244e+02],
         [-1.421e+03],
         [-3.676e+02],
         [-3.582e+02]],

        [[-1.399e+03],
         [ 1.460e+03],
         [-1.727e+03],
         [ 5.506e+03],
         [-1.120e+03],
         [ 2.496e+03],
         [ 2.423e+03],
         [-1.578e+03],
         [-1.843e+01],
         [-1.536e+03]],

        [[-1.412e+03],
         [-1.254e+03],
         [-9.777e+02],
         [-1.448e+03],
         [ 4.492e+03],
         [-1.670e+03],
         [ 5.111e+03],
         [-1.104e+03],
         [ 1.761e+02],
         [-1.328e+03]],

        [[-6.596e+02],
         [-1.073e+03],
         [ 8.469e+03],
         [-1.287e+03],
         [ 1.739e+03],
         [-3.818e+01],
         [-1.336e+03],
         [ 6.504e+03],
         [-1.045e+03],
         [ 4.518e+03]],

        [[-1.801e+03],
         [-1.703e+03],
         [-1.709e+02],
         [-1.036e+03],
         [-5.353e+02],
         [-1.176e+03],
         [-1.290e+03],
         [-1.939e+03],
         [-5.811e+01],
         [-9.158e+02]],

        [[-1.091e+03],
         [-5.061e+02],
         [-1.523e+03],
         [-1.187e+03],
         [ 1.444e+04],
         [ 4.272e+00],
         [ 1.146e+04],
         [ 2.566e+01],
         [-1.107e+03],
         [-1.940e+02]],

        [[ 1.779e+02],
         [-1.047e+03],
         [-5.396e+02],
         [-1.555e+03],
         [ 7.697e+03],
         [-1.770e+03],
         [-5.002e+02],
         [-1.730e+03],
         [-5.870e+02],
         [ 9.601e+02]],

        [[ 6.399e+02],
         [-1.448e+02],
         [ 1.810e+03],
         [ 3.923e+03],
         [ 1.706e+03],
         [-8.931e+02],
         [ 7.006e+03],
         [ 3.198e+03],
         [ 2.431e+03],
         [-1.044e+03]],

        [[-1.385e+03],
         [-1.075e+02],
         [ 6.219e+03],
         [-1.018e+03],
         [-1.587e+03],
         [-1.093e+03],
         [-1.188e+03],
         [-7.879e+02],
         [ 1.622e+03],
         [-4.963e+02]]], device='cuda:0')
v tensor([[[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [6.664e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.204e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [4.272e+00],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-1732.383],
        [ 1815.188],
        [-1778.659],
        [-1538.355],
        [  965.860],
        [-1165.104],
        [ -127.739],
        [ -330.679],
        [ 1852.012],
        [-1809.854],
        [-1352.645],
        [ 1689.247],
        [ -866.195],
        [-1868.826],
        [-1576.053],
        [-1517.451],
        [-1680.407],
        [ -944.400],
        [ 1780.191],
        [-1407.031],
        [ -257.568],
        [ 1780.747],
        [  311.688],
        [ -446.277],
        [ -856.260],
        [-1627.992],
        [ -418.796],
        [ 1076.468],
        [-1264.392],
        [-1342.559]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[ 5.209e-03,  6.515e-02, -9.079e-04,  ..., -3.249e-01, -4.659e-02,
          4.511e-01],
        [-1.100e-02,  7.564e-02,  4.417e-01,  ...,  3.327e-02,  2.540e-02,
         -4.414e-01],
        [ 3.956e-02, -2.541e-01, -3.159e-01,  ..., -7.803e-03, -2.186e-01,
         -2.755e-01],
        ...,
        [-2.179e-01,  2.677e-01, -7.095e-01,  ...,  1.127e-01,  6.401e-03,
          1.427e-01],
        [-4.089e-01,  2.407e-02, -2.497e-01,  ..., -3.139e-02,  1.206e+00,
         -2.146e-01],
        [-7.464e-02,  3.891e-02, -3.404e-01,  ...,  3.667e-02, -5.479e-01,
          2.181e-01]], device='cuda:0')
s after update for 1 param tensor([[1.746, 2.113, 2.288,  ..., 1.663, 1.927, 1.388],
        [1.274, 1.539, 1.655,  ..., 1.811, 1.826, 1.628],
        [1.260, 1.552, 1.491,  ..., 1.792, 1.034, 1.312],
        ...,
        [1.375, 2.140, 1.967,  ..., 1.153, 1.148, 2.008],
        [1.128, 1.531, 1.441,  ..., 1.275, 1.526, 1.649],
        [1.453, 1.873, 2.029,  ..., 2.023, 1.588, 2.399]], device='cuda:0')
b after update for 1 param tensor([[170.933, 188.056, 195.689,  ..., 166.815, 179.577, 152.417],
        [146.026, 160.454, 166.397,  ..., 174.063, 174.806, 165.036],
        [145.229, 161.130, 157.934,  ..., 173.171, 131.559, 148.169],
        ...,
        [151.691, 189.215, 181.410,  ..., 138.913, 138.606, 183.329],
        [137.359, 160.045, 155.264,  ..., 146.094, 159.799, 166.102],
        [155.925, 177.028, 184.267,  ..., 183.974, 163.018, 200.351]],
       device='cuda:0')
clipping threshold 2.018500398535765
a after update for 1 param tensor([ 0.125, -0.031, -0.235,  0.076,  0.446,  0.161, -0.457, -0.626, -0.052,
         0.632, -0.356,  0.130, -0.285,  0.054,  0.329,  0.501,  0.475,  0.149,
        -0.529,  0.113,  0.215,  0.234, -0.726, -0.233, -0.139, -0.523,  0.142,
        -0.034,  0.040,  0.389, -0.039,  0.221,  0.185, -0.070, -0.513, -0.430,
         0.242,  0.188, -0.227,  0.177,  0.144, -0.451,  0.512, -0.047,  0.200,
        -0.014, -0.161,  0.081,  0.199,  0.385, -0.478,  0.345,  0.348, -0.222,
        -0.312,  0.271, -0.241, -0.021,  0.294, -0.295, -0.516, -0.587, -0.223,
        -0.057, -0.071, -0.021,  0.415,  0.156, -0.289,  0.133,  0.320,  0.024,
        -0.147,  0.025, -0.432,  0.191, -0.048,  0.324, -1.000,  0.008, -0.306,
        -0.899,  0.026, -0.312, -0.071,  0.023, -0.192,  0.067,  0.492, -0.502,
         0.277,  0.217, -0.280,  0.204,  0.002,  0.146,  0.119, -0.504,  0.067,
        -0.171,  0.137, -0.019, -0.330, -0.211,  0.275, -0.692, -0.425, -0.018,
         0.084,  0.288,  0.110,  0.091,  0.196,  0.221, -0.285,  0.527,  0.021,
        -0.102, -0.114,  0.004,  0.165, -0.213,  0.036,  0.130, -0.030,  0.933,
         0.067, -0.118, -0.729, -0.151, -0.124,  0.277, -0.604, -0.088, -0.353,
        -0.305,  0.154,  0.106,  0.369,  0.191, -0.051,  0.149,  0.058,  0.035,
        -0.080, -0.063, -0.547,  0.633,  0.154,  0.088, -0.082, -0.106,  0.740,
         0.048,  0.743,  0.252,  0.766, -0.057,  0.090, -0.276, -0.174, -0.379,
         0.591,  0.548, -0.016, -0.005, -0.080,  0.345,  0.451,  0.096,  0.084,
         0.340,  0.458, -0.296, -0.187,  0.023,  0.148, -0.466,  0.031, -0.042,
        -0.766, -0.031,  0.057,  0.120,  0.037,  0.170,  0.096,  0.305, -0.318,
        -0.052, -0.790, -0.111,  0.510,  0.380,  0.115, -0.102,  0.438, -0.283,
         0.179, -0.858, -0.208,  0.182, -0.532, -0.448,  0.427,  0.201, -0.115,
        -0.094,  0.263, -0.099, -0.153, -0.223,  0.332, -0.163,  0.315, -0.337,
         0.378, -0.614, -0.286, -0.447, -0.097,  0.039,  0.728, -0.695,  0.861,
        -0.294, -0.331,  0.313,  0.134, -0.190,  0.294,  0.378, -0.182,  0.138,
        -0.205, -0.121,  0.190, -0.052, -0.276,  0.292,  0.575, -0.072,  0.150,
        -0.572, -0.542, -0.182, -0.534,  0.166, -0.122, -0.109, -0.201, -0.201,
         0.288,  0.343,  0.297, -0.250, -0.247, -0.446, -0.066, -0.332, -0.042,
         0.101,  0.174, -0.283,  0.240,  0.092, -0.218, -0.189, -0.293,  0.123,
         0.031, -0.357,  0.899, -0.270, -0.094,  0.554, -0.026, -0.111, -0.856,
         0.392, -0.373, -0.252, -0.241,  0.414, -0.236, -0.867, -0.249,  1.281,
        -0.579,  0.665,  0.273, -0.111,  0.300,  0.066, -0.074, -0.720,  0.803,
        -0.528, -0.302, -0.284], device='cuda:0')
s after update for 1 param tensor([1.719, 1.962, 1.751, 1.169, 1.202, 2.041, 1.854, 1.952, 1.940, 2.237,
        1.578, 1.914, 1.638, 1.552, 2.163, 1.701, 1.433, 1.812, 1.584, 1.570,
        2.076, 1.734, 1.529, 2.082, 1.832, 2.083, 2.129, 1.483, 1.901, 1.958,
        1.586, 2.211, 1.483, 1.458, 1.623, 2.064, 1.859, 1.407, 1.800, 2.084,
        1.253, 1.655, 1.850, 1.849, 1.684, 2.153, 1.198, 1.346, 1.282, 1.803,
        1.836, 1.819, 1.741, 1.777, 1.883, 1.515, 1.826, 2.273, 2.134, 1.877,
        1.918, 1.730, 1.881, 1.903, 1.118, 1.658, 1.625, 2.176, 2.122, 1.561,
        1.799, 1.600, 1.464, 1.867, 2.188, 1.519, 1.417, 1.966, 1.735, 1.332,
        2.416, 1.459, 1.972, 1.190, 1.568, 1.575, 1.344, 1.751, 1.850, 1.447,
        1.390, 1.691, 1.626, 1.518, 1.890, 1.432, 1.746, 1.247, 1.820, 1.591,
        0.785, 1.942, 1.952, 1.679, 2.102, 1.770, 1.271, 1.456, 1.602, 1.372,
        2.161, 2.205, 1.893, 1.637, 1.418, 1.730, 1.715, 1.785, 1.339, 1.891,
        1.776, 1.873, 1.895, 1.184, 1.570, 1.706, 1.265, 1.953, 1.776, 1.520,
        2.504, 1.813, 2.111, 1.882, 1.753, 1.433, 2.084, 1.778, 2.061, 1.664,
        1.371, 1.476, 1.906, 1.568, 2.152, 1.378, 1.841, 1.894, 1.091, 1.526,
        1.814, 1.677, 2.258, 1.038, 1.505, 2.041, 1.352, 1.764, 1.789, 2.277,
        1.275, 1.587, 1.666, 2.037, 1.418, 1.826, 1.661, 2.174, 1.618, 1.815,
        1.348, 1.569, 1.690, 1.843, 1.787, 1.609, 2.165, 1.827, 1.688, 1.103,
        1.866, 1.371, 1.012, 1.555, 2.050, 2.005, 2.295, 1.791, 1.829, 1.644,
        2.028, 1.511, 2.164, 1.381, 1.500, 1.551, 2.087, 2.098, 1.893, 1.849,
        1.703, 2.017, 2.077, 1.694, 1.433, 1.593, 1.325, 1.938, 1.719, 1.473,
        1.614, 1.464, 1.689, 1.169, 1.776, 1.619, 2.194, 1.644, 1.586, 1.757,
        2.156, 1.591, 1.939, 2.076, 2.127, 1.439, 1.846, 1.436, 1.271, 1.310,
        2.013, 1.413, 1.580, 1.921, 1.281, 1.863, 1.847, 1.850, 1.829, 1.789,
        2.045, 2.044, 1.633, 1.117, 1.540, 1.243, 2.050, 1.361, 0.838, 1.445,
        1.469, 1.295, 1.572, 1.769, 1.575, 1.815, 1.942, 1.997, 1.841, 1.543,
        1.611, 1.372, 2.202, 1.776, 1.718, 1.410, 1.761, 1.724, 1.601, 1.520,
        1.392, 1.789, 1.785, 1.299, 1.717, 1.380, 1.832, 1.384, 1.887, 2.029,
        1.682, 1.671, 1.886, 0.896, 1.401, 1.794, 1.414, 1.906, 1.779, 1.414,
        1.749, 1.599, 1.640, 1.586, 1.837, 2.204, 1.526, 1.847, 0.967, 2.061],
       device='cuda:0')
b after update for 1 param tensor([169.601, 181.211, 171.179, 139.835, 141.841, 184.784, 176.139, 180.723,
        180.167, 193.492, 162.507, 178.948, 165.582, 161.176, 190.246, 168.714,
        154.867, 174.142, 162.829, 162.106, 186.386, 170.361, 159.948, 186.638,
        175.088, 186.677, 188.746, 157.518, 178.361, 181.019, 162.889, 192.352,
        157.508, 156.203, 164.785, 185.859, 176.397, 153.464, 173.552, 186.721,
        144.780, 166.405, 175.925, 175.905, 167.853, 189.821, 141.565, 150.077,
        146.469, 173.708, 175.303, 174.446, 170.704, 172.454, 177.507, 159.207,
        174.794, 195.045, 188.948, 177.207, 179.139, 170.123, 177.395, 178.456,
        136.757, 166.555, 164.898, 190.813, 188.453, 161.642, 173.519, 163.617,
        156.539, 176.761, 191.344, 159.411, 153.994, 181.373, 170.390, 149.316,
        201.073, 156.257, 181.650, 141.142, 161.964, 162.354, 149.941, 171.158,
        175.968, 155.621, 152.520, 168.215, 164.930, 159.360, 177.841, 154.788,
        170.915, 144.452, 174.504, 163.142, 114.604, 180.283, 180.737, 167.608,
        187.549, 172.124, 145.823, 156.087, 163.729, 151.548, 190.153, 192.073,
        178.000, 165.485, 154.054, 170.153, 169.409, 172.807, 149.662, 177.887,
        172.377, 177.016, 178.056, 140.744, 162.064, 168.979, 145.494, 180.801,
        172.384, 159.509, 204.707, 174.202, 187.945, 177.483, 171.256, 154.871,
        186.741, 172.508, 185.713, 166.863, 151.465, 157.148, 178.593, 161.966,
        189.772, 151.856, 175.504, 178.018, 135.103, 159.823, 174.220, 167.509,
        194.384, 131.820, 158.678, 184.806, 150.408, 171.823, 173.037, 195.208,
        146.074, 162.940, 166.959, 184.627, 154.048, 174.794, 166.716, 190.754,
        164.524, 174.270, 150.203, 162.057, 168.181, 175.620, 172.937, 164.094,
        190.319, 174.835, 168.084, 135.882, 176.683, 151.438, 130.124, 161.288,
        185.215, 183.170, 195.961, 173.102, 174.947, 165.884, 184.224, 158.999,
        190.277, 152.033, 158.452, 161.088, 186.867, 187.374, 177.984, 175.918,
        168.803, 183.714, 186.410, 168.343, 154.856, 163.283, 148.883, 180.104,
        169.597, 156.984, 164.340, 156.539, 168.138, 139.849, 172.401, 164.590,
        191.629, 165.857, 162.916, 171.488, 189.927, 163.169, 180.147, 186.399,
        188.655, 155.197, 175.748, 155.009, 145.856, 148.074, 183.542, 153.748,
        162.599, 179.271, 146.399, 176.558, 175.828, 175.928, 174.943, 173.008,
        184.997, 184.936, 165.293, 136.743, 160.533, 144.216, 185.193, 150.896,
        118.435, 155.474, 156.812, 147.232, 162.214, 172.044, 162.331, 174.272,
        180.290, 182.801, 175.535, 160.686, 164.166, 151.541, 191.941, 172.393,
        169.555, 153.595, 171.657, 169.866, 163.704, 159.506, 152.609, 173.031,
        172.849, 147.414, 169.511, 151.976, 175.099, 152.191, 177.685, 184.272,
        167.758, 167.199, 177.645, 122.473, 153.132, 173.243, 153.804, 178.608,
        172.541, 153.806, 171.099, 163.557, 165.665, 162.897, 175.335, 192.055,
        159.803, 175.815, 127.234, 185.690], device='cuda:0')
clipping threshold 2.018500398535765
a after update for 1 param tensor([[[ 0.259],
         [-0.464],
         [-0.359],
         [-0.236],
         [-0.399],
         [ 0.473],
         [-0.225],
         [-0.003],
         [-0.010],
         [ 0.284]],

        [[-0.238],
         [ 0.456],
         [-0.184],
         [-0.234],
         [ 0.243],
         [ 0.637],
         [ 0.540],
         [-0.209],
         [-0.186],
         [-0.070]],

        [[-0.186],
         [-0.531],
         [ 0.008],
         [-0.681],
         [ 0.429],
         [ 0.244],
         [-0.392],
         [-0.495],
         [ 0.531],
         [-0.394]],

        [[ 0.428],
         [-0.029],
         [ 0.262],
         [ 0.069],
         [ 0.508],
         [ 0.285],
         [ 0.028],
         [ 0.141],
         [ 0.170],
         [-0.310]],

        [[ 0.317],
         [ 0.026],
         [ 0.005],
         [-0.101],
         [ 0.004],
         [-0.468],
         [ 0.071],
         [-0.071],
         [-0.094],
         [ 0.484]],

        [[ 0.424],
         [-0.066],
         [-0.108],
         [-0.501],
         [-0.050],
         [-0.555],
         [-0.260],
         [ 0.581],
         [-0.188],
         [ 0.814]],

        [[ 0.123],
         [ 0.242],
         [-0.427],
         [-0.122],
         [-0.429],
         [ 0.414],
         [ 0.100],
         [ 0.512],
         [-0.420],
         [-0.025]],

        [[-0.043],
         [-0.009],
         [ 0.385],
         [ 0.428],
         [-0.398],
         [ 0.459],
         [-0.724],
         [-0.362],
         [-0.235],
         [ 0.153]],

        [[-0.307],
         [ 0.031],
         [-0.273],
         [ 0.099],
         [ 0.261],
         [ 0.226],
         [-0.124],
         [ 0.772],
         [ 0.401],
         [ 0.289]],

        [[-0.343],
         [-0.241],
         [ 0.301],
         [ 0.212],
         [ 0.142],
         [ 0.012],
         [ 0.247],
         [ 0.088],
         [-0.081],
         [-0.133]],

        [[ 0.562],
         [-0.004],
         [ 0.194],
         [ 0.241],
         [-0.220],
         [-0.278],
         [ 0.109],
         [-0.397],
         [-0.455],
         [-0.357]],

        [[ 0.707],
         [ 0.762],
         [ 0.426],
         [ 0.565],
         [-0.110],
         [ 0.599],
         [ 0.268],
         [-0.057],
         [ 0.238],
         [ 0.062]],

        [[ 0.296],
         [ 0.453],
         [ 0.245],
         [ 0.346],
         [ 0.030],
         [-0.276],
         [-0.110],
         [ 0.255],
         [-0.051],
         [ 0.161]],

        [[-0.107],
         [ 0.494],
         [ 0.538],
         [ 0.081],
         [-0.438],
         [-0.422],
         [ 0.519],
         [ 0.070],
         [-0.034],
         [-0.175]],

        [[ 0.540],
         [ 0.101],
         [ 0.221],
         [-0.025],
         [ 0.248],
         [ 0.529],
         [-0.131],
         [-0.085],
         [ 0.254],
         [ 0.146]],

        [[ 0.016],
         [-0.604],
         [ 0.463],
         [ 0.580],
         [-0.051],
         [ 0.166],
         [-0.516],
         [ 0.372],
         [ 0.356],
         [ 0.041]],

        [[-1.045],
         [ 0.315],
         [-0.378],
         [ 0.081],
         [ 0.225],
         [ 0.157],
         [ 0.612],
         [-0.073],
         [ 0.035],
         [ 0.637]],

        [[-0.009],
         [ 0.227],
         [-0.076],
         [-0.363],
         [ 0.335],
         [ 0.467],
         [ 0.369],
         [ 0.023],
         [ 0.071],
         [-0.143]],

        [[ 0.142],
         [-0.746],
         [ 0.246],
         [ 0.339],
         [ 0.086],
         [-0.725],
         [ 0.477],
         [-0.478],
         [ 0.201],
         [ 0.431]],

        [[ 0.965],
         [ 0.127],
         [-0.212],
         [-0.527],
         [-0.164],
         [-0.387],
         [ 0.069],
         [-0.069],
         [-0.009],
         [ 0.247]],

        [[-0.483],
         [-0.483],
         [ 0.096],
         [ 0.035],
         [ 0.134],
         [-0.237],
         [-0.238],
         [-0.601],
         [-0.187],
         [-0.140]],

        [[ 0.017],
         [-0.114],
         [ 0.144],
         [ 0.051],
         [-0.507],
         [-0.814],
         [ 0.088],
         [-0.487],
         [ 0.161],
         [-0.119]],

        [[ 0.161],
         [ 0.537],
         [ 0.522],
         [ 0.370],
         [ 0.215],
         [ 0.110],
         [ 0.134],
         [ 0.257],
         [-0.583],
         [-0.070]],

        [[-0.035],
         [-0.246],
         [-0.018],
         [ 0.013],
         [ 0.065],
         [-0.114],
         [ 0.580],
         [ 0.024],
         [-0.206],
         [ 0.485]],

        [[-0.342],
         [ 0.055],
         [ 0.299],
         [-0.176],
         [ 0.018],
         [-0.323],
         [ 0.281],
         [-0.018],
         [ 0.229],
         [ 0.150]],

        [[-0.216],
         [-0.535],
         [-0.055],
         [ 0.465],
         [-0.328],
         [ 0.286],
         [-0.115],
         [-0.562],
         [ 0.177],
         [ 0.268]],

        [[ 0.195],
         [ 0.402],
         [ 0.076],
         [ 0.280],
         [-0.337],
         [-1.019],
         [ 0.462],
         [ 0.067],
         [-0.421],
         [-0.125]],

        [[ 0.221],
         [ 0.480],
         [ 0.030],
         [ 0.173],
         [ 0.316],
         [-0.043],
         [ 0.182],
         [ 0.164],
         [-0.125],
         [ 0.099]],

        [[ 0.060],
         [ 0.477],
         [ 0.009],
         [-0.221],
         [-0.153],
         [ 0.002],
         [-0.262],
         [ 0.297],
         [-0.320],
         [ 0.356]],

        [[ 0.011],
         [-0.366],
         [-0.365],
         [-0.083],
         [ 0.356],
         [ 0.206],
         [-0.127],
         [ 0.591],
         [ 0.741],
         [-0.592]]], device='cuda:0')
s after update for 1 param tensor([[[1.370],
         [1.724],
         [2.137],
         [1.637],
         [1.565],
         [1.723],
         [2.365],
         [1.261],
         [1.313],
         [0.990]],

        [[2.404],
         [1.377],
         [1.667],
         [1.976],
         [1.449],
         [1.580],
         [2.213],
         [1.940],
         [1.108],
         [1.853]],

        [[1.873],
         [1.675],
         [1.945],
         [0.796],
         [1.560],
         [1.731],
         [1.337],
         [1.618],
         [1.409],
         [2.301]],

        [[1.178],
         [1.637],
         [2.256],
         [1.381],
         [1.508],
         [1.634],
         [0.939],
         [2.381],
         [1.700],
         [1.216]],

        [[1.792],
         [1.493],
         [2.055],
         [1.478],
         [1.686],
         [1.179],
         [1.705],
         [1.659],
         [1.942],
         [2.069]],

        [[1.622],
         [1.780],
         [1.435],
         [2.022],
         [1.669],
         [1.355],
         [1.801],
         [1.968],
         [1.913],
         [1.725]],

        [[1.823],
         [1.318],
         [1.196],
         [1.670],
         [1.346],
         [1.471],
         [1.977],
         [1.651],
         [0.989],
         [1.701]],

        [[1.696],
         [1.603],
         [1.941],
         [2.104],
         [2.132],
         [1.953],
         [2.011],
         [2.040],
         [1.838],
         [2.076]],

        [[1.696],
         [1.500],
         [2.038],
         [1.950],
         [2.192],
         [1.449],
         [1.448],
         [1.057],
         [1.732],
         [1.619]],

        [[1.421],
         [2.085],
         [1.658],
         [1.451],
         [1.677],
         [2.267],
         [2.233],
         [1.865],
         [1.647],
         [1.723]],

        [[1.669],
         [1.646],
         [1.655],
         [1.374],
         [1.746],
         [2.368],
         [1.829],
         [1.646],
         [1.699],
         [2.069]],

        [[1.810],
         [1.595],
         [2.239],
         [1.749],
         [2.020],
         [1.627],
         [1.584],
         [2.255],
         [1.350],
         [1.702]],

        [[1.373],
         [2.056],
         [2.041],
         [2.147],
         [1.313],
         [1.638],
         [1.690],
         [1.375],
         [1.880],
         [1.723]],

        [[1.468],
         [1.762],
         [1.864],
         [1.801],
         [1.720],
         [1.474],
         [1.850],
         [1.482],
         [1.616],
         [1.402]],

        [[1.330],
         [1.731],
         [1.877],
         [1.479],
         [1.812],
         [1.847],
         [1.862],
         [1.515],
         [1.762],
         [2.105]],

        [[1.838],
         [1.233],
         [1.826],
         [2.121],
         [1.626],
         [1.690],
         [1.674],
         [1.648],
         [1.820],
         [1.674]],

        [[1.699],
         [1.505],
         [1.787],
         [1.475],
         [1.724],
         [1.863],
         [1.673],
         [1.504],
         [1.831],
         [1.746]],

        [[1.564],
         [2.145],
         [1.863],
         [1.800],
         [1.819],
         [1.689],
         [1.605],
         [1.402],
         [2.359],
         [1.896]],

        [[1.045],
         [2.164],
         [1.670],
         [1.655],
         [1.601],
         [2.235],
         [1.521],
         [1.251],
         [2.099],
         [2.031]],

        [[2.151],
         [1.567],
         [1.503],
         [1.669],
         [1.277],
         [1.831],
         [1.744],
         [1.644],
         [1.745],
         [1.705]],

        [[1.849],
         [2.024],
         [1.584],
         [1.534],
         [1.836],
         [1.650],
         [1.682],
         [1.771],
         [1.506],
         [2.250]],

        [[1.758],
         [2.068],
         [1.891],
         [1.472],
         [2.166],
         [2.109],
         [2.516],
         [1.759],
         [0.726],
         [2.210]],

        [[1.933],
         [2.067],
         [1.920],
         [2.113],
         [1.667],
         [2.009],
         [1.885],
         [1.850],
         [1.286],
         [1.940]],

        [[1.505],
         [1.376],
         [1.867],
         [1.913],
         [1.471],
         [1.763],
         [2.007],
         [1.171],
         [1.943],
         [1.463]],

        [[1.201],
         [1.139],
         [1.973],
         [1.548],
         [1.707],
         [1.723],
         [1.411],
         [2.388],
         [1.521],
         [2.165]],

        [[1.978],
         [1.802],
         [0.954],
         [1.522],
         [0.839],
         [1.946],
         [1.414],
         [2.164],
         [1.406],
         [1.754]],

        [[1.668],
         [1.099],
         [1.812],
         [1.595],
         [2.121],
         [1.450],
         [2.346],
         [1.999],
         [1.448],
         [1.640]],

        [[1.670],
         [1.600],
         [1.135],
         [1.642],
         [2.238],
         [1.871],
         [1.849],
         [1.836],
         [1.228],
         [1.474]],

        [[1.601],
         [1.291],
         [1.760],
         [1.751],
         [2.032],
         [1.760],
         [1.676],
         [1.803],
         [1.741],
         [1.244]],

        [[1.470],
         [1.536],
         [2.272],
         [1.206],
         [1.877],
         [1.179],
         [1.840],
         [2.026],
         [1.661],
         [2.088]]], device='cuda:0')
b after update for 1 param tensor([[[151.414],
         [169.852],
         [189.102],
         [165.498],
         [161.803],
         [169.778],
         [198.941],
         [145.276],
         [148.225],
         [128.730]],

        [[200.581],
         [151.781],
         [167.015],
         [181.841],
         [155.701],
         [162.593],
         [192.443],
         [180.190],
         [136.181],
         [176.093]],

        [[177.015],
         [167.409],
         [180.402],
         [115.434],
         [161.567],
         [170.175],
         [149.551],
         [164.566],
         [153.554],
         [196.238]],

        [[140.378],
         [165.496],
         [194.305],
         [152.016],
         [158.837],
         [165.332],
         [125.384],
         [199.610],
         [168.656],
         [142.662]],

        [[173.175],
         [158.073],
         [185.427],
         [157.266],
         [167.984],
         [140.433],
         [168.903],
         [166.614],
         [180.251],
         [186.087]],

        [[164.725],
         [172.577],
         [154.977],
         [183.948],
         [167.102],
         [150.579],
         [173.606],
         [181.483],
         [178.912],
         [169.910]],

        [[174.639],
         [148.534],
         [141.490],
         [167.184],
         [150.054],
         [156.875],
         [181.885],
         [166.198],
         [128.651],
         [168.710]],

        [[168.481],
         [163.798],
         [180.237],
         [187.656],
         [188.888],
         [180.799],
         [183.450],
         [184.774],
         [175.376],
         [186.371]],

        [[168.445],
         [158.420],
         [184.658],
         [180.631],
         [191.516],
         [155.730],
         [155.677],
         [132.982],
         [170.225],
         [164.576]],

        [[154.214],
         [186.772],
         [166.571],
         [155.815],
         [167.543],
         [194.770],
         [193.306],
         [176.653],
         [166.032],
         [169.781]],

        [[167.131],
         [165.956],
         [166.402],
         [151.659],
         [170.920],
         [199.080],
         [174.960],
         [165.966],
         [168.637],
         [186.067]],

        [[174.038],
         [163.359],
         [193.551],
         [171.073],
         [183.841],
         [164.985],
         [162.813],
         [194.260],
         [150.276],
         [168.782]],

        [[151.556],
         [185.505],
         [184.787],
         [189.532],
         [148.241],
         [165.561],
         [168.180],
         [151.702],
         [177.386],
         [169.795]],

        [[156.725],
         [171.713],
         [176.613],
         [173.588],
         [169.665],
         [157.062],
         [175.953],
         [157.465],
         [164.442],
         [153.169]],

        [[149.165],
         [170.207],
         [177.218],
         [157.330],
         [174.148],
         [175.810],
         [176.509],
         [159.217],
         [171.713],
         [187.674]],

        [[175.359],
         [143.636],
         [174.817],
         [188.373],
         [164.973],
         [168.177],
         [167.380],
         [166.078],
         [174.508],
         [167.355]],

        [[168.600],
         [158.690],
         [172.928],
         [157.089],
         [169.830],
         [176.551],
         [167.315],
         [158.627],
         [175.032],
         [170.954]],

        [[161.782],
         [189.438],
         [176.583],
         [173.529],
         [174.447],
         [168.134],
         [163.896],
         [153.161],
         [198.679],
         [178.104]],

        [[132.267],
         [190.290],
         [167.176],
         [166.426],
         [163.655],
         [193.396],
         [159.552],
         [144.701],
         [187.402],
         [184.348]],

        [[189.736],
         [161.952],
         [158.573],
         [167.125],
         [146.196],
         [175.046],
         [170.832],
         [165.838],
         [170.904],
         [168.931]],

        [[175.901],
         [184.058],
         [162.797],
         [160.223],
         [175.261],
         [166.155],
         [167.784],
         [172.127],
         [158.759],
         [194.049]],

        [[171.502],
         [186.030],
         [177.901],
         [156.949],
         [190.381],
         [187.881],
         [205.199],
         [171.565],
         [110.203],
         [192.297]],

        [[179.833],
         [185.965],
         [179.242],
         [188.047],
         [167.031],
         [183.371],
         [177.597],
         [175.958],
         [146.685],
         [180.169]],

        [[158.689],
         [151.719],
         [176.763],
         [178.919],
         [156.875],
         [171.747],
         [183.249],
         [139.966],
         [180.308],
         [156.463]],

        [[141.737],
         [138.027],
         [181.694],
         [160.936],
         [169.018],
         [169.785],
         [153.633],
         [199.914],
         [159.549],
         [190.330]],

        [[181.914],
         [173.661],
         [126.350],
         [159.568],
         [118.477],
         [180.477],
         [153.835],
         [190.295],
         [153.372],
         [171.303]],

        [[167.075],
         [135.637],
         [174.147],
         [163.348],
         [188.383],
         [155.781],
         [198.137],
         [182.906],
         [155.664],
         [165.641]],

        [[167.147],
         [163.646],
         [137.833],
         [165.779],
         [193.513],
         [176.922],
         [175.893],
         [175.277],
         [143.355],
         [157.056]],

        [[163.688],
         [146.983],
         [171.596],
         [171.192],
         [184.412],
         [171.606],
         [167.486],
         [173.684],
         [170.690],
         [144.292]],

        [[156.832],
         [160.332],
         [194.976],
         [142.042],
         [177.230],
         [140.458],
         [175.470],
         [184.106],
         [166.711],
         [186.928]]], device='cuda:0')
clipping threshold 2.018500398535765
a after update for 1 param tensor([[ 0.294],
        [-0.574],
        [ 0.091],
        [ 0.247],
        [ 0.129],
        [-0.174],
        [-0.299],
        [-0.045],
        [ 0.299],
        [-0.657],
        [ 0.035],
        [ 0.830],
        [ 0.176],
        [-0.020],
        [-0.059],
        [ 0.212],
        [ 0.006],
        [ 0.233],
        [ 0.072],
        [-0.356],
        [ 0.889],
        [ 0.434],
        [-0.093],
        [-0.377],
        [ 0.118],
        [ 0.034],
        [-0.330],
        [ 0.311],
        [ 0.052],
        [ 0.353]], device='cuda:0')
s after update for 1 param tensor([[1.875],
        [1.942],
        [1.885],
        [1.632],
        [1.285],
        [1.930],
        [1.665],
        [1.383],
        [1.749],
        [1.920],
        [1.926],
        [2.007],
        [1.602],
        [2.086],
        [1.849],
        [1.781],
        [1.987],
        [1.424],
        [1.966],
        [1.843],
        [1.401],
        [1.743],
        [2.168],
        [1.735],
        [1.412],
        [1.970],
        [1.568],
        [1.829],
        [1.356],
        [1.545]], device='cuda:0')
b after update for 1 param tensor([[177.153],
        [180.250],
        [177.602],
        [165.263],
        [146.632],
        [179.691],
        [166.912],
        [152.126],
        [171.096],
        [179.262],
        [179.535],
        [183.269],
        [163.706],
        [186.828],
        [175.918],
        [172.627],
        [182.363],
        [154.354],
        [181.399],
        [175.591],
        [153.129],
        [170.767],
        [190.456],
        [170.382],
        [153.704],
        [181.579],
        [161.959],
        [174.961],
        [150.627],
        [160.803]], device='cuda:0')
clipping threshold 2.018500398535765
||w||^2 3.571530130528112
exp ma of ||w||^2 6.090588413081585
||w|| 1.8898492348671923
exp ma of ||w|| 2.389437664432093
||w||^2 4.628975851159837
exp ma of ||w||^2 6.174197266032346
||w|| 2.151505484808216
exp ma of ||w|| 2.40467001579692
||w||^2 2.2136380655940915
exp ma of ||w||^2 6.419601344639165
||w|| 1.4878299854466206
exp ma of ||w|| 2.4537309070968885
||w||^2 5.428084029455577
exp ma of ||w||^2 5.855910590353102
||w|| 2.3298248924448326
exp ma of ||w|| 2.3465678612496763
||w||^2 9.4534956556621
exp ma of ||w||^2 5.830922376739833
||w|| 3.0746537456536633
exp ma of ||w|| 2.339912609011354
||w||^2 4.391457271247528
exp ma of ||w||^2 6.11905752904517
||w|| 2.095580413930119
exp ma of ||w|| 2.3984431582586265
||w||^2 9.679189533760711
exp ma of ||w||^2 6.0914257841994255
||w|| 3.111139587636773
exp ma of ||w|| 2.4088262443436443
||w||^2 5.51296225846689
exp ma of ||w||^2 6.033717972070042
||w|| 2.3479698163449396
exp ma of ||w|| 2.3842641781637033
||w||^2 15.628934974985242
exp ma of ||w||^2 6.220079712895138
||w|| 3.953344783216516
exp ma of ||w|| 2.410811329819617
||w||^2 6.19850248978155
exp ma of ||w||^2 5.792711297081092
||w|| 2.48967919414963
exp ma of ||w|| 2.3245856450832627
cuda
Objective function 139.20 = squared loss an data 79.58 + 0.5*rho*h**2 13.381650 + alpha*h 43.123012 + L2reg 2.69 + L1reg 0.43 ; SHD = 175 ; DAG True
Proportion of microbatches that were clipped  0.7789907811741873
iteration 1 in inner loop, alpha 83.35645917681265 rho 100.0 h 0.5173325772664867
9630
cuda
Objective function 259.63 = squared loss an data 79.58 + 0.5*rho*h**2 133.816498 + alpha*h 43.123012 + L2reg 2.69 + L1reg 0.43 ; SHD = 175 ; DAG True
||w||^2 3499587322.540408
exp ma of ||w||^2 209752706924.4226
||w|| 59157.30996707345
exp ma of ||w|| 293858.5952068211
||w||^2 48338.61764300787
exp ma of ||w||^2 1725692031.3794658
||w|| 219.86045038389207
exp ma of ||w|| 4126.101520893322
||w||^2 3.7372244887135544
exp ma of ||w||^2 6.8552081293553595
||w|| 1.9331902360382318
exp ma of ||w|| 2.540587242208853
||w||^2 8.326785466433911
exp ma of ||w||^2 6.846934837849399
||w|| 2.8856169992627074
exp ma of ||w|| 2.5400492482583
||w||^2 4.098816004527254
exp ma of ||w||^2 7.319514553459302
||w|| 2.024553285178549
exp ma of ||w|| 2.6203098045313493
||w||^2 3.6636257515730644
exp ma of ||w||^2 6.774840182581384
||w|| 1.9140600177562521
exp ma of ||w|| 2.516404161045709
||w||^2 6.799499168518352
exp ma of ||w||^2 6.971147269133081
||w|| 2.607584930259866
exp ma of ||w|| 2.55324000274678
||w||^2 7.222044637425037
exp ma of ||w||^2 7.081457553367557
||w|| 2.687386209204966
exp ma of ||w|| 2.5768705626806296
||w||^2 13.569906898708558
exp ma of ||w||^2 6.3528756968460245
||w|| 3.6837354544956886
exp ma of ||w|| 2.4434205772541446
||w||^2 8.269672446391079
exp ma of ||w||^2 6.285533087217105
||w|| 2.8757038175707663
exp ma of ||w|| 2.4444786029988266
||w||^2 3.6028849021959988
exp ma of ||w||^2 6.511746248539781
||w|| 1.8981266823360339
exp ma of ||w|| 2.4796060681189016
||w||^2 6.402407088157644
exp ma of ||w||^2 6.831235066106078
||w|| 2.5302978259797095
exp ma of ||w|| 2.539981595468002
||w||^2 6.512450755953877
exp ma of ||w||^2 7.095360943714595
||w|| 2.5519503827374614
exp ma of ||w|| 2.5880927078496967
||w||^2 5.6328911578416925
exp ma of ||w||^2 6.972182329630711
||w|| 2.373371264223466
exp ma of ||w|| 2.5731883064709096
cuda
Objective function 136.61 = squared loss an data 83.17 + 0.5*rho*h**2 29.830281 + alpha*h 20.360242 + L2reg 2.85 + L1reg 0.39 ; SHD = 170 ; DAG True
Proportion of microbatches that were clipped  0.7830346475507766
iteration 2 in inner loop, alpha 83.35645917681265 rho 1000.0 h 0.24425511675032396
9630
cuda
Objective function 405.08 = squared loss an data 83.17 + 0.5*rho*h**2 298.302810 + alpha*h 20.360242 + L2reg 2.85 + L1reg 0.39 ; SHD = 170 ; DAG True
||w||^2 178847671439.91324
exp ma of ||w||^2 2558987501730.8867
||w|| 422903.85602393514
exp ma of ||w|| 1122432.035926821
||w||^2 53437167.99341129
exp ma of ||w||^2 85493085811.30745
||w|| 7310.073049799933
exp ma of ||w|| 74778.93026988066
||w||^2 8.77411720480197
exp ma of ||w||^2 316.7287357812747
||w|| 2.9621136380635313
exp ma of ||w|| 3.147092955867353
||w||^2 14.576655992957377
exp ma of ||w||^2 110.09047880169365
||w|| 3.8179387099529736
exp ma of ||w|| 3.1924817794951488
||w||^2 8.649508195881042
exp ma of ||w||^2 58.98569478406084
||w|| 2.941004623573557
exp ma of ||w|| 3.1337905712885776
||w||^2 11.279690269724211
exp ma of ||w||^2 8.936714393116874
||w|| 3.3585250140090084
exp ma of ||w|| 2.928824291786936
||w||^2 4.975613303770784
exp ma of ||w||^2 8.895588836302446
||w|| 2.2306082811132
exp ma of ||w|| 2.918918081084936
||w||^2 8.754490325016615
exp ma of ||w||^2 9.267294520815044
||w|| 2.958798797657018
exp ma of ||w|| 2.9727393861402165
||w||^2 17.898174381617117
exp ma of ||w||^2 8.834385541060577
||w|| 4.230623403426157
exp ma of ||w|| 2.9098663707049215
||w||^2 16.029587331533545
exp ma of ||w||^2 9.879637508792836
||w|| 4.003696708235221
exp ma of ||w|| 3.0858980277297903
||w||^2 6.629414143590297
exp ma of ||w||^2 9.883661503651417
||w|| 2.574764871515513
exp ma of ||w|| 3.0832432706846413
||w||^2 5.8274812865998875
exp ma of ||w||^2 9.537070944848043
||w|| 2.414017664931201
exp ma of ||w|| 3.026046298958365
v before min max tensor([[ 3955.286, -1629.829,  1138.124,  ...,  -984.965,  -829.610,
         -1144.243],
        [-2190.813,  1213.149,  -716.781,  ..., -1258.906, -2702.193,
         -1823.658],
        [-2512.065,  2384.686, -1290.636,  ..., 24202.923, -2379.151,
         -1964.482],
        ...,
        [ 5015.418, -2194.436, -2176.990,  ..., -2110.550, -1768.638,
         -2298.047],
        [-2259.623,  7373.333, -2519.641,  ...,  3231.446, -2458.487,
          3781.070],
        [  565.737,  6561.981, -2288.943,  ..., -1222.948,  -329.876,
          8158.718]], device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        ...,
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e+01]], device='cuda:0')
v before min max tensor([ 9.205e+01,  7.995e+02, -2.590e+03, -2.297e+03, -3.598e+02, -1.763e+02,
         9.363e+02, -2.266e+03, -2.196e+03,  2.762e+03,  1.978e+03,  1.412e+03,
        -8.102e+02, -1.484e+03, -2.405e+03,  4.744e+03, -1.999e+03, -1.414e+03,
         9.103e+02,  5.464e+03,  1.056e+03, -1.501e+03, -5.901e+02, -2.700e+03,
         2.049e+03,  1.601e+03, -2.368e+03, -4.842e+02,  6.266e+02, -2.180e+03,
         2.019e+03, -8.518e+02, -9.148e+02, -1.914e+03,  2.268e+03, -5.235e+02,
        -1.479e+03, -1.641e+03, -1.693e+03,  3.183e+03, -8.766e+02,  4.321e+02,
        -1.172e+03, -1.316e+03,  2.528e+03,  1.833e+03,  1.195e+04, -1.541e+03,
         1.359e+04, -7.405e+02, -8.385e+02,  1.252e+03, -1.373e+03, -2.219e+03,
         8.995e+02, -1.407e+03,  1.501e+03, -2.498e+03, -2.327e+03, -2.422e+03,
        -1.776e+03, -2.324e+03, -6.952e+02, -4.842e+02, -1.573e+03, -1.903e+03,
         6.245e+02, -1.726e+03,  5.273e+03, -1.105e+03,  5.278e+03, -2.396e+03,
        -1.342e+03, -1.525e+03, -1.412e+03, -1.546e+03,  2.398e+03, -1.858e+03,
        -2.431e+03, -1.735e+03, -2.171e+03, -2.113e+03, -2.244e+03,  2.491e+03,
        -1.875e+03, -1.573e+03, -9.426e+02,  1.314e+03, -1.634e+03, -7.130e+02,
        -6.217e+02,  1.051e+03, -2.849e+02, -9.423e+02,  2.310e+02, -2.064e+03,
        -1.661e+03, -2.039e+03, -1.284e+03, -1.115e+03, -2.771e+02,  8.482e+03,
        -1.701e+03,  2.424e+02, -1.264e+03, -2.103e+03, -2.574e+03, -1.118e+03,
        -2.421e+03,  1.079e+03, -1.753e+03, -8.762e+02, -2.765e+02,  1.739e+03,
        -6.835e+02,  3.639e+03,  2.473e+03,  1.509e+03, -1.926e+03, -1.703e+03,
        -1.571e+03,  1.950e+03, -2.208e+03, -2.432e+03, -2.085e+03, -5.939e+02,
        -1.206e+03, -1.171e+03,  5.498e+03, -1.111e+03,  1.247e+03,  3.145e+03,
        -2.165e+03, -1.009e+03,  1.617e+03, -1.803e+03, -2.050e+03, -1.168e+03,
        -8.233e+02, -2.225e+03, -1.619e+03, -9.309e+02, -1.416e+03,  1.317e+03,
        -6.885e+02, -1.606e+03, -1.175e+03, -2.592e+03, -1.390e+03, -2.347e+03,
        -1.677e+03, -9.697e+02,  1.040e+04, -1.884e+03, -1.978e+03, -1.207e+01,
         2.353e+03, -2.693e+03,  5.464e+02, -2.247e+03, -1.706e+02, -1.854e+03,
        -5.141e+02, -1.519e+03, -1.279e+03, -1.538e+03, -8.170e+02, -2.597e+03,
        -2.313e+03, -1.644e+03,  1.221e+04, -2.347e+03,  3.569e+03, -1.796e+03,
        -1.472e+03, -1.342e+03, -1.080e+03, -1.641e+03,  6.314e+02, -1.576e+03,
        -2.254e+03,  3.091e+02, -2.095e+03, -1.795e+03, -1.736e+03, -1.784e+03,
        -9.614e+02, -1.353e+03, -1.792e+03, -2.057e+03,  4.496e+03, -1.121e+03,
         6.048e+03, -2.159e+03,  2.472e+02,  2.560e+01, -2.242e+03,  2.405e+03,
        -8.834e+02, -1.400e+03, -1.733e+03,  9.037e+01, -2.391e+03, -4.346e+01,
         7.818e+02, -9.835e+02, -1.546e+03,  6.798e+03,  1.386e+03, -1.628e+03,
        -1.883e+03, -1.690e+03, -1.378e+03,  1.568e+02,  8.231e+01, -6.958e+02,
        -1.946e+03, -5.211e+02, -1.596e+03,  1.040e+03,  4.235e+03, -1.767e+03,
        -1.942e+03, -7.018e+02,  6.265e+03,  1.811e+03, -2.228e+03, -1.056e+03,
        -2.174e+03, -1.565e+03, -1.931e+03, -1.788e+03, -1.438e+03,  3.984e+03,
         7.122e+02, -1.443e+03,  5.014e+03, -1.901e+03, -2.203e+03,  7.099e+03,
         5.393e+03,  2.922e+03, -1.880e+03, -2.526e+03, -6.325e+02,  1.559e+02,
        -2.945e+03, -9.794e+02, -2.361e+03, -2.600e+03,  7.705e+02, -2.013e+03,
        -2.836e+02, -1.906e+03, -2.274e+03, -2.205e+03, -2.198e+03,  1.832e+03,
        -2.108e+03, -1.570e+03,  3.763e+02, -1.737e+03,  3.254e+03, -8.807e+02,
        -1.639e+03, -1.677e+03,  1.606e+02, -9.589e+02,  7.696e+02, -1.679e+03,
        -1.997e+03, -1.315e+03, -1.151e+03,  3.396e+03, -1.902e+03,  1.528e+03,
        -2.053e+03, -1.120e+03, -8.098e+02, -2.128e+03,  6.746e+02, -1.539e+03,
        -2.262e+03, -1.980e+03, -8.625e+01,  1.408e+03,  1.297e+02, -9.380e+02,
        -1.324e+03,  3.006e+03, -2.322e+03, -1.992e+03, -2.176e+01,  5.305e+03,
        -2.072e+03, -1.054e+03, -1.377e+03, -2.175e+02,  3.017e+03, -7.478e+02],
       device='cuda:0')
v tensor([1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[-1840.483],
         [-1979.256],
         [-1694.498],
         [ -850.570],
         [ 1183.625],
         [ 2750.201],
         [-1665.666],
         [-1683.141],
         [-2051.241],
         [ 2863.018]],

        [[-2076.815],
         [-1044.346],
         [ 2150.728],
         [-1857.647],
         [ -912.277],
         [ 3091.864],
         [11076.696],
         [-2709.857],
         [-2439.297],
         [-1734.233]],

        [[  161.651],
         [-1964.630],
         [-1584.541],
         [-2089.999],
         [-1922.569],
         [10155.120],
         [-1474.340],
         [ -587.650],
         [-1233.835],
         [ 3961.768]],

        [[ -179.720],
         [-2104.977],
         [-1175.856],
         [-1523.133],
         [ 1789.014],
         [-1320.920],
         [ 5261.948],
         [ 1358.102],
         [-2236.308],
         [-1477.444]],

        [[-1050.352],
         [-1630.811],
         [ 1363.456],
         [-1989.717],
         [-1086.146],
         [-1867.786],
         [-2241.649],
         [  814.442],
         [-1928.849],
         [ -890.151]],

        [[20675.173],
         [-1326.537],
         [ 7294.301],
         [-1865.309],
         [-2456.534],
         [ 7551.761],
         [-1872.594],
         [ -391.990],
         [ 3566.252],
         [-1194.652]],

        [[ 1195.119],
         [-2066.222],
         [  170.013],
         [17162.996],
         [ 3608.669],
         [-1338.636],
         [-1716.321],
         [ -706.520],
         [ 1394.951],
         [ -954.840]],

        [[ -663.847],
         [-1472.401],
         [-1763.954],
         [-2002.362],
         [ -906.174],
         [  873.274],
         [-2208.032],
         [  765.103],
         [-1938.759],
         [ 1680.745]],

        [[ 5497.755],
         [  277.108],
         [-1704.026],
         [-2523.619],
         [  370.453],
         [-2723.016],
         [-2146.222],
         [-1960.775],
         [-1585.240],
         [ 4519.349]],

        [[ 1120.514],
         [  132.640],
         [-1858.438],
         [ -730.567],
         [-1910.012],
         [ -819.899],
         [-1975.408],
         [-1731.288],
         [-1602.101],
         [21218.266]],

        [[-2503.116],
         [-1785.610],
         [-2089.283],
         [ -422.154],
         [-2016.093],
         [-2183.164],
         [ 1283.876],
         [ -974.684],
         [ 1237.837],
         [ 3264.067]],

        [[ 8765.078],
         [-2001.612],
         [-1408.964],
         [-1528.856],
         [-1243.275],
         [-2229.871],
         [ 7262.354],
         [-1955.537],
         [ 6019.068],
         [ 1092.822]],

        [[-1423.036],
         [ -369.387],
         [ 1942.102],
         [-1431.711],
         [-1224.058],
         [ -644.916],
         [-1441.023],
         [ 4921.876],
         [-1198.492],
         [ 1263.005]],

        [[-1687.902],
         [ -802.056],
         [-1840.976],
         [-2329.585],
         [-1740.578],
         [ 1030.136],
         [ 3397.217],
         [-1712.617],
         [ -615.969],
         [-1468.475]],

        [[ -376.130],
         [-1867.488],
         [ 1416.422],
         [  471.277],
         [  737.564],
         [-2712.653],
         [-1320.452],
         [  653.335],
         [-2034.851],
         [ 6311.033]],

        [[-2053.004],
         [ 4866.151],
         [ -517.793],
         [-1664.921],
         [ -572.181],
         [-1644.253],
         [-1935.739],
         [-1347.492],
         [ 6396.873],
         [  120.189]],

        [[-1519.400],
         [-1619.539],
         [-1651.270],
         [-1500.580],
         [-2367.195],
         [-1824.536],
         [  715.971],
         [-2314.140],
         [ 2381.368],
         [11290.603]],

        [[-1261.256],
         [ -217.345],
         [ 1471.747],
         [ -522.140],
         [-1293.977],
         [-2411.354],
         [-2786.437],
         [-1383.216],
         [ 2614.300],
         [  835.835]],

        [[-1508.848],
         [-1825.374],
         [-2023.773],
         [ 1547.296],
         [  426.525],
         [  491.683],
         [-2305.484],
         [ -320.413],
         [-1948.295],
         [-2110.719]],

        [[-1412.258],
         [ 1749.582],
         [-1992.414],
         [-2636.414],
         [ -934.264],
         [-1535.579],
         [ -972.482],
         [ -735.789],
         [ -985.581],
         [ -795.284]],

        [[  301.852],
         [-1364.306],
         [ 1065.742],
         [11594.269],
         [ 1577.720],
         [-1991.865],
         [ 5849.287],
         [-1252.042],
         [ -189.112],
         [ 2593.463]],

        [[-1928.489],
         [ 2430.796],
         [-2150.651],
         [-1616.661],
         [-2719.130],
         [ 2421.368],
         [  627.863],
         [-1168.579],
         [10017.580],
         [-1407.361]],

        [[ 1536.975],
         [ -791.197],
         [-1741.106],
         [-1037.008],
         [-1907.823],
         [-2005.885],
         [-2230.469],
         [-1048.626],
         [ 2381.805],
         [  590.971]],

        [[-1814.422],
         [-1017.936],
         [-1732.165],
         [ 4325.446],
         [ 4447.627],
         [-1788.475],
         [-1680.924],
         [-2240.927],
         [-1267.839],
         [ 2284.023]],

        [[ -496.378],
         [-2188.415],
         [-1081.769],
         [ 7554.584],
         [  755.693],
         [-1401.141],
         [-2620.087],
         [-1526.586],
         [ 6506.947],
         [  102.826]],

        [[-1871.069],
         [-2548.313],
         [ -459.537],
         [ -828.505],
         [-2355.723],
         [-1693.657],
         [-1960.751],
         [17521.984],
         [ -191.492],
         [ 5467.246]],

        [[-2514.331],
         [-1843.427],
         [ 7282.674],
         [ 3096.887],
         [-1977.323],
         [-2324.070],
         [-1455.487],
         [-1255.641],
         [-2319.359],
         [ 2036.667]],

        [[  609.672],
         [ -294.654],
         [-1319.697],
         [ -591.320],
         [ 8050.369],
         [ 7131.281],
         [-1154.038],
         [  463.717],
         [  131.541],
         [ -258.367]],

        [[-2451.389],
         [-1082.591],
         [ 2211.551],
         [  955.360],
         [10699.489],
         [-2525.190],
         [ 3465.640],
         [ 2595.083],
         [-1713.835],
         [ -472.977]],

        [[ 7256.970],
         [ 6860.119],
         [ -845.067],
         [-2124.705],
         [-1215.553],
         [ 1536.060],
         [10068.102],
         [-2000.604],
         [ -838.341],
         [ -550.901]]], device='cuda:0')
v tensor([[[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[ 1681.200],
        [  768.072],
        [ -265.166],
        [ 2292.156],
        [19215.468],
        [ -917.905],
        [-1868.692],
        [-1965.801],
        [-2271.317],
        [  809.831],
        [ -918.548],
        [ 1754.866],
        [ -785.238],
        [-1611.351],
        [-2246.733],
        [  739.496],
        [-1656.954],
        [-1420.623],
        [-2382.495],
        [ -413.586],
        [ -400.951],
        [-1335.306],
        [-2248.095],
        [-2269.633],
        [ 1659.836],
        [-1582.286],
        [-2266.565],
        [  356.095],
        [ -779.282],
        [ -112.013]], device='cuda:0')
v tensor([[1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-5.716e-02, -1.103e-01,  3.166e-01,  ..., -4.251e-01,  4.238e-01,
         -1.462e-01],
        [ 3.315e-02,  5.851e-01,  3.566e-01,  ..., -6.241e-01,  4.918e-02,
          1.493e-01],
        [ 1.404e-01, -5.093e-01,  3.493e-01,  ..., -4.490e-01, -1.706e-01,
          3.368e-01],
        ...,
        [-5.804e-04, -2.164e-01, -2.399e-01,  ...,  7.074e-01,  1.915e-02,
          3.016e-02],
        [ 2.592e-01,  4.882e-01,  2.078e-01,  ..., -1.635e-01,  1.374e-01,
         -7.231e-02],
        [-1.001e-01,  1.284e-01,  9.959e-02,  ...,  8.350e-01,  1.867e-01,
         -2.193e-01]], device='cuda:0')
s after update for 1 param tensor([[1.783, 1.283, 1.809,  ..., 1.828, 1.645, 1.942],
        [2.457, 1.866, 1.158,  ..., 1.518, 2.222, 1.436],
        [2.210, 1.605, 1.559,  ..., 2.464, 1.860, 1.636],
        ...,
        [1.839, 1.762, 1.720,  ..., 1.918, 1.570, 1.798],
        [1.810, 1.970, 1.970,  ..., 1.640, 1.986, 1.709],
        [2.242, 1.742, 1.898,  ..., 1.772, 1.373, 2.561]], device='cuda:0')
b after update for 1 param tensor([[177.162, 150.281, 178.426,  ..., 179.386, 170.149, 184.870],
        [207.983, 181.234, 142.779,  ..., 163.467, 197.759, 158.975],
        [197.215, 168.073, 165.665,  ..., 208.242, 180.938, 169.679],
        ...,
        [179.926, 176.128, 173.990,  ..., 183.729, 166.235, 177.927],
        [178.491, 186.202, 186.208,  ..., 169.922, 186.966, 173.433],
        [198.646, 175.104, 182.760,  ..., 176.592, 155.443, 212.337]],
       device='cuda:0')
clipping threshold 2.289746767584684
a after update for 1 param tensor([-0.341,  0.524, -0.374, -0.218, -0.479,  0.188, -0.235,  0.815,  0.339,
        -0.159,  0.191,  0.304, -0.973,  0.955,  0.467,  0.203,  0.101,  0.085,
         0.648,  0.062, -0.342,  0.165,  1.257,  1.003,  0.767,  0.022, -0.714,
        -0.174,  0.216,  0.512, -0.793,  0.908, -0.487,  0.046,  0.531, -0.145,
         0.331, -1.053, -0.745,  0.396, -0.262, -0.343, -0.545, -0.711,  0.170,
         0.456,  0.684,  0.010, -0.811,  0.077,  0.015,  0.886, -0.186,  0.125,
        -0.737,  0.473, -0.472,  0.466,  0.105,  0.423, -0.587, -0.018, -0.516,
        -0.607, -1.344,  1.307, -0.015, -0.249,  0.772, -0.040, -0.105, -0.754,
         0.519,  0.352,  0.217,  0.722, -0.156,  0.621, -0.362, -0.080, -0.076,
         0.315, -0.096,  0.761, -0.528, -0.828,  0.183, -0.329, -0.551, -1.160,
         0.233,  0.470,  0.024,  0.099, -0.268, -0.282,  0.114, -0.418, -0.137,
         0.710, -0.041,  1.493, -0.082, -0.632,  0.225, -0.005, -0.436,  0.432,
         0.387, -0.647,  0.318,  0.290, -0.901, -0.474, -0.463,  0.335,  0.630,
        -0.288,  0.341, -0.230, -0.577, -0.054, -0.227,  0.073,  0.066,  1.250,
         0.140,  0.065,  0.014, -0.194, -0.105,  0.018,  0.041,  0.637,  0.104,
        -0.553, -0.086,  0.866,  0.031, -0.234, -0.450, -0.222, -0.631,  0.554,
         0.025, -0.203,  0.629, -0.134,  0.026,  0.545,  0.213, -0.055,  0.730,
        -0.228,  0.218, -0.078,  0.189, -0.217, -0.844, -0.692,  0.077,  0.121,
         0.271,  0.192,  0.053,  0.023, -0.479, -0.251, -0.038,  0.324, -0.102,
        -0.110, -0.144, -0.901, -0.081,  0.715,  0.198, -0.099,  0.301,  0.437,
        -0.082, -0.107,  0.393,  0.250, -0.344, -0.467, -0.382,  0.951, -0.404,
         0.596,  0.086,  0.545,  0.207, -0.361, -0.407,  0.689,  0.741,  0.675,
         0.268,  0.631,  0.584, -0.424,  0.700, -0.165,  1.152,  0.915,  0.423,
        -0.298, -0.014,  0.262,  0.174,  1.140,  0.595, -0.392,  0.323,  0.183,
        -0.546, -0.460,  0.481, -0.385, -0.535, -0.257,  0.674, -0.138,  0.144,
         0.403,  0.460,  0.227,  0.179,  0.633,  0.054, -0.374, -0.461,  0.161,
        -0.243, -0.764,  0.199, -0.372, -0.071,  0.353,  0.815,  0.499,  0.268,
         0.468, -0.729,  0.454,  0.460,  0.545, -1.261,  0.052, -0.855, -0.216,
         1.169,  0.214, -0.202,  0.557, -0.068,  0.997,  0.575, -0.403,  0.079,
        -0.261,  0.112, -0.055, -0.827, -1.120, -0.702,  0.231,  0.080, -0.527,
         0.324, -0.002, -1.046, -0.242,  0.058, -0.747,  0.328, -0.037, -0.105,
        -1.128,  0.259, -0.540, -0.210, -0.049,  0.240, -0.748,  0.231,  0.616,
         0.256, -0.924,  0.096, -1.043, -1.331, -0.111, -0.145,  0.491, -0.834,
         0.325,  1.114,  0.645], device='cuda:0')
s after update for 1 param tensor([1.882, 1.875, 2.030, 1.819, 1.535, 1.543, 2.297, 1.778, 1.767, 2.178,
        2.288, 1.621, 1.402, 1.377, 2.087, 1.641, 1.564, 1.952, 1.842, 1.770,
        2.110, 1.244, 1.755, 2.144, 1.943, 2.017, 2.123, 1.855, 1.765, 1.916,
        2.008, 1.523, 0.831, 1.523, 2.037, 2.199, 1.281, 1.611, 1.400, 2.165,
        1.663, 1.937, 1.820, 1.318, 2.365, 1.665, 1.870, 2.315, 2.095, 1.523,
        1.936, 1.797, 1.684, 2.027, 1.854, 2.152, 2.000, 1.989, 2.039, 1.910,
        1.745, 1.960, 1.193, 1.845, 1.445, 1.681, 2.072, 1.365, 2.115, 1.645,
        1.909, 1.875, 1.295, 1.758, 1.588, 1.492, 2.270, 1.503, 1.918, 1.984,
        1.739, 1.741, 1.827, 1.947, 1.833, 1.532, 1.718, 2.016, 1.319, 1.549,
        1.871, 2.065, 1.186, 1.185, 1.499, 1.616, 1.498, 1.775, 1.770, 1.621,
        2.099, 2.014, 1.414, 1.526, 1.147, 1.865, 2.021, 1.866, 1.930, 2.474,
        1.956, 1.378, 1.316, 2.336, 1.200, 2.331, 2.298, 1.587, 1.745, 1.541,
        1.690, 1.926, 1.755, 1.902, 1.630, 1.475, 1.437, 1.773, 1.988, 1.324,
        1.779, 2.187, 1.721, 1.901, 1.822, 1.566, 1.717, 1.635, 0.644, 1.740,
        2.155, 1.538, 1.680, 1.712, 2.122, 2.195, 1.311, 2.044, 1.689, 1.874,
        1.863, 1.563, 2.171, 1.669, 1.568, 1.574, 2.187, 2.183, 1.597, 1.758,
        1.436, 1.551, 0.981, 1.526, 0.999, 1.697, 1.501, 2.067, 1.872, 1.599,
        1.961, 1.935, 1.799, 1.423, 1.275, 1.855, 1.489, 1.429, 1.699, 1.388,
        1.824, 1.835, 2.257, 1.404, 1.723, 1.456, 1.398, 1.214, 1.895, 1.754,
        1.829, 1.634, 2.481, 2.028, 1.983, 2.322, 1.759, 1.507, 1.626, 1.681,
        1.793, 2.157, 1.997, 1.787, 2.011, 1.832, 1.784, 1.673, 1.933, 1.274,
        1.649, 2.054, 1.696, 2.041, 1.300, 1.324, 1.610, 0.765, 1.446, 2.230,
        2.064, 2.341, 2.082, 1.714, 1.980, 2.064, 1.865, 1.696, 1.763, 1.580,
        1.731, 1.678, 1.493, 1.608, 1.965, 1.247, 2.137, 1.620, 2.014, 2.061,
        2.035, 2.124, 1.470, 1.976, 1.975, 1.903, 2.302, 1.517, 1.857, 2.115,
        1.777, 1.749, 1.546, 1.772, 1.787, 2.014, 1.909, 1.737, 1.661, 1.599,
        1.789, 1.358, 2.017, 1.811, 1.483, 1.502, 1.985, 1.046, 2.326, 1.531,
        1.730, 2.010, 1.191, 2.095, 1.656, 1.652, 1.754, 1.133, 1.395, 1.904,
        1.699, 1.389, 1.915, 1.550, 2.235, 1.430, 1.801, 1.440, 1.498, 2.136,
        1.891, 1.560, 1.584, 2.031, 1.631, 1.202, 1.721, 1.735, 1.856, 1.396],
       device='cuda:0')
b after update for 1 param tensor([182.014, 181.652, 189.012, 178.958, 164.392, 164.811, 201.068, 176.918,
        176.378, 195.791, 200.704, 168.917, 157.086, 155.666, 191.674, 169.954,
        165.905, 185.371, 180.065, 176.505, 192.725, 148.005, 175.782, 194.283,
        184.917, 188.441, 193.336, 180.678, 176.269, 183.625, 188.022, 163.722,
        120.932, 163.725, 189.360, 196.760, 150.170, 168.377, 156.969, 195.200,
        171.104, 184.662, 178.966, 152.304, 204.054, 171.189, 181.446, 201.888,
        192.047, 163.759, 184.626, 177.875, 172.172, 188.902, 180.646, 194.628,
        187.625, 187.118, 189.456, 183.382, 175.264, 185.741, 144.892, 180.231,
        159.460, 172.020, 190.971, 155.007, 192.963, 170.177, 183.302, 181.684,
        150.964, 175.912, 167.177, 162.052, 199.891, 162.653, 183.761, 186.895,
        174.946, 175.045, 179.311, 185.129, 179.636, 164.193, 173.921, 188.369,
        152.348, 165.118, 181.481, 190.657, 144.486, 144.440, 162.415, 168.655,
        162.397, 176.768, 176.491, 168.931, 192.197, 188.265, 157.763, 163.876,
        142.091, 181.183, 188.615, 181.239, 184.298, 208.685, 185.572, 155.754,
        152.178, 202.767, 145.344, 202.546, 201.106, 167.160, 175.254, 164.690,
        172.467, 184.116, 175.744, 182.989, 169.411, 161.154, 159.052, 176.645,
        187.048, 152.654, 176.978, 196.184, 174.062, 182.942, 179.084, 166.008,
        173.841, 169.632, 106.444, 175.030, 194.778, 164.535, 171.956, 173.588,
        193.254, 196.548, 151.940, 189.685, 172.445, 181.606, 181.071, 165.851,
        195.474, 171.398, 166.115, 166.428, 196.217, 196.031, 167.644, 175.890,
        158.969, 165.255, 131.380, 163.879, 132.640, 172.833, 162.526, 190.731,
        181.541, 167.767, 185.813, 184.541, 177.942, 158.252, 149.818, 180.700,
        161.912, 158.618, 172.923, 156.321, 179.176, 179.738, 199.303, 157.186,
        174.139, 160.094, 156.852, 146.169, 182.631, 175.719, 179.452, 169.599,
        208.976, 188.958, 186.814, 202.162, 175.946, 162.879, 169.205, 172.027,
        177.678, 194.860, 187.500, 177.365, 188.127, 179.557, 177.207, 171.596,
        184.446, 149.735, 170.371, 190.150, 172.787, 189.534, 151.280, 152.638,
        168.336, 116.075, 159.541, 198.112, 190.620, 203.015, 191.437, 173.711,
        186.696, 190.614, 181.187, 172.790, 176.186, 166.786, 174.542, 171.851,
        162.103, 168.260, 185.963, 148.132, 193.971, 168.885, 188.298, 190.461,
        189.263, 193.376, 160.834, 186.523, 186.474, 183.040, 201.319, 163.424,
        180.808, 192.930, 176.866, 175.447, 164.940, 176.622, 177.351, 188.268,
        183.304, 174.865, 171.014, 167.761, 177.479, 154.629, 188.404, 178.560,
        161.586, 162.577, 186.934, 135.720, 202.357, 164.167, 174.531, 188.103,
        144.801, 192.045, 170.738, 170.549, 175.723, 141.228, 156.723, 183.067,
        172.948, 156.346, 183.606, 165.170, 198.345, 158.631, 178.035, 159.204,
        162.411, 193.912, 182.460, 165.699, 166.978, 189.081, 169.446, 145.487,
        174.069, 174.745, 180.737, 156.752], device='cuda:0')
clipping threshold 2.289746767584684
a after update for 1 param tensor([[[-8.565e-02],
         [ 9.139e-01],
         [-1.377e-01],
         [ 2.405e-01],
         [ 1.581e-01],
         [-2.122e-01],
         [-6.685e-01],
         [ 6.072e-02],
         [ 1.637e-01],
         [ 7.604e-01]],

        [[ 1.101e-01],
         [-7.270e-01],
         [ 1.494e+00],
         [-6.549e-01],
         [ 4.115e-01],
         [-1.059e-01],
         [ 1.637e-01],
         [-2.661e-01],
         [-6.826e-01],
         [ 6.129e-01]],

        [[-4.754e-01],
         [ 1.958e-01],
         [ 7.349e-01],
         [ 8.545e-01],
         [ 1.948e-01],
         [ 3.430e-01],
         [ 1.982e-01],
         [ 1.556e-01],
         [-5.575e-01],
         [-3.640e-01]],

        [[ 1.022e-01],
         [-8.976e-01],
         [-8.209e-01],
         [ 8.725e-01],
         [-1.954e-01],
         [ 8.799e-01],
         [ 3.719e-02],
         [ 3.375e-01],
         [ 5.055e-01],
         [ 1.125e-01]],

        [[ 4.646e-01],
         [-1.054e+00],
         [ 3.445e-01],
         [ 6.187e-02],
         [ 4.438e-01],
         [ 7.148e-01],
         [-2.506e-01],
         [-1.648e-01],
         [ 3.735e-01],
         [ 5.810e-01]],

        [[-1.246e-01],
         [-7.091e-01],
         [ 2.556e-03],
         [-1.256e-01],
         [-4.541e-01],
         [-7.908e-01],
         [-4.717e-01],
         [-5.811e-01],
         [-5.444e-02],
         [-5.539e-01]],

        [[-5.090e-01],
         [ 2.424e-01],
         [-3.555e-01],
         [-4.497e-01],
         [ 7.776e-01],
         [ 3.679e-01],
         [ 6.222e-01],
         [-2.131e-01],
         [-3.727e-01],
         [ 2.397e-01]],

        [[-1.138e-01],
         [ 5.222e-01],
         [ 1.395e-01],
         [-6.402e-01],
         [ 1.341e-01],
         [-6.255e-01],
         [-4.792e-01],
         [ 5.212e-01],
         [-8.552e-01],
         [ 2.873e-01]],

        [[-5.665e-02],
         [ 3.045e-01],
         [-7.060e-01],
         [-1.457e-01],
         [ 4.067e-01],
         [ 1.441e+00],
         [ 1.950e-01],
         [ 4.779e-01],
         [-1.250e-01],
         [-1.808e-01]],

        [[ 1.946e-01],
         [ 5.097e-01],
         [ 2.083e-01],
         [ 8.586e-01],
         [ 7.965e-01],
         [ 2.205e-01],
         [ 2.679e-01],
         [-1.563e-02],
         [ 4.056e-01],
         [ 1.108e+00]],

        [[ 8.418e-01],
         [-6.329e-01],
         [ 9.622e-01],
         [-3.854e-01],
         [-1.770e-01],
         [ 4.657e-01],
         [ 2.251e-01],
         [-2.553e-01],
         [ 3.732e-01],
         [ 9.110e-01]],

        [[-6.159e-01],
         [-3.159e-01],
         [ 3.888e-01],
         [ 5.233e-01],
         [-4.676e-01],
         [-6.595e-02],
         [ 5.470e-02],
         [ 1.216e+00],
         [ 1.028e+00],
         [-5.793e-01]],

        [[-1.245e-01],
         [ 9.392e-01],
         [-3.660e-01],
         [ 8.262e-01],
         [ 4.645e-01],
         [-4.371e-01],
         [-2.603e-01],
         [ 8.263e-01],
         [ 6.347e-01],
         [-4.432e-01]],

        [[ 7.273e-01],
         [ 6.155e-01],
         [-1.150e-01],
         [-1.206e-01],
         [ 4.531e-01],
         [ 3.672e-01],
         [ 8.055e-01],
         [ 2.616e-01],
         [-4.097e-01],
         [-5.753e-01]],

        [[-5.542e-01],
         [ 1.698e-01],
         [-4.227e-01],
         [-6.195e-01],
         [-8.545e-02],
         [-1.988e-01],
         [ 6.870e-01],
         [ 1.092e+00],
         [ 2.523e-01],
         [-1.706e-01]],

        [[ 9.428e-01],
         [-1.339e-01],
         [-8.159e-02],
         [ 4.004e-01],
         [-7.440e-01],
         [ 3.734e-01],
         [-4.572e-01],
         [ 4.987e-01],
         [ 9.881e-02],
         [ 1.012e-01]],

        [[ 1.905e+00],
         [-2.711e-01],
         [-4.635e-01],
         [ 2.268e-01],
         [ 9.100e-01],
         [ 7.084e-01],
         [ 2.827e-01],
         [ 1.018e-02],
         [-6.660e-01],
         [ 1.366e-01]],

        [[-1.306e-01],
         [ 4.699e-02],
         [ 7.325e-03],
         [-4.022e-01],
         [-1.998e-01],
         [ 3.770e-01],
         [ 9.361e-01],
         [-7.322e-01],
         [ 5.798e-01],
         [-9.578e-01]],

        [[-1.675e-01],
         [ 2.140e-01],
         [-3.240e-01],
         [-1.861e-01],
         [ 1.821e-01],
         [ 5.974e-01],
         [ 1.185e+00],
         [ 7.676e-04],
         [-4.033e-02],
         [ 1.082e+00]],

        [[ 2.446e-01],
         [ 1.776e-01],
         [-2.649e-02],
         [ 6.710e-01],
         [-1.499e-01],
         [ 2.001e-01],
         [ 2.295e-01],
         [-2.321e-01],
         [-4.847e-01],
         [ 5.649e-01]],

        [[-1.781e-01],
         [ 4.149e-01],
         [ 7.734e-01],
         [-1.689e-01],
         [ 2.487e-01],
         [ 4.352e-01],
         [-2.716e-01],
         [ 1.506e-01],
         [-3.435e-02],
         [ 7.166e-01]],

        [[-1.210e+00],
         [ 2.195e-01],
         [ 2.909e-01],
         [-5.619e-01],
         [ 4.149e-02],
         [-5.527e-01],
         [-3.359e-01],
         [-3.307e-01],
         [ 6.022e-03],
         [-8.366e-01]],

        [[-1.107e-01],
         [ 7.348e-01],
         [ 1.260e-01],
         [ 2.185e-01],
         [ 8.411e-02],
         [-4.947e-01],
         [-6.239e-02],
         [-3.168e-01],
         [ 8.291e-01],
         [ 5.015e-02]],

        [[ 1.192e+00],
         [ 3.796e-01],
         [-3.346e-01],
         [ 6.959e-01],
         [-2.962e-01],
         [-3.892e-01],
         [-4.907e-01],
         [-6.353e-01],
         [-9.615e-01],
         [ 9.328e-01]],

        [[-4.207e-01],
         [ 2.469e-01],
         [ 4.713e-01],
         [ 4.480e-01],
         [ 3.314e-01],
         [ 5.697e-02],
         [-4.342e-01],
         [-5.818e-01],
         [ 1.250e-01],
         [-2.933e-01]],

        [[-2.978e-01],
         [ 1.960e-01],
         [-1.006e+00],
         [-4.432e-01],
         [ 1.880e-01],
         [-6.967e-01],
         [ 1.311e-01],
         [-4.128e-01],
         [ 1.671e-01],
         [ 6.826e-01]],

        [[ 5.235e-02],
         [ 3.091e-01],
         [-5.189e-02],
         [-5.034e-01],
         [ 1.389e-01],
         [-4.447e-01],
         [ 1.611e-01],
         [-8.139e-01],
         [-8.218e-01],
         [ 7.966e-01]],

        [[-4.252e-01],
         [ 7.450e-02],
         [-1.510e+00],
         [ 9.856e-01],
         [-1.584e-01],
         [-7.724e-01],
         [-6.864e-01],
         [ 1.439e-01],
         [-7.297e-01],
         [ 1.269e+00]],

        [[-6.955e-01],
         [ 6.588e-02],
         [-2.575e-01],
         [-3.032e-02],
         [ 2.635e-01],
         [-2.329e-01],
         [-8.288e-01],
         [ 5.257e-01],
         [ 1.678e-01],
         [ 4.351e-01]],

        [[-2.234e-01],
         [-3.157e-01],
         [-3.966e-02],
         [ 3.397e-01],
         [ 8.179e-01],
         [-4.678e-01],
         [ 1.149e+00],
         [ 2.322e-01],
         [ 1.182e-01],
         [ 2.267e-01]]], device='cuda:0')
s after update for 1 param tensor([[[1.802],
         [1.626],
         [1.524],
         [1.930],
         [1.894],
         [2.173],
         [1.411],
         [1.470],
         [1.694],
         [1.772]],

        [[1.661],
         [1.777],
         [1.993],
         [1.512],
         [1.463],
         [2.192],
         [2.276],
         [2.122],
         [1.958],
         [1.543]],

        [[1.527],
         [1.541],
         [1.888],
         [1.760],
         [1.777],
         [2.023],
         [1.454],
         [2.002],
         [1.036],
         [1.772]],

        [[1.975],
         [1.829],
         [1.176],
         [1.191],
         [1.980],
         [1.186],
         [1.724],
         [1.950],
         [1.848],
         [1.323]],

        [[1.568],
         [1.514],
         [1.775],
         [1.827],
         [1.357],
         [1.634],
         [1.752],
         [1.851],
         [1.671],
         [1.390]],

        [[2.113],
         [1.098],
         [2.035],
         [1.509],
         [1.985],
         [2.062],
         [1.978],
         [1.659],
         [1.885],
         [2.013]],

        [[1.859],
         [1.635],
         [2.055],
         [2.358],
         [1.883],
         [1.280],
         [1.804],
         [1.224],
         [1.813],
         [0.857]],

        [[1.126],
         [1.721],
         [1.532],
         [1.759],
         [1.522],
         [1.834],
         [1.834],
         [2.022],
         [1.516],
         [1.720]],

        [[2.100],
         [1.666],
         [1.605],
         [2.243],
         [1.978],
         [2.190],
         [1.741],
         [1.959],
         [1.524],
         [1.864]],

        [[1.728],
         [1.826],
         [1.634],
         [0.930],
         [1.644],
         [1.556],
         [1.892],
         [1.854],
         [1.978],
         [2.166]],

        [[2.214],
         [1.443],
         [1.893],
         [1.143],
         [1.600],
         [1.730],
         [2.212],
         [2.130],
         [2.404],
         [1.765]],

        [[2.427],
         [1.585],
         [1.357],
         [1.246],
         [1.561],
         [1.924],
         [1.762],
         [1.730],
         [1.972],
         [1.585]],

        [[1.396],
         [1.445],
         [2.210],
         [1.554],
         [1.693],
         [1.162],
         [1.279],
         [1.923],
         [2.158],
         [2.012]],

        [[1.320],
         [1.342],
         [1.519],
         [1.859],
         [1.973],
         [2.372],
         [1.930],
         [1.496],
         [2.074],
         [1.617]],

        [[2.152],
         [1.568],
         [2.227],
         [2.056],
         [1.297],
         [2.293],
         [1.283],
         [1.654],
         [1.940],
         [1.919]],

        [[1.682],
         [2.032],
         [1.773],
         [1.937],
         [1.705],
         [1.457],
         [1.622],
         [1.165],
         [2.031],
         [1.386]],

        [[1.194],
         [1.773],
         [1.680],
         [1.391],
         [2.166],
         [1.999],
         [1.675],
         [1.866],
         [2.581],
         [2.219]],

        [[1.393],
         [1.592],
         [1.438],
         [2.050],
         [1.374],
         [1.889],
         [2.247],
         [1.531],
         [1.913],
         [2.087]],

        [[1.212],
         [2.086],
         [1.680],
         [2.189],
         [2.046],
         [1.868],
         [1.802],
         [0.968],
         [1.533],
         [1.807]],

        [[1.365],
         [1.743],
         [1.799],
         [2.073],
         [1.992],
         [1.676],
         [1.980],
         [1.081],
         [0.806],
         [1.055]],

        [[1.979],
         [1.758],
         [1.948],
         [2.033],
         [2.245],
         [1.644],
         [1.889],
         [1.239],
         [1.453],
         [2.099]],

        [[1.616],
         [1.749],
         [2.110],
         [1.703],
         [2.127],
         [2.064],
         [1.698],
         [1.268],
         [1.680],
         [1.829]],

        [[1.609],
         [1.777],
         [1.475],
         [1.403],
         [1.511],
         [1.723],
         [1.767],
         [1.138],
         [2.140],
         [1.866]],

        [[1.556],
         [2.227],
         [1.358],
         [2.073],
         [2.244],
         [2.201],
         [1.396],
         [2.017],
         [1.331],
         [1.712]],

        [[1.859],
         [1.821],
         [0.945],
         [1.585],
         [2.010],
         [1.096],
         [2.049],
         [1.598],
         [1.769],
         [1.916]],

        [[1.508],
         [2.248],
         [1.506],
         [1.339],
         [1.871],
         [1.647],
         [1.533],
         [2.082],
         [1.388],
         [2.049]],

        [[2.068],
         [1.463],
         [2.066],
         [2.445],
         [1.556],
         [1.944],
         [1.651],
         [1.464],
         [2.136],
         [1.983]],

        [[1.807],
         [1.692],
         [1.975],
         [1.960],
         [1.787],
         [2.299],
         [1.871],
         [2.040],
         [1.671],
         [1.872]],

        [[1.982],
         [1.260],
         [1.913],
         [2.161],
         [2.333],
         [1.977],
         [1.869],
         [1.680],
         [1.754],
         [1.377]],

        [[2.006],
         [2.352],
         [2.029],
         [1.662],
         [1.505],
         [1.535],
         [1.745],
         [2.008],
         [1.585],
         [1.287]]], device='cuda:0')
b after update for 1 param tensor([[[178.124],
         [169.203],
         [163.811],
         [184.335],
         [182.582],
         [195.571],
         [157.593],
         [160.858],
         [172.688],
         [176.603]],

        [[170.998],
         [176.874],
         [187.314],
         [163.146],
         [160.501],
         [196.423],
         [200.166],
         [193.249],
         [185.628],
         [164.785]],

        [[163.931],
         [164.707],
         [182.291],
         [176.007],
         [176.876],
         [188.716],
         [159.995],
         [187.704],
         [135.068],
         [176.609]],

        [[186.463],
         [179.420],
         [143.873],
         [144.790],
         [186.703],
         [144.496],
         [174.218],
         [185.264],
         [180.373],
         [152.598]],

        [[166.114],
         [163.265],
         [176.742],
         [179.318],
         [154.576],
         [169.621],
         [175.629],
         [180.484],
         [171.493],
         [156.448]],

        [[192.837],
         [139.015],
         [189.252],
         [162.953],
         [186.932],
         [190.504],
         [186.594],
         [170.873],
         [182.140],
         [188.221]],

        [[180.914],
         [169.629],
         [190.188],
         [203.715],
         [182.055],
         [150.128],
         [178.214],
         [146.754],
         [178.632],
         [122.841]],

        [[140.771],
         [174.046],
         [164.197],
         [175.972],
         [163.674],
         [179.697],
         [179.665],
         [188.648],
         [163.363],
         [174.007]],

        [[192.279],
         [171.265],
         [168.106],
         [198.712],
         [186.597],
         [196.342],
         [175.059],
         [185.688],
         [163.765],
         [181.152]],

        [[174.414],
         [179.268],
         [169.577],
         [127.922],
         [170.099],
         [165.502],
         [182.472],
         [180.645],
         [186.583],
         [195.284]],

        [[197.425],
         [159.364],
         [182.559],
         [141.858],
         [167.833],
         [174.483],
         [197.307],
         [193.644],
         [205.701],
         [176.270]],

        [[206.700],
         [167.041],
         [154.543],
         [148.120],
         [165.746],
         [184.015],
         [176.128],
         [174.513],
         [186.296],
         [167.020]],

        [[156.780],
         [159.470],
         [197.230],
         [165.374],
         [172.643],
         [143.015],
         [150.024],
         [183.993],
         [194.912],
         [188.176]],

        [[152.458],
         [153.724],
         [163.517],
         [180.898],
         [186.353],
         [204.322],
         [184.331],
         [162.267],
         [191.065],
         [168.699]],

        [[194.647],
         [166.149],
         [198.006],
         [190.258],
         [151.070],
         [200.903],
         [150.303],
         [170.616],
         [184.781],
         [183.779]],

        [[172.059],
         [189.132],
         [176.647],
         [184.628],
         [173.264],
         [160.137],
         [168.965],
         [143.182],
         [189.062],
         [156.203]],

        [[144.974],
         [176.655],
         [171.977],
         [156.483],
         [195.263],
         [187.595],
         [171.718],
         [181.238],
         [213.130],
         [197.618]],

        [[156.584],
         [167.382],
         [159.108],
         [189.971],
         [155.519],
         [182.361],
         [198.858],
         [164.186],
         [183.503],
         [191.655]],

        [[146.060],
         [191.600],
         [171.966],
         [196.294],
         [189.796],
         [181.317],
         [178.112],
         [130.533],
         [164.283],
         [178.348]],

        [[155.029],
         [175.179],
         [177.955],
         [191.045],
         [187.257],
         [171.740],
         [186.673],
         [137.912],
         [119.091],
         [136.246]],

        [[186.621],
         [175.927],
         [185.164],
         [189.166],
         [198.790],
         [170.099],
         [182.344],
         [147.684],
         [159.909],
         [192.213]],

        [[168.672],
         [175.483],
         [192.727],
         [173.156],
         [193.482],
         [190.614],
         [172.891],
         [149.372],
         [171.967],
         [179.449]],

        [[168.314],
         [176.848],
         [161.147],
         [157.149],
         [163.067],
         [174.157],
         [176.368],
         [141.536],
         [194.105],
         [181.226]],

        [[165.496],
         [198.015],
         [154.583],
         [191.015],
         [198.738],
         [196.851],
         [156.780],
         [188.417],
         [153.089],
         [173.616]],

        [[180.920],
         [179.049],
         [128.945],
         [167.032],
         [188.105],
         [138.901],
         [189.921],
         [167.729],
         [176.469],
         [183.668]],

        [[162.938],
         [198.909],
         [162.799],
         [153.518],
         [181.474],
         [170.264],
         [164.272],
         [191.445],
         [156.320],
         [189.909]],

        [[190.806],
         [160.493],
         [190.681],
         [207.437],
         [165.489],
         [184.972],
         [170.453],
         [160.516],
         [193.888],
         [186.855]],

        [[178.336],
         [172.561],
         [186.473],
         [185.749],
         [177.368],
         [201.147],
         [181.458],
         [189.512],
         [171.483],
         [181.529]],

        [[186.796],
         [148.949],
         [183.495],
         [195.053],
         [202.636],
         [186.572],
         [181.364],
         [171.963],
         [175.711],
         [155.713]],

        [[187.895],
         [203.463],
         [188.973],
         [171.042],
         [162.773],
         [164.381],
         [175.279],
         [187.993],
         [167.017],
         [150.522]]], device='cuda:0')
clipping threshold 2.289746767584684
a after update for 1 param tensor([[-0.342],
        [-0.446],
        [-0.572],
        [-0.239],
        [ 0.041],
        [ 0.183],
        [ 0.433],
        [-0.057],
        [ 0.065],
        [ 0.362],
        [-0.405],
        [ 0.660],
        [-0.361],
        [-0.168],
        [-0.024],
        [-0.222],
        [-0.737],
        [-0.223],
        [ 0.223],
        [ 0.497],
        [ 0.379],
        [ 0.645],
        [ 0.681],
        [-0.381],
        [-0.036],
        [ 0.462],
        [-0.178],
        [-0.643],
        [ 1.071],
        [-0.111]], device='cuda:0')
s after update for 1 param tensor([[2.100],
        [1.890],
        [1.442],
        [2.258],
        [2.387],
        [1.610],
        [1.498],
        [1.537],
        [1.812],
        [1.711],
        [1.184],
        [1.783],
        [1.802],
        [1.711],
        [1.757],
        [1.999],
        [1.335],
        [1.681],
        [1.920],
        [1.884],
        [1.834],
        [1.617],
        [1.777],
        [1.788],
        [2.278],
        [1.350],
        [1.915],
        [1.649],
        [1.693],
        [2.061]], device='cuda:0')
b after update for 1 param tensor([[192.254],
        [182.416],
        [159.340],
        [199.383],
        [204.964],
        [168.341],
        [162.390],
        [164.470],
        [178.599],
        [173.564],
        [144.352],
        [177.147],
        [178.085],
        [173.545],
        [175.873],
        [187.590],
        [153.291],
        [171.998],
        [183.857],
        [182.101],
        [179.656],
        [168.737],
        [176.878],
        [177.419],
        [200.260],
        [154.163],
        [183.578],
        [170.383],
        [172.625],
        [190.454]], device='cuda:0')
clipping threshold 2.289746767584684
cuda
Objective function 135.57 = squared loss an data 85.73 + 0.5*rho*h**2 39.081814 + alpha*h 7.369561 + L2reg 3.03 + L1reg 0.36 ; SHD = 179 ; DAG True
Proportion of microbatches that were clipped  0.7847859022406507
iteration 3 in inner loop, alpha 83.35645917681265 rho 10000.0 h 0.08841019640106396
iteration 3 in outer loop, alpha = 967.4584231874522, rho = 10000.0, h = 0.08841019640106396
cuda
9630
cuda
Objective function 213.74 = squared loss an data 85.73 + 0.5*rho*h**2 39.081814 + alpha*h 85.533189 + L2reg 3.03 + L1reg 0.36 ; SHD = 179 ; DAG True
||w||^2 3692793659408.572
exp ma of ||w||^2 1037313933250.959
||w|| 1921664.2941493636
exp ma of ||w|| 493637.65373831335
||w||^2 7.412606423156659
exp ma of ||w||^2 994.1937820087248
||w|| 2.7226102224072877
exp ma of ||w|| 3.5300667050383208
||w||^2 6.873965104498317
exp ma of ||w||^2 9.436085129153199
||w|| 2.621824766169226
exp ma of ||w|| 3.0110638185254133
||w||^2 6.98743342908376
exp ma of ||w||^2 9.31879119853819
||w|| 2.6433753855787794
exp ma of ||w|| 3.003320687619455
||w||^2 7.8801640143267235
exp ma of ||w||^2 9.640861372444569
||w|| 2.8071629832139644
exp ma of ||w|| 3.048024292033179
||w||^2 11.364903479542475
exp ma of ||w||^2 10.764349966998553
||w|| 3.371187250738599
exp ma of ||w|| 3.221060611529439
||w||^2 12.591043026847743
exp ma of ||w||^2 10.596864379141046
||w|| 3.548385974897283
exp ma of ||w|| 3.1841234511881145
||w||^2 11.892238244391812
exp ma of ||w||^2 10.091263334609385
||w|| 3.448512468353828
exp ma of ||w|| 3.124618499598097
||w||^2 12.989258969020014
exp ma of ||w||^2 10.033210713764221
||w|| 3.6040614546675
exp ma of ||w|| 3.116521450605794
||w||^2 6.311965929249605
exp ma of ||w||^2 9.310439710039375
||w|| 2.5123626189803105
exp ma of ||w|| 2.9981076587996345
||w||^2 7.533606281400582
exp ma of ||w||^2 10.837541714777883
||w|| 2.744741569146462
exp ma of ||w|| 3.248155517797253
cuda
Objective function 162.71 = squared loss an data 85.12 + 0.5*rho*h**2 17.283645 + alpha*h 56.880706 + L2reg 3.06 + L1reg 0.36 ; SHD = 173 ; DAG True
Proportion of microbatches that were clipped  0.787081909907047
iteration 1 in inner loop, alpha 967.4584231874522 rho 10000.0 h 0.05879395381379737
9630
cuda
Objective function 318.26 = squared loss an data 85.12 + 0.5*rho*h**2 172.836450 + alpha*h 56.880706 + L2reg 3.06 + L1reg 0.36 ; SHD = 173 ; DAG True
||w||^2 47.28506312281162
exp ma of ||w||^2 86241.55016436044
||w|| 6.876413536343755
exp ma of ||w|| 6.70477985002114
||w||^2 23.586795321781466
exp ma of ||w||^2 100.46169796641507
||w|| 4.856623860438593
exp ma of ||w|| 4.604452749921715
||w||^2 16.219905709720045
exp ma of ||w||^2 13.242245603229282
||w|| 4.027394407022988
exp ma of ||w|| 3.5817091627674342
||w||^2 11.408087304779587
exp ma of ||w||^2 13.336928705549331
||w|| 3.3775860173768466
exp ma of ||w|| 3.594543509168214
||w||^2 22.07795021348044
exp ma of ||w||^2 13.420504741020492
||w|| 4.698717932955802
exp ma of ||w|| 3.6067793051329953
v before min max tensor([[ 7252.612,  5484.161, -1986.954,  ..., -3561.863,  -149.657,
          -521.906],
        [23434.410, -3704.218,  8773.977,  ..., -3413.376, -1766.011,
          5377.653],
        [40593.567,  -460.605, -2269.861,  ..., -1502.326,  2461.706,
          -799.729],
        ...,
        [ 2066.871, 12460.366,  1194.682,  ..., -2556.693, -1969.076,
           288.829],
        [-2995.097, -3403.700,   336.114,  ..., -1968.134, 11598.403,
         -2386.052],
        [16313.650, -3260.669, 18926.636,  ...,  9995.762, -2522.177,
         -1214.683]], device='cuda:0')
v tensor([[1.000e+01, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e-12],
        ...,
        [1.000e+01, 1.000e+01, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e-12, 1.000e+01,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e-12]], device='cuda:0')
v before min max tensor([ 4.876e+03, -2.705e+03,  1.728e+03, -1.815e+03,  2.280e+03, -3.293e+03,
        -2.416e+03, -1.813e+03, -2.949e+03,  8.650e+03,  1.976e+03, -2.076e+03,
         6.034e+03, -2.884e+03,  5.757e+03,  2.032e+03, -2.389e+03, -1.904e+03,
         1.095e+04, -6.129e+02, -1.431e+03, -2.708e+02, -2.929e+02, -2.248e+03,
        -1.405e+03, -2.478e+03, -1.401e+03,  2.730e+03,  3.219e+01,  1.352e+03,
         4.366e+04, -1.434e+03, -3.664e+02,  4.856e+03, -1.650e+03, -1.481e+03,
         2.011e+04, -4.200e+03, -1.925e+03,  3.272e+04,  7.497e+02,  1.781e+04,
        -3.852e+03, -2.657e+03,  2.148e+02, -2.380e+03,  8.966e+03,  7.245e+02,
        -3.293e+02, -1.804e+03, -7.583e+01, -3.185e+03, -4.089e+03, -4.300e+03,
        -3.351e+03, -2.268e+03,  1.186e+03,  1.258e+04,  9.665e+03, -3.791e+03,
        -9.404e+02,  1.264e+04,  7.229e+03, -2.433e+03, -3.513e+03, -3.432e+03,
         1.541e+03, -2.844e+03, -2.352e+03, -3.106e+03, -9.668e+02, -2.339e+03,
        -5.108e+00, -3.075e+03,  1.745e+04, -2.926e+03,  6.197e+03, -2.528e+03,
        -2.430e+03, -1.462e+03, -2.425e+03, -2.990e+03, -1.113e+03,  5.377e+02,
         2.712e+04,  1.209e+03,  3.295e+04,  2.084e+03, -2.473e+03, -8.011e+02,
        -3.551e+03, -3.043e+03, -2.327e+03, -2.163e+03, -2.506e+03, -2.799e+03,
         8.254e+03, -3.088e+03,  5.417e+04, -3.120e+03, -3.145e+03, -3.021e+03,
         1.245e+04,  2.326e+03,  2.017e+03,  1.740e+02,  3.794e+02, -3.275e+03,
         5.441e+03, -1.532e+03, -9.686e+02,  1.318e+04, -2.564e+03,  1.512e+03,
        -7.443e+02, -2.904e+03, -7.089e+02, -2.818e+03, -2.397e+03, -2.476e+03,
         3.107e+04, -9.718e+02,  5.624e+03,  1.090e+04, -2.408e+03, -3.203e+03,
        -2.736e+03, -9.981e+02, -2.863e+03, -3.509e+03,  9.236e+02,  2.114e+03,
        -3.987e+03, -2.546e+03,  1.909e+04,  1.275e+03, -3.836e+03, -2.940e+03,
        -2.983e+03, -1.091e+03,  5.161e+03, -2.426e+03, -2.693e+03, -3.008e+03,
         5.365e+03, -3.107e+03,  4.943e+03, -2.806e+03, -1.490e+03, -3.332e+03,
        -2.196e+03, -2.663e+03, -3.004e+03,  6.903e+03, -3.909e+02, -1.656e+03,
         8.166e+02, -3.277e+03, -1.987e+03, -3.496e+03, -3.172e+03, -1.388e+03,
         4.851e+03, -1.465e+03,  3.885e+03, -2.849e+03, -2.913e+03,  1.501e+03,
         1.807e+03, -1.305e+03,  1.225e+03, -2.789e+03,  5.530e+03, -2.720e+03,
        -3.401e+03,  5.527e+02, -2.891e+03,  3.433e+03, -2.894e+03, -1.801e+03,
        -2.472e+03,  1.925e+03, -2.292e+02, -2.256e+03, -2.833e+03,  8.484e+03,
        -1.883e+03, -1.739e+03,  2.475e+03,  1.295e+04, -1.520e+03, -2.969e+03,
        -2.047e+03, -2.825e+03, -2.851e+03,  2.937e+04, -2.631e+03, -1.784e+03,
        -1.013e+03,  9.067e+03,  1.681e+03, -1.162e+03,  3.235e+03,  2.831e+03,
         1.417e+04, -3.389e+03,  7.259e+03, -3.059e+03,  3.202e+01,  9.324e+03,
         1.736e+03, -6.203e+02,  1.460e+04,  7.322e+03, -2.614e+03,  1.446e+04,
        -3.802e+03, -3.301e+03, -1.101e+03,  1.083e+04,  5.694e+03, -1.555e+02,
        -3.369e+03, -1.475e+03,  1.770e+04,  1.888e+04, -3.038e+03, -3.293e+03,
         8.113e+03, -2.917e+03,  1.212e+04, -1.574e+03, -1.571e+03, -2.344e+03,
        -4.124e+02,  6.443e+03,  4.637e+02,  2.033e+03,  4.071e+03,  1.301e+03,
        -3.782e+03, -2.370e+03, -9.746e+02,  3.268e+04,  6.244e+03,  1.320e+03,
        -1.177e+03,  1.039e+03, -1.263e+03,  3.708e+02, -1.879e+03,  4.898e+03,
        -2.166e+03, -2.433e+03,  1.459e+03,  6.083e+03, -2.955e+03,  1.590e+02,
        -1.736e+03, -3.887e+03,  3.734e+03,  1.667e+02,  3.163e+03, -2.553e+03,
         8.093e+03, -2.047e+03,  8.656e+03, -3.830e+03,  1.477e+03,  3.556e+03,
        -2.725e+03, -2.426e+03,  1.075e+04,  1.072e+03,  4.719e+02,  6.166e+03,
        -3.013e+03,  4.711e+03, -3.576e+02, -2.732e+03, -1.361e+03,  3.032e+03,
         7.938e+03,  1.525e+04, -7.673e+02, -3.643e+03,  7.310e+03, -2.368e+03,
        -1.196e+03, -2.806e+03, -2.487e+03, -3.599e+03, -2.771e+03, -2.779e+03,
        -2.813e+03,  1.418e+03, -1.279e+03, -3.561e+03,  2.806e+03, -2.998e+03],
       device='cuda:0')
v tensor([1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[-2.905e+03],
         [ 5.943e+03],
         [ 1.196e+04],
         [ 8.774e+02],
         [-3.579e+03],
         [-2.348e+03],
         [-1.751e+03],
         [ 9.063e+02],
         [ 3.288e+02],
         [-9.569e+02]],

        [[ 4.316e+02],
         [ 1.415e+03],
         [-2.453e+03],
         [-6.468e+02],
         [-8.782e+02],
         [ 3.903e+02],
         [ 4.446e+03],
         [ 2.110e+04],
         [ 1.023e+04],
         [-7.177e+02]],

        [[ 1.550e+03],
         [-2.926e+03],
         [-2.409e+03],
         [ 8.298e+03],
         [ 6.050e+03],
         [ 6.953e+03],
         [-3.732e+03],
         [-1.643e+03],
         [-9.198e+02],
         [ 4.643e+03]],

        [[-2.893e+03],
         [-3.298e+03],
         [ 1.217e+04],
         [ 9.654e+03],
         [-2.074e+03],
         [-2.265e+03],
         [-3.411e+03],
         [ 2.295e+03],
         [-2.262e+03],
         [-1.922e+03]],

        [[ 1.084e+04],
         [-2.874e+03],
         [-8.740e+02],
         [-3.508e+03],
         [-3.892e+03],
         [-2.188e+03],
         [-2.129e+03],
         [-2.864e+03],
         [-3.455e+03],
         [-1.243e+03]],

        [[ 3.158e+04],
         [ 7.982e+03],
         [ 1.519e+02],
         [ 6.261e+03],
         [-1.810e+03],
         [ 9.525e+03],
         [ 1.940e+03],
         [-5.899e+01],
         [ 5.787e+03],
         [ 1.100e+04]],

        [[-2.096e+03],
         [-3.788e+03],
         [-1.765e+03],
         [-2.979e+03],
         [ 8.056e+03],
         [ 4.801e+02],
         [-2.205e+03],
         [ 5.559e+03],
         [ 6.922e+03],
         [-1.665e+03]],

        [[ 4.618e+03],
         [ 7.417e+03],
         [-2.647e+03],
         [ 1.072e+04],
         [-1.650e+02],
         [-1.757e+03],
         [-2.439e+03],
         [-1.000e+03],
         [-2.639e+03],
         [ 2.600e+03]],

        [[-2.494e+03],
         [-3.119e+03],
         [-2.581e+03],
         [ 2.683e+03],
         [ 2.823e+03],
         [-2.198e+03],
         [-3.034e+03],
         [ 1.537e+01],
         [-2.641e+03],
         [-6.158e+02]],

        [[-2.016e+03],
         [-2.879e+03],
         [ 1.486e+04],
         [ 1.021e+04],
         [ 1.035e+03],
         [ 2.548e+04],
         [-3.254e+02],
         [ 2.532e+04],
         [-1.717e+03],
         [ 1.430e+04]],

        [[ 6.482e+03],
         [ 4.204e+03],
         [-3.069e+03],
         [ 6.061e+03],
         [-3.736e+03],
         [-2.992e+03],
         [ 5.854e+03],
         [-1.261e+02],
         [-1.651e+03],
         [-2.552e+03]],

        [[-2.644e+03],
         [-1.545e+03],
         [-3.346e+03],
         [ 5.761e+03],
         [ 3.227e+02],
         [-1.444e+03],
         [-1.531e+03],
         [-2.774e+03],
         [ 4.098e+03],
         [-1.685e+03]],

        [[ 1.304e+04],
         [ 6.492e+02],
         [ 4.587e+03],
         [-3.246e+03],
         [ 1.091e+03],
         [ 1.189e+03],
         [-2.800e+03],
         [-2.329e+03],
         [ 3.073e+03],
         [-2.255e+02]],

        [[-5.581e+02],
         [ 7.283e+03],
         [ 1.731e+03],
         [-2.194e+03],
         [-2.683e+03],
         [-5.249e+02],
         [-9.768e+02],
         [ 3.635e+03],
         [ 8.304e+03],
         [ 3.916e+03]],

        [[-2.814e+03],
         [-2.932e+03],
         [-3.302e+03],
         [-7.637e+02],
         [ 1.684e+04],
         [ 1.970e+04],
         [-2.541e+03],
         [-2.129e+03],
         [ 8.651e+03],
         [ 8.147e+03]],

        [[-3.297e+03],
         [ 3.827e+03],
         [ 5.721e+03],
         [-3.177e+02],
         [ 4.146e+03],
         [-2.866e+03],
         [ 9.527e+03],
         [-3.729e+03],
         [-1.579e+03],
         [ 1.183e+04]],

        [[ 5.175e+03],
         [ 1.605e+03],
         [-1.809e+03],
         [-3.691e+03],
         [-3.267e+03],
         [ 1.274e+04],
         [-3.219e+03],
         [-3.779e+02],
         [-6.638e+02],
         [-1.090e+03]],

        [[ 3.904e+03],
         [ 9.569e+03],
         [ 1.422e+03],
         [-5.255e+02],
         [-9.214e+02],
         [-2.199e+03],
         [ 9.208e+02],
         [ 1.828e+03],
         [ 2.664e+03],
         [-3.262e+03]],

        [[-1.757e+03],
         [-1.672e+03],
         [-1.961e+03],
         [ 1.835e+04],
         [-2.804e+03],
         [ 5.422e+03],
         [-6.333e+02],
         [ 9.111e+03],
         [-1.594e+03],
         [-3.239e+03]],

        [[ 2.150e+03],
         [-1.365e+03],
         [-7.405e+02],
         [-3.025e+03],
         [-2.539e+03],
         [ 2.156e+03],
         [-2.007e+03],
         [-2.881e+03],
         [ 6.907e+02],
         [ 8.469e+03]],

        [[-3.791e+03],
         [-1.230e+03],
         [-3.443e+03],
         [-3.364e+03],
         [-3.063e+03],
         [-2.147e+03],
         [-1.823e+03],
         [-3.259e+03],
         [ 7.714e+01],
         [ 8.894e+03]],

        [[-7.732e+02],
         [-3.551e+03],
         [ 4.173e+03],
         [-3.565e+03],
         [-3.810e+02],
         [ 2.439e+03],
         [ 6.535e+03],
         [-2.906e+03],
         [-2.448e+03],
         [ 9.169e+03]],

        [[-1.661e+03],
         [-2.733e+03],
         [-2.807e+03],
         [-2.689e+03],
         [-1.656e+03],
         [-1.829e+03],
         [ 9.527e+03],
         [ 2.093e+02],
         [-4.054e+03],
         [-6.329e+02]],

        [[-2.648e+03],
         [ 5.183e+03],
         [-1.738e+03],
         [-1.988e+03],
         [-3.264e+03],
         [-8.834e+02],
         [-8.198e+02],
         [ 3.265e+03],
         [ 2.203e+02],
         [ 7.725e+03]],

        [[-1.972e+03],
         [-2.279e+03],
         [-2.039e+03],
         [-2.405e+03],
         [-1.404e+03],
         [ 2.377e+03],
         [ 1.446e+03],
         [-3.051e+03],
         [-2.416e+03],
         [ 2.886e+03]],

        [[-3.063e+03],
         [-3.346e+03],
         [-3.259e+03],
         [ 3.547e+03],
         [-3.241e+03],
         [-2.786e+03],
         [-1.816e+03],
         [ 4.679e+03],
         [-5.984e+02],
         [ 4.499e+03]],

        [[-1.682e+03],
         [-2.087e+03],
         [ 7.664e+02],
         [ 9.035e+03],
         [-1.609e+02],
         [-1.582e+02],
         [ 2.088e+04],
         [-3.666e+03],
         [ 8.331e+03],
         [-4.871e+02]],

        [[-1.476e+03],
         [-2.616e+03],
         [-2.968e+03],
         [-3.321e+03],
         [-3.250e+03],
         [ 1.903e+04],
         [ 2.272e+03],
         [-2.564e+03],
         [ 9.306e+02],
         [ 3.444e+03]],

        [[ 6.488e+03],
         [-2.626e+03],
         [-3.417e+03],
         [ 4.537e+02],
         [-3.712e+03],
         [-4.151e+03],
         [-2.163e+03],
         [-4.054e+03],
         [ 1.685e+03],
         [ 3.621e+03]],

        [[ 3.368e+03],
         [ 8.489e+01],
         [-8.583e+02],
         [ 2.085e+02],
         [ 7.092e+03],
         [-1.214e+03],
         [-2.962e+03],
         [ 9.612e+03],
         [-3.693e+03],
         [-2.162e+03]]], device='cuda:0')
v tensor([[[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[ -839.514],
        [ -799.078],
        [-1261.575],
        [ 2587.907],
        [ 6194.094],
        [-2507.533],
        [-2859.552],
        [ 3816.412],
        [-1987.094],
        [ -965.593],
        [-1217.365],
        [ 3726.198],
        [-2520.139],
        [-2680.247],
        [-2567.071],
        [ 3218.521],
        [-3008.456],
        [-1512.966],
        [ 7791.671],
        [-2430.926],
        [10809.001],
        [  750.051],
        [  887.774],
        [ 9575.610],
        [ 4944.649],
        [ -993.215],
        [ 1142.388],
        [-1797.467],
        [  -20.575],
        [-3701.107]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.396,  1.377,  0.437,  ..., -1.337,  0.095, -0.632],
        [-0.052, -0.537, -0.312,  ..., -0.173,  0.170, -0.265],
        [-0.332, -0.245,  0.291,  ..., -0.722, -0.041,  0.660],
        ...,
        [-0.487, -0.396,  0.728,  ...,  0.965,  0.161, -0.016],
        [-0.317,  0.151,  0.621,  ..., -1.842,  0.182,  0.469],
        [-0.073, -0.206,  0.157,  ..., -1.182,  0.340, -0.107]],
       device='cuda:0')
s after update for 1 param tensor([[2.820, 1.963, 1.960,  ..., 1.963, 1.284, 1.287],
        [1.670, 2.069, 1.791,  ..., 1.867, 1.023, 1.793],
        [2.860, 1.686, 1.852,  ..., 1.298, 1.318, 1.554],
        ...,
        [2.235, 1.846, 1.828,  ..., 1.611, 1.949, 2.132],
        [1.787, 1.896, 1.612,  ..., 1.702, 2.231, 2.692],
        [2.148, 1.864, 1.620,  ..., 1.854, 1.484, 2.402]], device='cuda:0')
b after update for 1 param tensor([[224.903, 187.635, 187.496,  ..., 187.644, 151.758, 151.938],
        [173.083, 192.641, 179.227,  ..., 183.016, 135.482, 179.340],
        [226.503, 173.871, 182.265,  ..., 152.567, 153.749, 166.937],
        ...,
        [200.218, 181.951, 181.069,  ..., 170.004, 186.946, 195.550],
        [179.011, 184.390, 170.056,  ..., 174.713, 200.019, 219.750],
        [196.291, 182.852, 170.475,  ..., 182.338, 163.131, 207.556]],
       device='cuda:0')
clipping threshold 2.748717219777392
a after update for 1 param tensor([ 3.574e-01,  3.361e-01,  1.355e+00, -8.656e-01, -4.093e-01, -7.010e-02,
         3.489e-03,  1.346e-01, -2.691e-01,  1.372e-01, -2.160e+00, -3.410e-01,
         9.034e-01,  1.108e-01,  5.688e-02,  4.653e-01, -1.564e+00,  2.628e-01,
         7.466e-01,  9.531e-01,  2.042e+00,  2.190e-01,  1.102e-01,  5.098e-01,
        -5.272e-01, -3.789e-01, -7.124e-01,  7.311e-01, -4.643e-01,  1.127e+00,
        -3.732e-01,  1.418e+00, -1.747e-01, -1.572e-01, -7.014e-01, -1.415e+00,
        -5.630e-01,  2.191e-01,  5.147e-01,  1.126e-01, -5.490e-01, -4.643e-01,
         1.048e+00,  1.397e+00,  4.742e-01, -6.145e-01,  1.363e+00,  2.487e+00,
        -1.044e+00, -8.864e-01, -1.682e-01, -4.357e-01, -1.280e-01, -4.600e-01,
         2.132e-01,  5.692e-01, -8.610e-01,  2.118e-03,  5.637e-01,  8.363e-03,
        -1.442e-01, -8.276e-01,  9.765e-01,  6.057e-01,  7.683e-01, -2.013e-01,
         2.280e-01,  3.469e-02,  7.221e-01,  3.373e-02, -1.001e-01,  1.681e-01,
         1.374e+00,  2.480e-01,  6.587e-02,  3.229e-01, -9.137e-02,  1.162e+00,
        -6.272e-01,  1.071e+00,  1.112e+00, -6.501e-01, -2.287e-01, -1.881e+00,
        -1.371e+00,  1.246e+00, -7.002e-02, -3.721e-01,  3.752e-01,  1.556e+00,
         2.433e-02, -9.091e-01,  1.771e-01, -8.713e-01,  9.451e-01, -1.175e+00,
         1.791e-01,  8.277e-01, -1.326e+00,  8.246e-01,  2.140e-01, -5.872e-02,
        -2.759e-01,  3.835e-01, -1.111e+00, -1.984e-02, -1.098e-01, -8.111e-01,
         1.113e-01, -1.347e+00, -2.148e-01,  6.982e-02,  8.779e-01, -4.826e-01,
         6.800e-01,  8.413e-01, -5.748e-01,  1.039e-01,  6.096e-01,  2.830e-01,
         3.621e-01, -9.180e-01,  1.450e+00,  3.930e-01,  1.132e+00,  2.558e-01,
         1.057e-01,  1.331e+00, -2.277e-02, -6.098e-01, -3.294e+00, -7.095e-02,
        -9.740e-01,  3.885e-01, -1.392e-01, -8.308e-01,  6.735e-01, -1.291e+00,
         5.098e-01,  9.829e-01, -6.483e-01,  2.856e-01, -5.311e-02, -5.252e-01,
         5.676e-01, -7.303e-01,  8.565e-01,  4.896e-03,  1.092e+00,  1.491e+00,
        -4.356e-01, -2.230e-01, -3.472e-01, -6.567e-01, -5.337e-01, -6.245e-01,
        -1.282e-01, -7.592e-01, -3.153e-01,  7.108e-01,  3.515e-02, -2.229e-01,
         5.552e-01,  7.426e-01, -1.395e-01,  3.141e-01,  1.214e+00,  1.854e-01,
         1.380e-01, -5.761e-01,  1.123e-01,  4.315e-01,  5.387e-01, -4.709e-02,
        -5.029e-01, -9.363e-01, -4.121e-02,  1.481e+00, -1.308e+00,  5.421e-01,
         1.000e+00,  1.095e+00,  9.705e-03, -5.886e-02, -2.326e-01, -8.078e-01,
        -4.440e-01, -2.116e-02,  6.331e-01, -4.679e-01, -6.436e-01,  1.194e+00,
        -3.330e-01, -1.242e+00,  7.660e-01,  6.917e-03, -2.895e-01,  3.602e-01,
         7.694e-01, -4.293e-01, -2.551e-01,  5.531e-01, -9.228e-01, -1.174e+00,
         2.926e-01, -8.695e-01,  1.444e+00,  5.933e-01, -4.956e-01,  1.509e-01,
        -2.725e-01,  9.365e-02,  4.452e-01, -7.139e-01, -1.046e+00,  5.863e-01,
         1.677e-01,  6.408e-01,  2.215e-01, -3.701e-01, -8.450e-01, -4.826e-01,
         6.262e-01, -4.976e-01,  1.282e+00,  1.157e+00,  4.535e-01,  8.016e-02,
         9.736e-01, -1.394e+00, -7.374e-01,  3.506e-01, -1.523e+00, -1.878e-01,
        -1.007e+00,  1.115e+00,  7.613e-02,  1.239e+00,  1.065e+00, -1.062e-01,
        -4.505e-01, -6.793e-01, -2.035e-01, -5.269e-01, -2.558e-01,  3.948e-02,
         2.844e-02,  1.530e-01, -4.067e-01, -1.100e+00, -2.815e-01,  3.760e-01,
         1.147e+00, -5.254e-01, -1.731e+00,  6.324e-01, -1.487e+00, -9.523e-01,
        -1.495e+00,  5.355e-01,  1.537e+00, -5.716e-01, -3.473e-01,  3.051e-01,
         4.797e-01, -1.082e-01, -6.843e-01,  9.182e-01, -1.071e+00, -8.608e-01,
         1.727e+00, -8.274e-03,  4.211e-01,  1.320e-01, -3.291e-01,  3.500e-01,
         1.213e+00, -5.797e-01, -8.736e-02, -5.164e-01,  3.434e-01, -1.267e+00,
        -1.094e+00, -3.682e-01,  5.666e-01,  3.409e-01,  8.780e-01, -1.161e+00,
         1.109e+00,  3.931e-01, -6.177e-01, -9.280e-01,  7.190e-01, -3.470e-02,
         3.472e-01,  8.324e-01, -2.031e-01,  5.781e-01,  7.107e-01,  8.260e-01],
       device='cuda:0')
s after update for 1 param tensor([1.928, 1.793, 2.143, 1.893, 1.922, 1.924, 1.511, 1.944, 2.321, 2.046,
        1.553, 1.141, 1.574, 1.821, 1.802, 2.133, 1.446, 1.537, 1.819, 1.107,
        1.634, 1.730, 1.941, 1.467, 1.411, 1.523, 1.162, 1.603, 2.084, 2.066,
        2.288, 1.498, 1.612, 1.811, 1.656, 1.691, 1.821, 2.273, 1.539, 2.028,
        1.799, 1.894, 2.173, 1.989, 1.870, 1.353, 1.979, 1.591, 1.695, 1.792,
        1.880, 1.830, 2.250, 2.325, 1.883, 1.749, 1.860, 2.220, 1.951, 2.063,
        1.179, 1.955, 1.624, 1.523, 2.012, 1.855, 1.744, 2.069, 1.288, 1.825,
        0.999, 2.034, 1.523, 1.686, 2.039, 1.585, 2.306, 1.754, 1.623, 0.826,
        1.785, 1.939, 1.755, 1.740, 2.221, 1.616, 2.545, 2.076, 1.554, 1.119,
        1.929, 1.823, 1.820, 1.579, 1.563, 1.555, 2.096, 1.695, 2.624, 1.778,
        1.697, 1.698, 1.975, 1.919, 2.193, 1.861, 1.726, 1.788, 2.622, 1.519,
        1.302, 1.636, 1.396, 1.998, 1.783, 1.594, 1.152, 1.577, 1.338, 1.394,
        2.274, 1.492, 1.913, 1.632, 1.646, 1.762, 1.625, 1.392, 1.853, 1.952,
        2.445, 1.863, 2.194, 1.481, 1.846, 2.227, 2.073, 1.753, 1.799, 1.610,
        1.741, 1.444, 1.530, 1.695, 2.208, 1.684, 2.208, 1.521, 1.803, 1.798,
        1.654, 1.884, 1.622, 2.254, 1.466, 1.793, 2.153, 1.870, 1.870, 1.891,
        1.751, 1.476, 1.827, 1.558, 1.520, 1.740, 1.806, 2.340, 1.137, 1.866,
        2.211, 1.742, 1.233, 1.568, 1.838, 1.640, 1.561, 2.095, 1.588, 1.699,
        1.342, 2.084, 1.684, 1.217, 1.577, 1.941, 1.855, 2.097, 2.293, 1.910,
        1.530, 1.933, 1.496, 1.790, 1.646, 1.600, 1.469, 1.481, 1.448, 1.822,
        2.072, 1.509, 1.865, 2.022, 1.821, 1.879, 1.679, 1.920, 1.997, 1.930,
        1.596, 1.905, 1.962, 1.862, 1.719, 1.861, 2.145, 1.877, 2.075, 1.822,
        2.053, 2.009, 1.818, 1.837, 1.836, 2.167, 1.644, 1.777, 1.662, 1.576,
        1.993, 1.263, 1.834, 1.536, 1.391, 2.339, 1.644, 2.178, 1.870, 2.043,
        2.092, 1.792, 1.092, 1.985, 1.971, 2.245, 2.131, 1.769, 1.316, 1.831,
        1.364, 2.063, 1.385, 1.345, 1.730, 2.273, 1.633, 2.153, 1.224, 2.109,
        1.715, 2.044, 2.620, 2.291, 1.776, 1.108, 2.166, 2.188, 1.855, 1.908,
        1.471, 1.506, 1.870, 1.728, 2.142, 2.166, 1.636, 2.060, 0.702, 1.523,
        2.061, 1.960, 1.667, 2.114, 1.569, 1.971, 2.352, 1.952, 1.018, 1.569,
        1.383, 1.973, 1.644, 1.793, 1.708, 1.599, 1.525, 2.102, 2.100, 2.073],
       device='cuda:0')
b after update for 1 param tensor([185.974, 179.333, 196.039, 184.283, 185.649, 185.768, 164.602, 186.738,
        204.029, 191.541, 166.891, 143.047, 168.041, 180.711, 179.758, 195.597,
        161.060, 166.036, 180.634, 140.907, 171.189, 176.157, 186.572, 162.188,
        159.062, 165.276, 144.337, 169.575, 193.313, 192.510, 202.574, 163.935,
        170.047, 180.240, 172.356, 174.159, 180.709, 201.919, 166.138, 190.733,
        179.609, 184.321, 197.440, 188.878, 183.150, 155.758, 188.417, 168.910,
        174.368, 179.261, 183.616, 181.147, 200.887, 204.191, 183.772, 177.128,
        182.644, 199.523, 187.052, 192.346, 145.403, 187.247, 170.683, 165.299,
        189.943, 182.418, 176.861, 192.617, 151.974, 180.919, 133.842, 190.997,
        165.249, 173.897, 191.251, 168.595, 203.392, 177.360, 170.638, 121.696,
        178.931, 186.473, 177.442, 176.635, 199.597, 170.231, 213.634, 192.940,
        166.923, 141.646, 186.012, 180.817, 180.680, 168.289, 167.449, 167.010,
        193.901, 174.381, 216.936, 178.557, 174.455, 174.525, 188.187, 185.528,
        198.319, 182.720, 175.936, 179.089, 216.870, 165.051, 152.786, 171.285,
        158.237, 189.310, 178.807, 169.104, 143.771, 168.160, 154.925, 158.129,
        201.973, 163.577, 185.212, 171.073, 171.799, 177.762, 170.716, 158.019,
        182.292, 187.118, 209.417, 182.815, 198.352, 162.977, 181.967, 199.879,
        192.829, 177.337, 179.605, 169.910, 176.689, 160.939, 165.668, 174.383,
        199.003, 173.798, 199.016, 165.172, 179.826, 179.568, 172.217, 183.816,
        170.560, 201.060, 162.155, 179.330, 196.508, 183.146, 183.124, 184.170,
        177.233, 162.700, 181.003, 167.185, 165.127, 176.670, 179.956, 204.869,
        142.775, 182.963, 199.143, 176.745, 148.691, 167.722, 181.588, 171.486,
        167.327, 193.835, 168.761, 174.582, 155.130, 193.348, 173.767, 147.744,
        168.168, 186.575, 182.422, 193.917, 202.803, 185.069, 165.680, 186.209,
        163.821, 179.190, 171.832, 169.408, 162.328, 162.987, 161.183, 180.748,
        192.791, 164.507, 182.909, 190.433, 180.738, 183.585, 173.515, 185.569,
        189.243, 186.064, 169.201, 184.855, 187.592, 182.750, 175.578, 182.692,
        196.151, 183.486, 192.907, 180.767, 191.872, 189.843, 180.573, 181.507,
        181.482, 197.145, 171.714, 178.503, 172.668, 168.131, 189.046, 150.526,
        181.353, 165.982, 157.941, 204.825, 171.693, 197.642, 183.128, 191.441,
        193.727, 179.265, 139.948, 188.698, 187.997, 200.676, 195.514, 178.099,
        153.606, 181.204, 156.432, 192.360, 157.637, 155.308, 176.156, 201.901,
        171.133, 196.511, 148.153, 194.499, 175.375, 191.446, 216.792, 202.700,
        178.482, 140.994, 197.088, 198.113, 182.411, 184.993, 162.404, 164.361,
        183.137, 176.059, 196.014, 197.084, 171.282, 192.215, 112.225, 165.277,
        192.268, 187.489, 172.901, 194.734, 167.744, 188.004, 205.381, 187.124,
        135.109, 167.776, 157.511, 188.097, 171.696, 179.332, 175.036, 169.346,
        165.405, 194.159, 194.080, 192.845], device='cuda:0')
clipping threshold 2.748717219777392
a after update for 1 param tensor([[[ 0.361],
         [ 0.783],
         [-0.617],
         [ 0.385],
         [-0.314],
         [ 0.102],
         [ 0.255],
         [ 0.398],
         [ 1.142],
         [ 0.095]],

        [[-0.285],
         [ 0.277],
         [ 0.913],
         [-0.700],
         [ 0.920],
         [ 0.598],
         [ 0.518],
         [ 0.654],
         [-1.320],
         [ 0.505]],

        [[-0.033],
         [ 0.073],
         [-1.406],
         [ 1.223],
         [ 0.082],
         [ 0.781],
         [ 0.270],
         [ 0.713],
         [ 0.272],
         [-0.302]],

        [[-0.847],
         [-0.364],
         [ 0.315],
         [ 1.185],
         [-1.055],
         [-0.150],
         [ 0.204],
         [-1.343],
         [ 0.095],
         [ 0.204]],

        [[-0.808],
         [-0.411],
         [ 0.847],
         [-0.077],
         [ 0.398],
         [-0.004],
         [-0.600],
         [ 1.153],
         [-0.394],
         [ 1.123]],

        [[-1.596],
         [-2.174],
         [-0.123],
         [ 0.599],
         [-0.162],
         [ 0.043],
         [-0.526],
         [-0.286],
         [-0.148],
         [-0.503]],

        [[ 0.448],
         [ 1.253],
         [-0.258],
         [ 0.078],
         [-0.149],
         [ 0.946],
         [ 0.302],
         [-1.822],
         [-0.301],
         [ 0.795]],

        [[ 0.149],
         [ 1.258],
         [ 0.763],
         [-0.769],
         [-0.227],
         [-0.046],
         [-0.920],
         [ 1.284],
         [ 0.062],
         [ 0.458]],

        [[-1.082],
         [-0.247],
         [ 0.158],
         [ 0.110],
         [-0.363],
         [ 0.166],
         [ 0.680],
         [ 0.262],
         [-1.278],
         [ 0.086]],

        [[-0.386],
         [ 0.005],
         [-0.294],
         [ 0.594],
         [ 0.153],
         [ 0.162],
         [ 0.594],
         [-1.093],
         [-0.503],
         [ 0.099]],

        [[ 0.461],
         [ 1.312],
         [ 0.164],
         [-0.632],
         [ 0.130],
         [ 0.236],
         [ 0.176],
         [-0.042],
         [-0.880],
         [-0.023]],

        [[-1.260],
         [-0.886],
         [ 1.273],
         [ 1.157],
         [-0.537],
         [-0.171],
         [ 0.574],
         [ 0.687],
         [-0.453],
         [ 0.024]],

        [[ 0.766],
         [-0.452],
         [-1.129],
         [ 0.821],
         [ 0.176],
         [-1.241],
         [-0.386],
         [ 1.651],
         [-0.541],
         [ 0.591]],

        [[-1.464],
         [ 0.958],
         [ 0.253],
         [-1.177],
         [ 1.246],
         [-0.345],
         [ 0.380],
         [-1.165],
         [-1.063],
         [-0.210]],

        [[-0.979],
         [ 0.057],
         [ 1.382],
         [ 0.053],
         [ 0.379],
         [ 1.443],
         [ 0.234],
         [ 0.246],
         [ 1.387],
         [ 0.056]],

        [[ 0.492],
         [ 0.198],
         [-0.840],
         [-0.756],
         [ 1.182],
         [-0.927],
         [-0.313],
         [-0.779],
         [ 0.016],
         [-0.740]],

        [[ 1.406],
         [ 0.029],
         [-0.039],
         [-0.646],
         [ 0.794],
         [ 0.521],
         [ 0.539],
         [ 1.144],
         [-0.035],
         [ 0.484]],

        [[ 0.421],
         [-0.913],
         [ 0.727],
         [ 0.050],
         [-0.825],
         [ 0.330],
         [-0.303],
         [ 1.043],
         [-0.075],
         [ 0.008]],

        [[ 0.086],
         [-0.762],
         [ 1.078],
         [ 0.042],
         [ 0.264],
         [ 0.441],
         [ 0.871],
         [ 0.359],
         [-0.704],
         [-1.172]],

        [[-0.407],
         [ 0.064],
         [-0.051],
         [-1.135],
         [-0.416],
         [ 0.107],
         [-0.345],
         [-0.828],
         [ 0.897],
         [-1.504]],

        [[ 1.984],
         [-0.208],
         [ 0.433],
         [-0.857],
         [ 0.160],
         [-0.296],
         [-1.210],
         [-0.111],
         [ 0.852],
         [ 0.019]],

        [[ 0.496],
         [ 1.158],
         [ 0.979],
         [ 0.627],
         [-0.215],
         [ 0.231],
         [-0.441],
         [-0.971],
         [-0.267],
         [-1.470]],

        [[-0.868],
         [-0.559],
         [-1.272],
         [-1.727],
         [-0.293],
         [-0.014],
         [ 0.908],
         [ 0.341],
         [ 0.796],
         [ 0.369]],

        [[-0.369],
         [-1.115],
         [-0.373],
         [ 0.098],
         [ 0.071],
         [-1.028],
         [ 0.305],
         [ 0.046],
         [-0.458],
         [ 1.797]],

        [[ 0.728],
         [-0.254],
         [-0.345],
         [ 1.097],
         [ 0.490],
         [ 0.173],
         [-0.155],
         [ 0.726],
         [ 0.265],
         [-0.012]],

        [[ 1.259],
         [ 0.110],
         [-1.227],
         [ 0.690],
         [-0.582],
         [-0.315],
         [-0.313],
         [-0.043],
         [ 0.840],
         [ 0.319]],

        [[-0.697],
         [ 0.299],
         [-0.322],
         [ 0.070],
         [-0.558],
         [ 0.492],
         [ 0.517],
         [-0.013],
         [-0.546],
         [ 0.254]],

        [[-0.513],
         [-0.225],
         [-3.427],
         [ 0.313],
         [-0.362],
         [ 0.798],
         [ 0.276],
         [ 0.458],
         [ 0.216],
         [-0.149]],

        [[-0.955],
         [ 0.153],
         [-1.327],
         [ 0.496],
         [ 1.439],
         [-0.547],
         [ 0.106],
         [-0.342],
         [-0.788],
         [ 0.025]],

        [[ 0.604],
         [-0.354],
         [ 0.694],
         [ 0.430],
         [-1.413],
         [-0.446],
         [-0.276],
         [ 0.029],
         [ 0.150],
         [ 1.233]]], device='cuda:0')
s after update for 1 param tensor([[[1.661],
         [1.760],
         [2.027],
         [2.011],
         [1.974],
         [1.786],
         [1.487],
         [1.680],
         [1.671],
         [1.487]],

        [[2.107],
         [2.095],
         [1.500],
         [1.635],
         [1.551],
         [1.727],
         [1.994],
         [1.910],
         [2.118],
         [2.105]],

        [[1.534],
         [2.092],
         [1.517],
         [2.311],
         [2.040],
         [1.814],
         [2.022],
         [1.008],
         [1.080],
         [2.227]],

        [[1.695],
         [1.781],
         [1.963],
         [2.106],
         [1.895],
         [1.759],
         [1.963],
         [1.696],
         [1.663],
         [1.327]],

        [[2.232],
         [1.572],
         [1.859],
         [1.988],
         [2.102],
         [1.607],
         [1.353],
         [1.837],
         [1.871],
         [1.486]],

        [[2.172],
         [1.827],
         [1.760],
         [1.624],
         [1.736],
         [1.647],
         [1.815],
         [1.794],
         [2.417],
         [2.182]],

        [[1.703],
         [2.063],
         [1.090],
         [1.726],
         [2.121],
         [2.250],
         [1.456],
         [1.261],
         [1.922],
         [1.829]],

        [[2.324],
         [2.238],
         [1.577],
         [1.959],
         [1.193],
         [1.792],
         [1.648],
         [0.912],
         [1.517],
         [2.082]],

        [[1.675],
         [1.682],
         [1.533],
         [1.905],
         [2.016],
         [1.561],
         [1.713],
         [1.926],
         [1.474],
         [1.876]],

        [[1.925],
         [1.604],
         [1.621],
         [1.940],
         [2.220],
         [1.920],
         [1.595],
         [1.855],
         [1.964],
         [1.547]],

        [[2.148],
         [2.090],
         [1.658],
         [2.150],
         [2.018],
         [2.053],
         [1.684],
         [1.312],
         [1.623],
         [1.631]],

        [[2.074],
         [1.637],
         [1.812],
         [1.908],
         [1.884],
         [1.483],
         [1.975],
         [1.763],
         [2.118],
         [1.637]],

        [[2.125],
         [1.181],
         [2.333],
         [1.756],
         [1.983],
         [1.921],
         [1.537],
         [1.425],
         [2.613],
         [1.682]],

        [[1.363],
         [2.148],
         [1.705],
         [1.808],
         [1.642],
         [1.442],
         [1.971],
         [1.867],
         [2.089],
         [2.113]],

        [[1.595],
         [1.590],
         [1.851],
         [1.507],
         [2.144],
         [2.149],
         [1.541],
         [1.610],
         [1.835],
         [2.108]],

        [[1.810],
         [2.425],
         [2.031],
         [1.564],
         [1.925],
         [1.560],
         [1.777],
         [2.014],
         [2.039],
         [1.831]],

        [[2.188],
         [2.072],
         [1.158],
         [2.141],
         [1.908],
         [2.271],
         [1.747],
         [2.003],
         [1.373],
         [1.590]],

        [[2.288],
         [2.177],
         [1.604],
         [1.124],
         [1.725],
         [1.368],
         [1.709],
         [1.599],
         [1.806],
         [1.897]],

        [[1.186],
         [1.304],
         [2.163],
         [1.907],
         [1.951],
         [2.462],
         [1.458],
         [1.926],
         [1.158],
         [1.791]],

        [[2.094],
         [1.727],
         [2.188],
         [1.934],
         [1.969],
         [2.239],
         [1.902],
         [1.640],
         [1.630],
         [2.247]],

        [[2.045],
         [1.902],
         [1.860],
         [1.921],
         [1.944],
         [1.532],
         [1.490],
         [1.772],
         [2.046],
         [2.137]],

        [[1.863],
         [1.922],
         [2.062],
         [1.945],
         [1.661],
         [2.066],
         [1.894],
         [1.573],
         [1.813],
         [2.076]],

        [[1.026],
         [1.577],
         [1.812],
         [1.500],
         [1.377],
         [1.621],
         [1.590],
         [1.443],
         [2.206],
         [1.954]],

        [[1.452],
         [2.149],
         [1.031],
         [1.892],
         [1.785],
         [1.156],
         [1.649],
         [2.474],
         [1.370],
         [1.930]],

        [[1.613],
         [1.394],
         [1.924],
         [1.370],
         [1.805],
         [2.184],
         [2.356],
         [1.668],
         [1.352],
         [1.678]],

        [[1.835],
         [2.039],
         [1.934],
         [1.931],
         [1.846],
         [1.636],
         [0.981],
         [2.059],
         [1.096],
         [1.759]],

        [[1.505],
         [1.330],
         [2.064],
         [2.286],
         [1.584],
         [1.421],
         [2.175],
         [1.994],
         [1.700],
         [1.607]],

        [[1.791],
         [1.886],
         [1.605],
         [1.795],
         [1.823],
         [2.091],
         [1.713],
         [1.676],
         [2.103],
         [1.392]],

        [[1.594],
         [1.783],
         [1.878],
         [1.737],
         [2.005],
         [2.241],
         [1.836],
         [2.208],
         [2.261],
         [1.606]],

        [[1.941],
         [2.063],
         [1.157],
         [2.085],
         [2.059],
         [1.639],
         [1.602],
         [2.406],
         [2.067],
         [1.181]]], device='cuda:0')
b after update for 1 param tensor([[[172.577],
         [177.686],
         [190.677],
         [189.926],
         [188.181],
         [178.962],
         [163.322],
         [173.577],
         [173.106],
         [163.327]],

        [[194.410],
         [193.862],
         [164.011],
         [171.236],
         [166.793],
         [175.979],
         [189.122],
         [185.096],
         [194.899],
         [194.293]],

        [[165.893],
         [193.710],
         [164.924],
         [203.592],
         [191.282],
         [180.374],
         [190.446],
         [134.459],
         [139.169],
         [199.845]],

        [[174.375],
         [178.727],
         [187.630],
         [194.371],
         [184.363],
         [177.643],
         [187.643],
         [174.418],
         [172.713],
         [154.277]],

        [[200.061],
         [167.890],
         [182.594],
         [188.835],
         [194.153],
         [169.768],
         [155.767],
         [181.507],
         [183.204],
         [163.236]],

        [[197.360],
         [181.038],
         [177.677],
         [170.668],
         [176.463],
         [171.872],
         [180.442],
         [179.389],
         [208.193],
         [197.839]],

        [[174.759],
         [192.344],
         [139.840],
         [175.924],
         [195.030],
         [200.864],
         [161.624],
         [150.395],
         [185.682],
         [181.101]],

        [[204.161],
         [200.344],
         [168.162],
         [187.464],
         [146.295],
         [179.261],
         [171.910],
         [127.920],
         [164.938],
         [193.253]],

        [[173.350],
         [173.700],
         [165.798],
         [184.840],
         [190.160],
         [167.344],
         [175.284],
         [185.872],
         [162.610],
         [183.435]],

        [[185.806],
         [169.588],
         [170.484],
         [186.530],
         [199.520],
         [185.553],
         [169.146],
         [182.384],
         [187.691],
         [166.579]],

        [[196.273],
         [193.612],
         [172.442],
         [196.372],
         [190.226],
         [191.897],
         [173.785],
         [153.389],
         [170.617],
         [171.027]],

        [[192.880],
         [171.376],
         [180.282],
         [184.967],
         [183.803],
         [163.107],
         [188.233],
         [177.827],
         [194.897],
         [171.374]],

        [[195.242],
         [145.525],
         [204.567],
         [177.466],
         [188.581],
         [185.642],
         [166.054],
         [159.852],
         [216.506],
         [173.672]],

        [[156.329],
         [196.289],
         [174.876],
         [180.098],
         [171.587],
         [160.822],
         [188.015],
         [182.994],
         [193.555],
         [194.668]],

        [[169.138],
         [168.855],
         [182.229],
         [164.410],
         [196.085],
         [196.321],
         [166.266],
         [169.933],
         [181.429],
         [194.453]],

        [[180.183],
         [208.571],
         [190.873],
         [167.502],
         [185.821],
         [167.293],
         [178.545],
         [190.040],
         [191.229],
         [181.230]],

        [[198.118],
         [192.783],
         [144.093],
         [195.967],
         [184.972],
         [201.829],
         [177.010],
         [189.522],
         [156.954],
         [168.864]],

        [[202.572],
         [197.605],
         [169.630],
         [141.961],
         [175.881],
         [156.635],
         [175.079],
         [169.369],
         [179.990],
         [184.461]],

        [[145.818],
         [152.948],
         [196.947],
         [184.934],
         [187.071],
         [210.142],
         [161.707],
         [185.840],
         [144.086],
         [179.251]],

        [[193.775],
         [176.010],
         [198.113],
         [186.258],
         [187.908],
         [200.390],
         [184.703],
         [171.501],
         [170.980],
         [200.737]],

        [[191.506],
         [184.720],
         [182.646],
         [185.637],
         [186.732],
         [165.740],
         [163.453],
         [178.283],
         [191.577],
         [195.768]],

        [[182.794],
         [185.672],
         [192.315],
         [186.789],
         [172.610],
         [192.508],
         [184.298],
         [167.972],
         [180.315],
         [192.967]],

        [[135.662],
         [168.180],
         [180.273],
         [164.020],
         [157.158],
         [170.529],
         [168.879],
         [160.861],
         [198.911],
         [187.193]],

        [[161.372],
         [196.317],
         [136.013],
         [184.194],
         [178.922],
         [143.983],
         [171.984],
         [210.647],
         [156.767],
         [186.053]],

        [[170.115],
         [158.095],
         [185.768],
         [156.776],
         [179.913],
         [197.922],
         [205.548],
         [172.969],
         [155.710],
         [173.466]],

        [[181.393],
         [191.226],
         [186.260],
         [186.091],
         [181.952],
         [171.283],
         [132.674],
         [192.154],
         [140.180],
         [177.610]],

        [[164.294],
         [154.452],
         [192.422],
         [202.481],
         [168.541],
         [159.661],
         [197.510],
         [189.117],
         [174.619],
         [169.747]],

        [[179.219],
         [183.922],
         [169.672],
         [179.431],
         [180.822],
         [193.661],
         [175.293],
         [173.365],
         [194.203],
         [157.999]],

        [[169.081],
         [178.819],
         [183.507],
         [176.514],
         [189.627],
         [200.475],
         [181.486],
         [198.985],
         [201.373],
         [169.723]],

        [[186.599],
         [192.359],
         [144.064],
         [193.381],
         [192.160],
         [171.434],
         [169.504],
         [207.714],
         [192.529],
         [145.563]]], device='cuda:0')
clipping threshold 2.748717219777392
a after update for 1 param tensor([[-0.229],
        [-0.775],
        [-0.215],
        [-0.767],
        [ 0.194],
        [-1.440],
        [ 0.260],
        [-1.125],
        [-0.170],
        [ 0.641],
        [-0.481],
        [-0.239],
        [ 1.000],
        [ 0.500],
        [-1.507],
        [ 0.403],
        [-0.677],
        [-0.695],
        [-0.359],
        [-0.602],
        [-1.534],
        [-0.413],
        [ 0.224],
        [-0.058],
        [ 1.834],
        [-0.379],
        [ 1.057],
        [-1.293],
        [ 0.109],
        [-0.226]], device='cuda:0')
s after update for 1 param tensor([[1.790],
        [1.842],
        [1.620],
        [2.293],
        [2.029],
        [1.757],
        [2.124],
        [2.043],
        [1.951],
        [1.358],
        [1.877],
        [1.610],
        [1.387],
        [1.570],
        [1.535],
        [1.952],
        [1.983],
        [1.025],
        [1.775],
        [1.347],
        [2.507],
        [1.716],
        [1.357],
        [2.418],
        [1.715],
        [1.403],
        [2.024],
        [1.309],
        [1.379],
        [2.233]], device='cuda:0')
b after update for 1 param tensor([[179.161],
        [181.742],
        [170.442],
        [202.804],
        [190.743],
        [177.534],
        [195.189],
        [191.432],
        [187.084],
        [156.050],
        [183.476],
        [169.932],
        [157.700],
        [167.785],
        [165.923],
        [187.123],
        [188.603],
        [135.579],
        [178.433],
        [155.454],
        [212.043],
        [175.420],
        [156.024],
        [208.268],
        [175.363],
        [158.634],
        [190.522],
        [153.212],
        [157.290],
        [200.112]], device='cuda:0')
clipping threshold 2.748717219777392
||w||^2 19.829108328018417
exp ma of ||w||^2 12.314804346287284
||w|| 4.452988696147613
exp ma of ||w|| 3.4648247844359146
||w||^2 13.464069399566263
exp ma of ||w||^2 11.67635367577848
||w|| 3.6693418210308866
exp ma of ||w|| 3.378869172320594
||w||^2 11.797942851372454
exp ma of ||w||^2 14.016199502435065
||w|| 3.4348133648529515
exp ma of ||w|| 3.7078267631262394
||w||^2 12.354463147351145
exp ma of ||w||^2 14.937316141089546
||w|| 3.514891626686539
exp ma of ||w|| 3.8323261047190793
||w||^2 13.937716126231265
exp ma of ||w||^2 14.538148693260014
||w|| 3.733325076420652
exp ma of ||w|| 3.7809081761494907
||w||^2 21.840588975193736
exp ma of ||w||^2 14.665266959499688
||w|| 4.673391592322832
exp ma of ||w|| 3.787730989211126
||w||^2 15.980526355426052
exp ma of ||w||^2 15.56945791975127
||w|| 3.997565053307582
exp ma of ||w|| 3.916366705602847
||w||^2 15.971482049706665
exp ma of ||w||^2 17.690963422189014
||w|| 3.9964336663713893
exp ma of ||w|| 4.175915110470379
cuda
Objective function 156.21 = squared loss an data 85.16 + 0.5*rho*h**2 40.105983 + alpha*h 27.400084 + L2reg 3.20 + L1reg 0.35 ; SHD = 171 ; DAG True
Proportion of microbatches that were clipped  0.7859148151118954
iteration 2 in inner loop, alpha 967.4584231874522 rho 100000.0 h 0.028321717046594586
iteration 4 in outer loop, alpha = 29289.175469782036, rho = 1000000.0, h = 0.028321717046594586
Threshold 0.3
[[0.006 0.076 0.119 0.125 0.109 0.015 0.093 0.083 0.084 0.073 0.126 0.078
  0.071 0.02  0.012 0.062 0.106 0.085 0.522 0.023 0.08  0.011 0.127 0.023
  0.018 0.032 0.061 0.02  0.142 0.062]
 [0.057 0.006 0.15  0.269 0.078 0.016 0.202 0.261 0.095 0.059 0.254 0.246
  0.153 0.021 0.01  0.128 0.37  0.092 0.253 0.055 0.103 0.019 0.2   0.093
  0.045 0.108 0.339 0.018 0.34  0.103]
 [0.05  0.053 0.009 0.032 0.082 0.016 0.071 0.102 0.036 0.038 0.121 0.067
  0.074 0.007 0.011 0.067 0.09  0.039 0.118 0.027 0.061 0.012 0.08  0.026
  0.018 0.028 0.044 0.025 0.094 0.062]
 [0.051 0.024 0.137 0.007 0.067 0.016 0.07  0.115 0.101 0.033 0.138 0.086
  0.053 0.021 0.007 0.083 0.149 0.035 0.224 0.024 0.016 0.021 0.133 0.037
  0.011 0.024 0.075 0.008 0.081 0.02 ]
 [0.07  0.068 0.101 0.129 0.006 0.008 0.053 0.157 0.118 0.044 0.173 0.119
  0.022 0.053 0.017 0.087 0.09  0.037 0.289 0.046 0.084 0.007 0.163 0.044
  0.029 0.063 0.033 0.023 0.169 0.04 ]
 [0.367 0.388 0.416 0.517 0.736 0.008 0.742 0.467 0.21  0.248 0.376 0.539
  0.131 0.185 0.068 0.464 0.545 0.252 0.65  0.222 0.301 0.185 0.56  0.205
  0.207 0.148 0.348 0.053 0.757 0.523]
 [0.098 0.047 0.152 0.118 0.116 0.007 0.006 0.133 0.046 0.049 0.185 0.222
  0.08  0.019 0.009 0.081 0.201 0.034 0.213 0.043 0.033 0.011 0.099 0.019
  0.027 0.03  0.087 0.029 0.209 0.028]
 [0.062 0.033 0.104 0.063 0.064 0.016 0.067 0.005 0.069 0.055 0.071 0.07
  0.084 0.023 0.009 0.107 0.222 0.045 0.146 0.021 0.063 0.011 0.049 0.07
  0.022 0.037 0.024 0.016 0.106 0.025]
 [0.146 0.054 0.133 0.076 0.074 0.033 0.149 0.118 0.005 0.046 0.113 0.075
  0.076 0.022 0.012 0.114 0.163 0.019 0.242 0.043 0.023 0.012 0.2   0.059
  0.021 0.021 0.073 0.017 0.108 0.081]
 [0.103 0.103 0.235 0.147 0.254 0.035 0.116 0.12  0.129 0.007 0.198 0.154
  0.19  0.022 0.023 0.132 0.284 0.081 0.475 0.023 0.147 0.043 0.23  0.084
  0.034 0.075 0.114 0.021 0.184 0.075]
 [0.034 0.026 0.069 0.065 0.054 0.011 0.047 0.118 0.06  0.038 0.009 0.114
  0.032 0.017 0.023 0.161 0.085 0.023 0.181 0.051 0.042 0.019 0.157 0.037
  0.01  0.007 0.031 0.008 0.301 0.04 ]
 [0.114 0.033 0.082 0.103 0.064 0.015 0.047 0.098 0.071 0.042 0.063 0.005
  0.046 0.016 0.007 0.03  0.163 0.032 0.203 0.013 0.038 0.015 0.044 0.031
  0.008 0.034 0.065 0.012 0.046 0.064]
 [0.107 0.069 0.103 0.187 0.338 0.048 0.109 0.066 0.092 0.04  0.114 0.208
  0.008 0.024 0.007 0.381 0.117 0.093 0.381 0.035 0.116 0.014 0.145 0.057
  0.023 0.039 0.079 0.022 0.119 0.116]
 [0.299 0.328 1.    0.438 0.144 0.034 0.352 0.212 0.257 0.291 0.372 0.426
  0.258 0.003 0.036 0.368 0.338 0.202 0.713 0.098 0.198 0.039 0.491 0.109
  0.163 0.181 0.171 0.059 0.329 0.608]
 [0.572 0.666 0.623 1.227 0.236 0.162 0.54  0.576 0.477 0.239 0.386 0.814
  0.751 0.218 0.006 0.557 0.696 0.404 1.363 0.405 0.612 0.173 0.35  1.359
  0.124 0.4   0.42  0.153 0.607 0.371]
 [0.162 0.045 0.119 0.119 0.066 0.014 0.077 0.066 0.043 0.055 0.061 0.208
  0.024 0.022 0.007 0.003 0.18  0.026 0.15  0.015 0.051 0.014 0.107 0.029
  0.023 0.035 0.051 0.011 0.163 0.034]
 [0.075 0.017 0.097 0.031 0.086 0.016 0.036 0.035 0.04  0.028 0.058 0.046
  0.056 0.016 0.01  0.031 0.006 0.024 0.381 0.017 0.037 0.008 0.097 0.027
  0.009 0.027 0.056 0.016 0.147 0.043]
 [0.091 0.148 0.108 0.285 0.162 0.045 0.223 0.17  0.396 0.118 0.341 0.216
  0.081 0.052 0.012 0.292 0.216 0.008 0.47  0.074 0.076 0.052 0.137 0.133
  0.04  0.089 0.261 0.028 0.397 0.117]
 [0.012 0.023 0.064 0.027 0.031 0.01  0.038 0.044 0.031 0.02  0.026 0.025
  0.015 0.008 0.005 0.044 0.028 0.009 0.006 0.012 0.02  0.008 0.016 0.005
  0.012 0.023 0.026 0.005 0.064 0.025]
 [0.236 0.217 0.294 0.299 0.139 0.032 0.229 0.301 0.226 0.243 0.152 0.504
  0.19  0.089 0.027 0.423 0.477 0.092 0.567 0.008 0.252 0.046 0.179 0.554
  0.043 0.207 0.184 0.015 0.817 0.226]
 [0.127 0.088 0.173 0.181 0.065 0.023 0.197 0.155 0.31  0.05  0.164 0.154
  0.066 0.028 0.012 0.118 0.271 0.085 0.38  0.031 0.004 0.028 0.062 0.064
  0.031 0.038 0.251 0.046 0.309 0.089]
 [0.561 0.304 0.686 0.416 0.861 0.035 0.484 0.666 0.581 0.21  0.341 0.5
  0.41  0.14  0.044 0.385 0.703 0.099 0.813 0.148 0.251 0.006 0.526 0.376
  0.052 0.196 0.257 0.036 0.544 0.171]
 [0.044 0.039 0.106 0.084 0.053 0.015 0.047 0.122 0.056 0.028 0.059 0.182
  0.073 0.018 0.017 0.083 0.077 0.034 0.457 0.032 0.072 0.014 0.006 0.03
  0.02  0.023 0.048 0.015 0.122 0.032]
 [0.356 0.124 0.266 0.157 0.135 0.02  0.312 0.132 0.187 0.095 0.145 0.313
  0.127 0.047 0.005 0.285 0.223 0.075 0.965 0.009 0.127 0.022 0.158 0.005
  0.065 0.131 0.213 0.024 0.421 0.097]
 [0.331 0.203 0.494 0.45  0.165 0.034 0.163 0.376 0.385 0.18  0.613 0.557
  0.33  0.06  0.044 0.524 0.39  0.173 0.56  0.168 0.209 0.141 0.268 0.092
  0.005 0.708 0.162 0.048 0.646 0.235]
 [0.211 0.073 0.233 0.416 0.079 0.049 0.146 0.168 0.307 0.103 0.769 0.198
  0.163 0.066 0.012 0.176 0.376 0.096 0.224 0.046 0.187 0.027 0.355 0.051
  0.015 0.007 0.167 0.023 0.508 0.148]
 [0.129 0.019 0.175 0.111 0.202 0.02  0.09  0.251 0.101 0.049 0.126 0.145
  0.105 0.031 0.014 0.123 0.152 0.03  0.452 0.044 0.025 0.034 0.167 0.034
  0.047 0.042 0.006 0.009 0.299 0.071]
 [0.432 0.399 0.257 0.82  0.271 0.111 0.209 0.337 0.394 0.287 0.682 0.75
  0.342 0.114 0.066 0.42  0.494 0.313 0.594 0.388 0.243 0.255 0.509 0.296
  0.133 0.335 0.712 0.004 0.478 0.348]
 [0.049 0.014 0.077 0.071 0.04  0.005 0.04  0.077 0.046 0.034 0.02  0.143
  0.039 0.015 0.006 0.047 0.064 0.021 0.185 0.01  0.018 0.02  0.066 0.017
  0.008 0.017 0.031 0.016 0.006 0.034]
 [0.081 0.078 0.126 0.335 0.166 0.017 0.339 0.275 0.081 0.073 0.23  0.142
  0.042 0.009 0.015 0.206 0.162 0.075 0.245 0.034 0.092 0.046 0.239 0.083
  0.034 0.043 0.107 0.017 0.263 0.005]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.522 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.37  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.339 0.    0.34  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.367 0.388 0.416 0.517 0.736 0.    0.742 0.467 0.    0.    0.376 0.539
  0.    0.    0.    0.464 0.545 0.    0.65  0.    0.301 0.    0.56  0.
  0.    0.    0.348 0.    0.757 0.523]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.475 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.301 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.338 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.381 0.    0.    0.381 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.328 1.    0.438 0.    0.    0.352 0.    0.    0.    0.372 0.426
  0.    0.    0.    0.368 0.338 0.    0.713 0.    0.    0.    0.491 0.
  0.    0.    0.    0.    0.329 0.608]
 [0.572 0.666 0.623 1.227 0.    0.    0.54  0.576 0.477 0.    0.386 0.814
  0.751 0.    0.    0.557 0.696 0.404 1.363 0.405 0.612 0.    0.35  1.359
  0.    0.4   0.42  0.    0.607 0.371]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.381 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.396 0.    0.341 0.
  0.    0.    0.    0.    0.    0.    0.47  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.397 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.301 0.    0.    0.    0.504
  0.    0.    0.    0.423 0.477 0.    0.567 0.    0.    0.    0.    0.554
  0.    0.    0.    0.    0.817 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.31  0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.38  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.309 0.   ]
 [0.561 0.304 0.686 0.416 0.861 0.    0.484 0.666 0.581 0.    0.341 0.5
  0.41  0.    0.    0.385 0.703 0.    0.813 0.    0.    0.    0.526 0.376
  0.    0.    0.    0.    0.544 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.457 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.356 0.    0.    0.    0.    0.    0.312 0.    0.    0.    0.    0.313
  0.    0.    0.    0.    0.    0.    0.965 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.421 0.   ]
 [0.331 0.    0.494 0.45  0.    0.    0.    0.376 0.385 0.    0.613 0.557
  0.33  0.    0.    0.524 0.39  0.    0.56  0.    0.    0.    0.    0.
  0.    0.708 0.    0.    0.646 0.   ]
 [0.    0.    0.    0.416 0.    0.    0.    0.    0.307 0.    0.769 0.
  0.    0.    0.    0.    0.376 0.    0.    0.    0.    0.    0.355 0.
  0.    0.    0.    0.    0.508 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.452 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.432 0.399 0.    0.82  0.    0.    0.    0.337 0.394 0.    0.682 0.75
  0.342 0.    0.    0.42  0.494 0.313 0.594 0.388 0.    0.    0.509 0.
  0.    0.335 0.712 0.    0.478 0.348]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.335 0.    0.    0.339 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.717391304347826, 'tpr': 0.325, 'fpr': 0.3142857142857143, 'f1': 0.3023255813953488, 'shd': 171, 'npred': 138, 'ntrue': 120}
[0.076 0.119 0.125 0.109 0.015 0.093 0.083 0.084 0.073 0.126 0.078 0.071
 0.02  0.012 0.062 0.106 0.085 0.522 0.023 0.08  0.011 0.127 0.023 0.018
 0.032 0.061 0.02  0.142 0.062 0.057 0.15  0.269 0.078 0.016 0.202 0.261
 0.095 0.059 0.254 0.246 0.153 0.021 0.01  0.128 0.37  0.092 0.253 0.055
 0.103 0.019 0.2   0.093 0.045 0.108 0.339 0.018 0.34  0.103 0.05  0.053
 0.032 0.082 0.016 0.071 0.102 0.036 0.038 0.121 0.067 0.074 0.007 0.011
 0.067 0.09  0.039 0.118 0.027 0.061 0.012 0.08  0.026 0.018 0.028 0.044
 0.025 0.094 0.062 0.051 0.024 0.137 0.067 0.016 0.07  0.115 0.101 0.033
 0.138 0.086 0.053 0.021 0.007 0.083 0.149 0.035 0.224 0.024 0.016 0.021
 0.133 0.037 0.011 0.024 0.075 0.008 0.081 0.02  0.07  0.068 0.101 0.129
 0.008 0.053 0.157 0.118 0.044 0.173 0.119 0.022 0.053 0.017 0.087 0.09
 0.037 0.289 0.046 0.084 0.007 0.163 0.044 0.029 0.063 0.033 0.023 0.169
 0.04  0.367 0.388 0.416 0.517 0.736 0.742 0.467 0.21  0.248 0.376 0.539
 0.131 0.185 0.068 0.464 0.545 0.252 0.65  0.222 0.301 0.185 0.56  0.205
 0.207 0.148 0.348 0.053 0.757 0.523 0.098 0.047 0.152 0.118 0.116 0.007
 0.133 0.046 0.049 0.185 0.222 0.08  0.019 0.009 0.081 0.201 0.034 0.213
 0.043 0.033 0.011 0.099 0.019 0.027 0.03  0.087 0.029 0.209 0.028 0.062
 0.033 0.104 0.063 0.064 0.016 0.067 0.069 0.055 0.071 0.07  0.084 0.023
 0.009 0.107 0.222 0.045 0.146 0.021 0.063 0.011 0.049 0.07  0.022 0.037
 0.024 0.016 0.106 0.025 0.146 0.054 0.133 0.076 0.074 0.033 0.149 0.118
 0.046 0.113 0.075 0.076 0.022 0.012 0.114 0.163 0.019 0.242 0.043 0.023
 0.012 0.2   0.059 0.021 0.021 0.073 0.017 0.108 0.081 0.103 0.103 0.235
 0.147 0.254 0.035 0.116 0.12  0.129 0.198 0.154 0.19  0.022 0.023 0.132
 0.284 0.081 0.475 0.023 0.147 0.043 0.23  0.084 0.034 0.075 0.114 0.021
 0.184 0.075 0.034 0.026 0.069 0.065 0.054 0.011 0.047 0.118 0.06  0.038
 0.114 0.032 0.017 0.023 0.161 0.085 0.023 0.181 0.051 0.042 0.019 0.157
 0.037 0.01  0.007 0.031 0.008 0.301 0.04  0.114 0.033 0.082 0.103 0.064
 0.015 0.047 0.098 0.071 0.042 0.063 0.046 0.016 0.007 0.03  0.163 0.032
 0.203 0.013 0.038 0.015 0.044 0.031 0.008 0.034 0.065 0.012 0.046 0.064
 0.107 0.069 0.103 0.187 0.338 0.048 0.109 0.066 0.092 0.04  0.114 0.208
 0.024 0.007 0.381 0.117 0.093 0.381 0.035 0.116 0.014 0.145 0.057 0.023
 0.039 0.079 0.022 0.119 0.116 0.299 0.328 1.    0.438 0.144 0.034 0.352
 0.212 0.257 0.291 0.372 0.426 0.258 0.036 0.368 0.338 0.202 0.713 0.098
 0.198 0.039 0.491 0.109 0.163 0.181 0.171 0.059 0.329 0.608 0.572 0.666
 0.623 1.227 0.236 0.162 0.54  0.576 0.477 0.239 0.386 0.814 0.751 0.218
 0.557 0.696 0.404 1.363 0.405 0.612 0.173 0.35  1.359 0.124 0.4   0.42
 0.153 0.607 0.371 0.162 0.045 0.119 0.119 0.066 0.014 0.077 0.066 0.043
 0.055 0.061 0.208 0.024 0.022 0.007 0.18  0.026 0.15  0.015 0.051 0.014
 0.107 0.029 0.023 0.035 0.051 0.011 0.163 0.034 0.075 0.017 0.097 0.031
 0.086 0.016 0.036 0.035 0.04  0.028 0.058 0.046 0.056 0.016 0.01  0.031
 0.024 0.381 0.017 0.037 0.008 0.097 0.027 0.009 0.027 0.056 0.016 0.147
 0.043 0.091 0.148 0.108 0.285 0.162 0.045 0.223 0.17  0.396 0.118 0.341
 0.216 0.081 0.052 0.012 0.292 0.216 0.47  0.074 0.076 0.052 0.137 0.133
 0.04  0.089 0.261 0.028 0.397 0.117 0.012 0.023 0.064 0.027 0.031 0.01
 0.038 0.044 0.031 0.02  0.026 0.025 0.015 0.008 0.005 0.044 0.028 0.009
 0.012 0.02  0.008 0.016 0.005 0.012 0.023 0.026 0.005 0.064 0.025 0.236
 0.217 0.294 0.299 0.139 0.032 0.229 0.301 0.226 0.243 0.152 0.504 0.19
 0.089 0.027 0.423 0.477 0.092 0.567 0.252 0.046 0.179 0.554 0.043 0.207
 0.184 0.015 0.817 0.226 0.127 0.088 0.173 0.181 0.065 0.023 0.197 0.155
 0.31  0.05  0.164 0.154 0.066 0.028 0.012 0.118 0.271 0.085 0.38  0.031
 0.028 0.062 0.064 0.031 0.038 0.251 0.046 0.309 0.089 0.561 0.304 0.686
 0.416 0.861 0.035 0.484 0.666 0.581 0.21  0.341 0.5   0.41  0.14  0.044
 0.385 0.703 0.099 0.813 0.148 0.251 0.526 0.376 0.052 0.196 0.257 0.036
 0.544 0.171 0.044 0.039 0.106 0.084 0.053 0.015 0.047 0.122 0.056 0.028
 0.059 0.182 0.073 0.018 0.017 0.083 0.077 0.034 0.457 0.032 0.072 0.014
 0.03  0.02  0.023 0.048 0.015 0.122 0.032 0.356 0.124 0.266 0.157 0.135
 0.02  0.312 0.132 0.187 0.095 0.145 0.313 0.127 0.047 0.005 0.285 0.223
 0.075 0.965 0.009 0.127 0.022 0.158 0.065 0.131 0.213 0.024 0.421 0.097
 0.331 0.203 0.494 0.45  0.165 0.034 0.163 0.376 0.385 0.18  0.613 0.557
 0.33  0.06  0.044 0.524 0.39  0.173 0.56  0.168 0.209 0.141 0.268 0.092
 0.708 0.162 0.048 0.646 0.235 0.211 0.073 0.233 0.416 0.079 0.049 0.146
 0.168 0.307 0.103 0.769 0.198 0.163 0.066 0.012 0.176 0.376 0.096 0.224
 0.046 0.187 0.027 0.355 0.051 0.015 0.167 0.023 0.508 0.148 0.129 0.019
 0.175 0.111 0.202 0.02  0.09  0.251 0.101 0.049 0.126 0.145 0.105 0.031
 0.014 0.123 0.152 0.03  0.452 0.044 0.025 0.034 0.167 0.034 0.047 0.042
 0.009 0.299 0.071 0.432 0.399 0.257 0.82  0.271 0.111 0.209 0.337 0.394
 0.287 0.682 0.75  0.342 0.114 0.066 0.42  0.494 0.313 0.594 0.388 0.243
 0.255 0.509 0.296 0.133 0.335 0.712 0.478 0.348 0.049 0.014 0.077 0.071
 0.04  0.005 0.04  0.077 0.046 0.034 0.02  0.143 0.039 0.015 0.006 0.047
 0.064 0.021 0.185 0.01  0.018 0.02  0.066 0.017 0.008 0.017 0.031 0.016
 0.034 0.081 0.078 0.126 0.335 0.166 0.017 0.339 0.275 0.081 0.073 0.23
 0.142 0.042 0.009 0.015 0.206 0.162 0.075 0.245 0.034 0.092 0.046 0.239
 0.083 0.034 0.043 0.107 0.017 0.263]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.641688888888889, 0.25994867538392197)
Iterations 2250
Achieves (5.825547298234785, 1e-05)-DP
