samples  5000  graph  13 26 ER mlp  minibatch size  75  noise  0.5  minibatches per NN training  111 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3261408551088962
iteration 1 in outer loop, alpha = 1.3261408551088962, rho = 1.0, h = 1.3261408551088962
cuda
iteration 1 in inner loop,alpha 1.3261408551088962 rho 1.0 h 0.8833022561168153
iteration 2 in inner loop,alpha 1.3261408551088962 rho 10.0 h 0.3976662043074768
iteration 3 in inner loop,alpha 1.3261408551088962 rho 100.0 h 0.14473978390051023
iteration 2 in outer loop, alpha = 15.80011924515992, rho = 100.0, h = 0.14473978390051023
cuda
iteration 1 in inner loop,alpha 15.80011924515992 rho 100.0 h 0.09206134723974557
iteration 2 in inner loop,alpha 15.80011924515992 rho 1000.0 h 0.03605103482185257
iteration 3 in outer loop, alpha = 51.85115406701249, rho = 1000.0, h = 0.03605103482185257
cuda
iteration 1 in inner loop,alpha 51.85115406701249 rho 1000.0 h 0.022520091840645406
iteration 2 in inner loop,alpha 51.85115406701249 rho 10000.0 h 0.007430765387507421
iteration 4 in outer loop, alpha = 126.1588079420867, rho = 10000.0, h = 0.007430765387507421
cuda
iteration 1 in inner loop,alpha 126.1588079420867 rho 10000.0 h 0.003776628811836602
iteration 2 in inner loop,alpha 126.1588079420867 rho 100000.0 h 0.0013610307532996302
iteration 5 in outer loop, alpha = 262.2618832720497, rho = 100000.0, h = 0.0013610307532996302
cuda
iteration 1 in inner loop,alpha 262.2618832720497 rho 100000.0 h 0.0007716999804809888
iteration 6 in outer loop, alpha = 1033.9618637530384, rho = 1000000.0, h = 0.0007716999804809888
Threshold 0.3
[[0.001 0.    0.182 2.569 0.635 0.    0.017 0.    0.    0.437 0.    1.888
  0.   ]
 [0.631 0.001 0.321 0.04  0.172 0.002 0.04  0.    0.    2.061 0.001 0.198
  0.   ]
 [0.    0.    0.002 0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.687 0.001 0.109 0.001 0.089 0.    0.    1.627 0.    0.283
  0.   ]
 [0.    0.    0.255 0.    0.001 0.    0.002 0.    0.    0.    0.    0.
  0.   ]
 [0.077 0.029 0.336 0.065 0.609 0.001 0.013 0.    0.114 2.26  0.033 0.126
  0.012]
 [0.009 0.012 1.092 0.025 0.088 0.043 0.027 0.001 0.002 0.973 0.001 0.299
  0.003]
 [0.009 0.007 0.009 0.01  1.505 3.724 0.081 0.001 0.648 0.449 0.002 0.053
  0.   ]
 [0.021 1.541 2.239 0.017 0.092 0.004 0.359 0.    0.    1.386 0.002 0.194
  0.022]
 [0.    0.    1.123 0.    0.242 0.    0.001 0.    0.    0.004 0.    2.362
  0.   ]
 [1.749 0.008 2.559 0.069 0.554 0.    0.012 0.001 0.003 0.986 0.    0.091
  0.001]
 [0.    0.    0.95  0.    0.976 0.    0.001 0.    0.    0.    0.    0.001
  0.   ]
 [0.007 1.023 2.329 0.018 0.272 0.003 0.091 0.    0.006 2.319 0.    0.415
  0.   ]]
[[0.    0.    0.    2.569 0.635 0.    0.    0.    0.    0.437 0.    1.888
  0.   ]
 [0.631 0.    0.321 0.    0.    0.    0.    0.    0.    2.061 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.687 0.    0.    0.    0.    0.    0.    1.627 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.336 0.    0.609 0.    0.    0.    0.    2.26  0.    0.
  0.   ]
 [0.    0.    1.092 0.    0.    0.    0.    0.    0.    0.973 0.    0.
  0.   ]
 [0.    0.    0.    0.    1.505 3.724 0.    0.    0.648 0.449 0.    0.
  0.   ]
 [0.    1.541 2.239 0.    0.    0.    0.359 0.    0.    1.386 0.    0.
  0.   ]
 [0.    0.    1.123 0.    0.    0.    0.    0.    0.    0.    0.    2.362
  0.   ]
 [1.749 0.    2.559 0.    0.554 0.    0.    0.    0.    0.986 0.    0.
  0.   ]
 [0.    0.    0.95  0.    0.976 0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    1.023 2.329 0.    0.    0.    0.    0.    0.    2.319 0.    0.415
  0.   ]]
{'fdr': 0.4117647058823529, 'tpr': 0.7692307692307693, 'fpr': 0.2692307692307692, 'f1': 0.6666666666666667, 'shd': 16, 'npred': 34, 'ntrue': 26}
[2.709e-04 1.824e-01 2.569e+00 6.347e-01 4.409e-04 1.688e-02 1.418e-04
 1.314e-04 4.370e-01 2.441e-05 1.888e+00 8.225e-05 6.311e-01 3.206e-01
 3.991e-02 1.718e-01 1.888e-03 4.013e-02 3.257e-04 1.429e-04 2.061e+00
 1.297e-03 1.978e-01 1.038e-04 3.463e-05 1.430e-05 1.019e-04 1.623e-04
 1.429e-05 9.834e-04 5.300e-06 2.169e-05 1.810e-04 1.877e-06 1.476e-05
 1.439e-05 1.233e-04 6.370e-05 2.687e+00 1.093e-01 5.654e-04 8.929e-02
 1.822e-04 4.822e-05 1.627e+00 8.417e-06 2.825e-01 5.350e-05 3.871e-05
 3.550e-05 2.549e-01 1.370e-04 9.870e-06 1.644e-03 7.721e-06 1.615e-05
 1.463e-04 4.828e-06 2.880e-04 1.052e-05 7.735e-02 2.929e-02 3.360e-01
 6.516e-02 6.090e-01 1.331e-02 1.129e-04 1.145e-01 2.260e+00 3.342e-02
 1.261e-01 1.247e-02 9.295e-03 1.173e-02 1.092e+00 2.524e-02 8.756e-02
 4.262e-02 9.732e-04 2.381e-03 9.727e-01 1.082e-03 2.986e-01 2.923e-03
 8.812e-03 6.955e-03 9.492e-03 1.023e-02 1.505e+00 3.724e+00 8.077e-02
 6.482e-01 4.494e-01 1.951e-03 5.338e-02 2.108e-04 2.096e-02 1.541e+00
 2.239e+00 1.656e-02 9.200e-02 4.081e-03 3.586e-01 3.273e-04 1.386e+00
 1.763e-03 1.943e-01 2.219e-02 4.106e-05 8.852e-05 1.123e+00 1.152e-04
 2.416e-01 2.212e-04 1.069e-03 4.351e-05 1.334e-05 6.935e-06 2.362e+00
 3.005e-05 1.749e+00 7.818e-03 2.559e+00 6.923e-02 5.540e-01 3.162e-04
 1.167e-02 8.957e-04 2.919e-03 9.857e-01 9.067e-02 8.555e-04 2.191e-06
 6.270e-06 9.500e-01 2.399e-05 9.764e-01 7.224e-06 1.035e-03 1.079e-06
 2.817e-06 4.822e-05 8.203e-07 9.864e-07 7.113e-03 1.023e+00 2.329e+00
 1.810e-02 2.718e-01 2.717e-03 9.136e-02 2.993e-05 5.610e-03 2.319e+00
 3.833e-04 4.147e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.8715976331360946, 0.7667248028902397)
cuda
1963
cuda
Objective function 480.43 = squared loss an data 285.64 + 0.5*rho*h**2 194.315677 + alpha*h 0.000000 + L2reg 0.24 + L1reg 0.24 ; SHD = 91 ; DAG False
||w||^2 0.6889076657254372
exp ma of ||w||^2 3300760.601069947
||w|| 0.8300046178940435
exp ma of ||w|| 3.525619523460455
||w||^2 0.7142404424157114
exp ma of ||w||^2 2567402.268961145
||w|| 0.8451274711046325
exp ma of ||w|| 2.9360466777463463
||w||^2 0.48765862570890656
exp ma of ||w||^2 521.466361322567
||w|| 0.6983255871790082
exp ma of ||w|| 0.6498782978150961
||w||^2 0.5508814895281183
exp ma of ||w||^2 0.3700027560521092
||w|| 0.7422139108963927
exp ma of ||w|| 0.5954597983114203
||w||^2 0.4523771924429471
exp ma of ||w||^2 0.35290911137961933
||w|| 0.6725899140211271
exp ma of ||w|| 0.5830095959325093
||w||^2 0.48602046817781974
exp ma of ||w||^2 0.42442989435663825
||w|| 0.6971516823316284
exp ma of ||w|| 0.6310775053330551
||w||^2 0.160581513046812
exp ma of ||w||^2 0.45915794265223137
||w|| 0.40072623204229096
exp ma of ||w|| 0.6550470608917732
cuda
Objective function 38.55 = squared loss an data 37.07 + 0.5*rho*h**2 1.050590 + alpha*h 0.000000 + L2reg 0.31 + L1reg 0.11 ; SHD = 36 ; DAG False
Proportion of microbatches that were clipped  0.7588099012315571
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.4495445761929453
iteration 1 in outer loop, alpha = 1.4495445761929453, rho = 1.0, h = 1.4495445761929453
cuda
1963
cuda
Objective function 40.65 = squared loss an data 37.07 + 0.5*rho*h**2 1.050590 + alpha*h 2.101179 + L2reg 0.31 + L1reg 0.11 ; SHD = 36 ; DAG False
||w||^2 26231864392.324745
exp ma of ||w||^2 47400452753.53313
||w|| 161962.54008975267
exp ma of ||w|| 191613.30960313685
||w||^2 1.638416086274535
exp ma of ||w||^2 0.6750092253700217
||w|| 1.2800062836855666
exp ma of ||w|| 0.7684170193361854
||w||^2 0.46441418067739365
exp ma of ||w||^2 0.7110770892651282
||w|| 0.6814794059084938
exp ma of ||w|| 0.8030456178441913
||w||^2 0.4885539173333998
exp ma of ||w||^2 0.8181823562078453
||w|| 0.698966320600213
exp ma of ||w|| 0.858486273834268
||w||^2 0.8256546321293144
exp ma of ||w||^2 0.7083052082611464
||w|| 0.908655397898078
exp ma of ||w|| 0.8047857585473468
cuda
Objective function 13.61 = squared loss an data 10.49 + 0.5*rho*h**2 0.693640 + alpha*h 1.707315 + L2reg 0.62 + L1reg 0.10 ; SHD = 31 ; DAG False
Proportion of microbatches that were clipped  0.7621556886227545
iteration 1 in inner loop, alpha 1.4495445761929453 rho 1.0 h 1.1778287102636646
1963
cuda
Objective function 19.86 = squared loss an data 10.49 + 0.5*rho*h**2 6.936402 + alpha*h 1.707315 + L2reg 0.62 + L1reg 0.10 ; SHD = 31 ; DAG False
||w||^2 41891360.41685528
exp ma of ||w||^2 641471083.3277541
||w|| 6472.353545415708
exp ma of ||w|| 18486.610363864154
||w||^2 11947354.075445665
exp ma of ||w||^2 194945067.02075133
||w|| 3456.4944778555146
exp ma of ||w|| 8943.090125810962
||w||^2 240.58769337607643
exp ma of ||w||^2 214722.99046118013
||w|| 15.510889509505134
exp ma of ||w|| 67.75095700175865
||w||^2 0.7830767719714737
exp ma of ||w||^2 9.143741505990286
||w|| 0.8849162513884993
exp ma of ||w|| 1.0592877231694269
||w||^2 1.065059423060134
exp ma of ||w||^2 1.058390268152404
||w|| 1.0320171621926324
exp ma of ||w|| 0.9820201501864743
||w||^2 2.935745397914463
exp ma of ||w||^2 1.111638879156702
||w|| 1.713401703604401
exp ma of ||w|| 1.006338179081047
||w||^2 0.6289124203447617
exp ma of ||w||^2 0.9679488448372014
||w|| 0.7930399865988863
exp ma of ||w|| 0.9378514135330033
||w||^2 0.8917881939395261
exp ma of ||w||^2 1.0822056360561525
||w|| 0.9443453785239414
exp ma of ||w|| 0.9969520924665921
||w||^2 0.4642906228269147
exp ma of ||w||^2 1.098452832653822
||w|| 0.6813887457442445
exp ma of ||w|| 0.9998313217365035
cuda
Objective function 13.97 = squared loss an data 11.38 + 0.5*rho*h**2 1.055356 + alpha*h 0.665957 + L2reg 0.77 + L1reg 0.09 ; SHD = 28 ; DAG False
Proportion of microbatches that were clipped  0.761331531752856
iteration 2 in inner loop, alpha 1.4495445761929453 rho 10.0 h 0.4594248859477936
1963
cuda
Objective function 23.47 = squared loss an data 11.38 + 0.5*rho*h**2 10.553561 + alpha*h 0.665957 + L2reg 0.77 + L1reg 0.09 ; SHD = 28 ; DAG False
||w||^2 1579746647.8136635
exp ma of ||w||^2 8712343805.031021
||w|| 39746.026817955826
exp ma of ||w|| 77073.84619259796
||w||^2 1.8486439178785519
exp ma of ||w||^2 13537.453246902865
||w|| 1.359648453784489
exp ma of ||w|| 7.275835718634503
||w||^2 1.4388023537367327
exp ma of ||w||^2 152.15530333458753
||w|| 1.1995008769220357
exp ma of ||w|| 1.469899888637055
||w||^2 5.869858929205031
exp ma of ||w||^2 1.5751863691585584
||w|| 2.4227791746680154
exp ma of ||w|| 1.18423313642624
cuda
Objective function 15.50 = squared loss an data 13.12 + 0.5*rho*h**2 1.189971 + alpha*h 0.223622 + L2reg 0.88 + L1reg 0.09 ; SHD = 30 ; DAG True
Proportion of microbatches that were clipped  0.7726285300506879
iteration 3 in inner loop, alpha 1.4495445761929453 rho 100.0 h 0.15427062638104516
iteration 2 in outer loop, alpha = 16.87660721429746, rho = 100.0, h = 0.15427062638104516
cuda
1963
cuda
Objective function 17.88 = squared loss an data 13.12 + 0.5*rho*h**2 1.189971 + alpha*h 2.603565 + L2reg 0.88 + L1reg 0.09 ; SHD = 30 ; DAG True
||w||^2 0.9759658559800775
exp ma of ||w||^2 1.748806651439758
||w|| 0.9879098420301711
exp ma of ||w|| 1.2489396071305523
||w||^2 0.43134955311547785
exp ma of ||w||^2 1.7052752591678138
||w|| 0.656772070900916
exp ma of ||w|| 1.238817943048922
||w||^2 1.4419676519056954
exp ma of ||w||^2 1.7286236201226586
||w|| 1.2008195750843236
exp ma of ||w|| 1.2625000722705653
||w||^2 1.5085729857018104
exp ma of ||w||^2 1.8366704803386555
||w|| 1.2282397916131078
exp ma of ||w|| 1.28074718507045
||w||^2 2.3375116739883426
exp ma of ||w||^2 1.6657888568197716
||w|| 1.5288923029397272
exp ma of ||w|| 1.2174991475730128
||w||^2 1.4905303705348791
exp ma of ||w||^2 1.6092592045863083
||w|| 1.2208727904801873
exp ma of ||w|| 1.2008333115524805
cuda
Objective function 16.30 = squared loss an data 13.34 + 0.5*rho*h**2 0.393098 + alpha*h 1.496410 + L2reg 0.98 + L1reg 0.09 ; SHD = 26 ; DAG True
Proportion of microbatches that were clipped  0.7718677564962791
iteration 1 in inner loop, alpha 16.87660721429746 rho 100.0 h 0.0886677315450548
1963
cuda
Objective function 19.84 = squared loss an data 13.34 + 0.5*rho*h**2 3.930983 + alpha*h 1.496410 + L2reg 0.98 + L1reg 0.09 ; SHD = 26 ; DAG True
||w||^2 254289950170.77582
exp ma of ||w||^2 270944715070.17593
||w|| 504271.70272659144
exp ma of ||w|| 440565.23005720304
||w||^2 204077043748.25275
exp ma of ||w||^2 257321411561.2563
||w|| 451748.8724371681
exp ma of ||w|| 431899.4965775771
||w||^2 6.043585519594939
exp ma of ||w||^2 77748.35320503851
||w|| 2.4583705008795844
exp ma of ||w|| 13.130254595224446
||w||^2 2.6083226145504197
exp ma of ||w||^2 16213.68091344865
||w|| 1.6150302209402831
exp ma of ||w|| 4.391817654415347
||w||^2 1.745371161892705
exp ma of ||w||^2 23.29610459357554
||w|| 1.3211249607409228
exp ma of ||w|| 1.467545919769135
||w||^2 0.5756907182659484
exp ma of ||w||^2 1.9322677668737518
||w|| 0.7587428538483565
exp ma of ||w|| 1.307598198406807
||w||^2 2.7908639420875487
exp ma of ||w||^2 1.8157607325703127
||w|| 1.6705879031309752
exp ma of ||w|| 1.2774471446222664
||w||^2 1.8019095599395922
exp ma of ||w||^2 1.7875835410622818
||w|| 1.3423522488302362
exp ma of ||w|| 1.2688845725638145
||w||^2 2.2463189248070297
exp ma of ||w||^2 1.7719821397478548
||w|| 1.4987724726612208
exp ma of ||w|| 1.260489802623825
||w||^2 6.943967558976656
exp ma of ||w||^2 1.7772681396496126
||w|| 2.635140899264526
exp ma of ||w|| 1.2653001602297869
||w||^2 2.7175093252608846
exp ma of ||w||^2 1.7457488632440046
||w|| 1.64848698061613
exp ma of ||w|| 1.2581612216968334
||w||^2 1.04664252558761
exp ma of ||w||^2 1.7231010596774108
||w|| 1.0230554850972697
exp ma of ||w|| 1.2510204500447115
cuda
Objective function 16.32 = squared loss an data 14.09 + 0.5*rho*h**2 0.549569 + alpha*h 0.559515 + L2reg 1.03 + L1reg 0.09 ; SHD = 24 ; DAG True
Proportion of microbatches that were clipped  0.7717958045349824
iteration 2 in inner loop, alpha 16.87660721429746 rho 1000.0 h 0.03315326098933191
iteration 3 in outer loop, alpha = 50.02986820362938, rho = 1000.0, h = 0.03315326098933191
cuda
1963
cuda
Objective function 17.42 = squared loss an data 14.09 + 0.5*rho*h**2 0.549569 + alpha*h 1.658653 + L2reg 1.03 + L1reg 0.09 ; SHD = 24 ; DAG True
||w||^2 4.1507940164589066
exp ma of ||w||^2 1.5905159185788889
||w|| 2.0373497531005587
exp ma of ||w|| 1.1961918720358868
||w||^2 0.4222914271944553
exp ma of ||w||^2 1.653825338944003
||w|| 0.649839539574544
exp ma of ||w|| 1.2136953385763922
cuda
Objective function 16.45 = squared loss an data 14.06 + 0.5*rho*h**2 0.203426 + alpha*h 1.009131 + L2reg 1.08 + L1reg 0.10 ; SHD = 28 ; DAG True
Proportion of microbatches that were clipped  0.7735894647819258
iteration 1 in inner loop, alpha 50.02986820362938 rho 1000.0 h 0.020170573421687266
1963
cuda
Objective function 18.28 = squared loss an data 14.06 + 0.5*rho*h**2 2.034260 + alpha*h 1.009131 + L2reg 1.08 + L1reg 0.10 ; SHD = 28 ; DAG True
||w||^2 549322445419.184
exp ma of ||w||^2 136891805197.55132
||w|| 741162.9007304561
exp ma of ||w|| 152444.48023119452
||w||^2 1174600950222.3425
exp ma of ||w||^2 206282861652.72842
||w|| 1083790.0858664203
exp ma of ||w|| 224027.38114476248
||w||^2 519209589490.47644
exp ma of ||w||^2 209412128931.1059
||w|| 720561.9955912721
exp ma of ||w|| 228992.72728922757
||w||^2 1.7871074420296198
exp ma of ||w||^2 16.77101528666904
||w|| 1.3368273792938339
exp ma of ||w|| 1.3810164886900795
||w||^2 2.0663586414574895
exp ma of ||w||^2 1.6746782502294162
||w|| 1.437483440411572
exp ma of ||w|| 1.2363593744336425
||w||^2 2.163584219020031
exp ma of ||w||^2 1.7258501118239338
||w|| 1.470912716315972
exp ma of ||w|| 1.2599787841148784
||w||^2 1.4811867293250156
exp ma of ||w||^2 1.6693241920017423
||w|| 1.2170401510735032
exp ma of ||w|| 1.2284613910235034
||w||^2 0.9600703076201353
exp ma of ||w||^2 1.598964361111267
||w|| 0.9798317751635407
exp ma of ||w|| 1.2097185178620704
||w||^2 1.3529731499347528
exp ma of ||w||^2 1.621449784531189
||w|| 1.1631737402188689
exp ma of ||w|| 1.2238500357502866
||w||^2 0.650776902864107
exp ma of ||w||^2 1.5792495709267982
||w|| 0.8067074456481154
exp ma of ||w|| 1.1968574637820681
||w||^2 1.105803695678563
exp ma of ||w||^2 1.5797718752322758
||w|| 1.051572011646641
exp ma of ||w|| 1.2002002054181988
||w||^2 5.98984364758668
exp ma of ||w||^2 1.5804708150549986
||w|| 2.447415707963541
exp ma of ||w|| 1.1937895115838895
||w||^2 1.3541235708416341
exp ma of ||w||^2 1.5731702265327765
||w|| 1.1636681532299638
exp ma of ||w|| 1.1946967033457445
cuda
Objective function 16.10 = squared loss an data 14.11 + 0.5*rho*h**2 0.373794 + alpha*h 0.432574 + L2reg 1.09 + L1reg 0.10 ; SHD = 31 ; DAG True
Proportion of microbatches that were clipped  0.7679970617042116
iteration 2 in inner loop, alpha 50.02986820362938 rho 10000.0 h 0.008646322280952745
1963
cuda
Objective function 19.47 = squared loss an data 14.11 + 0.5*rho*h**2 3.737944 + alpha*h 0.432574 + L2reg 1.09 + L1reg 0.10 ; SHD = 31 ; DAG True
||w||^2 2187485109.0134945
exp ma of ||w||^2 1713316626849.8508
||w|| 46770.55814306148
exp ma of ||w|| 292804.46687763924
||w||^2 2.13599182708092
exp ma of ||w||^2 1585.7360632890377
||w|| 1.4615032764523384
exp ma of ||w|| 1.3985669311375095
||w||^2 1.2814937206241475
exp ma of ||w||^2 1.7046810802161656
||w|| 1.132030794909815
exp ma of ||w|| 1.2426922749660112
||w||^2 1.4140199497544754
exp ma of ||w||^2 1.6067024619133616
||w|| 1.1891257081379056
exp ma of ||w|| 1.210770150187907
cuda
Objective function 16.00 = squared loss an data 14.22 + 0.5*rho*h**2 0.418619 + alpha*h 0.144762 + L2reg 1.11 + L1reg 0.10 ; SHD = 27 ; DAG True
Proportion of microbatches that were clipped  0.7689709253227168
iteration 3 in inner loop, alpha 50.02986820362938 rho 100000.0 h 0.0028935079965179966
iteration 4 in outer loop, alpha = 339.380667855429, rho = 100000.0, h = 0.0028935079965179966
cuda
1963
cuda
Objective function 16.84 = squared loss an data 14.22 + 0.5*rho*h**2 0.418619 + alpha*h 0.982001 + L2reg 1.11 + L1reg 0.10 ; SHD = 27 ; DAG True
||w||^2 150303.31513484954
exp ma of ||w||^2 13536768631.404064
||w|| 387.68971502330254
exp ma of ||w|| 2614.9999944685405
||w||^2 172.4664825170112
exp ma of ||w||^2 466995867.1558403
||w|| 13.132649485804881
exp ma of ||w|| 149.97570885953576
||w||^2 3.0583493925735303
exp ma of ||w||^2 3068397.318769306
||w|| 1.7488137100827894
exp ma of ||w|| 3.5820997503755074
||w||^2 1.4762210055983198
exp ma of ||w||^2 43714.45230262612
||w|| 1.2149983562121884
exp ma of ||w|| 1.6298893165248025
||w||^2 0.9987156182078741
exp ma of ||w||^2 942.2128929387673
||w|| 0.9993576027668345
exp ma of ||w|| 1.3556626291801963
||w||^2 0.553016349859457
exp ma of ||w||^2 1.4571162320069764
||w|| 0.7436506907543736
exp ma of ||w|| 1.1574954224176246
||w||^2 1.1753129378139362
exp ma of ||w||^2 1.3887127254806235
||w|| 1.0841185072739679
exp ma of ||w|| 1.127994468971378
||w||^2 0.8266534387308212
exp ma of ||w||^2 1.494471103653748
||w|| 0.9092048387084295
exp ma of ||w|| 1.1749589915362355
v before min max tensor([[-22.699, -16.261,  -8.768,  ...,  27.675,   4.364, -10.618],
        [224.816,  -6.482, -16.750,  ..., -16.926,   4.746, -15.002],
        [-18.971, -20.764, 120.449,  ...,  28.979, -14.776, -14.726],
        ...,
        [ 24.160,  78.188, -26.758,  ...,  -1.948, -13.839, 168.294],
        [ -5.291, -17.677,  28.031,  ..., -18.715,  64.358, -24.279],
        [-15.292,  80.289, 226.181,  ..., -17.261, -16.240, -12.158]],
       device='cuda:0')
v tensor([[1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 4.364e+00,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 4.746e+00,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        ...,
        [1.000e+01, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e-12, 1.000e+01,
         1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e-12]], device='cuda:0')
v before min max tensor([-1.176e+01, -1.018e+01,  4.773e+01,  4.335e+00, -1.783e+01, -6.967e+00,
         1.880e+01,  7.865e+00, -8.445e+00,  7.068e+00,  4.182e+00, -1.205e+01,
        -8.322e+00, -1.734e+01,  1.043e+00, -1.226e+01, -1.782e+01, -1.218e+01,
         7.747e+00,  5.027e+01, -8.132e+00, -1.550e+01, -7.524e-01, -5.070e+00,
        -1.416e+01, -1.345e+01, -1.658e+01,  5.020e+00, -9.616e+00, -9.547e+00,
         2.194e+00,  4.697e+00, -7.849e+00, -2.084e+01, -1.021e+01,  3.818e+01,
        -1.621e+01, -1.154e+01, -1.881e+01, -1.089e+01, -9.617e+00, -1.090e+01,
        -8.718e+00, -1.786e+01, -1.200e+01, -1.308e+01,  5.594e-02, -6.971e+00,
        -7.133e+00, -1.889e+01, -1.060e+01,  3.306e+01,  8.954e+00, -3.100e+00,
         6.392e+00, -7.519e+00,  3.231e+01, -1.223e+01, -1.618e+01,  1.552e+00,
         4.530e+00, -6.390e+00, -2.062e+01, -1.578e+01, -7.517e+00, -9.959e+00,
        -1.279e+01, -1.066e+01,  4.289e+01,  4.273e+01, -8.754e+00, -1.347e+01,
        -7.594e+00, -1.170e+01, -1.203e+01,  3.515e-01, -1.359e+01, -3.029e+00,
        -1.018e+00, -3.822e+00, -1.701e+01, -5.246e+00,  2.429e+01,  6.096e+00,
        -4.459e+00,  5.729e+00,  2.364e+01, -1.098e+01, -1.415e+00,  1.407e+00,
        -1.569e+01, -1.934e+01, -8.415e+00,  5.747e+01,  1.425e+01, -1.191e+01,
         5.161e+01, -1.311e+01, -1.319e+01,  5.134e+00,  7.834e+00,  2.186e+01,
        -1.022e+01,  4.728e+01,  8.450e+01, -8.622e+00, -9.489e+00, -9.857e+00,
         1.168e+01, -7.810e+00, -5.121e+00, -1.395e+01, -5.440e-01, -5.256e-01,
        -8.254e+00, -4.402e+00, -1.508e+01, -1.283e+01,  5.980e+01, -1.080e+01,
        -1.422e+01, -1.536e+01,  1.849e+01, -1.434e+01, -5.614e+00, -1.751e+01,
         2.868e+01,  2.626e+01, -3.628e+00, -1.162e+01], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 4.335e+00, 1.000e-12, 1.000e-12,
        1.000e+01, 7.865e+00, 1.000e-12, 7.068e+00, 4.182e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.043e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        7.747e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 5.020e+00, 1.000e-12, 1.000e-12,
        2.194e+00, 4.697e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 5.594e-02, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 8.954e+00, 1.000e-12,
        6.392e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.552e+00,
        4.530e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 3.515e-01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 6.096e+00,
        1.000e-12, 5.729e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.407e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 5.134e+00, 7.834e+00, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 4.584e+00],
         [-8.407e+00],
         [-1.398e+01],
         [-1.405e+01],
         [ 4.595e+01],
         [-8.803e+00],
         [-8.929e+00],
         [-1.390e+01],
         [ 9.464e+01],
         [-2.112e+01]],

        [[-1.319e+00],
         [-4.503e+00],
         [ 1.261e+01],
         [-1.533e+01],
         [-8.867e+00],
         [-2.590e+00],
         [-9.797e+00],
         [-2.260e+00],
         [-8.804e+00],
         [-8.561e+00]],

        [[-1.766e+01],
         [-1.134e+01],
         [ 1.337e+01],
         [-1.086e+01],
         [-2.539e+00],
         [ 3.175e+01],
         [ 4.709e+00],
         [ 1.287e+01],
         [-6.947e+00],
         [-8.893e+00]],

        [[ 1.014e-01],
         [ 3.177e+01],
         [-1.436e+01],
         [-6.477e+00],
         [-1.686e+01],
         [-3.180e+00],
         [-2.613e+00],
         [ 1.507e+01],
         [-7.755e+00],
         [ 1.249e+01]],

        [[ 1.953e+01],
         [ 3.967e+01],
         [-5.989e+00],
         [-7.984e+00],
         [ 1.018e+01],
         [-1.498e+01],
         [-1.854e+01],
         [-7.405e+00],
         [ 1.082e+02],
         [-2.673e+00]],

        [[ 3.194e+01],
         [-3.243e+00],
         [-1.678e+01],
         [-1.238e+01],
         [-1.161e+01],
         [-1.433e+01],
         [-1.337e+01],
         [-1.593e+00],
         [-1.071e+01],
         [-7.155e+00]],

        [[-1.209e+01],
         [-1.709e+01],
         [-1.537e+01],
         [-1.310e+01],
         [-1.372e+01],
         [ 7.115e+01],
         [-1.281e+01],
         [-1.206e+01],
         [-6.164e+00],
         [ 2.789e+01]],

        [[ 1.550e+01],
         [-7.743e+00],
         [ 1.099e+01],
         [ 4.090e-01],
         [-1.160e+01],
         [ 9.834e+00],
         [ 4.951e+01],
         [-9.304e+00],
         [-6.669e+00],
         [ 2.988e+00]],

        [[-2.304e+00],
         [-3.983e+00],
         [-1.240e+01],
         [-1.033e+01],
         [-8.009e+00],
         [-1.455e+01],
         [-6.275e+00],
         [-1.461e+01],
         [ 6.219e+00],
         [-1.702e+01]],

        [[ 1.206e+00],
         [ 6.304e+01],
         [-1.257e+01],
         [-1.461e+01],
         [-1.327e+01],
         [ 5.962e+00],
         [-1.466e+01],
         [-1.492e+01],
         [ 1.790e+00],
         [ 1.816e+01]],

        [[-8.524e+00],
         [-7.994e+00],
         [-1.452e+01],
         [-7.934e+00],
         [-9.804e+00],
         [-1.129e+01],
         [-7.727e+00],
         [-1.309e+01],
         [-1.243e+01],
         [-2.805e+00]],

        [[-8.803e-01],
         [-1.188e+01],
         [-1.257e+01],
         [-1.766e+01],
         [-1.800e+01],
         [-1.233e+01],
         [ 3.165e+01],
         [-8.970e+00],
         [ 2.463e+00],
         [-1.058e+01]],

        [[-8.153e+00],
         [ 3.842e+01],
         [-1.185e+01],
         [-1.320e+01],
         [ 1.656e+01],
         [ 1.195e+01],
         [-1.737e+01],
         [-1.300e+01],
         [-9.209e+00],
         [-1.458e+01]]], device='cuda:0')
v tensor([[[4.584e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [4.709e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.014e-01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [4.090e-01],
         [1.000e-12],
         [9.834e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [2.988e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.219e+00],
         [1.000e-12]],

        [[1.206e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.962e+00],
         [1.000e-12],
         [1.000e-12],
         [1.790e+00],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [2.463e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-17.938],
        [-10.625],
        [  2.712],
        [-11.347],
        [ 12.593],
        [ -6.643],
        [-16.925],
        [ -8.507],
        [-20.540],
        [ 60.499],
        [ 81.941],
        [-17.297],
        [ -9.340]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [2.712e+00],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[ 0.014, -0.004, -0.006,  ..., -0.005,  0.013, -0.017],
        [ 0.011,  0.013, -0.003,  ...,  0.004,  0.010, -0.023],
        [-0.007,  0.001,  0.022,  ..., -0.013,  0.014, -0.037],
        ...,
        [-0.014,  0.013,  0.007,  ..., -0.011,  0.013,  0.011],
        [ 0.003,  0.001,  0.006,  ..., -0.004, -0.002, -0.004],
        [-0.023,  0.005,  0.012,  ..., -0.019,  0.008, -0.005]],
       device='cuda:0')
s after update for 1 param tensor([[2.123, 1.560, 2.152,  ..., 2.386, 1.423, 1.166],
        [2.414, 1.756, 2.650,  ..., 1.569, 1.578, 1.643],
        [2.280, 1.911, 2.828,  ..., 2.581, 1.337, 1.344],
        ...,
        [2.062, 2.034, 2.419,  ..., 1.336, 1.385, 2.619],
        [1.990, 2.180, 2.315,  ..., 1.742, 1.832, 2.247],
        [1.394, 2.091, 2.409,  ..., 1.688, 1.641, 1.989]], device='cuda:0')
b after update for 1 param tensor([[85.572, 73.358, 86.145,  ..., 90.720, 70.047, 63.413],
        [91.250, 77.811, 95.598,  ..., 73.553, 73.763, 75.271],
        [88.671, 81.184, 98.765,  ..., 94.344, 67.900, 68.080],
        ...,
        [84.320, 83.761, 91.338,  ..., 67.875, 69.112, 95.045],
        [82.852, 86.714, 89.348,  ..., 77.517, 79.478, 88.029],
        [69.344, 84.919, 91.142,  ..., 76.292, 75.227, 82.823]],
       device='cuda:0')
clipping threshold 0.9522550009710194
a after update for 1 param tensor([-0.035,  0.035, -0.009,  0.035, -0.085,  0.008, -0.038,  0.039,  0.009,
         0.007,  0.011, -0.009,  0.018,  0.043, -0.043,  0.038, -0.001, -0.039,
         0.103, -0.019, -0.045, -0.034,  0.029,  0.015,  0.031,  0.036,  0.027,
        -0.028, -0.055,  0.053, -0.017,  0.072, -0.033,  0.014,  0.067,  0.020,
        -0.003, -0.008,  0.001, -0.007,  0.010, -0.076, -0.042, -0.035, -0.017,
         0.027,  0.006, -0.055, -0.021,  0.045,  0.006,  0.046, -0.028, -0.016,
         0.002,  0.024,  0.003, -0.022, -0.042, -0.034, -0.035,  0.021, -0.086,
        -0.055, -0.017,  0.068,  0.043,  0.012,  0.007, -0.011, -0.009, -0.062,
         0.031, -0.062,  0.031,  0.020, -0.016,  0.060,  0.010, -0.022, -0.011,
        -0.020, -0.027,  0.004,  0.066, -0.078, -0.013, -0.029, -0.020, -0.045,
        -0.047, -0.003,  0.026, -0.019,  0.012, -0.011, -0.002, -0.058, -0.014,
         0.074, -0.033,  0.055,  0.030, -0.059,  0.009,  0.010, -0.021,  0.049,
         0.013, -0.055, -0.008,  0.003, -0.015,  0.015,  0.008, -0.017,  0.026,
        -0.012,  0.043, -0.023, -0.027, -0.020, -0.057, -0.016, -0.017,  0.055,
         0.000,  0.007, -0.021,  0.013], device='cuda:0')
s after update for 1 param tensor([1.186, 0.920, 1.646, 1.648, 1.779, 0.825, 1.500, 2.095, 1.261, 1.518,
        1.148, 1.354, 1.677, 1.572, 1.305, 1.133, 1.667, 1.168, 1.575, 1.799,
        0.839, 1.665, 1.590, 1.221, 1.322, 1.331, 1.628, 1.599, 1.092, 1.249,
        1.489, 1.914, 1.560, 1.897, 1.625, 1.555, 1.746, 1.522, 1.738, 1.357,
        1.458, 1.577, 1.378, 1.625, 1.259, 1.734, 1.219, 1.667, 1.521, 1.735,
        0.974, 1.733, 1.596, 1.522, 1.578, 1.036, 1.608, 1.157, 1.499, 1.124,
        0.999, 1.260, 1.966, 1.686, 1.638, 0.900, 1.345, 0.978, 1.854, 1.487,
        1.426, 1.939, 1.931, 1.079, 1.510, 1.154, 1.284, 1.295, 1.264, 0.890,
        1.552, 1.245, 1.756, 1.184, 1.073, 1.719, 2.263, 0.993, 1.507, 1.231,
        1.448, 1.809, 0.822, 1.484, 1.986, 1.425, 2.149, 1.186, 1.330, 1.031,
        1.897, 2.161, 0.938, 1.826, 1.919, 0.994, 2.088, 0.891, 1.534, 1.236,
        1.240, 1.546, 1.644, 1.483, 1.354, 1.024, 1.365, 1.163, 1.654, 1.121,
        1.567, 1.390, 1.743, 1.603, 1.963, 2.016, 2.118, 1.778, 1.717, 1.511],
       device='cuda:0')
b after update for 1 param tensor([63.960, 56.326, 75.345, 75.401, 78.339, 53.357, 71.934, 84.998, 65.952,
        72.353, 62.925, 68.338, 76.061, 73.631, 67.080, 62.508, 75.828, 63.460,
        73.694, 78.774, 53.797, 75.788, 74.041, 64.901, 67.522, 67.759, 74.943,
        74.268, 61.355, 65.620, 71.657, 81.256, 73.347, 80.878, 74.873, 73.226,
        77.601, 72.445, 77.429, 68.415, 70.901, 73.747, 68.946, 74.873, 65.886,
        77.338, 64.845, 75.814, 72.422, 77.350, 57.954, 77.306, 74.181, 72.457,
        73.774, 59.785, 74.470, 63.170, 71.893, 62.264, 58.689, 65.924, 82.337,
        76.262, 75.166, 55.722, 68.120, 58.080, 79.972, 71.625, 70.135, 81.779,
        81.613, 60.997, 72.172, 63.080, 66.557, 66.834, 66.034, 55.404, 73.162,
        65.520, 77.820, 63.907, 60.825, 76.996, 88.339, 58.519, 72.096, 65.153,
        70.674, 78.978, 53.240, 71.533, 82.753, 70.096, 86.092, 63.959, 67.719,
        59.634, 80.876, 86.331, 56.865, 79.361, 81.356, 58.563, 84.855, 55.438,
        72.725, 65.281, 65.404, 73.018, 75.293, 71.519, 68.328, 59.434, 68.609,
        63.323, 75.535, 62.177, 73.523, 69.244, 77.536, 74.347, 82.287, 83.387,
        85.472, 78.305, 76.944, 72.198], device='cuda:0')
clipping threshold 0.9522550009710194
a after update for 1 param tensor([[[ 0.003],
         [-0.019],
         [-0.008],
         [-0.026],
         [ 0.023],
         [ 0.070],
         [-0.076],
         [-0.009],
         [-0.023],
         [ 0.048]],

        [[-0.001],
         [-0.043],
         [ 0.078],
         [-0.040],
         [-0.026],
         [-0.031],
         [-0.057],
         [-0.019],
         [-0.012],
         [-0.007]],

        [[-0.048],
         [ 0.022],
         [ 0.035],
         [ 0.020],
         [ 0.043],
         [-0.005],
         [ 0.023],
         [-0.036],
         [-0.052],
         [-0.040]],

        [[-0.006],
         [ 0.001],
         [-0.044],
         [ 0.002],
         [-0.003],
         [ 0.031],
         [ 0.045],
         [ 0.047],
         [-0.017],
         [ 0.007]],

        [[ 0.003],
         [-0.064],
         [ 0.026],
         [ 0.002],
         [ 0.016],
         [ 0.020],
         [ 0.007],
         [ 0.038],
         [ 0.036],
         [-0.027]],

        [[-0.004],
         [-0.019],
         [-0.025],
         [ 0.006],
         [ 0.022],
         [-0.029],
         [-0.016],
         [-0.036],
         [-0.032],
         [-0.025]],

        [[-0.005],
         [ 0.020],
         [-0.003],
         [-0.018],
         [-0.003],
         [ 0.018],
         [ 0.007],
         [ 0.018],
         [-0.037],
         [ 0.002]],

        [[-0.012],
         [ 0.027],
         [ 0.027],
         [-0.065],
         [-0.035],
         [ 0.017],
         [ 0.074],
         [-0.027],
         [-0.029],
         [ 0.015]],

        [[-0.022],
         [ 0.018],
         [-0.049],
         [ 0.004],
         [ 0.070],
         [-0.016],
         [-0.010],
         [ 0.022],
         [ 0.007],
         [ 0.002]],

        [[ 0.006],
         [ 0.021],
         [ 0.032],
         [ 0.082],
         [ 0.003],
         [ 0.038],
         [ 0.045],
         [-0.028],
         [ 0.054],
         [ 0.067]],

        [[ 0.033],
         [-0.044],
         [ 0.058],
         [-0.002],
         [ 0.003],
         [ 0.039],
         [ 0.036],
         [-0.059],
         [ 0.005],
         [ 0.026]],

        [[-0.058],
         [-0.008],
         [ 0.029],
         [ 0.038],
         [-0.017],
         [-0.065],
         [ 0.035],
         [ 0.065],
         [ 0.024],
         [ 0.007]],

        [[-0.024],
         [ 0.040],
         [-0.002],
         [ 0.007],
         [ 0.019],
         [-0.035],
         [-0.020],
         [-0.026],
         [ 0.041],
         [ 0.027]]], device='cuda:0')
s after update for 1 param tensor([[[1.678],
         [1.312],
         [1.355],
         [1.331],
         [1.436],
         [1.666],
         [1.189],
         [1.332],
         [1.929],
         [1.910]],

        [[0.992],
         [1.038],
         [1.831],
         [1.639],
         [1.641],
         [1.377],
         [1.324],
         [1.116],
         [1.891],
         [1.396]],

        [[1.654],
         [1.734],
         [1.342],
         [1.359],
         [1.196],
         [1.441],
         [1.626],
         [1.437],
         [1.449],
         [1.314]],

        [[0.947],
         [1.903],
         [1.299],
         [0.886],
         [1.731],
         [1.340],
         [1.359],
         [2.177],
         [0.983],
         [2.120]],

        [[1.537],
         [1.831],
         [1.302],
         [0.966],
         [2.012],
         [1.450],
         [1.721],
         [1.088],
         [1.605],
         [1.783]],

        [[2.171],
         [1.497],
         [1.532],
         [1.551],
         [1.506],
         [1.321],
         [1.213],
         [1.362],
         [1.439],
         [1.149]],

        [[1.256],
         [1.647],
         [1.389],
         [1.540],
         [1.300],
         [1.802],
         [1.282],
         [1.138],
         [1.576],
         [2.049]],

        [[1.313],
         [1.234],
         [1.515],
         [0.704],
         [1.747],
         [2.000],
         [1.644],
         [1.298],
         [1.595],
         [1.438]],

        [[1.529],
         [1.703],
         [1.617],
         [1.464],
         [1.268],
         [1.401],
         [0.569],
         [1.328],
         [1.542],
         [1.661]],

        [[1.063],
         [1.841],
         [1.433],
         [1.786],
         [1.312],
         [1.403],
         [1.341],
         [1.659],
         [1.830],
         [1.558]],

        [[1.271],
         [1.370],
         [1.318],
         [1.384],
         [1.434],
         [1.364],
         [1.117],
         [1.849],
         [1.495],
         [1.956]],

        [[1.363],
         [1.207],
         [1.236],
         [1.928],
         [1.667],
         [1.117],
         [2.033],
         [0.811],
         [1.678],
         [0.958]],

        [[1.439],
         [1.856],
         [1.080],
         [1.344],
         [2.099],
         [1.504],
         [1.592],
         [1.743],
         [1.313],
         [1.355]]], device='cuda:0')
b after update for 1 param tensor([[[76.064],
         [67.267],
         [68.350],
         [67.755],
         [70.372],
         [75.793],
         [64.043],
         [67.786],
         [81.572],
         [81.155]],

        [[58.502],
         [59.831],
         [79.466],
         [75.183],
         [75.221],
         [68.917],
         [67.570],
         [62.034],
         [80.750],
         [69.382]],

        [[75.519],
         [77.325],
         [68.041],
         [68.456],
         [64.213],
         [70.505],
         [74.876],
         [70.407],
         [70.703],
         [67.307]],

        [[57.137],
         [81.011],
         [66.946],
         [55.289],
         [77.265],
         [67.993],
         [68.470],
         [86.643],
         [58.238],
         [85.517]],

        [[72.807],
         [79.458],
         [67.020],
         [57.731],
         [83.300],
         [70.716],
         [77.033],
         [61.252],
         [74.401],
         [78.422]],

        [[86.528],
         [71.855],
         [72.691],
         [73.145],
         [72.078],
         [67.499],
         [64.684],
         [68.537],
         [70.452],
         [62.959]],

        [[65.825],
         [75.358],
         [69.221],
         [72.867],
         [66.966],
         [78.826],
         [66.491],
         [62.635],
         [73.726],
         [84.067]],

        [[67.295],
         [65.242],
         [72.290],
         [49.291],
         [77.618],
         [83.051],
         [75.288],
         [66.917],
         [74.161],
         [70.430]],

        [[72.619],
         [76.628],
         [74.670],
         [71.059],
         [66.128],
         [69.508],
         [44.306],
         [67.665],
         [72.920],
         [75.696]],

        [[60.540],
         [79.684],
         [70.302],
         [78.484],
         [67.274],
         [69.564],
         [68.015],
         [75.631],
         [79.436],
         [73.307]],

        [[66.207],
         [68.733],
         [67.433],
         [69.082],
         [70.325],
         [68.599],
         [62.068],
         [79.866],
         [71.810],
         [82.135]],

        [[68.572],
         [64.519],
         [65.289],
         [81.535],
         [75.832],
         [62.060],
         [83.737],
         [52.897],
         [76.082],
         [57.475]],

        [[70.451],
         [80.016],
         [61.037],
         [68.076],
         [85.082],
         [72.029],
         [74.105],
         [77.530],
         [67.305],
         [68.357]]], device='cuda:0')
clipping threshold 0.9522550009710194
a after update for 1 param tensor([[ 0.016],
        [ 0.019],
        [-0.021],
        [ 0.051],
        [-0.013],
        [-0.026],
        [-0.018],
        [-0.025],
        [-0.003],
        [ 0.040],
        [-0.003],
        [ 0.024],
        [ 0.008]], device='cuda:0')
s after update for 1 param tensor([[1.685],
        [1.215],
        [1.718],
        [1.612],
        [1.535],
        [1.093],
        [1.790],
        [1.266],
        [1.858],
        [1.611],
        [2.018],
        [1.644],
        [1.210]], device='cuda:0')
b after update for 1 param tensor([[76.223],
        [64.744],
        [76.973],
        [74.559],
        [72.771],
        [61.391],
        [78.565],
        [66.085],
        [80.060],
        [74.548],
        [83.417],
        [75.300],
        [64.601]], device='cuda:0')
clipping threshold 0.9522550009710194
cuda
Objective function 15.88 = squared loss an data 13.92 + 0.5*rho*h**2 0.155050 + alpha*h 0.597637 + L2reg 1.11 + L1reg 0.09 ; SHD = 24 ; DAG True
Proportion of microbatches that were clipped  0.7692589875275129
iteration 1 in inner loop, alpha 339.380667855429 rho 100000.0 h 0.0017609644095060162
iteration 5 in outer loop, alpha = 2100.345077361445, rho = 1000000.0, h = 0.0017609644095060162
Threshold 0.3
[[0.004 0.327 0.738 1.749 0.633 0.206 0.551 0.071 0.068 0.673 0.614 0.18
  0.065]
 [0.013 0.001 0.103 0.042 0.038 0.028 0.219 0.005 0.004 0.489 0.049 0.049
  0.004]
 [0.002 0.048 0.004 0.003 0.039 0.028 0.388 0.006 0.006 0.248 0.004 0.009
  0.004]
 [0.002 0.086 1.468 0.004 0.075 0.172 0.357 0.009 0.039 0.373 0.043 0.016
  0.02 ]
 [0.005 0.088 0.134 0.066 0.005 0.039 0.168 0.003 0.025 0.134 0.009 0.004
  0.008]
 [0.011 0.17  0.103 0.029 0.213 0.004 0.308 0.001 0.035 1.028 0.071 0.031
  0.017]
 [0.004 0.022 0.014 0.009 0.031 0.015 0.004 0.002 0.004 0.017 0.015 0.013
  0.005]
 [0.06  0.524 0.544 0.392 1.211 2.854 0.979 0.004 0.575 0.79  0.309 0.155
  0.159]
 [0.063 1.093 0.704 0.136 0.262 0.15  0.798 0.005 0.003 0.531 0.118 0.066
  0.048]
 [0.002 0.006 0.021 0.011 0.024 0.005 0.297 0.001 0.006 0.01  0.009 0.009
  0.002]
 [0.004 0.109 0.871 0.124 0.329 0.056 0.331 0.015 0.058 0.44  0.003 0.031
  0.019]
 [0.027 0.116 0.4   0.259 0.723 0.151 0.35  0.025 0.045 0.359 0.115 0.005
  0.044]
 [0.084 0.737 1.094 0.193 0.533 0.182 0.373 0.025 0.083 1.319 0.283 0.13
  0.004]]
[[0.    0.327 0.738 1.749 0.633 0.    0.551 0.    0.    0.673 0.614 0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.489 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.388 0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    1.468 0.    0.    0.    0.357 0.    0.    0.373 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.308 0.    0.    1.028 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.524 0.544 0.392 1.211 2.854 0.979 0.    0.575 0.79  0.309 0.
  0.   ]
 [0.    1.093 0.704 0.    0.    0.    0.798 0.    0.    0.531 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.871 0.    0.329 0.    0.331 0.    0.    0.44  0.    0.
  0.   ]
 [0.    0.    0.4   0.    0.723 0.    0.35  0.    0.    0.359 0.    0.
  0.   ]
 [0.    0.737 1.094 0.    0.533 0.    0.373 0.    0.    1.319 0.    0.
  0.   ]]
{'fdr': 0.525, 'tpr': 0.7307692307692307, 'fpr': 0.40384615384615385, 'f1': 0.5757575757575758, 'shd': 24, 'npred': 40, 'ntrue': 26}
[3.269e-01 7.382e-01 1.749e+00 6.332e-01 2.063e-01 5.506e-01 7.126e-02
 6.832e-02 6.733e-01 6.141e-01 1.795e-01 6.498e-02 1.269e-02 1.026e-01
 4.230e-02 3.829e-02 2.850e-02 2.191e-01 5.117e-03 4.350e-03 4.888e-01
 4.858e-02 4.935e-02 3.562e-03 1.728e-03 4.849e-02 3.197e-03 3.902e-02
 2.797e-02 3.881e-01 5.578e-03 5.685e-03 2.485e-01 4.046e-03 9.486e-03
 3.598e-03 2.192e-03 8.565e-02 1.468e+00 7.516e-02 1.723e-01 3.566e-01
 8.957e-03 3.923e-02 3.735e-01 4.278e-02 1.595e-02 2.028e-02 4.564e-03
 8.788e-02 1.341e-01 6.557e-02 3.885e-02 1.676e-01 3.184e-03 2.533e-02
 1.343e-01 9.069e-03 4.309e-03 7.908e-03 1.146e-02 1.700e-01 1.032e-01
 2.854e-02 2.132e-01 3.077e-01 5.974e-04 3.480e-02 1.028e+00 7.067e-02
 3.139e-02 1.704e-02 4.241e-03 2.249e-02 1.401e-02 8.715e-03 3.110e-02
 1.505e-02 2.416e-03 3.675e-03 1.701e-02 1.512e-02 1.279e-02 4.884e-03
 6.000e-02 5.245e-01 5.436e-01 3.919e-01 1.211e+00 2.854e+00 9.794e-01
 5.745e-01 7.896e-01 3.088e-01 1.552e-01 1.589e-01 6.281e-02 1.093e+00
 7.038e-01 1.357e-01 2.617e-01 1.497e-01 7.983e-01 5.272e-03 5.306e-01
 1.176e-01 6.629e-02 4.817e-02 2.270e-03 6.420e-03 2.052e-02 1.053e-02
 2.376e-02 5.498e-03 2.971e-01 1.442e-03 5.706e-03 8.610e-03 9.320e-03
 1.820e-03 3.627e-03 1.094e-01 8.712e-01 1.243e-01 3.291e-01 5.604e-02
 3.312e-01 1.523e-02 5.762e-02 4.397e-01 3.124e-02 1.925e-02 2.719e-02
 1.159e-01 3.999e-01 2.595e-01 7.233e-01 1.509e-01 3.503e-01 2.499e-02
 4.525e-02 3.594e-01 1.154e-01 4.411e-02 8.409e-02 7.369e-01 1.094e+00
 1.929e-01 5.330e-01 1.819e-01 3.728e-01 2.534e-02 8.344e-02 1.319e+00
 2.832e-01 1.302e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.8218934911242602, 0.6740294408675905)
Iterations 1110
Achieves (23.82233853610087, 1e-05)-DP
