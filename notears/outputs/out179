samples  5000  graph  20 60 ER mim  minibatch size  100  noise  1.0  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.09213537717594988
iteration 3 in outer loop, alpha = 15.699588471753614, rho = 100.0, h = 0.09213537717594988
cuda
iteration 1 in inner loop,alpha 15.699588471753614 rho 100.0 h 0.05367211647120129
iteration 2 in inner loop,alpha 15.699588471753614 rho 1000.0 h 0.01978300963797608
iteration 4 in outer loop, alpha = 35.48259810972969, rho = 1000.0, h = 0.01978300963797608
cuda
iteration 1 in inner loop,alpha 35.48259810972969 rho 1000.0 h 0.01090959889101839
iteration 2 in inner loop,alpha 35.48259810972969 rho 10000.0 h 0.0033893587627424893
iteration 5 in outer loop, alpha = 69.37618573715459, rho = 10000.0, h = 0.0033893587627424893
cuda
iteration 1 in inner loop,alpha 69.37618573715459 rho 10000.0 h 0.0014690778404897742
iteration 2 in inner loop,alpha 69.37618573715459 rho 100000.0 h 0.0005756136893566577
iteration 6 in outer loop, alpha = 126.93755467282035, rho = 100000.0, h = 0.0005756136893566577
cuda
iteration 1 in inner loop,alpha 126.93755467282035 rho 100000.0 h 0.00035750960914171515
iteration 7 in outer loop, alpha = 484.4471638145355, rho = 1000000.0, h = 0.00035750960914171515
Threshold 0.3
[[0.    0.025 0.009 0.053 0.156 0.183 0.051 0.002 0.041 0.057 0.479 0.632
  1.079 0.003 0.094 0.004 0.074 2.571 0.516 0.019]
 [0.    0.001 0.001 0.038 0.079 0.461 0.172 0.    0.007 0.049 0.043 0.
  0.    0.    0.071 0.142 0.02  0.    0.086 0.801]
 [0.002 0.288 0.002 0.124 0.539 0.101 0.079 0.001 1.469 0.822 0.131 0.006
  0.006 0.    0.034 0.217 0.052 0.039 0.136 0.114]
 [0.    0.006 0.    0.002 0.001 0.005 0.001 0.    0.    0.001 0.002 0.
  0.    0.    0.05  0.003 0.    0.    0.002 0.024]
 [0.    0.005 0.    0.167 0.002 0.009 0.004 0.    0.    0.013 0.009 0.
  0.    0.    0.037 0.107 0.004 0.    0.001 0.004]
 [0.    0.002 0.005 0.103 0.029 0.005 0.009 0.    0.02  0.031 0.003 0.
  0.003 0.    1.19  0.082 0.006 0.    0.298 0.041]
 [0.    0.003 0.    0.219 0.296 0.237 0.003 0.    0.002 0.005 0.003 0.001
  0.002 0.    0.092 0.017 0.001 0.    0.004 0.013]
 [0.003 0.099 0.003 1.172 0.045 0.058 0.186 0.    0.367 0.067 0.078 2.159
  1.36  0.036 0.109 0.014 0.033 2.226 0.047 0.039]
 [0.    0.025 0.    1.436 1.382 0.016 0.09  0.    0.003 0.408 1.237 0.
  0.001 0.    0.421 0.144 0.005 0.    0.006 0.017]
 [0.    0.003 0.001 0.05  0.013 0.168 0.117 0.    0.004 0.004 1.068 0.001
  0.002 0.    0.095 0.007 0.039 0.    0.006 0.668]
 [0.    0.007 0.    0.037 0.011 0.02  0.334 0.    0.    0.001 0.002 0.
  0.    0.    0.016 0.004 0.326 0.    0.003 0.739]
 [0.    2.091 0.005 1.58  1.3   0.057 0.577 0.    1.037 0.033 1.095 0.001
  0.001 0.003 0.003 0.911 0.379 0.    0.067 0.122]
 [0.    0.034 0.004 0.18  0.079 0.119 0.451 0.    0.005 0.575 0.056 0.576
  0.003 0.    1.592 0.153 0.235 0.    0.005 0.061]
 [0.004 1.038 4.616 0.095 0.039 0.17  0.09  0.002 0.842 0.188 0.049 0.022
  0.052 0.    0.056 0.672 0.004 0.087 0.709 1.197]
 [0.    0.001 0.    0.014 0.021 0.001 0.003 0.    0.002 0.005 0.002 0.
  0.    0.    0.002 0.011 0.001 0.    0.001 0.003]
 [0.    0.001 0.002 0.078 0.006 0.015 0.006 0.    0.004 0.299 0.035 0.
  0.    0.    0.038 0.003 0.006 0.    0.001 0.09 ]
 [0.    0.01  0.001 0.995 0.104 0.004 0.923 0.    0.001 0.002 0.002 0.001
  0.002 0.    0.551 0.003 0.002 0.    0.003 0.021]
 [0.    0.052 0.004 0.288 0.051 0.05  0.062 0.    0.105 0.057 0.107 0.875
  2.056 0.004 0.003 0.072 0.048 0.002 0.025 1.252]
 [0.    0.004 0.01  0.096 0.769 0.016 0.01  0.002 0.416 0.23  0.106 0.001
  0.006 0.    0.692 1.043 0.003 0.008 0.004 0.064]
 [0.    0.    0.    0.012 0.009 0.022 0.068 0.    0.001 0.001 0.001 0.
  0.    0.    0.056 0.002 0.003 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.479 0.632
  1.079 0.    0.    0.    0.    2.571 0.516 0.   ]
 [0.    0.    0.    0.    0.    0.461 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.801]
 [0.    0.    0.    0.    0.539 0.    0.    0.    1.469 0.822 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.19  0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.172 0.    0.    0.    0.    0.367 0.    0.    2.159
  1.36  0.    0.    0.    0.    2.226 0.    0.   ]
 [0.    0.    0.    1.436 1.382 0.    0.    0.    0.    0.408 1.237 0.
  0.    0.    0.421 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.068 0.
  0.    0.    0.    0.    0.    0.    0.    0.668]
 [0.    0.    0.    0.    0.    0.    0.334 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.326 0.    0.    0.739]
 [0.    2.091 0.    1.58  1.3   0.    0.577 0.    1.037 0.    1.095 0.
  0.    0.    0.    0.911 0.379 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.451 0.    0.    0.575 0.    0.576
  0.    0.    1.592 0.    0.    0.    0.    0.   ]
 [0.    1.038 4.616 0.    0.    0.    0.    0.    0.842 0.    0.    0.
  0.    0.    0.    0.672 0.    0.    0.709 1.197]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.995 0.    0.    0.923 0.    0.    0.    0.    0.
  0.    0.    0.551 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.875
  2.056 0.    0.    0.    0.    0.    0.    1.252]
 [0.    0.    0.    0.    0.769 0.    0.    0.    0.416 0.    0.    0.
  0.    0.    0.692 1.043 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.1111111111111111, 'tpr': 0.8, 'fpr': 0.046153846153846156, 'f1': 0.8421052631578948, 'shd': 15, 'npred': 54, 'ntrue': 60}
[2.473e-02 8.744e-03 5.335e-02 1.558e-01 1.833e-01 5.147e-02 2.248e-03
 4.052e-02 5.687e-02 4.786e-01 6.321e-01 1.079e+00 3.201e-03 9.395e-02
 4.130e-03 7.374e-02 2.571e+00 5.164e-01 1.885e-02 1.788e-06 1.072e-03
 3.784e-02 7.915e-02 4.610e-01 1.723e-01 5.583e-06 7.481e-03 4.933e-02
 4.335e-02 8.972e-05 1.280e-04 3.444e-05 7.136e-02 1.424e-01 1.974e-02
 1.041e-04 8.600e-02 8.008e-01 1.749e-03 2.885e-01 1.241e-01 5.387e-01
 1.013e-01 7.943e-02 5.248e-04 1.469e+00 8.223e-01 1.313e-01 5.791e-03
 6.260e-03 3.010e-05 3.409e-02 2.165e-01 5.185e-02 3.888e-02 1.360e-01
 1.142e-01 7.386e-06 6.429e-03 3.397e-05 1.368e-03 4.500e-03 1.347e-03
 2.524e-06 3.877e-04 6.996e-04 2.072e-03 1.107e-04 2.367e-04 6.214e-06
 4.961e-02 2.879e-03 3.727e-04 3.123e-05 2.032e-03 2.394e-02 9.397e-05
 5.287e-03 1.630e-04 1.673e-01 8.836e-03 3.963e-03 4.696e-06 4.961e-04
 1.331e-02 9.226e-03 1.137e-04 2.474e-04 5.917e-06 3.650e-02 1.071e-01
 3.536e-03 8.809e-05 6.770e-04 4.239e-03 8.440e-05 1.797e-03 5.476e-03
 1.027e-01 2.939e-02 8.763e-03 2.719e-05 1.976e-02 3.070e-02 2.958e-03
 3.462e-04 2.965e-03 2.215e-04 1.190e+00 8.212e-02 6.465e-03 4.223e-04
 2.979e-01 4.142e-02 1.373e-05 2.723e-03 3.374e-04 2.185e-01 2.961e-01
 2.369e-01 1.592e-05 1.558e-03 4.921e-03 3.022e-03 7.439e-04 2.363e-03
 6.438e-05 9.195e-02 1.677e-02 1.133e-03 5.753e-05 3.653e-03 1.324e-02
 3.149e-03 9.937e-02 3.352e-03 1.172e+00 4.476e-02 5.850e-02 1.864e-01
 3.666e-01 6.685e-02 7.848e-02 2.159e+00 1.360e+00 3.608e-02 1.094e-01
 1.362e-02 3.321e-02 2.226e+00 4.695e-02 3.947e-02 1.924e-05 2.504e-02
 2.253e-04 1.436e+00 1.382e+00 1.630e-02 8.952e-02 2.722e-05 4.084e-01
 1.237e+00 2.504e-04 5.919e-04 2.315e-05 4.209e-01 1.442e-01 5.017e-03
 1.387e-04 6.010e-03 1.665e-02 2.270e-05 3.188e-03 6.238e-04 5.030e-02
 1.330e-02 1.677e-01 1.173e-01 1.742e-05 3.520e-03 1.068e+00 1.051e-03
 1.853e-03 7.056e-06 9.493e-02 7.058e-03 3.920e-02 1.464e-04 5.820e-03
 6.676e-01 1.240e-05 7.369e-03 1.743e-04 3.737e-02 1.059e-02 2.044e-02
 3.342e-01 2.558e-06 4.342e-04 9.021e-04 1.344e-04 9.555e-05 6.346e-06
 1.645e-02 4.479e-03 3.262e-01 1.044e-04 2.884e-03 7.387e-01 4.749e-06
 2.091e+00 5.032e-03 1.580e+00 1.300e+00 5.702e-02 5.765e-01 8.234e-06
 1.037e+00 3.350e-02 1.095e+00 1.143e-03 3.119e-03 3.017e-03 9.111e-01
 3.790e-01 1.627e-04 6.729e-02 1.222e-01 2.781e-06 3.375e-02 4.056e-03
 1.804e-01 7.928e-02 1.187e-01 4.509e-01 4.892e-06 4.913e-03 5.751e-01
 5.629e-02 5.758e-01 4.272e-04 1.592e+00 1.526e-01 2.347e-01 7.653e-05
 5.335e-03 6.053e-02 3.960e-03 1.038e+00 4.616e+00 9.548e-02 3.868e-02
 1.701e-01 8.997e-02 2.194e-03 8.419e-01 1.884e-01 4.863e-02 2.151e-02
 5.218e-02 5.631e-02 6.724e-01 3.854e-03 8.670e-02 7.088e-01 1.197e+00
 2.647e-06 7.907e-04 2.654e-04 1.375e-02 2.145e-02 8.300e-04 2.626e-03
 1.895e-06 1.599e-03 5.354e-03 1.595e-03 2.788e-04 3.893e-04 1.903e-05
 1.112e-02 8.807e-04 3.346e-05 1.285e-03 2.779e-03 1.459e-05 8.637e-04
 1.979e-03 7.766e-02 5.588e-03 1.524e-02 5.608e-03 8.803e-05 4.066e-03
 2.988e-01 3.544e-02 4.194e-04 4.137e-04 2.705e-05 3.789e-02 5.862e-03
 1.396e-04 1.459e-03 8.960e-02 6.699e-05 1.038e-02 1.202e-03 9.946e-01
 1.044e-01 3.560e-03 9.227e-01 2.092e-05 6.371e-04 2.385e-03 2.312e-03
 8.896e-04 2.433e-03 2.261e-05 5.510e-01 2.926e-03 9.034e-05 2.734e-03
 2.089e-02 4.523e-06 5.162e-02 3.965e-03 2.883e-01 5.088e-02 4.956e-02
 6.179e-02 1.604e-05 1.045e-01 5.746e-02 1.073e-01 8.755e-01 2.056e+00
 4.111e-03 3.044e-03 7.244e-02 4.837e-02 2.546e-02 1.252e+00 9.568e-05
 4.115e-03 9.869e-03 9.613e-02 7.694e-01 1.572e-02 1.042e-02 1.753e-03
 4.158e-01 2.301e-01 1.062e-01 6.765e-04 5.660e-03 1.317e-04 6.915e-01
 1.043e+00 3.031e-03 8.023e-03 6.351e-02 3.205e-06 3.316e-04 1.046e-04
 1.166e-02 8.689e-03 2.217e-02 6.833e-02 3.390e-06 6.366e-04 5.993e-04
 6.773e-04 4.552e-05 4.777e-04 1.218e-05 5.641e-02 1.575e-03 3.095e-03
 3.211e-04 2.053e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9316666666666666, 0.864389065798652)
cuda
4420
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
||w||^2 0.03476063198677594
exp ma of ||w||^2 53.207742677100526
||w|| 0.1864420338517469
exp ma of ||w|| 0.1691273036089788
||w||^2 0.025164230215553744
exp ma of ||w||^2 0.03363897298290356
||w|| 0.1586323744244968
exp ma of ||w|| 0.17795487219858883
||w||^2 0.058216951237482994
exp ma of ||w||^2 0.03592967902435918
||w|| 0.24128189164850933
exp ma of ||w|| 0.18528215183653834
||w||^2 0.044169705855967
exp ma of ||w||^2 0.05081662902690257
||w|| 0.2101659007926048
exp ma of ||w|| 0.22176560434563625
||w||^2 0.03717775299227953
exp ma of ||w||^2 0.056479063514690786
||w|| 0.19281533391377234
exp ma of ||w|| 0.2318110764695799
cuda
Objective function 22.90 = squared loss an data 21.93 + 0.5*rho*h**2 0.586628 + alpha*h 0.000000 + L2reg 0.16 + L1reg 0.23 ; SHD = 78 ; DAG False
Proportion of microbatches that were clipped  0.7095798854232973
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.0831690874969055
iteration 1 in outer loop, alpha = 1.0831690874969055, rho = 1.0, h = 1.0831690874969055
cuda
4420
cuda
Objective function 24.07 = squared loss an data 21.93 + 0.5*rho*h**2 0.586628 + alpha*h 1.173255 + L2reg 0.16 + L1reg 0.23 ; SHD = 78 ; DAG False
||w||^2 9.518489597037673
exp ma of ||w||^2 1775.7581286480968
||w|| 3.0852049521932368
exp ma of ||w|| 9.811555553382805
||w||^2 0.6710979568062512
exp ma of ||w||^2 281.1933424944637
||w|| 0.8192056865075164
exp ma of ||w|| 3.290315523126322
||w||^2 0.02964067752207461
exp ma of ||w||^2 0.03464257428910778
||w|| 0.17216468140148436
exp ma of ||w|| 0.18346647888432632
||w||^2 0.05312862985902359
exp ma of ||w||^2 0.04200937416364266
||w|| 0.230496485567619
exp ma of ||w|| 0.20127428814174275
||w||^2 0.02431921588178031
exp ma of ||w||^2 0.044578953381300926
||w|| 0.15594619547068247
exp ma of ||w|| 0.2075388557012487
cuda
Objective function 22.81 = squared loss an data 20.95 + 0.5*rho*h**2 0.427243 + alpha*h 1.001265 + L2reg 0.19 + L1reg 0.23 ; SHD = 66 ; DAG False
Proportion of microbatches that were clipped  0.7019137204022057
iteration 1 in inner loop, alpha 1.0831690874969055 rho 1.0 h 0.9243843437136867
4420
cuda
Objective function 26.65 = squared loss an data 20.95 + 0.5*rho*h**2 4.272432 + alpha*h 1.001265 + L2reg 0.19 + L1reg 0.23 ; SHD = 66 ; DAG False
||w||^2 9160.540926985619
exp ma of ||w||^2 169350.83342040124
||w|| 95.7107147971721
exp ma of ||w|| 194.1188989314069
||w||^2 0.020228665535262463
exp ma of ||w||^2 0.03159763231850565
||w|| 0.14222751328509706
exp ma of ||w|| 0.17149407377644485
||w||^2 0.06957044989978876
exp ma of ||w||^2 0.039049421179619064
||w|| 0.26376210853681914
exp ma of ||w|| 0.19409461029925412
||w||^2 0.0272719641988753
exp ma of ||w||^2 0.0474958873756082
||w|| 0.16514225443197542
exp ma of ||w|| 0.21326830131108176
||w||^2 0.05734554385508311
exp ma of ||w||^2 0.05871107093319432
||w|| 0.2394692962679832
exp ma of ||w|| 0.23685799796513884
cuda
Objective function 22.19 = squared loss an data 20.88 + 0.5*rho*h**2 0.565733 + alpha*h 0.364349 + L2reg 0.19 + L1reg 0.19 ; SHD = 57 ; DAG True
Proportion of microbatches that were clipped  0.7132087325270928
iteration 2 in inner loop, alpha 1.0831690874969055 rho 10.0 h 0.33637272021429254
4420
cuda
Objective function 27.28 = squared loss an data 20.88 + 0.5*rho*h**2 5.657330 + alpha*h 0.364349 + L2reg 0.19 + L1reg 0.19 ; SHD = 57 ; DAG True
||w||^2 75327590.04460834
exp ma of ||w||^2 266357584.60928872
||w|| 8679.146850042827
exp ma of ||w|| 13692.877494534747
||w||^2 0.032364345838430814
exp ma of ||w||^2 0.038038526709060036
||w|| 0.17990093340066587
exp ma of ||w|| 0.19190415513058764
||w||^2 0.04683996901809924
exp ma of ||w||^2 0.040442698153840786
||w|| 0.2164254352383269
exp ma of ||w|| 0.19803083457341256
||w||^2 0.05712249387248802
exp ma of ||w||^2 0.06212120230371238
||w|| 0.23900312523581782
exp ma of ||w|| 0.2432269621761202
cuda
Objective function 21.99 = squared loss an data 21.10 + 0.5*rho*h**2 0.450078 + alpha*h 0.102767 + L2reg 0.20 + L1reg 0.15 ; SHD = 58 ; DAG True
Proportion of microbatches that were clipped  0.7159818778315888
iteration 3 in inner loop, alpha 1.0831690874969055 rho 100.0 h 0.09487657099272795
iteration 2 in outer loop, alpha = 10.5708261867697, rho = 100.0, h = 0.09487657099272795
cuda
4420
cuda
Objective function 22.89 = squared loss an data 21.10 + 0.5*rho*h**2 0.450078 + alpha*h 1.002924 + L2reg 0.20 + L1reg 0.15 ; SHD = 58 ; DAG True
||w||^2 308899425.7284075
exp ma of ||w||^2 410711659.5482363
||w|| 17575.534863224148
exp ma of ||w|| 17734.542904561164
||w||^2 201785243.6424423
exp ma of ||w||^2 392489289.0868649
||w|| 14205.113292136824
exp ma of ||w|| 17364.847280443766
||w||^2 1439270.1800963164
exp ma of ||w||^2 21646457.625372626
||w|| 1199.695869833816
exp ma of ||w|| 3415.567274682368
||w||^2 8.001742825288966
exp ma of ||w||^2 1127.2285602858587
||w|| 2.8287351988634364
exp ma of ||w|| 6.747125723579767
||w||^2 0.5110376766908289
exp ma of ||w||^2 90.47065909861273
||w|| 0.7148689926768602
exp ma of ||w|| 1.5590923788930395
||w||^2 0.10574730107279942
exp ma of ||w||^2 0.20254099500966388
||w|| 0.32518810106275325
exp ma of ||w|| 0.23997615353309093
||w||^2 0.046057595448898994
exp ma of ||w||^2 0.07001916255297523
||w|| 0.21461033397508844
exp ma of ||w|| 0.21836273202887524
||w||^2 0.023847182218008953
exp ma of ||w||^2 0.05174663956685361
||w|| 0.15442532893929334
exp ma of ||w|| 0.2222024601821655
||w||^2 0.06047946353929388
exp ma of ||w||^2 0.06304928430270858
||w|| 0.24592572768885707
exp ma of ||w|| 0.24464553334437977
||w||^2 0.07480031689311946
exp ma of ||w||^2 0.06408552831241987
||w|| 0.2734964659609324
exp ma of ||w|| 0.24679012744491058
cuda
Objective function 22.42 = squared loss an data 21.22 + 0.5*rho*h**2 0.193234 + alpha*h 0.657152 + L2reg 0.22 + L1reg 0.14 ; SHD = 56 ; DAG True
Proportion of microbatches that were clipped  0.7057287416353843
iteration 1 in inner loop, alpha 10.5708261867697 rho 100.0 h 0.06216657602618625
4420
cuda
Objective function 24.16 = squared loss an data 21.22 + 0.5*rho*h**2 1.932342 + alpha*h 0.657152 + L2reg 0.22 + L1reg 0.14 ; SHD = 56 ; DAG True
||w||^2 6694496.3662096765
exp ma of ||w||^2 49284376.6167042
||w|| 2587.372483081954
exp ma of ||w|| 5279.42724785998
||w||^2 2.8611822517944683
exp ma of ||w||^2 1249.6729686183592
||w|| 1.691502956484105
exp ma of ||w|| 7.297634018602122
||w||^2 0.06818977249047839
exp ma of ||w||^2 0.2232675320973705
||w|| 0.26113171483080794
exp ma of ||w|| 0.295325082204283
cuda
Objective function 22.09 = squared loss an data 21.24 + 0.5*rho*h**2 0.259390 + alpha*h 0.240769 + L2reg 0.23 + L1reg 0.12 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.713534194165471
iteration 2 in inner loop, alpha 10.5708261867697 rho 1000.0 h 0.022776757619141108
iteration 3 in outer loop, alpha = 33.34758380591081, rho = 1000.0, h = 0.022776757619141108
cuda
4420
cuda
Objective function 22.61 = squared loss an data 21.24 + 0.5*rho*h**2 0.259390 + alpha*h 0.759550 + L2reg 0.23 + L1reg 0.12 ; SHD = 54 ; DAG True
||w||^2 80.6549359520609
exp ma of ||w||^2 14791.268108609302
||w|| 8.980809314981634
exp ma of ||w|| 31.26037590900712
||w||^2 0.042891103293710695
exp ma of ||w||^2 0.04456083546537241
||w|| 0.2071016738071199
exp ma of ||w|| 0.20700916638836603
||w||^2 0.11611906030612364
exp ma of ||w||^2 0.046889144677909965
||w|| 0.3407624690398337
exp ma of ||w|| 0.211425337692178
||w||^2 0.03429376544360385
exp ma of ||w||^2 0.0524117450374944
||w|| 0.1851857592894331
exp ma of ||w|| 0.22346546337495246
||w||^2 0.04094860420090211
exp ma of ||w||^2 0.0549675665444632
||w|| 0.20235761463533342
exp ma of ||w|| 0.22793606619209378
cuda
Objective function 22.15 = squared loss an data 21.14 + 0.5*rho*h**2 0.122455 + alpha*h 0.521876 + L2reg 0.25 + L1reg 0.12 ; SHD = 55 ; DAG True
Proportion of microbatches that were clipped  0.7095107379299209
iteration 1 in inner loop, alpha 33.34758380591081 rho 1000.0 h 0.015649591264445206
4420
cuda
Objective function 23.25 = squared loss an data 21.14 + 0.5*rho*h**2 1.224549 + alpha*h 0.521876 + L2reg 0.25 + L1reg 0.12 ; SHD = 55 ; DAG True
||w||^2 25626848303.687115
exp ma of ||w||^2 4391773720.347574
||w|| 160083.87896252112
exp ma of ||w|| 33067.856581246124
||w||^2 481103271.0411134
exp ma of ||w||^2 2678237469.8020635
||w|| 21934.066450184593
exp ma of ||w|| 38049.75655431456
||w||^2 0.03172820909677927
exp ma of ||w||^2 0.06059501011690382
||w|| 0.17812413956782858
exp ma of ||w|| 0.24319230189510221
||w||^2 0.05166679350589114
exp ma of ||w||^2 0.05430273305129567
||w|| 0.22730330729202147
exp ma of ||w|| 0.22934208006257373
||w||^2 0.048355999462573386
exp ma of ||w||^2 0.051003430811604425
||w|| 0.21989997604041112
exp ma of ||w|| 0.22112947490412763
cuda
Objective function 21.82 = squared loss an data 21.01 + 0.5*rho*h**2 0.207682 + alpha*h 0.214921 + L2reg 0.28 + L1reg 0.11 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.7102372233720745
iteration 2 in inner loop, alpha 33.34758380591081 rho 10000.0 h 0.006444880082526083
4420
cuda
Objective function 23.69 = squared loss an data 21.01 + 0.5*rho*h**2 2.076824 + alpha*h 0.214921 + L2reg 0.28 + L1reg 0.11 ; SHD = 54 ; DAG True
||w||^2 51891850489.88446
exp ma of ||w||^2 8564319978.721874
||w|| 227797.82810616185
exp ma of ||w|| 37636.03893351881
||w||^2 39730154.840216815
exp ma of ||w||^2 9610234928.562792
||w|| 6303.186086434131
exp ma of ||w|| 18355.564989099847
v before min max tensor([[ 8.258e-03, -2.526e-05, -3.147e-02,  ..., -1.089e-02,  1.207e-01,
         -2.921e-03],
        [ 6.402e-01, -4.667e-03, -5.904e-03,  ...,  1.669e-04, -2.724e-04,
          2.886e-02],
        [ 1.399e+00, -3.132e-02, -2.615e-03,  ...,  1.000e-02,  8.040e-04,
         -2.950e-04],
        ...,
        [ 2.339e-03, -4.042e-03, -5.639e-04,  ...,  3.267e-01, -2.528e-02,
         -2.679e-01],
        [-1.390e-03, -5.148e-03, -2.419e-02,  ..., -7.985e-04, -8.166e-03,
          1.083e-01],
        [ 1.361e-02, -9.301e-03, -2.814e-02,  ..., -1.544e-01, -1.053e-01,
          1.087e+00]], device='cuda:0')
v tensor([[8.258e-03, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.207e-01,
         1.000e-12],
        [6.402e-01, 1.000e-12, 1.000e-12,  ..., 1.669e-04, 1.000e-12,
         2.886e-02],
        [1.399e+00, 1.000e-12, 1.000e-12,  ..., 1.000e-02, 8.040e-04,
         1.000e-12],
        ...,
        [2.339e-03, 1.000e-12, 1.000e-12,  ..., 3.267e-01, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.083e-01],
        [1.361e-02, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.087e+00]], device='cuda:0')
v before min max tensor([-1.143e-05,  1.522e-05, -1.237e-03, -1.244e-02, -8.000e-04, -4.447e-05,
        -1.319e-03,  1.046e-03,  9.998e-03,  1.828e-01, -1.385e-04, -3.512e-05,
         1.792e-03, -1.310e-02, -3.757e-03, -1.533e-03, -1.311e-03, -4.422e-03,
         1.618e-03, -1.054e-05, -1.268e-01, -4.059e-03,  2.991e-02,  1.499e-03,
         4.399e-05, -8.456e-04, -1.860e-02, -2.955e-03, -1.561e-02, -1.854e-03,
        -1.607e-02,  1.411e-03, -4.178e-06, -7.295e-06,  6.447e-04, -5.046e-04,
        -6.103e-06, -8.203e-05,  2.182e-05, -1.065e-03,  3.738e-04, -2.148e-01,
        -2.795e-03, -1.165e-05, -7.275e-03,  9.856e-03, -6.334e-05,  2.217e-04,
        -2.532e-03, -2.776e-03, -3.763e-03, -1.656e-02, -4.423e-05,  1.345e-03,
        -8.467e-04, -7.970e-04,  9.420e-03, -5.301e-06, -3.657e-03, -4.451e-02,
         7.362e-03, -1.172e-02,  2.706e-02, -3.328e-02, -1.931e-04, -8.889e-03,
        -7.594e-03,  1.216e-02,  2.567e-03,  5.411e-02, -2.522e-05, -6.214e-04,
         8.659e-03,  3.503e-03,  8.574e-02, -4.374e-02, -1.652e-05, -7.765e-04,
        -5.105e-03, -8.422e-04, -1.355e-01,  1.289e-02, -7.998e-05, -1.495e-03,
        -1.570e-03, -1.130e-02, -8.409e-03, -1.062e-05,  1.317e-02,  1.413e-04,
        -6.004e-04,  2.576e-05, -2.227e-03, -1.800e-04, -1.458e-03,  6.269e-02,
         1.976e-02, -8.699e-04, -2.451e-03, -7.371e-03, -4.529e-03, -6.357e-04,
        -2.446e-03,  3.439e-02, -5.870e-05, -6.248e-04, -6.184e-03,  5.851e-03,
        -3.002e-02,  3.075e-06,  4.722e-03,  1.377e-02,  5.602e-05,  2.014e-04,
        -6.250e-03, -6.407e-05, -7.840e-05, -1.627e-04, -2.115e-03,  3.822e-02,
         1.052e-02, -1.235e-02, -2.489e-02, -2.235e-04, -5.140e-05, -4.647e-03,
        -2.097e-02, -5.461e-03, -2.360e-01,  8.357e-03, -4.435e-02, -1.733e-03,
        -2.027e-03,  7.194e-04,  2.372e-03, -6.855e-05, -1.713e-04, -1.628e-02,
        -3.362e-03, -2.034e-03,  8.538e-04, -2.572e-04, -1.324e-02,  1.208e-04,
        -3.434e-02, -1.459e-04, -1.650e-03, -3.069e-02, -7.531e-03, -3.748e-03,
        -1.900e-01, -7.516e-03, -4.432e-03, -1.104e-04, -1.180e-03, -6.073e-04,
         1.441e-01,  2.486e-03, -1.881e-03,  6.933e-02, -5.317e-05,  1.343e-03,
        -2.260e-03, -1.136e-02, -4.771e-03, -1.717e-04, -4.184e-02,  2.632e-02,
        -1.098e-02,  4.135e-06, -4.399e-04,  7.678e-03, -3.727e-02, -3.490e-03,
        -8.795e-03, -5.394e-06, -1.756e-02, -1.334e-02, -7.335e-06, -3.772e-03,
        -2.998e-03,  1.092e-03,  3.504e-03, -3.182e-06, -1.435e-04,  2.564e-04,
        -1.084e-03,  2.247e-01, -3.087e-03, -3.384e-04,  1.516e-01, -1.109e-02,
         5.245e-02,  3.343e-04,  2.722e-02,  2.641e-03, -2.677e-04,  2.256e-02,
        -1.682e-02, -4.079e-02], device='cuda:0')
v tensor([1.000e-12, 1.522e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.046e-03, 9.998e-03, 1.828e-01, 1.000e-12, 1.000e-12,
        1.792e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.618e-03, 1.000e-12, 1.000e-12, 1.000e-12, 2.991e-02, 1.499e-03,
        4.399e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.411e-03, 1.000e-12, 1.000e-12, 6.447e-04, 1.000e-12,
        1.000e-12, 1.000e-12, 2.182e-05, 1.000e-12, 3.738e-04, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 9.856e-03, 1.000e-12, 2.217e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.345e-03,
        1.000e-12, 1.000e-12, 9.420e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        7.362e-03, 1.000e-12, 2.706e-02, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.216e-02, 2.567e-03, 5.411e-02, 1.000e-12, 1.000e-12,
        8.659e-03, 3.503e-03, 8.574e-02, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.289e-02, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.317e-02, 1.413e-04,
        1.000e-12, 2.576e-05, 1.000e-12, 1.000e-12, 1.000e-12, 6.269e-02,
        1.976e-02, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 3.439e-02, 1.000e-12, 1.000e-12, 1.000e-12, 5.851e-03,
        1.000e-12, 3.075e-06, 4.722e-03, 1.377e-02, 5.602e-05, 2.014e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.822e-02,
        1.052e-02, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 8.357e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 7.194e-04, 2.372e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 8.538e-04, 1.000e-12, 1.000e-12, 1.208e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.441e-01, 2.486e-03, 1.000e-12, 6.933e-02, 1.000e-12, 1.343e-03,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.632e-02,
        1.000e-12, 4.135e-06, 1.000e-12, 7.678e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.092e-03, 3.504e-03, 1.000e-12, 1.000e-12, 2.564e-04,
        1.000e-12, 2.247e-01, 1.000e-12, 1.000e-12, 1.516e-01, 1.000e-12,
        5.245e-02, 3.343e-04, 2.722e-02, 2.641e-03, 1.000e-12, 2.256e-02,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 7.228e-03],
         [ 3.556e-02],
         [ 1.886e-04],
         [-6.505e-03],
         [-2.798e-03],
         [ 9.699e-05],
         [ 9.162e-04],
         [ 1.484e-03],
         [-2.508e-02],
         [ 8.946e-04]],

        [[-3.445e-04],
         [-1.106e-03],
         [-9.186e-05],
         [-6.424e-04],
         [-3.348e-03],
         [-4.700e-02],
         [-3.903e-03],
         [-5.809e-04],
         [-2.903e-03],
         [-1.770e-03]],

        [[-5.515e-02],
         [ 1.459e-03],
         [-4.426e-04],
         [-6.284e-04],
         [-5.070e-02],
         [ 4.588e-04],
         [ 2.024e-03],
         [ 5.731e-05],
         [-2.219e-03],
         [-2.595e-03]],

        [[-2.290e-03],
         [-3.120e-05],
         [ 9.905e-02],
         [-4.028e-04],
         [-3.034e-03],
         [-2.359e-02],
         [-3.716e-03],
         [-3.835e-03],
         [-6.122e-03],
         [-1.638e-04]],

        [[-4.235e-03],
         [ 2.085e-01],
         [ 3.492e-02],
         [-1.024e-02],
         [-3.130e-03],
         [-1.560e-02],
         [-9.026e-03],
         [-1.585e-04],
         [ 7.822e-03],
         [-3.945e-03]],

        [[-3.286e-02],
         [ 1.187e-01],
         [ 8.361e-05],
         [-4.698e-04],
         [-2.822e-02],
         [-4.275e-03],
         [-2.509e-04],
         [-1.192e-03],
         [-4.137e-04],
         [ 2.323e-02]],

        [[-9.577e-03],
         [ 1.589e-02],
         [-1.572e-03],
         [-5.354e-04],
         [-1.584e-02],
         [-1.958e-02],
         [-5.509e-04],
         [-2.742e-03],
         [-2.638e-02],
         [-4.462e-03]],

        [[-1.014e-02],
         [-4.278e-02],
         [ 2.430e-02],
         [-5.791e-03],
         [ 2.879e-04],
         [ 2.528e-02],
         [-1.409e-01],
         [-3.261e-04],
         [-4.719e-04],
         [-3.620e-04]],

        [[-1.856e-03],
         [-8.247e-04],
         [-2.081e-03],
         [-1.149e-03],
         [-3.816e-03],
         [-4.719e-04],
         [-3.108e-04],
         [-2.404e-03],
         [-6.363e-03],
         [ 5.021e-03]],

        [[-3.753e-02],
         [-1.169e-02],
         [-1.915e-03],
         [-2.131e-03],
         [-7.446e-03],
         [-2.976e-03],
         [ 5.039e-02],
         [-6.013e-03],
         [-3.442e-03],
         [-1.163e-02]],

        [[ 6.115e-03],
         [-1.920e-05],
         [ 6.557e-03],
         [-2.978e-03],
         [-2.312e-02],
         [-9.558e-02],
         [-2.760e-05],
         [-8.533e-04],
         [ 9.949e-02],
         [-5.495e-03]],

        [[-1.770e-02],
         [-5.426e-03],
         [-6.346e-02],
         [ 1.596e-03],
         [-9.852e-03],
         [-9.473e-02],
         [-1.821e-02],
         [ 1.361e-03],
         [-5.849e-02],
         [-2.913e-03]],

        [[ 6.896e-05],
         [-2.099e-02],
         [-5.043e-06],
         [ 2.369e-01],
         [ 3.252e-04],
         [-1.053e-04],
         [-6.184e-03],
         [ 1.883e-03],
         [ 1.977e-04],
         [-5.067e-03]],

        [[ 5.293e-04],
         [-1.808e-02],
         [ 5.702e-03],
         [-1.128e-02],
         [-2.124e-01],
         [ 3.256e-02],
         [-6.725e-04],
         [-5.798e-04],
         [ 9.633e-07],
         [-4.535e-03]],

        [[-2.076e-02],
         [-3.074e-04],
         [-7.427e-03],
         [-5.103e-03],
         [ 1.110e-03],
         [ 1.748e-04],
         [ 7.697e-05],
         [ 1.065e-02],
         [ 1.094e-03],
         [ 1.923e-02]],

        [[-6.350e-04],
         [-4.015e-03],
         [ 4.082e-04],
         [ 1.068e-02],
         [ 6.228e-04],
         [-8.685e-04],
         [-2.493e-02],
         [-8.035e-06],
         [-7.776e-02],
         [-3.118e-03]],

        [[-1.989e-04],
         [ 1.116e-03],
         [-4.116e-02],
         [-8.559e-03],
         [-1.354e-03],
         [ 7.590e-05],
         [-3.725e-02],
         [ 1.128e-02],
         [ 1.983e-03],
         [ 5.114e-02]],

        [[ 1.687e-03],
         [ 4.414e-02],
         [-2.318e-03],
         [-1.007e-03],
         [ 5.770e-02],
         [-1.751e-02],
         [ 6.970e-03],
         [-5.725e-03],
         [ 7.597e-06],
         [-1.281e-02]],

        [[-4.951e-03],
         [-8.487e-02],
         [-9.506e-03],
         [-1.358e-02],
         [-7.944e-05],
         [-2.180e-04],
         [-8.499e-03],
         [ 1.444e-03],
         [ 4.104e-02],
         [ 4.649e-03]],

        [[-3.666e-02],
         [-5.390e-03],
         [-1.003e-02],
         [-4.884e-04],
         [-6.914e-03],
         [ 2.778e-04],
         [ 8.077e-04],
         [-1.981e-03],
         [-7.123e-02],
         [-1.437e-03]]], device='cuda:0')
v tensor([[[7.228e-03],
         [3.556e-02],
         [1.886e-04],
         [1.000e-12],
         [1.000e-12],
         [9.699e-05],
         [9.162e-04],
         [1.484e-03],
         [1.000e-12],
         [8.946e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.459e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.588e-04],
         [2.024e-03],
         [5.731e-05],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [9.905e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [2.085e-01],
         [3.492e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.822e-03],
         [1.000e-12]],

        [[1.000e-12],
         [1.187e-01],
         [8.361e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.323e-02]],

        [[1.000e-12],
         [1.589e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [2.430e-02],
         [1.000e-12],
         [2.879e-04],
         [2.528e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.021e-03]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.039e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[6.115e-03],
         [1.000e-12],
         [6.557e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [9.949e-02],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.596e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.361e-03],
         [1.000e-12],
         [1.000e-12]],

        [[6.896e-05],
         [1.000e-12],
         [1.000e-12],
         [2.369e-01],
         [3.252e-04],
         [1.000e-12],
         [1.000e-12],
         [1.883e-03],
         [1.977e-04],
         [1.000e-12]],

        [[5.293e-04],
         [1.000e-12],
         [5.702e-03],
         [1.000e-12],
         [1.000e-12],
         [3.256e-02],
         [1.000e-12],
         [1.000e-12],
         [9.633e-07],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.110e-03],
         [1.748e-04],
         [7.697e-05],
         [1.065e-02],
         [1.094e-03],
         [1.923e-02]],

        [[1.000e-12],
         [1.000e-12],
         [4.082e-04],
         [1.068e-02],
         [6.228e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.116e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.590e-05],
         [1.000e-12],
         [1.128e-02],
         [1.983e-03],
         [5.114e-02]],

        [[1.687e-03],
         [4.414e-02],
         [1.000e-12],
         [1.000e-12],
         [5.770e-02],
         [1.000e-12],
         [6.970e-03],
         [1.000e-12],
         [7.597e-06],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.444e-03],
         [4.104e-02],
         [4.649e-03]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.778e-04],
         [8.077e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-0.002],
        [-0.004],
        [-0.001],
        [-0.001],
        [-0.000],
        [-0.009],
        [-0.003],
        [-0.001],
        [-0.001],
        [-0.020],
        [ 0.092],
        [-0.081],
        [-0.021],
        [-0.097],
        [-0.002],
        [-0.002],
        [ 0.003],
        [-0.002],
        [-0.067],
        [-0.001]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [9.161e-02],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [3.036e-03],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-3.311e-04, -5.412e-06, -7.681e-05,  ...,  4.182e-05, -3.491e-04,
          1.043e-05],
        [ 7.204e-04,  2.093e-05,  1.669e-05,  ...,  3.199e-05, -3.781e-06,
         -2.486e-04],
        [-9.591e-04, -1.093e-04, -7.554e-05,  ..., -1.702e-04,  3.096e-05,
          1.720e-05],
        ...,
        [-1.014e-04, -1.240e-04,  5.634e-05,  ...,  9.031e-04,  9.812e-06,
         -2.634e-04],
        [ 5.833e-05,  1.091e-05, -6.334e-07,  ..., -1.017e-05,  7.453e-05,
          4.642e-04],
        [-2.116e-04,  1.662e-04, -4.687e-05,  ...,  5.004e-06, -9.438e-05,
         -9.723e-04]], device='cuda:0')
s after update for 1 param tensor([[6.767e-02, 1.578e-05, 7.806e-03,  ..., 5.257e-03, 1.102e-01,
         8.493e-04],
        [2.539e-01, 1.449e-03, 1.494e-03,  ..., 4.086e-03, 6.778e-05,
         5.397e-02],
        [3.768e-01, 9.007e-03, 2.575e-03,  ..., 3.193e-02, 8.969e-03,
         7.764e-05],
        ...,
        [1.556e-02, 2.161e-03, 1.888e-03,  ..., 1.876e-01, 6.818e-03,
         7.741e-02],
        [5.718e-04, 1.279e-03, 6.093e-03,  ..., 2.081e-04, 6.114e-03,
         1.087e-01],
        [3.993e-02, 1.601e-02, 7.271e-03,  ..., 4.348e-02, 2.597e-02,
         3.299e-01]], device='cuda:0')
b after update for 1 param tensor([[2.737, 0.042, 0.930,  ..., 0.763, 3.493, 0.307],
        [5.302, 0.401, 0.407,  ..., 0.673, 0.087, 2.445],
        [6.459, 0.999, 0.534,  ..., 1.880, 0.997, 0.093],
        ...,
        [1.312, 0.489, 0.457,  ..., 4.558, 0.869, 2.928],
        [0.252, 0.376, 0.821,  ..., 0.152, 0.823, 3.469],
        [2.103, 1.331, 0.897,  ..., 2.194, 1.696, 6.044]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 1.527e-06,  2.621e-05,  1.397e-05, -1.094e-05,  4.611e-05, -4.537e-07,
         3.339e-07,  6.275e-05, -8.051e-05,  6.444e-04,  1.012e-05,  4.598e-06,
        -7.171e-05,  5.248e-06,  1.357e-05, -1.226e-06,  1.086e-05, -2.936e-05,
        -4.657e-05, -2.103e-05, -3.917e-05, -6.234e-05, -2.549e-04, -6.956e-05,
         1.691e-05, -4.724e-05,  7.222e-05, -7.516e-05,  4.271e-05, -2.166e-05,
        -2.402e-05, -1.033e-04,  3.375e-06, -7.798e-07, -2.289e-05,  1.653e-06,
        -1.560e-06, -1.423e-06, -2.071e-05, -7.311e-06, -6.017e-05, -2.778e-04,
         7.995e-06, -4.958e-06,  4.979e-05,  1.565e-04, -1.195e-05, -3.144e-05,
         3.914e-05,  2.772e-05, -1.188e-04,  3.845e-05,  5.388e-06, -5.550e-05,
        -1.376e-05, -7.434e-06,  1.640e-04,  1.101e-06, -2.324e-06,  1.581e-05,
         1.045e-04,  2.917e-05, -2.706e-04, -1.889e-05, -2.361e-05,  8.861e-05,
         1.574e-04, -1.841e-04,  1.654e-04,  2.846e-04,  1.797e-05, -1.786e-05,
         1.564e-04,  6.337e-05, -3.735e-04, -6.625e-05, -1.014e-06,  4.442e-06,
         2.309e-05, -2.126e-05,  2.375e-04, -2.360e-04, -4.551e-06, -3.181e-05,
        -1.087e-05,  5.505e-05, -3.518e-07, -3.233e-06,  1.161e-04, -4.570e-04,
        -4.492e-05,  5.923e-06,  1.480e-06, -1.792e-05,  3.739e-05, -2.869e-04,
         1.792e-04,  6.010e-05,  8.463e-05,  7.624e-06,  1.020e-04, -1.694e-05,
        -1.591e-05, -2.693e-04, -2.872e-06,  2.579e-05, -4.451e-05, -8.346e-05,
         1.274e-05, -2.601e-06, -1.015e-04, -1.191e-04, -7.552e-06, -1.835e-05,
        -4.383e-05,  9.842e-06,  2.139e-05,  2.916e-05, -8.219e-05, -2.380e-04,
         1.203e-04, -6.204e-05,  1.438e-04, -5.558e-06, -7.181e-07,  1.133e-04,
        -2.580e-05,  3.483e-05, -1.571e-04,  1.744e-04,  4.264e-05,  3.451e-05,
        -1.683e-04, -2.558e-05, -7.750e-05,  8.237e-06,  7.534e-06, -4.623e-06,
        -2.051e-06, -8.412e-06, -6.210e-05, -2.484e-06,  7.663e-05,  3.548e-05,
        -1.018e-04,  3.668e-05, -4.157e-05, -1.271e-04, -1.118e-05,  1.912e-05,
         1.810e-04,  5.587e-05,  1.486e-04,  1.851e-06,  1.946e-05, -4.193e-05,
        -3.543e-04, -6.892e-05,  2.380e-04, -2.677e-04, -3.975e-06,  3.607e-05,
        -8.191e-05,  2.549e-05, -1.121e-04, -7.212e-07, -1.902e-04,  2.727e-04,
        -1.355e-04, -6.540e-06,  3.024e-05,  1.816e-04,  3.566e-05,  5.128e-05,
        -5.771e-05, -2.011e-06,  6.135e-06,  7.475e-05, -7.924e-07, -1.030e-04,
         3.911e-05,  4.334e-05,  7.395e-05,  1.948e-06, -1.598e-05, -1.746e-04,
         1.273e-05, -6.650e-04, -1.115e-05,  6.696e-06,  4.618e-04, -1.758e-04,
         2.884e-04,  5.566e-05,  3.407e-04,  7.225e-05,  1.872e-05, -2.480e-04,
        -4.294e-04, -2.248e-04], device='cuda:0')
s after update for 1 param tensor([3.186e-06, 1.270e-03, 3.069e-04, 3.301e-03, 3.830e-04, 1.277e-05,
        5.090e-04, 1.026e-02, 3.191e-02, 1.377e-01, 6.412e-05, 9.952e-06,
        1.344e-02, 3.285e-03, 9.690e-04, 4.346e-04, 3.310e-04, 1.594e-03,
        1.272e-02, 8.917e-05, 4.615e-02, 1.428e-03, 5.519e-02, 1.227e-02,
        2.100e-03, 5.800e-04, 5.272e-03, 1.804e-03, 4.430e-03, 5.309e-04,
        4.467e-03, 1.198e-02, 1.882e-06, 1.882e-06, 8.030e-03, 4.893e-04,
        1.882e-06, 2.428e-05, 1.484e-03, 2.629e-04, 6.178e-03, 5.354e-02,
        6.906e-04, 1.437e-05, 1.795e-03, 3.222e-02, 2.487e-05, 4.709e-03,
        6.600e-04, 1.229e-03, 5.464e-03, 5.127e-03, 1.136e-05, 1.160e-02,
        2.297e-04, 1.976e-04, 3.571e-02, 1.882e-06, 9.850e-04, 1.107e-02,
        2.836e-02, 2.991e-03, 5.351e-02, 9.687e-03, 1.027e-04, 5.440e-03,
        3.476e-03, 3.572e-02, 1.657e-02, 7.370e-02, 2.950e-05, 2.642e-04,
        2.945e-02, 1.873e-02, 9.287e-02, 1.107e-02, 4.709e-06, 2.286e-04,
        1.517e-03, 3.526e-04, 3.353e-02, 3.678e-02, 2.328e-05, 4.099e-04,
        4.557e-04, 3.208e-03, 2.496e-03, 4.147e-06, 3.630e-02, 2.862e-02,
        7.022e-04, 1.605e-03, 5.818e-04, 2.048e-04, 6.033e-04, 8.012e-02,
        4.451e-02, 8.270e-04, 1.342e-03, 1.941e-03, 1.783e-03, 1.746e-04,
        6.102e-04, 5.898e-02, 1.539e-05, 2.322e-04, 1.918e-03, 2.420e-02,
        7.964e-03, 5.545e-04, 2.205e-02, 3.722e-02, 2.367e-03, 4.488e-03,
        1.813e-03, 1.925e-05, 8.846e-05, 1.580e-04, 1.219e-03, 6.219e-02,
        3.246e-02, 3.221e-03, 8.687e-03, 6.879e-05, 1.306e-05, 1.841e-03,
        5.810e-03, 1.377e-03, 6.750e-02, 3.006e-02, 1.984e-02, 5.852e-04,
        4.991e-03, 8.482e-03, 1.542e-02, 1.771e-04, 6.007e-05, 4.047e-03,
        9.756e-04, 5.843e-04, 9.246e-03, 9.749e-05, 4.837e-03, 3.479e-03,
        9.252e-03, 3.352e-04, 6.686e-04, 9.589e-03, 2.706e-03, 9.988e-04,
        4.687e-02, 1.945e-03, 3.210e-03, 2.726e-05, 3.686e-04, 4.996e-04,
        1.208e-01, 1.577e-02, 5.670e-03, 8.347e-02, 1.435e-05, 1.159e-02,
        3.822e-03, 2.933e-03, 2.107e-03, 4.366e-05, 1.519e-02, 5.153e-02,
        5.161e-03, 6.431e-04, 2.419e-04, 2.805e-02, 9.625e-03, 1.045e-03,
        2.206e-03, 1.912e-06, 4.429e-03, 4.282e-03, 1.882e-06, 4.490e-03,
        9.126e-04, 1.045e-02, 1.872e-02, 1.882e-06, 8.518e-05, 9.907e-03,
        2.758e-04, 1.512e-01, 9.654e-04, 8.622e-05, 1.300e-01, 4.434e-03,
        7.251e-02, 5.844e-03, 5.254e-02, 1.627e-02, 1.640e-04, 4.811e-02,
        3.149e-02, 1.343e-02], device='cuda:0')
b after update for 1 param tensor([0.019, 0.375, 0.184, 0.605, 0.206, 0.038, 0.237, 1.066, 1.880, 3.904,
        0.084, 0.033, 1.220, 0.603, 0.328, 0.219, 0.191, 0.420, 1.187, 0.099,
        2.260, 0.398, 2.472, 1.166, 0.482, 0.253, 0.764, 0.447, 0.700, 0.242,
        0.703, 1.152, 0.014, 0.014, 0.943, 0.233, 0.014, 0.052, 0.405, 0.171,
        0.827, 2.435, 0.277, 0.040, 0.446, 1.889, 0.052, 0.722, 0.270, 0.369,
        0.778, 0.753, 0.035, 1.133, 0.159, 0.148, 1.989, 0.014, 0.330, 1.107,
        1.772, 0.576, 2.434, 1.036, 0.107, 0.776, 0.620, 1.989, 1.355, 2.857,
        0.057, 0.171, 1.806, 1.440, 3.207, 1.107, 0.023, 0.159, 0.410, 0.198,
        1.927, 2.018, 0.051, 0.213, 0.225, 0.596, 0.526, 0.021, 2.005, 1.780,
        0.279, 0.422, 0.254, 0.151, 0.258, 2.979, 2.220, 0.303, 0.385, 0.464,
        0.444, 0.139, 0.260, 2.556, 0.041, 0.160, 0.461, 1.637, 0.939, 0.248,
        1.562, 2.030, 0.512, 0.705, 0.448, 0.046, 0.099, 0.132, 0.367, 2.624,
        1.896, 0.597, 0.981, 0.087, 0.038, 0.451, 0.802, 0.390, 2.734, 1.824,
        1.482, 0.255, 0.743, 0.969, 1.307, 0.140, 0.082, 0.669, 0.329, 0.254,
        1.012, 0.104, 0.732, 0.621, 1.012, 0.193, 0.272, 1.030, 0.547, 0.333,
        2.278, 0.464, 0.596, 0.055, 0.202, 0.235, 3.657, 1.321, 0.792, 3.040,
        0.040, 1.133, 0.651, 0.570, 0.483, 0.070, 1.297, 2.389, 0.756, 0.267,
        0.164, 1.762, 1.032, 0.340, 0.494, 0.015, 0.700, 0.689, 0.014, 0.705,
        0.318, 1.076, 1.440, 0.014, 0.097, 1.047, 0.175, 4.092, 0.327, 0.098,
        3.794, 0.701, 2.833, 0.804, 2.412, 1.342, 0.135, 2.308, 1.867, 1.219],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 2.267e-04],
         [-2.099e-04],
         [ 2.520e-05],
         [ 5.248e-05],
         [-1.172e-05],
         [ 1.570e-05],
         [ 7.847e-05],
         [ 4.299e-05],
         [-7.131e-05],
         [-3.133e-05]],

        [[ 3.127e-05],
         [ 2.663e-05],
         [ 4.328e-06],
         [-3.266e-05],
         [ 7.271e-05],
         [ 1.608e-04],
         [-2.267e-06],
         [-6.146e-06],
         [ 3.475e-05],
         [ 7.648e-05]],

        [[ 2.432e-05],
         [-6.409e-05],
         [-2.392e-05],
         [-7.187e-05],
         [ 5.124e-05],
         [ 3.155e-05],
         [ 3.180e-05],
         [-3.396e-05],
         [ 3.415e-06],
         [-2.504e-05]],

        [[-1.092e-05],
         [ 1.513e-05],
         [ 3.381e-04],
         [-1.013e-05],
         [ 1.595e-05],
         [ 8.995e-05],
         [-1.484e-08],
         [ 6.571e-05],
         [ 1.244e-05],
         [ 1.516e-05]],

        [[-5.747e-05],
         [ 4.608e-04],
         [ 2.057e-04],
         [-5.593e-05],
         [-7.903e-06],
         [-6.442e-05],
         [-2.324e-05],
         [ 7.099e-06],
         [-1.401e-04],
         [ 3.875e-05]],

        [[ 1.083e-04],
         [-4.753e-04],
         [ 9.951e-06],
         [-8.156e-07],
         [-6.932e-05],
         [ 6.230e-05],
         [-6.136e-05],
         [-2.712e-05],
         [ 2.310e-05],
         [ 2.201e-04]],

        [[ 1.903e-04],
         [ 1.673e-04],
         [-9.666e-05],
         [ 8.300e-06],
         [-4.467e-05],
         [ 4.165e-05],
         [ 1.930e-05],
         [ 5.507e-05],
         [-9.359e-05],
         [-5.254e-05]],

        [[-9.535e-06],
         [-3.580e-06],
         [ 2.072e-04],
         [ 1.089e-04],
         [-3.962e-05],
         [-1.777e-04],
         [-1.763e-04],
         [ 8.728e-06],
         [-6.510e-06],
         [-4.148e-06]],

        [[-4.461e-05],
         [ 1.702e-06],
         [-8.930e-05],
         [-1.844e-05],
         [-1.278e-05],
         [ 1.754e-05],
         [-1.964e-05],
         [ 8.061e-06],
         [ 1.642e-05],
         [ 6.504e-05]],

        [[-7.353e-06],
         [-3.638e-05],
         [-9.239e-06],
         [ 1.122e-05],
         [-4.317e-05],
         [ 3.027e-05],
         [-4.013e-04],
         [-8.596e-05],
         [-1.903e-05],
         [-2.276e-05]],

        [[-9.629e-05],
         [-7.290e-06],
         [-1.310e-04],
         [-1.022e-04],
         [-1.397e-04],
         [ 8.516e-05],
         [-6.230e-06],
         [-7.026e-05],
         [-4.433e-04],
         [ 1.370e-05]],

        [[ 2.101e-04],
         [ 1.983e-04],
         [-5.540e-05],
         [-7.783e-05],
         [ 8.112e-05],
         [ 1.661e-04],
         [-1.962e-05],
         [-8.225e-05],
         [-2.344e-04],
         [ 1.055e-05]],

        [[ 2.376e-05],
         [ 7.012e-05],
         [-7.860e-06],
         [ 4.327e-04],
         [ 5.057e-05],
         [-4.502e-05],
         [-4.489e-05],
         [-7.985e-05],
         [ 1.822e-05],
         [-1.122e-04]],

        [[-5.178e-05],
         [ 1.907e-04],
         [ 1.073e-04],
         [-1.565e-04],
         [-1.021e-04],
         [ 3.436e-04],
         [ 4.111e-05],
         [-2.434e-05],
         [-2.321e-06],
         [-2.480e-05]],

        [[ 4.398e-05],
         [ 1.001e-04],
         [ 2.297e-05],
         [-1.749e-05],
         [ 3.155e-05],
         [-1.214e-04],
         [ 1.995e-05],
         [-1.349e-04],
         [-7.043e-05],
         [-3.098e-04]],

        [[-3.023e-05],
         [-3.294e-05],
         [ 2.868e-05],
         [ 9.171e-05],
         [-3.961e-05],
         [ 2.111e-05],
         [ 2.864e-05],
         [-9.702e-06],
         [-4.238e-04],
         [ 3.829e-06]],

        [[ 2.560e-06],
         [-3.588e-05],
         [-3.779e-04],
         [-1.046e-04],
         [-9.955e-06],
         [-1.100e-05],
         [ 1.373e-04],
         [-1.987e-04],
         [-5.383e-05],
         [-2.559e-04]],

        [[-4.808e-05],
         [-1.580e-04],
         [ 3.812e-05],
         [ 1.194e-05],
         [ 2.668e-04],
         [-1.729e-05],
         [ 1.650e-04],
         [-2.946e-05],
         [ 6.320e-06],
         [ 1.078e-04]],

        [[ 3.583e-05],
         [-2.409e-05],
         [ 1.229e-06],
         [ 4.560e-05],
         [ 1.966e-05],
         [-4.908e-07],
         [-1.309e-05],
         [-7.321e-05],
         [-2.601e-04],
         [ 1.616e-04]],

        [[-4.116e-06],
         [ 7.609e-05],
         [ 1.938e-05],
         [-8.383e-06],
         [-3.316e-05],
         [ 6.954e-05],
         [ 3.305e-05],
         [-1.322e-05],
         [-3.756e-05],
         [ 8.731e-05]]], device='cuda:0')
s after update for 1 param tensor([[[2.772e-02],
         [5.972e-02],
         [4.345e-03],
         [2.042e-03],
         [6.950e-04],
         [3.114e-03],
         [9.615e-03],
         [1.218e-02],
         [6.405e-03],
         [9.460e-03]],

        [[2.644e-04],
         [5.138e-04],
         [2.269e-05],
         [2.942e-04],
         [1.504e-03],
         [1.300e-02],
         [1.559e-03],
         [1.434e-04],
         [8.874e-04],
         [2.470e-03]],

        [[1.614e-02],
         [1.210e-02],
         [3.368e-04],
         [5.520e-04],
         [1.274e-02],
         [6.773e-03],
         [1.423e-02],
         [2.395e-03],
         [5.534e-04],
         [6.467e-04]],

        [[1.092e-03],
         [4.119e-05],
         [1.002e-01],
         [1.001e-04],
         [7.537e-04],
         [6.157e-03],
         [9.168e-04],
         [1.159e-03],
         [1.527e-03],
         [1.239e-04]],

        [[2.428e-03],
         [1.453e-01],
         [5.918e-02],
         [3.033e-03],
         [7.745e-04],
         [5.488e-03],
         [2.369e-03],
         [4.882e-05],
         [2.828e-02],
         [1.192e-03]],

        [[1.472e-02],
         [1.100e-01],
         [2.892e-03],
         [1.256e-04],
         [8.016e-03],
         [2.586e-03],
         [3.571e-04],
         [4.738e-04],
         [1.906e-04],
         [4.830e-02]],

        [[8.134e-03],
         [3.989e-02],
         [1.916e-03],
         [2.626e-04],
         [3.953e-03],
         [5.018e-03],
         [1.782e-04],
         [8.275e-04],
         [1.146e-02],
         [2.316e-03]],

        [[2.505e-03],
         [1.085e-02],
         [4.983e-02],
         [3.275e-03],
         [5.370e-03],
         [5.036e-02],
         [3.486e-02],
         [2.706e-04],
         [1.172e-04],
         [9.982e-05]],

        [[5.649e-04],
         [2.121e-04],
         [1.769e-03],
         [3.238e-04],
         [1.016e-03],
         [1.192e-04],
         [3.632e-04],
         [6.159e-04],
         [1.601e-03],
         [2.242e-02]],

        [[9.740e-03],
         [2.905e-03],
         [6.728e-04],
         [5.312e-04],
         [1.875e-03],
         [7.344e-04],
         [7.292e-02],
         [5.034e-03],
         [8.987e-04],
         [2.893e-03]],

        [[2.473e-02],
         [2.948e-05],
         [2.573e-02],
         [3.232e-03],
         [1.988e-02],
         [3.277e-02],
         [1.418e-05],
         [5.036e-04],
         [1.001e-01],
         [4.287e-03]],

        [[5.221e-03],
         [3.485e-03],
         [1.837e-02],
         [1.291e-02],
         [5.275e-03],
         [2.337e-02],
         [4.824e-03],
         [1.172e-02],
         [2.051e-02],
         [9.014e-04]],

        [[2.630e-03],
         [5.331e-03],
         [2.337e-05],
         [1.551e-01],
         [5.761e-03],
         [5.096e-04],
         [2.068e-03],
         [1.397e-02],
         [4.469e-03],
         [9.711e-03]],

        [[7.288e-03],
         [8.683e-03],
         [2.389e-02],
         [6.723e-03],
         [5.390e-02],
         [5.836e-02],
         [1.498e-03],
         [2.681e-04],
         [3.104e-04],
         [1.213e-03]],

        [[5.135e-03],
         [2.886e-03],
         [1.964e-03],
         [1.962e-03],
         [1.054e-02],
         [4.491e-03],
         [2.800e-03],
         [3.264e-02],
         [1.046e-02],
         [4.596e-02]],

        [[1.927e-04],
         [1.900e-03],
         [6.389e-03],
         [3.274e-02],
         [7.895e-03],
         [6.457e-04],
         [6.186e-03],
         [1.705e-05],
         [3.937e-02],
         [8.138e-04]],

        [[6.821e-05],
         [1.057e-02],
         [2.747e-02],
         [3.591e-03],
         [3.379e-04],
         [2.755e-03],
         [1.155e-02],
         [3.543e-02],
         [1.408e-02],
         [7.202e-02]],

        [[1.299e-02],
         [6.727e-02],
         [1.111e-03],
         [2.585e-04],
         [7.633e-02],
         [4.392e-03],
         [2.645e-02],
         [1.621e-03],
         [8.716e-04],
         [4.948e-03]],

        [[1.282e-03],
         [2.350e-02],
         [2.369e-03],
         [5.403e-03],
         [6.030e-05],
         [5.381e-05],
         [2.106e-03],
         [1.203e-02],
         [6.485e-02],
         [2.327e-02]],

        [[1.495e-02],
         [1.463e-03],
         [2.501e-03],
         [1.490e-04],
         [1.979e-03],
         [5.300e-03],
         [8.989e-03],
         [8.936e-04],
         [1.759e-02],
         [1.975e-03]]], device='cuda:0')
b after update for 1 param tensor([[[1.752],
         [2.572],
         [0.694],
         [0.476],
         [0.277],
         [0.587],
         [1.032],
         [1.161],
         [0.842],
         [1.023]],

        [[0.171],
         [0.239],
         [0.050],
         [0.180],
         [0.408],
         [1.200],
         [0.416],
         [0.126],
         [0.313],
         [0.523]],

        [[1.337],
         [1.158],
         [0.193],
         [0.247],
         [1.188],
         [0.866],
         [1.255],
         [0.515],
         [0.248],
         [0.268]],

        [[0.348],
         [0.068],
         [3.331],
         [0.105],
         [0.289],
         [0.826],
         [0.319],
         [0.358],
         [0.411],
         [0.117]],

        [[0.519],
         [4.011],
         [2.560],
         [0.580],
         [0.293],
         [0.780],
         [0.512],
         [0.074],
         [1.769],
         [0.363]],

        [[1.277],
         [3.491],
         [0.566],
         [0.118],
         [0.942],
         [0.535],
         [0.199],
         [0.229],
         [0.145],
         [2.313]],

        [[0.949],
         [2.102],
         [0.461],
         [0.171],
         [0.662],
         [0.745],
         [0.140],
         [0.303],
         [1.126],
         [0.506]],

        [[0.527],
         [1.096],
         [2.349],
         [0.602],
         [0.771],
         [2.361],
         [1.965],
         [0.173],
         [0.114],
         [0.105]],

        [[0.250],
         [0.153],
         [0.443],
         [0.189],
         [0.335],
         [0.115],
         [0.201],
         [0.261],
         [0.421],
         [1.576]],

        [[1.039],
         [0.567],
         [0.273],
         [0.243],
         [0.456],
         [0.285],
         [2.842],
         [0.747],
         [0.315],
         [0.566]],

        [[1.655],
         [0.057],
         [1.688],
         [0.598],
         [1.484],
         [1.905],
         [0.040],
         [0.236],
         [3.330],
         [0.689]],

        [[0.760],
         [0.621],
         [1.426],
         [1.196],
         [0.764],
         [1.609],
         [0.731],
         [1.139],
         [1.507],
         [0.316]],

        [[0.540],
         [0.768],
         [0.051],
         [4.144],
         [0.799],
         [0.238],
         [0.479],
         [1.244],
         [0.703],
         [1.037]],

        [[0.898],
         [0.981],
         [1.626],
         [0.863],
         [2.443],
         [2.542],
         [0.407],
         [0.172],
         [0.185],
         [0.366]],

        [[0.754],
         [0.565],
         [0.466],
         [0.466],
         [1.080],
         [0.705],
         [0.557],
         [1.901],
         [1.076],
         [2.256]],

        [[0.146],
         [0.459],
         [0.841],
         [1.904],
         [0.935],
         [0.267],
         [0.828],
         [0.043],
         [2.088],
         [0.300]],

        [[0.087],
         [1.082],
         [1.744],
         [0.631],
         [0.193],
         [0.552],
         [1.131],
         [1.981],
         [1.249],
         [2.824]],

        [[1.199],
         [2.729],
         [0.351],
         [0.169],
         [2.907],
         [0.697],
         [1.711],
         [0.424],
         [0.311],
         [0.740]],

        [[0.377],
         [1.613],
         [0.512],
         [0.773],
         [0.082],
         [0.077],
         [0.483],
         [1.154],
         [2.680],
         [1.605]],

        [[1.287],
         [0.403],
         [0.526],
         [0.128],
         [0.468],
         [0.766],
         [0.998],
         [0.315],
         [1.396],
         [0.468]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 9.445e-06],
        [ 6.145e-05],
        [ 3.407e-05],
        [ 6.556e-05],
        [ 5.868e-06],
        [-3.517e-05],
        [ 4.731e-05],
        [ 1.571e-05],
        [ 8.241e-05],
        [ 7.576e-05],
        [ 2.592e-04],
        [ 1.501e-04],
        [-9.491e-05],
        [ 8.559e-05],
        [-4.882e-05],
        [ 5.341e-06],
        [-5.605e-05],
        [-4.731e-05],
        [-1.586e-04],
        [ 7.207e-05]], device='cuda:0')
s after update for 1 param tensor([[0.001],
        [0.004],
        [0.000],
        [0.000],
        [0.000],
        [0.002],
        [0.001],
        [0.000],
        [0.004],
        [0.014],
        [0.096],
        [0.025],
        [0.006],
        [0.027],
        [0.001],
        [0.001],
        [0.017],
        [0.001],
        [0.018],
        [0.007]], device='cuda:0')
b after update for 1 param tensor([[0.304],
        [0.670],
        [0.138],
        [0.233],
        [0.113],
        [0.510],
        [0.385],
        [0.195],
        [0.661],
        [1.259],
        [3.257],
        [1.669],
        [0.820],
        [1.730],
        [0.247],
        [0.273],
        [1.389],
        [0.314],
        [1.412],
        [0.861]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 0.5069783055112932
exp ma of ||w||^2 22.890306376405963
||w|| 0.7120240905413897
exp ma of ||w|| 0.7508628799556789
||w||^2 0.14859280559168822
exp ma of ||w||^2 0.16582804050635855
||w|| 0.3854773736442753
exp ma of ||w|| 0.38713071224752127
||w||^2 0.043432209312237
exp ma of ||w||^2 0.06099096704776472
||w|| 0.20840395704553452
exp ma of ||w|| 0.24380639505510118
cuda
Objective function 22.02 = squared loss an data 21.30 + 0.5*rho*h**2 0.277665 + alpha*h 0.078585 + L2reg 0.28 + L1reg 0.09 ; SHD = 50 ; DAG True
Proportion of microbatches that were clipped  0.7108981646905961
iteration 3 in inner loop, alpha 33.34758380591081 rho 100000.0 h 0.002356544780049319
iteration 4 in outer loop, alpha = 269.0020618108427, rho = 100000.0, h = 0.002356544780049319
cuda
4420
cuda
Objective function 22.58 = squared loss an data 21.30 + 0.5*rho*h**2 0.277665 + alpha*h 0.633915 + L2reg 0.28 + L1reg 0.09 ; SHD = 50 ; DAG True
||w||^2 17127415010.203762
exp ma of ||w||^2 4043535983.8517475
||w|| 130871.75023741282
exp ma of ||w|| 32405.53523636336
||w||^2 154471477.47874254
exp ma of ||w||^2 183439226954.912
||w|| 12428.655497628959
exp ma of ||w|| 157463.2403983395
||w||^2 5059263.461218513
exp ma of ||w||^2 6819378794.558252
||w|| 2249.2806541689083
exp ma of ||w|| 10426.79871132097
||w||^2 2.9156205940191042
exp ma of ||w||^2 113811.60051453441
||w|| 1.7075188414828997
exp ma of ||w|| 5.458859259041653
||w||^2 0.7586162651346209
exp ma of ||w||^2 670.4723079100468
||w|| 0.8709858007652139
exp ma of ||w|| 0.9869772838459818
||w||^2 0.5410479784438704
exp ma of ||w||^2 118.43398523988856
||w|| 0.7355596362252829
exp ma of ||w|| 0.8750902079693872
||w||^2 0.31784597470189657
exp ma of ||w||^2 7.715899283678204
||w|| 0.563778302794544
exp ma of ||w|| 0.6628284761269227
||w||^2 0.0801265582471809
exp ma of ||w||^2 0.06595836711741382
||w|| 0.28306634954932547
exp ma of ||w|| 0.2539372211560557
||w||^2 0.04418035102972249
exp ma of ||w||^2 0.06349401779052459
||w|| 0.21019122491132328
exp ma of ||w|| 0.24886885175098178
cuda
Objective function 22.38 = squared loss an data 21.45 + 0.5*rho*h**2 0.127226 + alpha*h 0.429100 + L2reg 0.29 + L1reg 0.09 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.7068633739576652
iteration 1 in inner loop, alpha 269.0020618108427 rho 100000.0 h 0.0015951565170411186
iteration 5 in outer loop, alpha = 1864.1585788519612, rho = 1000000.0, h = 0.0015951565170411186
Threshold 0.3
[[0.002 0.043 0.192 0.029 0.119 0.061 0.058 0.037 0.021 0.07  0.185 0.034
  0.148 0.008 0.019 0.069 0.035 0.138 0.072 0.024]
 [0.048 0.003 0.06  0.016 0.094 0.156 0.019 0.035 0.023 0.081 0.049 0.047
  0.082 0.066 0.06  0.041 0.023 0.16  0.064 0.028]
 [0.019 0.044 0.002 0.03  0.015 0.028 0.017 0.011 0.005 0.016 0.039 0.029
  0.106 0.004 0.025 0.071 0.025 0.065 0.023 0.013]
 [0.109 0.111 0.116 0.002 0.339 0.242 0.085 0.064 0.066 0.084 0.181 0.316
  0.223 0.054 0.034 0.101 0.136 0.129 0.08  0.051]
 [0.031 0.04  0.105 0.007 0.002 0.033 0.009 0.013 0.004 0.014 0.096 0.006
  0.116 0.051 0.028 0.057 0.017 0.031 0.044 0.016]
 [0.054 0.022 0.09  0.011 0.062 0.002 0.011 0.01  0.014 0.023 0.085 0.057
  0.136 0.012 0.004 0.047 0.03  0.075 0.097 0.014]
 [0.06  0.092 0.192 0.027 0.312 0.236 0.003 0.018 0.083 0.051 0.288 0.319
  0.244 0.069 0.016 0.164 0.519 0.103 0.069 0.084]
 [0.049 0.089 0.124 0.029 0.166 0.187 0.128 0.004 0.016 0.052 0.117 0.369
  0.288 0.062 0.026 0.138 0.158 0.746 0.044 0.089]
 [0.127 0.13  0.555 0.052 0.461 0.176 0.028 0.1   0.002 0.07  0.434 0.211
  0.349 0.035 0.118 0.246 0.102 0.336 0.513 0.081]
 [0.031 0.039 0.247 0.035 0.139 0.152 0.046 0.045 0.038 0.003 0.431 0.036
  0.116 0.026 0.038 0.22  0.074 0.157 0.078 0.04 ]
 [0.011 0.061 0.082 0.012 0.025 0.035 0.012 0.023 0.007 0.007 0.004 0.038
  0.038 0.028 0.03  0.04  0.03  0.039 0.009 0.014]
 [0.053 0.08  0.051 0.01  0.354 0.058 0.011 0.006 0.013 0.036 0.064 0.003
  0.203 0.039 0.036 0.055 0.056 0.228 0.033 0.012]
 [0.023 0.035 0.021 0.013 0.026 0.019 0.01  0.008 0.009 0.034 0.053 0.019
  0.003 0.009 0.005 0.053 0.029 0.078 0.048 0.015]
 [0.242 0.048 0.772 0.052 0.044 0.187 0.034 0.036 0.079 0.109 0.08  0.069
  0.164 0.002 0.043 0.11  0.074 0.185 0.14  0.141]
 [0.112 0.038 0.075 0.066 0.092 0.414 0.154 0.132 0.023 0.098 0.061 0.047
  0.654 0.061 0.003 0.074 0.098 0.192 0.085 0.178]
 [0.039 0.072 0.043 0.028 0.044 0.065 0.018 0.018 0.009 0.014 0.044 0.074
  0.055 0.022 0.031 0.002 0.038 0.136 0.018 0.049]
 [0.071 0.098 0.1   0.019 0.117 0.076 0.003 0.016 0.029 0.037 0.108 0.051
  0.09  0.046 0.025 0.066 0.002 0.078 0.02  0.02 ]
 [0.015 0.021 0.049 0.014 0.091 0.027 0.022 0.003 0.009 0.019 0.07  0.014
  0.038 0.018 0.016 0.02  0.029 0.003 0.043 0.009]
 [0.041 0.05  0.109 0.036 0.039 0.047 0.024 0.057 0.005 0.056 0.178 0.078
  0.053 0.014 0.025 0.228 0.088 0.032 0.002 0.025]
 [0.126 0.067 0.217 0.056 0.154 0.148 0.035 0.028 0.035 0.096 0.217 0.237
  0.18  0.019 0.017 0.069 0.141 0.391 0.108 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.339 0.    0.    0.    0.    0.    0.    0.316
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.312 0.    0.    0.    0.    0.    0.    0.319
  0.    0.    0.    0.    0.519 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.369
  0.    0.    0.    0.    0.    0.746 0.    0.   ]
 [0.    0.    0.555 0.    0.461 0.    0.    0.    0.    0.    0.434 0.
  0.349 0.    0.    0.    0.    0.336 0.513 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.431 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.354 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.772 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.414 0.    0.    0.    0.    0.    0.
  0.654 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.391 0.    0.   ]]
{'fdr': 0.5263157894736842, 'tpr': 0.15, 'fpr': 0.07692307692307693, 'f1': 0.22784810126582275, 'shd': 54, 'npred': 19, 'ntrue': 60}
[0.043 0.192 0.029 0.119 0.061 0.058 0.037 0.021 0.07  0.185 0.034 0.148
 0.008 0.019 0.069 0.035 0.138 0.072 0.024 0.048 0.06  0.016 0.094 0.156
 0.019 0.035 0.023 0.081 0.049 0.047 0.082 0.066 0.06  0.041 0.023 0.16
 0.064 0.028 0.019 0.044 0.03  0.015 0.028 0.017 0.011 0.005 0.016 0.039
 0.029 0.106 0.004 0.025 0.071 0.025 0.065 0.023 0.013 0.109 0.111 0.116
 0.339 0.242 0.085 0.064 0.066 0.084 0.181 0.316 0.223 0.054 0.034 0.101
 0.136 0.129 0.08  0.051 0.031 0.04  0.105 0.007 0.033 0.009 0.013 0.004
 0.014 0.096 0.006 0.116 0.051 0.028 0.057 0.017 0.031 0.044 0.016 0.054
 0.022 0.09  0.011 0.062 0.011 0.01  0.014 0.023 0.085 0.057 0.136 0.012
 0.004 0.047 0.03  0.075 0.097 0.014 0.06  0.092 0.192 0.027 0.312 0.236
 0.018 0.083 0.051 0.288 0.319 0.244 0.069 0.016 0.164 0.519 0.103 0.069
 0.084 0.049 0.089 0.124 0.029 0.166 0.187 0.128 0.016 0.052 0.117 0.369
 0.288 0.062 0.026 0.138 0.158 0.746 0.044 0.089 0.127 0.13  0.555 0.052
 0.461 0.176 0.028 0.1   0.07  0.434 0.211 0.349 0.035 0.118 0.246 0.102
 0.336 0.513 0.081 0.031 0.039 0.247 0.035 0.139 0.152 0.046 0.045 0.038
 0.431 0.036 0.116 0.026 0.038 0.22  0.074 0.157 0.078 0.04  0.011 0.061
 0.082 0.012 0.025 0.035 0.012 0.023 0.007 0.007 0.038 0.038 0.028 0.03
 0.04  0.03  0.039 0.009 0.014 0.053 0.08  0.051 0.01  0.354 0.058 0.011
 0.006 0.013 0.036 0.064 0.203 0.039 0.036 0.055 0.056 0.228 0.033 0.012
 0.023 0.035 0.021 0.013 0.026 0.019 0.01  0.008 0.009 0.034 0.053 0.019
 0.009 0.005 0.053 0.029 0.078 0.048 0.015 0.242 0.048 0.772 0.052 0.044
 0.187 0.034 0.036 0.079 0.109 0.08  0.069 0.164 0.043 0.11  0.074 0.185
 0.14  0.141 0.112 0.038 0.075 0.066 0.092 0.414 0.154 0.132 0.023 0.098
 0.061 0.047 0.654 0.061 0.074 0.098 0.192 0.085 0.178 0.039 0.072 0.043
 0.028 0.044 0.065 0.018 0.018 0.009 0.014 0.044 0.074 0.055 0.022 0.031
 0.038 0.136 0.018 0.049 0.071 0.098 0.1   0.019 0.117 0.076 0.003 0.016
 0.029 0.037 0.108 0.051 0.09  0.046 0.025 0.066 0.078 0.02  0.02  0.015
 0.021 0.049 0.014 0.091 0.027 0.022 0.003 0.009 0.019 0.07  0.014 0.038
 0.018 0.016 0.02  0.029 0.043 0.009 0.041 0.05  0.109 0.036 0.039 0.047
 0.024 0.057 0.005 0.056 0.178 0.078 0.053 0.014 0.025 0.228 0.088 0.032
 0.025 0.126 0.067 0.217 0.056 0.154 0.148 0.035 0.028 0.035 0.096 0.217
 0.237 0.18  0.019 0.017 0.069 0.141 0.391 0.108]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.5258333333333333, 0.2858439738565214)
Iterations 630
Achieves (4.040438258977602, 1e-05)-DP
