samples  5000  graph  30 60 ER mim  minibatch size  100  noise  0.8  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.144444827137683
iteration 1 in outer loop, alpha = 2.144444827137683, rho = 1.0, h = 2.144444827137683
cuda
iteration 1 in inner loop,alpha 2.144444827137683 rho 1.0 h 1.3256466874605835
iteration 2 in inner loop,alpha 2.144444827137683 rho 10.0 h 0.5131010328032026
iteration 2 in outer loop, alpha = 7.275455155169709, rho = 10.0, h = 0.5131010328032026
cuda
iteration 1 in inner loop,alpha 7.275455155169709 rho 10.0 h 0.28442514981965417
iteration 2 in inner loop,alpha 7.275455155169709 rho 100.0 h 0.09687583592673477
iteration 3 in outer loop, alpha = 16.963038747843186, rho = 100.0, h = 0.09687583592673477
cuda
iteration 1 in inner loop,alpha 16.963038747843186 rho 100.0 h 0.05565872169768937
iteration 2 in inner loop,alpha 16.963038747843186 rho 1000.0 h 0.017246591836673986
iteration 4 in outer loop, alpha = 34.20963058451717, rho = 1000.0, h = 0.017246591836673986
cuda
iteration 1 in inner loop,alpha 34.20963058451717 rho 1000.0 h 0.007536379858688491
iteration 2 in inner loop,alpha 34.20963058451717 rho 10000.0 h 0.0024562545468960195
iteration 5 in outer loop, alpha = 58.77217605347737, rho = 10000.0, h = 0.0024562545468960195
cuda
iteration 1 in inner loop,alpha 58.77217605347737 rho 10000.0 h 0.0010792499674714406
iteration 2 in inner loop,alpha 58.77217605347737 rho 100000.0 h 0.00040857954408224373
iteration 6 in outer loop, alpha = 99.63013046170174, rho = 100000.0, h = 0.00040857954408224373
cuda
iteration 1 in inner loop,alpha 99.63013046170174 rho 100000.0 h 0.0002314835665977455
iteration 7 in outer loop, alpha = 331.11369705944725, rho = 1000000.0, h = 0.0002314835665977455
Threshold 0.3
[[0.    0.002 0.099 0.045 0.058 0.006 0.115 0.023 0.059 0.348 0.009 0.059
  0.005 0.002 0.01  0.028 0.003 0.05  0.055 0.878 0.042 0.007 0.142 0.023
  0.001 0.035 0.022 0.018 0.023 0.023]
 [0.003 0.    0.018 0.004 0.012 0.682 1.88  0.087 0.112 0.005 0.    0.034
  0.014 0.001 0.003 0.06  0.001 0.073 0.026 0.052 0.006 0.005 0.102 0.054
  0.004 0.013 0.002 0.081 0.085 0.031]
 [0.    0.    0.004 0.01  0.107 0.    0.069 0.05  0.001 0.    0.    0.
  0.    0.    0.    0.005 0.    0.014 0.057 0.123 0.001 0.    0.009 0.021
  0.    0.    0.019 0.013 0.023 0.007]
 [0.    0.    0.006 0.003 0.02  0.002 0.021 0.    0.001 0.    0.    0.001
  0.    0.    0.    0.002 0.    0.    0.001 0.052 0.    0.    0.001 0.001
  0.    0.    0.    0.    0.059 0.   ]
 [0.    0.    0.003 0.007 0.003 0.004 0.013 0.012 0.002 0.    0.    0.
  0.    0.    0.    0.001 0.    0.001 0.001 0.161 0.001 0.    0.007 0.001
  0.    0.    0.001 0.009 0.013 0.001]
 [0.    0.    0.051 0.042 0.017 0.003 0.049 0.044 0.083 0.    0.001 0.067
  0.    0.    0.    0.003 0.    0.006 0.004 0.831 0.001 0.    0.03  0.009
  0.001 0.001 0.011 0.003 0.003 0.001]
 [0.    0.    0.01  0.027 0.026 0.002 0.003 0.02  0.005 0.    0.001 0.001
  0.    0.001 0.    0.017 0.002 0.01  0.009 0.094 0.001 0.    0.089 0.043
  0.    0.001 0.004 0.009 0.021 0.002]
 [0.001 0.    0.009 1.483 0.009 0.014 0.027 0.003 0.001 0.    0.    0.001
  0.    0.    0.    0.003 0.    0.    0.001 0.009 0.001 0.    0.005 0.001
  0.    0.    0.001 0.    0.043 0.002]
 [0.001 0.    0.051 0.009 0.042 0.005 0.043 0.058 0.003 0.001 0.    0.059
  0.    0.    0.    0.093 0.    0.046 0.755 1.091 0.    0.001 1.785 0.079
  0.    0.004 0.055 0.001 0.056 0.009]
 [0.001 0.    0.126 0.14  0.029 1.135 0.056 0.641 0.091 0.    0.    2.467
  0.001 0.003 0.    0.259 0.    0.18  0.026 0.099 0.009 0.003 0.061 0.005
  0.007 0.014 0.595 0.021 0.013 0.025]
 [0.008 0.    0.029 0.186 0.033 0.018 0.09  0.08  0.9   0.017 0.    0.036
  0.005 0.004 0.018 0.029 0.    0.056 0.064 0.103 0.051 0.013 0.033 0.068
  0.    0.038 0.093 4.127 0.041 0.015]
 [0.    0.    4.289 0.012 0.03  0.009 0.019 0.056 0.013 0.    0.    0.002
  0.001 0.001 0.    0.127 0.    0.089 0.034 0.151 0.002 0.001 0.005 0.065
  0.    0.    0.423 0.024 0.029 0.011]
 [0.005 0.004 0.    0.053 0.048 0.009 1.879 0.013 1.181 0.02  0.    0.052
  0.    0.001 0.009 0.122 0.004 0.096 0.003 0.008 0.022 0.022 0.07  0.359
  0.001 0.038 0.459 0.039 0.02  3.057]
 [0.001 0.003 0.082 0.003 1.229 0.042 0.024 0.129 0.064 0.    0.001 0.07
  0.003 0.    0.005 0.112 0.003 0.921 0.031 1.342 0.02  0.006 0.044 0.038
  0.003 0.005 0.137 0.068 0.011 0.009]
 [0.002 0.01  0.032 0.057 0.06  0.06  0.038 0.103 0.05  0.002 0.006 1.739
  0.012 0.007 0.    0.012 0.    0.187 0.022 0.007 0.016 0.013 0.116 0.756
  0.003 3.168 0.008 0.014 0.121 0.057]
 [0.    0.001 0.048 0.005 0.071 0.032 0.019 0.058 0.013 0.001 0.001 0.005
  0.003 0.001 0.    0.004 0.    0.09  0.013 0.027 0.003 0.002 0.455 0.012
  0.001 0.006 0.781 0.005 0.009 0.074]
 [0.003 0.    2.54  0.093 0.089 0.019 0.034 0.013 0.458 0.    0.002 0.004
  0.004 0.003 0.001 0.013 0.    0.12  0.004 0.035 2.852 0.402 0.124 0.016
  0.001 0.006 0.048 0.051 0.074 0.042]
 [0.001 0.    0.014 0.059 1.535 0.086 0.018 1.12  0.002 0.    0.    0.
  0.    0.    0.    0.003 0.    0.004 0.003 0.093 0.003 0.    0.015 0.001
  0.    0.001 0.003 0.011 0.207 0.001]
 [0.    0.    0.007 0.025 0.022 0.008 0.006 0.037 0.001 0.    0.    0.001
  0.    0.001 0.    0.001 0.    0.137 0.002 0.01  0.    0.    0.018 0.001
  0.    0.    0.01  0.006 0.035 0.001]
 [0.    0.    0.004 0.004 0.001 0.001 0.007 0.    0.001 0.    0.    0.
  0.    0.    0.    0.021 0.    0.001 0.014 0.003 0.    0.    0.075 0.009
  0.001 0.    0.006 0.003 0.009 0.001]
 [0.002 0.    0.037 0.    0.005 0.841 0.014 0.03  1.728 0.006 0.    0.063
  0.002 0.    0.    0.015 0.    0.074 0.024 0.082 0.002 0.001 0.012 0.029
  0.002 0.019 0.01  0.012 2.129 0.036]
 [0.    0.009 0.044 1.341 1.125 0.023 0.051 0.12  0.24  0.005 0.002 0.039
  0.003 0.002 0.008 0.057 0.    0.696 0.069 0.068 0.331 0.001 0.1   0.2
  0.    0.027 0.242 0.05  1.58  2.036]
 [0.001 0.    0.004 0.12  0.058 0.002 0.002 0.102 0.    0.    0.    0.002
  0.    0.    0.    0.005 0.    0.009 0.027 0.    0.    0.    0.004 0.
  0.    0.    0.167 0.    0.006 0.   ]
 [0.    0.    0.004 0.619 0.001 0.022 0.013 0.086 0.008 0.    0.001 0.004
  0.    0.001 0.    0.025 0.    0.127 1.181 0.024 0.004 0.    2.441 0.003
  0.    0.    0.379 0.005 0.178 0.   ]
 [0.003 0.006 0.058 0.077 0.017 0.099 1.496 0.025 0.004 0.033 0.    0.018
  0.008 0.009 0.004 0.085 0.003 0.057 0.06  0.059 0.04  0.07  1.744 0.038
  0.    0.035 0.08  0.061 0.01  0.053]
 [0.003 0.012 0.047 0.063 0.179 0.043 0.009 2.526 0.04  0.005 0.    0.304
  0.001 0.009 0.    0.154 0.004 1.164 0.071 0.037 0.005 0.003 0.17  1.564
  0.002 0.002 0.216 0.029 2.218 0.069]
 [0.001 0.002 0.04  0.013 0.181 0.017 0.032 0.066 0.002 0.    0.001 0.002
  0.    0.    0.    0.002 0.    0.623 0.057 0.03  0.001 0.001 0.011 0.003
  0.    0.    0.004 0.009 0.043 0.002]
 [0.    0.    0.036 0.088 0.012 0.035 0.029 1.363 0.166 0.001 0.    0.007
  0.001 0.    0.001 0.02  0.001 0.035 0.041 0.055 0.044 0.005 0.013 0.02
  0.    0.014 0.006 0.003 0.024 0.014]
 [0.001 0.    0.036 0.003 0.052 0.112 0.016 0.006 0.013 0.    0.    0.002
  0.001 0.001 0.    0.02  0.    0.002 0.009 0.059 0.    0.    0.047 0.001
  0.    0.    0.001 0.001 0.002 0.002]
 [0.002 0.002 0.03  0.079 0.062 0.618 0.201 0.048 0.036 0.002 0.    0.018
  0.    0.    0.001 0.001 0.    0.19  1.028 0.037 0.007 0.    0.044 0.387
  0.001 0.    0.041 0.016 0.068 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.348 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.878 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.682 1.88  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.831 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.483 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.755 1.091 0.    0.    1.785 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.135 0.    0.641 0.    0.    0.    2.467
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.595 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.9   0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    4.127 0.    0.   ]
 [0.    0.    4.289 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.423 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.879 0.    1.181 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.359
  0.    0.    0.459 0.    0.    3.057]
 [0.    0.    0.    0.    1.229 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.921 0.    1.342 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.739
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.756
  0.    3.168 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.455 0.
  0.    0.    0.781 0.    0.    0.   ]
 [0.    0.    2.54  0.    0.    0.    0.    0.    0.458 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    2.852 0.402 0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.535 0.    0.    1.12  0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.841 0.    0.    1.728 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    2.129 0.   ]
 [0.    0.    0.    1.341 1.125 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.696 0.    0.    0.331 0.    0.    0.
  0.    0.    0.    0.    1.58  2.036]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.619 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.181 0.    0.    0.    2.441 0.
  0.    0.    0.379 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.496 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.744 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    2.526 0.    0.    0.    0.304
  0.    0.    0.    0.    0.    1.164 0.    0.    0.    0.    0.    1.564
  0.    0.    0.    0.    2.218 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.623 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.363 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.618 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.028 0.    0.    0.    0.    0.387
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.11475409836065574, 'tpr': 0.9, 'fpr': 0.018666666666666668, 'f1': 0.8925619834710743, 'shd': 9, 'npred': 61, 'ntrue': 60}
[1.511e-03 9.943e-02 4.514e-02 5.799e-02 6.200e-03 1.148e-01 2.253e-02
 5.873e-02 3.481e-01 8.610e-03 5.900e-02 5.477e-03 2.294e-03 1.016e-02
 2.833e-02 2.530e-03 5.046e-02 5.535e-02 8.782e-01 4.212e-02 7.277e-03
 1.421e-01 2.310e-02 1.408e-03 3.461e-02 2.204e-02 1.846e-02 2.259e-02
 2.262e-02 3.000e-03 1.777e-02 4.056e-03 1.207e-02 6.820e-01 1.880e+00
 8.714e-02 1.120e-01 5.109e-03 3.948e-04 3.386e-02 1.421e-02 6.098e-04
 2.666e-03 5.965e-02 1.241e-03 7.328e-02 2.634e-02 5.194e-02 5.696e-03
 4.515e-03 1.018e-01 5.443e-02 3.561e-03 1.266e-02 2.101e-03 8.139e-02
 8.547e-02 3.118e-02 2.460e-05 3.151e-04 1.016e-02 1.074e-01 3.844e-04
 6.873e-02 4.978e-02 1.083e-03 6.169e-05 6.910e-05 5.965e-05 7.519e-05
 6.536e-05 2.137e-06 4.580e-03 8.566e-05 1.427e-02 5.731e-02 1.230e-01
 7.155e-04 1.827e-04 9.342e-03 2.060e-02 3.753e-04 1.339e-04 1.850e-02
 1.308e-02 2.293e-02 7.166e-03 6.588e-05 2.919e-04 5.945e-03 2.036e-02
 2.472e-03 2.064e-02 4.721e-04 7.701e-04 1.530e-04 7.801e-06 8.875e-04
 7.033e-05 1.759e-04 1.496e-06 1.625e-03 1.158e-04 7.082e-05 9.959e-04
 5.196e-02 4.607e-04 1.116e-04 1.074e-03 1.107e-03 1.211e-04 4.989e-05
 1.103e-04 5.962e-05 5.897e-02 4.532e-04 3.424e-04 4.145e-04 2.624e-03
 6.742e-03 3.892e-03 1.325e-02 1.245e-02 1.744e-03 1.694e-04 3.271e-05
 3.034e-04 7.545e-05 9.397e-05 7.346e-05 6.004e-04 1.151e-04 6.790e-04
 9.420e-04 1.615e-01 7.776e-04 8.871e-05 6.851e-03 1.191e-03 2.991e-04
 1.672e-04 1.125e-03 8.886e-03 1.253e-02 1.151e-03 9.950e-05 1.283e-04
 5.108e-02 4.203e-02 1.653e-02 4.909e-02 4.416e-02 8.318e-02 1.381e-04
 6.770e-04 6.729e-02 2.007e-05 4.219e-04 3.337e-05 3.474e-03 1.957e-05
 6.280e-03 3.928e-03 8.309e-01 6.238e-04 1.374e-04 2.999e-02 8.851e-03
 1.128e-03 5.782e-04 1.104e-02 3.071e-03 3.232e-03 1.169e-03 1.805e-04
 2.806e-05 1.043e-02 2.740e-02 2.576e-02 2.326e-03 2.005e-02 5.033e-03
 1.215e-04 6.096e-04 5.547e-04 3.443e-05 1.425e-03 4.891e-05 1.689e-02
 1.593e-03 9.583e-03 8.542e-03 9.357e-02 1.015e-03 1.290e-04 8.908e-02
 4.252e-02 2.577e-05 7.924e-04 3.772e-03 9.214e-03 2.078e-02 2.262e-03
 6.763e-04 4.115e-04 9.028e-03 1.483e+00 9.137e-03 1.372e-02 2.697e-02
 6.109e-04 1.075e-04 5.566e-06 1.257e-03 4.243e-05 7.154e-05 2.665e-06
 2.689e-03 8.528e-05 3.571e-04 9.073e-04 8.980e-03 6.241e-04 2.107e-04
 5.241e-03 5.272e-04 6.472e-05 1.078e-04 8.591e-04 3.264e-04 4.268e-02
 1.992e-03 5.868e-04 2.458e-04 5.093e-02 9.424e-03 4.210e-02 5.429e-03
 4.304e-02 5.823e-02 1.122e-03 2.976e-05 5.949e-02 1.344e-05 4.131e-04
 3.244e-04 9.291e-02 4.412e-06 4.648e-02 7.553e-01 1.091e+00 2.355e-04
 5.781e-04 1.785e+00 7.894e-02 3.849e-04 4.129e-03 5.524e-02 1.300e-03
 5.579e-02 8.700e-03 5.655e-04 4.202e-04 1.262e-01 1.404e-01 2.928e-02
 1.135e+00 5.555e-02 6.414e-01 9.063e-02 3.863e-04 2.467e+00 6.656e-04
 3.258e-03 4.155e-04 2.589e-01 4.359e-04 1.798e-01 2.590e-02 9.906e-02
 9.201e-03 3.082e-03 6.084e-02 4.796e-03 7.008e-03 1.356e-02 5.952e-01
 2.103e-02 1.318e-02 2.497e-02 7.827e-03 4.449e-04 2.884e-02 1.864e-01
 3.319e-02 1.846e-02 8.999e-02 7.979e-02 9.001e-01 1.696e-02 3.631e-02
 4.782e-03 4.167e-03 1.766e-02 2.946e-02 3.943e-04 5.562e-02 6.432e-02
 1.027e-01 5.109e-02 1.304e-02 3.295e-02 6.811e-02 3.264e-04 3.812e-02
 9.349e-02 4.127e+00 4.141e-02 1.475e-02 1.625e-04 4.892e-04 4.289e+00
 1.167e-02 2.967e-02 9.495e-03 1.929e-02 5.642e-02 1.261e-02 1.116e-04
 3.118e-04 7.295e-04 5.306e-04 3.842e-05 1.274e-01 5.490e-05 8.853e-02
 3.440e-02 1.507e-01 1.769e-03 1.308e-03 4.848e-03 6.455e-02 2.493e-04
 2.333e-04 4.234e-01 2.410e-02 2.889e-02 1.142e-02 4.706e-03 4.323e-03
 2.646e-04 5.316e-02 4.773e-02 9.286e-03 1.879e+00 1.291e-02 1.181e+00
 1.961e-02 3.864e-04 5.162e-02 5.425e-04 8.687e-03 1.218e-01 3.654e-03
 9.607e-02 3.285e-03 8.211e-03 2.199e-02 2.226e-02 6.954e-02 3.586e-01
 8.780e-04 3.838e-02 4.592e-01 3.950e-02 2.037e-02 3.057e+00 5.958e-04
 2.985e-03 8.230e-02 2.764e-03 1.229e+00 4.209e-02 2.368e-02 1.290e-01
 6.448e-02 4.879e-04 1.470e-03 6.981e-02 2.749e-03 5.151e-03 1.121e-01
 3.398e-03 9.207e-01 3.113e-02 1.342e+00 1.957e-02 6.250e-03 4.433e-02
 3.805e-02 3.449e-03 5.273e-03 1.368e-01 6.786e-02 1.111e-02 8.733e-03
 1.590e-03 9.692e-03 3.185e-02 5.742e-02 6.025e-02 6.023e-02 3.809e-02
 1.025e-01 4.984e-02 2.473e-03 5.543e-03 1.739e+00 1.158e-02 6.565e-03
 1.247e-02 4.112e-04 1.867e-01 2.237e-02 7.075e-03 1.587e-02 1.330e-02
 1.164e-01 7.563e-01 3.001e-03 3.168e+00 7.810e-03 1.398e-02 1.213e-01
 5.673e-02 4.555e-04 6.595e-04 4.793e-02 4.722e-03 7.129e-02 3.156e-02
 1.946e-02 5.779e-02 1.309e-02 5.162e-04 5.459e-04 4.946e-03 2.681e-03
 6.225e-04 2.625e-04 4.444e-04 9.007e-02 1.345e-02 2.746e-02 3.478e-03
 1.798e-03 4.550e-01 1.227e-02 5.737e-04 5.963e-03 7.805e-01 5.140e-03
 8.501e-03 7.394e-02 3.477e-03 5.950e-05 2.540e+00 9.293e-02 8.875e-02
 1.911e-02 3.386e-02 1.310e-02 4.578e-01 2.428e-04 2.435e-03 3.976e-03
 4.224e-03 2.793e-03 6.718e-04 1.325e-02 1.199e-01 4.440e-03 3.477e-02
 2.852e+00 4.025e-01 1.243e-01 1.625e-02 1.349e-03 5.882e-03 4.756e-02
 5.051e-02 7.404e-02 4.233e-02 5.553e-04 3.712e-04 1.408e-02 5.875e-02
 1.535e+00 8.602e-02 1.783e-02 1.120e+00 1.617e-03 5.845e-05 5.225e-05
 3.601e-04 1.175e-04 1.220e-04 1.985e-05 3.315e-03 1.095e-04 2.541e-03
 9.311e-02 2.651e-03 3.849e-04 1.541e-02 7.553e-04 5.205e-05 5.647e-04
 3.316e-03 1.077e-02 2.066e-01 1.389e-03 4.745e-04 4.495e-04 7.027e-03
 2.544e-02 2.169e-02 8.322e-03 6.439e-03 3.656e-02 1.008e-03 1.198e-04
 1.172e-04 5.819e-04 9.215e-05 1.063e-03 7.001e-05 8.833e-04 2.409e-05
 1.369e-01 1.041e-02 1.979e-04 6.395e-05 1.788e-02 6.710e-04 3.222e-04
 1.646e-04 1.024e-02 5.956e-03 3.520e-02 5.601e-04 9.244e-05 4.382e-05
 4.254e-03 4.460e-03 6.419e-04 1.009e-03 6.753e-03 3.055e-04 6.897e-04
 8.250e-05 3.542e-05 4.967e-04 1.902e-05 3.232e-05 1.395e-04 2.104e-02
 7.746e-05 9.496e-04 1.400e-02 1.502e-04 1.532e-04 7.481e-02 9.348e-03
 5.563e-04 4.161e-04 5.765e-03 2.575e-03 8.662e-03 1.246e-03 1.814e-03
 2.584e-04 3.665e-02 2.951e-04 5.140e-03 8.414e-01 1.418e-02 3.017e-02
 1.728e+00 5.567e-03 6.099e-05 6.305e-02 2.417e-03 4.099e-04 4.037e-04
 1.486e-02 1.854e-05 7.431e-02 2.407e-02 8.183e-02 1.488e-03 1.197e-02
 2.939e-02 2.258e-03 1.887e-02 1.034e-02 1.244e-02 2.129e+00 3.628e-02
 3.360e-04 9.133e-03 4.420e-02 1.341e+00 1.125e+00 2.283e-02 5.150e-02
 1.203e-01 2.395e-01 4.694e-03 1.907e-03 3.891e-02 3.016e-03 1.981e-03
 8.289e-03 5.719e-02 3.927e-05 6.964e-01 6.915e-02 6.753e-02 3.306e-01
 1.003e-01 1.998e-01 4.168e-04 2.721e-02 2.415e-01 4.959e-02 1.580e+00
 2.036e+00 7.756e-04 4.190e-05 3.565e-03 1.201e-01 5.841e-02 1.905e-03
 1.903e-03 1.023e-01 2.040e-04 2.548e-04 3.030e-05 2.015e-03 3.775e-05
 1.949e-04 1.468e-06 4.691e-03 8.953e-06 9.332e-03 2.728e-02 4.399e-04
 5.294e-05 4.111e-05 1.719e-04 1.744e-05 1.096e-04 1.668e-01 4.258e-04
 5.647e-03 8.382e-05 2.382e-04 2.156e-04 3.993e-03 6.187e-01 1.352e-03
 2.229e-02 1.270e-02 8.564e-02 8.281e-03 4.046e-04 9.589e-04 4.106e-03
 1.053e-04 5.038e-04 9.084e-06 2.463e-02 7.119e-05 1.274e-01 1.181e+00
 2.434e-02 3.947e-03 1.096e-04 2.441e+00 3.337e-04 3.265e-04 3.793e-01
 4.753e-03 1.780e-01 4.643e-04 3.384e-03 5.752e-03 5.830e-02 7.654e-02
 1.664e-02 9.900e-02 1.496e+00 2.549e-02 3.738e-03 3.332e-02 4.196e-04
 1.814e-02 7.782e-03 9.327e-03 4.312e-03 8.476e-02 3.077e-03 5.703e-02
 5.958e-02 5.895e-02 4.007e-02 6.960e-02 1.744e+00 3.839e-02 3.469e-02
 8.031e-02 6.147e-02 9.885e-03 5.343e-02 2.522e-03 1.241e-02 4.687e-02
 6.288e-02 1.786e-01 4.334e-02 8.741e-03 2.526e+00 4.000e-02 5.097e-03
 4.131e-04 3.037e-01 1.229e-03 9.186e-03 1.258e-05 1.538e-01 3.710e-03
 1.164e+00 7.136e-02 3.670e-02 5.136e-03 2.660e-03 1.697e-01 1.564e+00
 1.914e-03 2.162e-01 2.884e-02 2.218e+00 6.943e-02 5.445e-04 1.594e-03
 4.014e-02 1.284e-02 1.810e-01 1.669e-02 3.208e-02 6.613e-02 1.826e-03
 2.023e-04 6.127e-04 2.117e-03 5.195e-05 4.602e-04 2.099e-05 2.328e-03
 9.129e-05 6.226e-01 5.652e-02 3.003e-02 5.850e-04 8.776e-04 1.145e-02
 2.781e-03 5.683e-05 3.436e-04 9.334e-03 4.252e-02 2.061e-03 3.418e-04
 4.418e-04 3.612e-02 8.828e-02 1.153e-02 3.503e-02 2.908e-02 1.363e+00
 1.657e-01 6.435e-04 1.834e-06 6.716e-03 1.197e-03 3.236e-04 5.434e-04
 1.961e-02 8.605e-04 3.527e-02 4.054e-02 5.455e-02 4.361e-02 4.809e-03
 1.276e-02 1.952e-02 4.666e-04 1.434e-02 5.584e-03 2.411e-02 1.397e-02
 5.898e-04 2.858e-04 3.608e-02 2.715e-03 5.165e-02 1.121e-01 1.618e-02
 5.806e-03 1.315e-02 4.626e-04 2.637e-05 1.813e-03 9.016e-04 1.172e-03
 1.694e-06 1.989e-02 7.699e-06 2.071e-03 8.639e-03 5.927e-02 1.221e-04
 9.656e-05 4.730e-02 1.308e-03 3.407e-04 1.446e-04 8.793e-04 5.908e-04
 2.046e-03 2.358e-03 2.204e-03 2.996e-02 7.935e-02 6.219e-02 6.184e-01
 2.015e-01 4.791e-02 3.637e-02 2.346e-03 3.775e-04 1.847e-02 8.871e-06
 3.891e-04 7.251e-04 1.229e-03 2.175e-04 1.904e-01 1.028e+00 3.743e-02
 7.432e-03 1.108e-04 4.377e-02 3.871e-01 6.811e-04 4.275e-04 4.080e-02
 1.572e-02 6.811e-02]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9581481481481482, 0.9230829319887145)
cuda
9630
cuda
Objective function 291.21 = squared loss an data 35.20 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 416 ; DAG False
||w||^2 0.008169982152128039
exp ma of ||w||^2 4.594111664789125
||w|| 0.09038795357860492
exp ma of ||w|| 0.12374466720717543
||w||^2 0.02029060368732079
exp ma of ||w||^2 0.024039460363277032
||w|| 0.14244509007796932
exp ma of ||w|| 0.15327967548754895
||w||^2 0.024367374702357072
exp ma of ||w||^2 0.02441849538602763
||w|| 0.15610052755310302
exp ma of ||w|| 0.15429834646337176
||w||^2 0.03250612828177516
exp ma of ||w||^2 0.02588116448108221
||w|| 0.1802945597675514
exp ma of ||w|| 0.15908724742561436
||w||^2 0.01820270223762217
exp ma of ||w||^2 0.028154780188992725
||w|| 0.13491739041955328
exp ma of ||w|| 0.16586402448225715
||w||^2 0.027040959644428153
exp ma of ||w||^2 0.034430099771820405
||w|| 0.16444135624722922
exp ma of ||w|| 0.18296463515386607
||w||^2 0.03201801164377807
exp ma of ||w||^2 0.03861696011515071
||w|| 0.17893577519260387
exp ma of ||w|| 0.19396378192283842
cuda
Objective function 31.07 = squared loss an data 29.52 + 0.5*rho*h**2 0.840036 + alpha*h 0.000000 + L2reg 0.25 + L1reg 0.46 ; SHD = 79 ; DAG True
Proportion of microbatches that were clipped  0.7027627818355033
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.2961758585607583
iteration 1 in outer loop, alpha = 1.2961758585607583, rho = 1.0, h = 1.2961758585607583
cuda
9630
cuda
Objective function 32.75 = squared loss an data 29.52 + 0.5*rho*h**2 0.840036 + alpha*h 1.680072 + L2reg 0.25 + L1reg 0.46 ; SHD = 79 ; DAG True
||w||^2 3922.2085494099833
exp ma of ||w||^2 266316.1319667715
||w|| 62.62753826720306
exp ma of ||w|| 257.8485431576775
||w||^2 1505.4227199264058
exp ma of ||w||^2 155335.2787394847
||w|| 38.79977731800024
exp ma of ||w|| 177.68606073881477
||w||^2 20.13834861083363
exp ma of ||w||^2 6706.0556537072025
||w|| 4.487577142605309
exp ma of ||w|| 22.094853389858056
||w||^2 0.029578950453814656
exp ma of ||w||^2 0.02275512177472583
||w|| 0.17198532046024934
exp ma of ||w|| 0.14942201547897316
||w||^2 0.04015850506001395
exp ma of ||w||^2 0.027816522493585176
||w|| 0.20039587086567914
exp ma of ||w|| 0.16456451534245298
cuda
Objective function 31.74 = squared loss an data 28.93 + 0.5*rho*h**2 0.631962 + alpha*h 1.457218 + L2reg 0.27 + L1reg 0.45 ; SHD = 78 ; DAG True
Proportion of microbatches that were clipped  0.6993347395748823
iteration 1 in inner loop, alpha 1.2961758585607583 rho 1.0 h 1.1242437005420527
9630
cuda
Objective function 37.43 = squared loss an data 28.93 + 0.5*rho*h**2 6.319619 + alpha*h 1.457218 + L2reg 0.27 + L1reg 0.45 ; SHD = 78 ; DAG True
||w||^2 0.023716968483356547
exp ma of ||w||^2 0.023477873567041146
||w|| 0.15400314439438095
exp ma of ||w|| 0.15170072413049182
||w||^2 0.022627573816624124
exp ma of ||w||^2 0.030619627300902313
||w|| 0.1504246449775572
exp ma of ||w|| 0.17227013572011193
||w||^2 0.02850389480705125
exp ma of ||w||^2 0.037047643578318606
||w|| 0.1688309651901903
exp ma of ||w|| 0.18941893559133918
cuda
Objective function 30.83 = squared loss an data 29.05 + 0.5*rho*h**2 0.704501 + alpha*h 0.486541 + L2reg 0.23 + L1reg 0.36 ; SHD = 60 ; DAG True
Proportion of microbatches that were clipped  0.7071840423874084
iteration 2 in inner loop, alpha 1.2961758585607583 rho 10.0 h 0.375366806651666
9630
cuda
Objective function 37.17 = squared loss an data 29.05 + 0.5*rho*h**2 7.045012 + alpha*h 0.486541 + L2reg 0.23 + L1reg 0.36 ; SHD = 60 ; DAG True
||w||^2 8128714061.928814
exp ma of ||w||^2 3032149127.38772
||w|| 90159.38144158274
exp ma of ||w|| 32530.960974886257
||w||^2 9312564251.850906
exp ma of ||w||^2 3761557209.9478693
||w|| 96501.62823419564
exp ma of ||w|| 40378.99255192633
||w||^2 89697020.68883578
exp ma of ||w||^2 410548943.2834995
||w|| 9470.85110688769
exp ma of ||w|| 16870.802441290452
||w||^2 19665.60092188484
exp ma of ||w||^2 418850.53616288985
||w|| 140.234093293624
exp ma of ||w|| 353.64989648081075
||w||^2 45.925709903969015
exp ma of ||w||^2 10616.235654554062
||w|| 6.776851031560972
exp ma of ||w|| 28.072690214781435
||w||^2 0.14030362832246818
exp ma of ||w||^2 3.6864666118171914
||w|| 0.3745712593385512
exp ma of ||w|| 0.3720019923328315
||w||^2 0.03240886238672525
exp ma of ||w||^2 0.6257491490188406
||w|| 0.1800246160577082
exp ma of ||w|| 0.2632173311459988
||w||^2 0.03360370644748032
exp ma of ||w||^2 0.15850015858809882
||w|| 0.1833131376838014
exp ma of ||w|| 0.20617226853423676
||w||^2 0.022623541631331776
exp ma of ||w||^2 0.025604715747652926
||w|| 0.15041124170530532
exp ma of ||w|| 0.15784183148050404
cuda
Objective function 30.52 = squared loss an data 29.31 + 0.5*rho*h**2 0.583357 + alpha*h 0.140006 + L2reg 0.21 + L1reg 0.28 ; SHD = 61 ; DAG True
Proportion of microbatches that were clipped  0.7051543161522795
iteration 3 in inner loop, alpha 1.2961758585607583 rho 100.0 h 0.10801455517620084
iteration 2 in outer loop, alpha = 12.097631376180843, rho = 100.0, h = 0.10801455517620084
cuda
9630
cuda
Objective function 31.69 = squared loss an data 29.31 + 0.5*rho*h**2 0.583357 + alpha*h 1.306720 + L2reg 0.21 + L1reg 0.28 ; SHD = 61 ; DAG True
||w||^2 46467214.38213135
exp ma of ||w||^2 202306211.77436545
||w|| 6816.686466468247
exp ma of ||w|| 11942.544054622345
||w||^2 1956.3831468756978
exp ma of ||w||^2 87303.56601897265
||w|| 44.231020188050124
exp ma of ||w|| 108.69742318705552
||w||^2 0.04835500921778035
exp ma of ||w||^2 6.123274855334191
||w|| 0.21989772444884542
exp ma of ||w|| 0.39239576201192783
||w||^2 0.053428775038251025
exp ma of ||w||^2 4.5915122351723765
||w|| 0.2311466526650365
exp ma of ||w|| 0.35709842656671137
||w||^2 0.06101021890362069
exp ma of ||w||^2 1.6208247567754426
||w|| 0.24700246740391216
exp ma of ||w|| 0.28081678241312735
||w||^2 0.04837163146204986
exp ma of ||w||^2 0.1869925522491625
||w|| 0.21993551659986585
exp ma of ||w|| 0.21579639811746748
||w||^2 0.03802984581120743
exp ma of ||w||^2 0.039642392302139194
||w|| 0.19501242476110958
exp ma of ||w|| 0.1916479287124095
||w||^2 0.01701884863450382
exp ma of ||w||^2 0.02699507991862785
||w|| 0.13045630929358618
exp ma of ||w|| 0.16270260470039605
||w||^2 0.017662043874570297
exp ma of ||w||^2 0.03458899934958897
||w|| 0.1328986225457973
exp ma of ||w|| 0.18322089224505012
cuda
Objective function 30.94 = squared loss an data 29.24 + 0.5*rho*h**2 0.289962 + alpha*h 0.921268 + L2reg 0.23 + L1reg 0.26 ; SHD = 58 ; DAG True
Proportion of microbatches that were clipped  0.6954612005856515
iteration 1 in inner loop, alpha 12.097631376180843 rho 100.0 h 0.07615273297962943
9630
cuda
Objective function 33.55 = squared loss an data 29.24 + 0.5*rho*h**2 2.899619 + alpha*h 0.921268 + L2reg 0.23 + L1reg 0.26 ; SHD = 58 ; DAG True
||w||^2 2585.5875250500385
exp ma of ||w||^2 184932.27994576894
||w|| 50.848672795364465
exp ma of ||w|| 167.95882459271462
||w||^2 1.8083252316450689
exp ma of ||w||^2 324.5973374828754
||w|| 1.3447398379036255
exp ma of ||w|| 2.659971717059453
||w||^2 0.06716205851980765
exp ma of ||w||^2 0.09908372970478704
||w|| 0.25915643638506775
exp ma of ||w|| 0.26544051667182106
v before min max tensor([[ 7.750e+02, -1.121e+01, -1.918e+00,  ..., -6.448e+00, -1.080e+01,
          1.300e+01],
        [ 1.106e+02, -4.558e+00, -1.302e+01,  ...,  2.986e+01, -5.465e+00,
          1.513e+02],
        [-1.131e+01, -6.888e+00, -1.240e+01,  ..., -7.883e+00,  1.154e+01,
         -2.653e+00],
        ...,
        [ 4.934e+00, -1.136e+01, -9.787e+00,  ..., -2.686e+00,  3.656e-01,
          1.254e+02],
        [-1.619e+00, -1.390e+01,  2.720e+01,  ...,  6.851e+00, -1.331e+01,
          1.340e+01],
        [-1.481e+01, -7.207e+00, -1.039e+01,  ...,  4.073e+01,  3.306e+01,
         -1.361e+01]], device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e-12],
        ...,
        [4.934e+00, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 3.656e-01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 6.851e+00, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e-12]], device='cuda:0')
v before min max tensor([ -7.481,  -9.437,  34.065,  -6.009, -11.691,  -7.512,  10.422,  -4.144,
         15.205,  -9.254,  45.592, -12.691, -12.837,  -2.979,  -2.478,  62.335,
        -12.099, -11.822,  -2.628, -12.522,  -7.211,  -2.682, -13.977,  -3.032,
        -12.990, -10.738,   1.597,  -2.372,  -5.828,  -8.060,  -0.585,  -0.919,
        -10.152,  -6.295, -11.765,  17.312,  30.794,   6.403, -10.417,   3.501,
         -2.436, -12.484,  -1.991,  -5.507,  -1.022, -10.384,  -2.731, -11.555,
        131.063,  -6.226,  28.732, -14.289, -10.067,  -7.191,  22.015,  -4.662,
         10.195,  -6.888,  -9.301, -11.931,  -9.865,  -6.537,  63.577,   4.860,
         -7.342,  -3.134,   3.167,   2.226,  11.238,   9.075,  -6.355,  -7.267,
         -5.309,   1.934,  -4.365,  15.735,  -1.639,  -8.383, -14.148,  -8.083,
         -1.169, -10.369,  -5.153,  -9.362,  -6.222, -10.458, -10.604, -11.487,
          5.463,  -7.404, -14.863,  43.906,   1.116, -10.781,  -0.142, -13.533,
        -14.414,  -2.559,  11.361, -11.664,  50.443,  42.692,  13.499,  -6.204,
          4.813,  47.584,  -4.001, -11.507,  -0.755,   6.508,   7.066,  -9.225,
        -12.807,  -8.963,  22.390, -13.216,  18.285,   2.575,  11.925,   3.166,
         -7.985, -12.029,  -0.529,   4.546,  -5.959, -11.578,  -1.673, -11.593,
         -2.961,  -1.714,  -7.436, -11.043, -11.315,  11.531, -11.939,  -9.901,
         28.994, -15.718,  31.263,  -5.176,  -7.798, -13.860,   6.151,   6.535,
         -8.287,  -8.678, -13.263,  -1.585,  18.483,  -6.722, -13.178,   0.935,
         -0.177,  59.162,  -3.990,   2.424,  -7.057, -10.874, -11.749,  -1.003,
          3.260,  -3.118,  -9.994, -12.264,  -7.115,   8.580,  -4.494,   1.576,
        -11.006,   4.390, -10.304,  15.207, -12.110,  -6.065,  22.568,  10.574,
         -7.993,  -7.320,  -2.253,  -8.873,  -8.494, -12.097,   6.711,  40.002,
         33.437, -12.675, -13.698,  10.217,  -5.228,   2.184, -11.193,  -8.612,
         -7.761,  -9.350,  11.109,   4.932,  34.170,  -6.784,  -0.530, -15.182,
         -9.087,  -5.074, -11.954, -11.565,  -7.220,  18.071, -13.509, -12.127,
        -10.000,  -8.673,   2.533, -14.465, -13.820,  17.315,  -0.645,  67.179,
         -9.019,   8.142,  -9.075,  -7.764,  -2.699,  -9.873,  -9.601, -15.707,
        -11.136,  -9.678,  44.035,  23.591, -12.601, -12.154,  20.534,  -5.501,
         -1.572,  -5.488, -13.492,   3.196,  -5.419,  -2.170,  -9.368, -11.445,
         -3.165, -12.622,  29.248, -11.989, -12.719,  -7.655,  11.984, -10.186,
         -5.372,  -3.361,  -7.697,  -5.075,   3.086,   7.085,  -5.781,  -4.271,
          2.209, -10.003, -10.550,   8.437,  -8.730,  -1.184, -13.945,  -8.259,
        -10.442,  -4.786, -15.022, -11.787, -10.819, -11.978, 113.984,  -4.444,
        -15.882,   7.313,  -6.477,  -2.598, -12.067,  -4.445,   0.418,  -3.603,
         -6.051,  -0.912,  -7.369,  -8.942,  -4.930,   9.232,   8.995,  40.544,
        -13.372,  -6.540, -15.364, -12.093,  -8.934, -14.310,  -9.520,   9.471,
         11.376,  -9.114,  24.557,  -7.061], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.597e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 6.403e+00, 1.000e-12, 3.501e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 4.860e+00, 1.000e-12, 1.000e-12,
        3.167e+00, 2.226e+00, 1.000e+01, 9.075e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.934e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 5.463e+00, 1.000e-12,
        1.000e-12, 1.000e+01, 1.116e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 4.813e+00, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 6.508e+00, 7.066e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 2.575e+00, 1.000e+01, 3.166e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 4.546e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 6.151e+00, 6.535e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 9.351e-01, 1.000e-12, 1.000e+01, 1.000e-12, 2.424e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.260e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 8.580e+00, 1.000e-12, 1.576e+00,
        1.000e-12, 4.390e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 6.711e+00, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 2.184e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 4.932e+00, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        2.533e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 8.142e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 3.196e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        3.086e+00, 7.085e+00, 1.000e-12, 1.000e-12, 2.209e+00, 1.000e-12,
        1.000e-12, 8.437e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 7.313e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 4.177e-01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 9.232e+00, 8.995e+00, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 9.471e+00, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 23.701],
         [ -7.455],
         [ 29.242],
         [ -7.593],
         [ -1.564],
         [ 18.250],
         [-14.379],
         [  4.469],
         [-10.659],
         [  6.720]],

        [[-11.748],
         [-12.829],
         [ -6.013],
         [-11.400],
         [ -5.836],
         [ -3.895],
         [  2.636],
         [ -8.357],
         [ 11.063],
         [ 19.134]],

        [[ -6.747],
         [-10.581],
         [-10.784],
         [-10.479],
         [-10.785],
         [ -6.852],
         [  2.004],
         [-10.610],
         [ -0.787],
         [ -4.654]],

        [[-13.344],
         [  5.424],
         [  6.950],
         [ 24.930],
         [ -0.775],
         [ 49.718],
         [ -7.386],
         [ -9.603],
         [  8.513],
         [  7.711]],

        [[ -2.650],
         [  1.631],
         [  6.905],
         [ 16.425],
         [  8.183],
         [-15.711],
         [-12.548],
         [-14.893],
         [-14.179],
         [ -9.413]],

        [[  0.797],
         [-10.865],
         [ -3.317],
         [-10.644],
         [ -9.383],
         [ -2.600],
         [  1.166],
         [ -9.133],
         [ -8.270],
         [ 67.974]],

        [[ 33.025],
         [-10.919],
         [-13.659],
         [ 10.009],
         [ -0.134],
         [-10.994],
         [ -4.511],
         [-10.925],
         [ -9.878],
         [  7.791]],

        [[ -5.617],
         [-12.768],
         [ 27.979],
         [ -9.194],
         [ -3.135],
         [ 14.928],
         [ -6.502],
         [ -8.268],
         [ -6.109],
         [  8.626]],

        [[ -5.044],
         [-12.333],
         [ -8.797],
         [ -8.242],
         [-11.603],
         [-12.314],
         [ 15.984],
         [  8.152],
         [ 37.455],
         [  9.891]],

        [[-15.446],
         [ -6.110],
         [  4.562],
         [-13.838],
         [ -9.471],
         [ -8.179],
         [ -1.624],
         [ -9.238],
         [ -5.605],
         [ 19.030]],

        [[-12.702],
         [ -8.615],
         [-11.111],
         [ -9.018],
         [ -7.208],
         [  4.267],
         [ -7.075],
         [ -5.068],
         [ -4.743],
         [ 10.866]],

        [[ -3.851],
         [-10.676],
         [ -8.484],
         [  8.214],
         [ -4.589],
         [ -8.926],
         [  4.350],
         [ -2.216],
         [-10.095],
         [-12.005]],

        [[ -4.358],
         [-10.305],
         [  1.539],
         [  1.219],
         [ -6.673],
         [ 35.763],
         [ -4.109],
         [ -0.125],
         [ -9.618],
         [ -0.285]],

        [[-12.700],
         [ 38.979],
         [ -8.135],
         [-11.040],
         [ -8.817],
         [-10.773],
         [-12.740],
         [ -8.141],
         [-14.207],
         [ -9.039]],

        [[-14.353],
         [ -7.722],
         [ 20.257],
         [ -7.670],
         [ -4.957],
         [ -8.857],
         [ -1.628],
         [-11.705],
         [  4.719],
         [ -1.886]],

        [[-13.044],
         [-12.641],
         [-16.636],
         [  7.113],
         [ -8.522],
         [-13.778],
         [-10.172],
         [ -2.548],
         [ -6.609],
         [ -6.797]],

        [[-10.288],
         [-15.530],
         [ -2.651],
         [ -0.992],
         [-12.733],
         [-10.200],
         [ -1.696],
         [ -6.803],
         [ -8.967],
         [ -7.376]],

        [[ -9.182],
         [ -7.373],
         [-10.780],
         [-12.177],
         [ 71.033],
         [ -9.702],
         [ -0.327],
         [ -5.719],
         [ 18.143],
         [ -7.516]],

        [[  0.537],
         [-14.160],
         [ -2.020],
         [ 19.257],
         [ -7.831],
         [-10.228],
         [ -0.332],
         [ -8.650],
         [ 27.658],
         [ 31.194]],

        [[ 15.028],
         [-10.493],
         [-13.025],
         [-10.638],
         [ 24.249],
         [-15.445],
         [  2.361],
         [-10.283],
         [  3.247],
         [ -8.218]],

        [[ -8.281],
         [ -8.295],
         [ -0.806],
         [ -6.356],
         [  4.525],
         [-10.683],
         [ 11.427],
         [-15.164],
         [-13.682],
         [ -0.845]],

        [[ -8.760],
         [ -6.605],
         [  7.511],
         [  4.489],
         [-11.500],
         [ -8.946],
         [-10.741],
         [ -5.712],
         [-12.877],
         [-11.514]],

        [[ -7.625],
         [-10.541],
         [  4.058],
         [-12.112],
         [ -8.187],
         [-11.251],
         [ -5.672],
         [ -2.193],
         [ -3.405],
         [-14.539]],

        [[-10.738],
         [-12.269],
         [ -9.527],
         [ -9.170],
         [-10.956],
         [-13.397],
         [-12.074],
         [-11.443],
         [-11.062],
         [ 37.360]],

        [[ -8.626],
         [ -9.369],
         [-10.582],
         [ 14.077],
         [-13.931],
         [ -5.323],
         [ -6.934],
         [-10.919],
         [  6.967],
         [  6.477]],

        [[-12.188],
         [ -7.802],
         [117.875],
         [-13.640],
         [ -6.720],
         [ -4.907],
         [-17.202],
         [-13.301],
         [ -6.213],
         [ -6.624]],

        [[  7.901],
         [  3.192],
         [ 30.064],
         [-10.316],
         [ 13.556],
         [-11.447],
         [  9.660],
         [ -7.128],
         [-11.344],
         [ -2.378]],

        [[ -9.685],
         [ 11.670],
         [ -9.089],
         [ -2.652],
         [ 54.837],
         [-10.593],
         [ -5.303],
         [ -4.025],
         [-10.319],
         [ 21.243]],

        [[-11.798],
         [ -9.641],
         [-10.645],
         [ -1.269],
         [-10.665],
         [ -3.816],
         [ -0.806],
         [ -7.571],
         [-10.158],
         [  1.638]],

        [[  2.589],
         [ -8.767],
         [ -1.356],
         [-13.194],
         [-12.773],
         [-10.171],
         [ -9.674],
         [ -3.272],
         [ -3.772],
         [-12.994]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [4.469e+00],
         [1.000e-12],
         [6.720e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.636e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.004e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [5.424e+00],
         [6.950e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [8.513e+00],
         [7.711e+00]],

        [[1.000e-12],
         [1.631e+00],
         [6.905e+00],
         [1.000e+01],
         [8.183e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[7.971e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.166e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.791e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.626e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [8.152e+00],
         [1.000e+01],
         [9.891e+00]],

        [[1.000e-12],
         [1.000e-12],
         [4.562e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.267e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.214e+00],
         [1.000e-12],
         [1.000e-12],
         [4.350e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.539e+00],
         [1.219e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.719e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.113e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[5.370e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [2.361e+00],
         [1.000e-12],
         [3.247e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.525e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [7.511e+00],
         [4.489e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [4.058e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.967e+00],
         [6.477e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[7.901e+00],
         [3.192e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [9.660e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.638e+00]],

        [[2.589e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[ -8.931],
        [-11.193],
        [-11.618],
        [-13.658],
        [-10.713],
        [-12.152],
        [ -2.813],
        [ -9.220],
        [  6.638],
        [ -0.063],
        [-10.034],
        [-10.781],
        [ -7.386],
        [ -4.319],
        [ -8.792],
        [  6.397],
        [ 12.601],
        [ -4.195],
        [ -7.177],
        [ -5.948],
        [ -4.238],
        [ -9.873],
        [-11.569],
        [-10.030],
        [  8.089],
        [ -5.019],
        [ 46.232],
        [ -7.767],
        [ -2.767],
        [-15.598]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [6.638e+00],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [6.397e+00],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [8.089e+00],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.075,  0.017, -0.004,  ..., -0.039, -0.031,  0.032],
        [ 0.000, -0.018,  0.026,  ...,  0.026, -0.037, -0.022],
        [ 0.029,  0.023,  0.017,  ..., -0.009, -0.045,  0.012],
        ...,
        [ 0.026,  0.015, -0.032,  ..., -0.033, -0.009,  0.086],
        [-0.037, -0.046,  0.050,  ..., -0.014,  0.043, -0.030],
        [-0.016, -0.052,  0.018,  ...,  0.008, -0.010, -0.026]],
       device='cuda:0')
s after update for 1 param tensor([[2.633, 1.523, 1.681,  ..., 2.013, 1.714, 2.144],
        [2.613, 1.581, 1.698,  ..., 1.952, 1.696, 2.376],
        [2.376, 1.319, 2.058,  ..., 1.032, 2.227, 1.681],
        ...,
        [2.101, 1.488, 1.728,  ..., 1.555, 1.391, 2.657],
        [2.308, 1.840, 2.196,  ..., 2.068, 1.730, 2.581],
        [2.100, 2.018, 2.011,  ..., 1.938, 1.521, 1.935]], device='cuda:0')
b after update for 1 param tensor([[210.700, 160.222, 168.360,  ..., 184.241, 169.986, 190.114],
        [209.885, 163.255, 169.209,  ..., 181.430, 169.075, 200.160],
        [200.165, 149.109, 186.251,  ..., 131.899, 193.753, 168.332],
        ...,
        [188.203, 158.382, 170.677,  ..., 161.935, 153.127, 211.669],
        [197.282, 176.152, 192.408,  ..., 186.724, 170.809, 208.601],
        [188.178, 184.450, 184.149,  ..., 180.739, 160.121, 180.627]],
       device='cuda:0')
clipping threshold 0.25961975651177677
a after update for 1 param tensor([ 2.565e-02, -8.609e-02, -1.799e-02,  2.081e-02, -1.889e-02,  2.127e-03,
         3.530e-02,  9.234e-03,  5.832e-03,  2.072e-02, -3.029e-02, -4.682e-02,
         4.472e-02,  1.760e-02,  1.158e-02, -7.514e-03, -3.859e-02, -7.745e-02,
         9.242e-02,  3.655e-02,  2.301e-02,  5.804e-03, -4.424e-02,  1.465e-02,
        -5.104e-02, -1.903e-02,  2.728e-02, -2.649e-02, -3.183e-03, -5.753e-02,
         2.870e-02, -3.281e-02, -1.260e-03, -2.714e-02,  4.558e-02, -8.189e-03,
         6.659e-02,  4.743e-03,  1.046e-02,  4.261e-02, -3.375e-02,  3.025e-02,
        -1.142e-02, -4.023e-02, -2.489e-02,  2.334e-02,  1.151e-02,  5.657e-03,
        -6.010e-03, -1.840e-02,  1.933e-02, -2.305e-02, -5.111e-02, -2.181e-02,
         4.043e-03, -5.857e-03, -5.712e-02, -3.112e-03, -1.476e-02,  2.897e-02,
        -1.993e-02, -7.644e-02, -8.982e-02,  6.038e-02,  9.275e-03,  1.194e-03,
         5.522e-02,  1.268e-01,  1.147e-02, -4.883e-02,  7.253e-03,  5.277e-03,
         5.586e-03, -1.098e-01, -1.111e-01,  2.793e-02,  3.136e-02, -8.147e-03,
        -1.352e-01, -4.443e-02,  4.934e-03,  1.367e-02, -2.781e-03,  3.282e-02,
         1.711e-02,  2.342e-02, -7.406e-02,  2.599e-02, -8.411e-04,  2.009e-02,
        -1.978e-03,  5.590e-02,  1.426e-02, -9.838e-03,  8.057e-03, -3.999e-02,
         7.235e-02, -4.495e-02, -8.672e-02,  2.550e-02,  1.080e-02, -7.066e-02,
         2.057e-03,  1.985e-02,  2.995e-02,  1.045e-01,  5.263e-03, -2.127e-03,
        -2.613e-02, -4.359e-04,  3.947e-02,  8.836e-02,  1.973e-02, -8.126e-03,
        -2.193e-02, -6.251e-02,  5.627e-03,  6.071e-03, -1.584e-01,  4.071e-03,
        -3.573e-02, -5.430e-02,  4.338e-03, -4.291e-02, -4.793e-02,  4.075e-02,
        -1.307e-02,  7.305e-02,  5.509e-02,  4.927e-02,  2.176e-02,  1.753e-02,
         2.140e-02, -6.171e-02, -3.296e-03,  1.015e-02, -2.241e-02,  6.207e-02,
         2.252e-02,  4.979e-02,  5.299e-03, -4.724e-03, -1.742e-02,  3.811e-02,
        -5.972e-02, -2.438e-02, -2.209e-02,  1.155e-02,  2.277e-02,  4.956e-02,
         2.878e-02,  2.962e-02, -3.053e-02, -3.362e-02,  2.638e-02,  3.204e-03,
         7.166e-02, -2.070e-03,  7.389e-03, -1.888e-02,  8.656e-03, -2.193e-03,
         4.307e-02,  1.363e-01, -2.410e-03,  1.585e-03,  2.292e-02, -1.324e-02,
        -5.187e-02,  1.453e-03,  5.081e-03,  4.229e-02, -8.005e-02,  1.290e-01,
        -2.649e-02,  5.505e-04,  8.790e-03,  3.406e-02, -6.313e-02,  2.149e-02,
        -4.797e-02, -4.151e-03, -1.771e-03, -2.441e-02,  2.870e-02, -8.491e-02,
        -6.525e-02, -2.840e-02,  5.194e-03, -1.313e-02,  5.024e-02, -3.702e-02,
        -9.990e-03,  7.855e-03,  1.398e-02, -5.928e-02, -6.747e-02,  1.972e-02,
        -1.202e-04, -3.839e-02, -1.439e-02, -2.728e-03, -9.888e-03, -1.172e-02,
        -1.990e-02, -2.004e-02, -3.819e-02,  2.224e-02,  7.572e-02,  2.755e-02,
        -1.131e-02, -9.848e-02,  2.547e-02, -2.506e-02,  7.655e-04, -1.751e-02,
         4.064e-02,  2.182e-02, -3.684e-02,  1.071e-02,  3.497e-02,  1.918e-02,
        -3.157e-02, -3.151e-02, -7.611e-02,  3.371e-02,  1.777e-03, -1.299e-01,
         1.051e-01,  1.088e-02, -7.395e-02,  2.188e-03, -1.227e-02, -8.479e-03,
        -1.129e-02, -9.533e-03, -2.461e-02,  1.077e-01, -5.652e-02, -4.884e-02,
         2.134e-03, -3.994e-02,  2.642e-02,  2.438e-02,  2.018e-02, -2.939e-02,
        -6.279e-02, -9.650e-02, -8.426e-02, -5.945e-04, -2.322e-03, -7.785e-02,
        -1.866e-02,  1.198e-02, -6.484e-02,  2.515e-02,  1.736e-02, -3.334e-02,
         8.512e-03, -5.024e-03, -4.170e-02, -3.949e-02, -3.345e-02, -1.533e-03,
        -9.112e-02,  1.207e-02,  3.570e-04,  4.214e-02,  1.048e-02, -6.590e-02,
        -1.766e-02, -2.512e-02,  4.034e-02,  4.753e-02,  1.934e-02,  3.349e-03,
         4.174e-02,  6.332e-03,  1.081e-02,  5.412e-02,  3.424e-02, -1.371e-01,
         1.751e-02, -3.325e-02, -4.616e-02, -2.534e-02,  1.900e-03,  1.646e-02,
        -5.842e-03, -2.161e-02,  7.587e-03,  4.343e-02, -2.956e-02, -1.147e-01,
        -5.049e-02, -5.565e-02, -2.936e-03, -5.738e-02, -1.256e-01, -8.772e-03],
       device='cuda:0')
s after update for 1 param tensor([1.674, 1.939, 2.161, 1.483, 1.582, 1.354, 1.762, 1.866, 2.198, 1.876,
        1.748, 1.691, 2.079, 1.239, 1.649, 1.735, 1.662, 1.698, 1.809, 1.944,
        1.821, 0.364, 1.820, 1.676, 1.759, 1.396, 1.948, 1.858, 1.429, 1.474,
        2.014, 1.540, 1.414, 1.851, 1.549, 2.004, 1.695, 1.079, 1.546, 1.952,
        1.327, 1.777, 1.813, 2.051, 1.085, 1.433, 1.850, 1.698, 2.092, 1.152,
        2.340, 1.858, 1.776, 1.089, 1.995, 1.942, 1.912, 1.705, 2.021, 1.573,
        1.638, 1.788, 2.239, 1.703, 1.404, 1.503, 1.859, 1.941, 2.196, 1.389,
        0.874, 1.003, 0.973, 1.941, 1.464, 2.425, 2.077, 1.534, 1.843, 1.289,
        1.682, 1.348, 0.723, 1.234, 1.441, 1.448, 1.490, 1.519, 1.707, 1.621,
        1.935, 2.044, 2.176, 1.460, 1.309, 1.805, 1.885, 1.548, 2.375, 1.812,
        1.932, 1.983, 1.840, 0.809, 1.565, 2.059, 1.018, 1.506, 1.077, 0.998,
        1.245, 1.725, 1.794, 1.166, 2.212, 1.721, 1.696, 1.675, 2.043, 0.625,
        2.032, 1.643, 0.363, 1.418, 0.902, 1.691, 1.536, 1.859, 1.698, 1.693,
        1.156, 1.441, 1.545, 1.826, 1.552, 1.288, 1.834, 2.052, 1.569, 1.756,
        1.180, 1.804, 1.946, 1.826, 1.800, 1.791, 1.727, 0.505, 1.562, 1.295,
        1.743, 1.580, 1.269, 2.143, 1.651, 1.804, 1.413, 1.424, 1.649, 1.379,
        1.084, 1.629, 1.649, 1.638, 1.761, 1.747, 1.328, 1.059, 1.653, 1.953,
        1.341, 2.038, 1.835, 1.442, 1.409, 1.231, 1.516, 1.506, 1.666, 1.195,
        1.220, 1.825, 1.794, 1.800, 1.487, 1.687, 1.860, 1.736, 0.939, 1.672,
        1.902, 1.560, 1.715, 1.332, 2.053, 1.567, 1.578, 1.843, 1.112, 1.975,
        1.534, 1.535, 1.739, 1.565, 1.124, 1.942, 1.773, 1.578, 2.104, 1.616,
        1.001, 1.886, 1.949, 1.867, 0.112, 1.914, 1.179, 1.937, 1.481, 1.706,
        1.639, 1.864, 1.279, 2.100, 1.799, 1.374, 2.129, 2.272, 2.155, 1.581,
        2.036, 0.825, 1.406, 1.023, 1.765, 1.743, 1.649, 1.559, 1.222, 1.780,
        1.280, 1.645, 1.992, 1.588, 1.679, 1.060, 1.890, 1.754, 1.751, 0.457,
        1.380, 2.052, 2.020, 1.591, 1.864, 1.609, 1.414, 1.786, 1.442, 1.865,
        1.233, 2.077, 2.097, 1.334, 1.533, 0.909, 1.973, 1.539, 1.909, 1.666,
        1.692, 1.630, 2.068, 1.929, 1.405, 0.348, 1.569, 1.280, 1.057, 1.801,
        1.428, 2.192, 1.463, 1.691, 1.518, 1.661, 1.630, 1.402, 1.906, 1.187,
        2.039, 1.715, 1.220, 1.861, 1.256, 2.187, 1.960, 1.815, 2.057, 1.679],
       device='cuda:0')
b after update for 1 param tensor([167.980, 180.791, 190.871, 158.098, 163.300, 151.110, 172.365, 177.351,
        192.513, 177.837, 171.667, 168.837, 187.226, 144.512, 166.727, 171.040,
        167.389, 169.214, 174.654, 181.045, 175.197,  78.311, 175.188, 168.081,
        172.193, 153.443, 181.226, 176.996, 155.215, 157.643, 184.286, 161.157,
        154.414, 176.670, 161.599, 183.834, 169.036, 134.896, 161.440, 181.428,
        149.601, 173.066, 174.817, 185.934, 135.262, 155.433, 176.594, 169.218,
        187.787, 139.367, 198.645, 177.002, 173.037, 135.526, 183.385, 180.934,
        179.536, 169.567, 184.579, 162.829, 166.203, 173.624, 194.307, 169.447,
        153.874, 159.172, 177.051, 180.903, 192.415, 153.037, 121.358, 130.023,
        128.106, 180.918, 157.106, 202.192, 187.118, 160.818, 176.273, 147.395,
        168.406, 150.774, 110.384, 144.241, 155.866, 156.262, 158.497, 160.016,
        169.648, 165.341, 180.615, 185.633, 191.520, 156.899, 148.573, 174.450,
        178.254, 161.532, 200.092, 174.797, 180.494, 182.831, 176.154, 116.795,
        162.422, 186.316, 131.018, 159.345, 134.727, 129.730, 144.885, 170.544,
        173.922, 140.237, 193.120, 170.347, 169.112, 168.068, 185.581, 102.683,
        185.104, 166.437,  78.283, 154.593, 123.331, 168.829, 160.949, 177.049,
        169.197, 168.932, 139.581, 155.862, 161.398, 175.457, 161.786, 147.355,
        175.853, 186.018, 162.661, 172.074, 141.035, 174.383, 181.134, 175.437,
        174.210, 173.752, 170.627,  92.289, 162.260, 147.739, 171.421, 163.211,
        146.281, 190.087, 166.816, 174.409, 154.372, 154.965, 166.722, 152.502,
        135.208, 165.742, 166.740, 166.197, 172.311, 171.616, 149.627, 133.610,
        166.922, 181.474, 150.365, 185.353, 175.881, 155.946, 154.134, 144.069,
        159.888, 159.371, 167.583, 141.914, 143.422, 175.426, 173.910, 174.231,
        158.362, 168.669, 177.068, 171.060, 125.811, 167.881, 179.067, 162.186,
        170.068, 149.837, 186.036, 162.531, 163.134, 176.277, 136.946, 182.465,
        160.828, 160.889, 171.249, 162.441, 137.639, 180.954, 172.879, 163.093,
        188.365, 165.062, 129.888, 178.322, 181.269, 177.398,  43.493, 179.633,
        140.977, 180.698, 158.035, 169.610, 166.213, 177.255, 146.863, 188.182,
        174.174, 152.191, 189.440, 195.712, 190.610, 163.271, 185.267, 117.951,
        153.973, 131.310, 172.488, 171.431, 166.724, 162.111, 143.511, 173.260,
        146.905, 166.550, 183.240, 163.604, 168.234, 133.709, 178.495, 171.945,
        171.828,  87.767, 152.534, 186.014, 184.542, 163.790, 177.271, 164.699,
        154.390, 173.516, 155.936, 177.313, 144.190, 187.126, 188.024, 149.947,
        160.763, 123.821, 182.377, 161.087, 179.390, 167.594, 168.917, 165.790,
        186.716, 180.353, 153.898,  76.592, 162.653, 146.917, 133.518, 174.244,
        155.186, 192.262, 157.079, 168.846, 159.996, 167.363, 165.786, 153.760,
        179.255, 141.459, 185.397, 170.027, 143.441, 177.128, 145.513, 192.021,
        181.801, 174.947, 186.234, 168.239], device='cuda:0')
clipping threshold 0.25961975651177677
a after update for 1 param tensor([[[-0.018],
         [ 0.023],
         [-0.091],
         [ 0.037],
         [ 0.027],
         [-0.002],
         [-0.003],
         [ 0.058],
         [-0.066],
         [-0.067]],

        [[-0.033],
         [ 0.089],
         [ 0.040],
         [ 0.045],
         [ 0.074],
         [ 0.002],
         [-0.010],
         [-0.046],
         [ 0.009],
         [-0.068]],

        [[ 0.031],
         [ 0.030],
         [-0.030],
         [ 0.004],
         [ 0.064],
         [-0.019],
         [-0.014],
         [-0.109],
         [ 0.028],
         [-0.038]],

        [[-0.035],
         [ 0.030],
         [ 0.043],
         [ 0.066],
         [-0.018],
         [-0.044],
         [-0.033],
         [-0.038],
         [-0.001],
         [ 0.107]],

        [[-0.020],
         [-0.005],
         [ 0.027],
         [ 0.034],
         [-0.045],
         [ 0.022],
         [-0.015],
         [-0.063],
         [ 0.009],
         [ 0.032]],

        [[-0.015],
         [-0.073],
         [-0.032],
         [-0.013],
         [ 0.008],
         [ 0.016],
         [ 0.072],
         [-0.041],
         [-0.063],
         [ 0.011]],

        [[-0.055],
         [ 0.030],
         [ 0.075],
         [-0.039],
         [-0.032],
         [-0.112],
         [ 0.026],
         [-0.003],
         [-0.004],
         [ 0.046]],

        [[ 0.096],
         [-0.102],
         [ 0.001],
         [-0.034],
         [ 0.062],
         [ 0.002],
         [ 0.031],
         [-0.053],
         [-0.026],
         [-0.036]],

        [[-0.051],
         [-0.013],
         [ 0.068],
         [ 0.007],
         [-0.071],
         [ 0.041],
         [ 0.013],
         [ 0.003],
         [ 0.023],
         [ 0.024]],

        [[-0.017],
         [-0.010],
         [-0.006],
         [-0.025],
         [-0.036],
         [ 0.012],
         [ 0.080],
         [-0.054],
         [-0.012],
         [ 0.015]],

        [[ 0.050],
         [-0.064],
         [ 0.016],
         [ 0.023],
         [-0.041],
         [ 0.055],
         [-0.036],
         [-0.071],
         [-0.015],
         [-0.050]],

        [[-0.006],
         [-0.034],
         [-0.013],
         [-0.020],
         [-0.029],
         [-0.040],
         [ 0.092],
         [-0.004],
         [ 0.054],
         [-0.034]],

        [[-0.046],
         [ 0.039],
         [ 0.103],
         [-0.021],
         [ 0.072],
         [ 0.044],
         [ 0.020],
         [-0.011],
         [ 0.023],
         [-0.034]],

        [[-0.056],
         [ 0.020],
         [-0.005],
         [ 0.022],
         [-0.030],
         [-0.029],
         [ 0.070],
         [-0.004],
         [-0.024],
         [-0.007]],

        [[-0.014],
         [-0.011],
         [-0.039],
         [-0.026],
         [-0.017],
         [-0.012],
         [ 0.030],
         [ 0.031],
         [-0.066],
         [-0.001]],

        [[-0.043],
         [-0.008],
         [ 0.003],
         [ 0.027],
         [ 0.025],
         [-0.074],
         [-0.076],
         [-0.001],
         [-0.056],
         [-0.056]],

        [[-0.006],
         [-0.036],
         [-0.022],
         [ 0.013],
         [-0.057],
         [ 0.094],
         [-0.030],
         [ 0.000],
         [ 0.066],
         [ 0.023]],

        [[-0.022],
         [-0.009],
         [ 0.030],
         [ 0.011],
         [ 0.013],
         [ 0.024],
         [-0.006],
         [-0.002],
         [-0.069],
         [ 0.023]],

        [[ 0.074],
         [ 0.074],
         [ 0.011],
         [ 0.091],
         [-0.001],
         [ 0.035],
         [-0.013],
         [-0.007],
         [ 0.029],
         [ 0.005]],

        [[ 0.082],
         [ 0.011],
         [ 0.090],
         [-0.007],
         [ 0.016],
         [-0.121],
         [ 0.094],
         [-0.004],
         [-0.011],
         [-0.037]],

        [[ 0.028],
         [ 0.027],
         [ 0.044],
         [-0.005],
         [ 0.019],
         [-0.053],
         [-0.035],
         [-0.017],
         [-0.006],
         [ 0.041]],

        [[ 0.032],
         [-0.023],
         [-0.021],
         [ 0.061],
         [-0.022],
         [ 0.006],
         [ 0.034],
         [ 0.024],
         [ 0.052],
         [-0.087]],

        [[ 0.009],
         [ 0.032],
         [ 0.025],
         [ 0.011],
         [ 0.044],
         [ 0.072],
         [-0.005],
         [ 0.003],
         [ 0.026],
         [-0.103]],

        [[ 0.023],
         [-0.018],
         [-0.012],
         [ 0.010],
         [ 0.013],
         [ 0.057],
         [-0.001],
         [-0.075],
         [-0.028],
         [-0.006]],

        [[-0.029],
         [ 0.025],
         [-0.018],
         [ 0.004],
         [-0.004],
         [-0.020],
         [-0.183],
         [-0.024],
         [ 0.027],
         [ 0.015]],

        [[-0.025],
         [ 0.040],
         [-0.112],
         [ 0.007],
         [ 0.013],
         [-0.021],
         [ 0.001],
         [-0.010],
         [-0.018],
         [-0.007]],

        [[ 0.012],
         [ 0.070],
         [-0.029],
         [ 0.004],
         [ 0.013],
         [-0.038],
         [-0.032],
         [-0.003],
         [ 0.029],
         [ 0.008]],

        [[-0.019],
         [-0.058],
         [ 0.006],
         [ 0.011],
         [ 0.007],
         [ 0.057],
         [ 0.019],
         [-0.010],
         [ 0.026],
         [-0.018]],

        [[ 0.061],
         [ 0.025],
         [ 0.033],
         [-0.042],
         [-0.015],
         [-0.026],
         [ 0.031],
         [ 0.061],
         [ 0.064],
         [-0.012]],

        [[-0.019],
         [-0.012],
         [ 0.002],
         [ 0.027],
         [ 0.004],
         [-0.058],
         [ 0.032],
         [ 0.007],
         [-0.042],
         [ 0.153]]], device='cuda:0')
s after update for 1 param tensor([[[1.937],
         [1.280],
         [2.091],
         [1.757],
         [1.167],
         [1.684],
         [1.872],
         [2.316],
         [1.729],
         [2.031]],

        [[1.746],
         [1.699],
         [1.400],
         [1.725],
         [1.341],
         [1.829],
         [1.514],
         [1.444],
         [1.703],
         [1.636]],

        [[1.888],
         [1.657],
         [1.598],
         [1.914],
         [1.431],
         [1.899],
         [1.410],
         [1.825],
         [1.388],
         [1.331]],

        [[1.746],
         [1.875],
         [1.836],
         [1.360],
         [0.981],
         [1.840],
         [0.963],
         [1.255],
         [1.293],
         [2.134]],

        [[1.818],
         [1.826],
         [1.385],
         [1.816],
         [1.905],
         [2.127],
         [1.639],
         [2.011],
         [2.216],
         [1.677]],

        [[1.638],
         [1.692],
         [1.240],
         [1.424],
         [1.575],
         [1.104],
         [1.836],
         [1.541],
         [1.885],
         [2.132]],

        [[2.072],
         [1.549],
         [1.936],
         [1.793],
         [1.212],
         [1.870],
         [1.465],
         [1.479],
         [1.292],
         [1.507]],

        [[1.258],
         [1.688],
         [1.940],
         [1.603],
         [1.702],
         [2.007],
         [1.446],
         [1.799],
         [1.549],
         [1.858]],

        [[0.661],
         [1.643],
         [1.580],
         [1.077],
         [1.590],
         [1.605],
         [1.777],
         [1.716],
         [2.300],
         [1.785]],

        [[2.170],
         [0.943],
         [1.964],
         [1.818],
         [1.276],
         [1.724],
         [0.883],
         [1.527],
         [0.970],
         [1.208]],

        [[1.655],
         [1.377],
         [1.861],
         [1.834],
         [1.903],
         [1.271],
         [1.900],
         [2.142],
         [1.168],
         [1.763]],

        [[1.202],
         [1.518],
         [1.232],
         [1.720],
         [1.516],
         [1.185],
         [2.147],
         [1.375],
         [1.389],
         [1.576]],

        [[1.982],
         [1.709],
         [1.795],
         [1.152],
         [0.941],
         [1.887],
         [0.956],
         [1.342],
         [1.325],
         [2.189]],

        [[1.661],
         [2.004],
         [1.136],
         [1.855],
         [1.523],
         [1.517],
         [1.864],
         [1.060],
         [1.849],
         [1.710]],

        [[1.867],
         [1.561],
         [1.674],
         [1.643],
         [0.650],
         [1.531],
         [0.919],
         [1.629],
         [1.886],
         [1.799]],

        [[1.704],
         [2.006],
         [2.204],
         [1.642],
         [1.530],
         [2.048],
         [1.460],
         [1.571],
         [1.755],
         [1.582]],

        [[1.338],
         [2.140],
         [1.137],
         [0.851],
         [1.752],
         [1.966],
         [1.801],
         [1.288],
         [2.133],
         [1.394]],

        [[1.608],
         [1.173],
         [1.890],
         [1.627],
         [1.981],
         [1.657],
         [1.673],
         [1.746],
         [1.842],
         [1.959]],

        [[1.521],
         [1.866],
         [0.933],
         [1.795],
         [1.051],
         [1.702],
         [1.393],
         [1.334],
         [1.945],
         [1.606]],

        [[2.004],
         [1.556],
         [1.696],
         [1.650],
         [2.209],
         [2.157],
         [1.651],
         [1.555],
         [1.491],
         [1.346]],

        [[1.298],
         [1.548],
         [1.737],
         [1.092],
         [1.764],
         [1.467],
         [1.635],
         [2.206],
         [1.941],
         [1.524]],

        [[1.239],
         [1.386],
         [1.465],
         [2.013],
         [1.623],
         [1.335],
         [1.647],
         [1.415],
         [2.069],
         [1.754]],

        [[1.287],
         [1.372],
         [1.762],
         [1.575],
         [1.488],
         [1.511],
         [1.901],
         [1.040],
         [0.966],
         [1.911]],

        [[1.681],
         [1.681],
         [1.331],
         [1.788],
         [1.470],
         [1.972],
         [2.052],
         [1.608],
         [1.520],
         [2.020]],

        [[1.211],
         [1.694],
         [1.481],
         [2.022],
         [1.836],
         [1.440],
         [1.876],
         [1.479],
         [1.637],
         [1.475]],

        [[1.834],
         [1.670],
         [1.754],
         [1.837],
         [1.387],
         [1.256],
         [2.237],
         [1.922],
         [2.082],
         [1.141]],

        [[2.022],
         [1.479],
         [1.963],
         [1.371],
         [1.747],
         [1.632],
         [1.717],
         [1.371],
         [1.476],
         [0.310]],

        [[1.720],
         [1.599],
         [1.942],
         [1.517],
         [1.680],
         [1.483],
         [1.684],
         [1.216],
         [1.668],
         [1.557]],

        [[1.854],
         [2.124],
         [1.410],
         [2.070],
         [1.527],
         [1.505],
         [1.664],
         [1.275],
         [1.342],
         [1.322]],

        [[1.382],
         [1.496],
         [1.783],
         [1.759],
         [1.791],
         [1.579],
         [1.634],
         [0.881],
         [1.106],
         [1.713]]], device='cuda:0')
b after update for 1 param tensor([[[180.702],
         [146.930],
         [187.750],
         [172.109],
         [140.241],
         [168.512],
         [177.677],
         [197.616],
         [170.754],
         [185.029]],

        [[171.593],
         [169.243],
         [153.645],
         [170.532],
         [150.364],
         [175.626],
         [159.758],
         [156.011],
         [169.468],
         [166.067]],

        [[178.434],
         [167.158],
         [164.121],
         [179.648],
         [155.309],
         [178.950],
         [154.207],
         [175.391],
         [152.954],
         [149.821]],

        [[171.595],
         [177.808],
         [175.932],
         [151.421],
         [128.618],
         [176.119],
         [127.447],
         [145.485],
         [147.652],
         [189.660]],

        [[175.082],
         [175.448],
         [152.811],
         [174.999],
         [179.228],
         [189.361],
         [166.255],
         [184.139],
         [193.307],
         [168.150]],

        [[166.204],
         [168.904],
         [144.570],
         [154.962],
         [162.952],
         [136.412],
         [175.963],
         [161.178],
         [178.283],
         [189.591]],

        [[186.887],
         [161.625],
         [180.689],
         [173.886],
         [142.977],
         [177.552],
         [157.154],
         [157.891],
         [147.588],
         [159.395]],

        [[145.641],
         [168.683],
         [180.876],
         [164.421],
         [169.420],
         [183.935],
         [156.148],
         [174.179],
         [161.584],
         [176.982]],

        [[105.601],
         [166.421],
         [163.199],
         [134.750],
         [163.749],
         [164.487],
         [173.107],
         [170.111],
         [196.931],
         [173.457]],

        [[191.294],
         [126.116],
         [181.976],
         [175.061],
         [146.702],
         [170.490],
         [122.041],
         [160.447],
         [127.883],
         [142.725]],

        [[167.036],
         [152.389],
         [177.154],
         [175.828],
         [179.100],
         [146.381],
         [178.965],
         [190.017],
         [140.325],
         [172.406]],

        [[142.343],
         [159.984],
         [144.108],
         [170.282],
         [159.867],
         [141.376],
         [190.239],
         [152.246],
         [153.016],
         [163.007]],

        [[182.810],
         [169.741],
         [173.967],
         [139.336],
         [125.965],
         [178.386],
         [126.948],
         [150.422],
         [149.478],
         [192.106]],

        [[167.331],
         [183.825],
         [138.364],
         [176.839],
         [160.241],
         [159.952],
         [177.281],
         [133.668],
         [176.546],
         [169.804]],

        [[177.416],
         [162.239],
         [167.978],
         [166.412],
         [104.666],
         [160.643],
         [124.496],
         [165.744],
         [178.322],
         [174.181]],

        [[169.489],
         [183.884],
         [192.759],
         [166.368],
         [160.629],
         [185.823],
         [156.914],
         [162.757],
         [171.997],
         [163.301]],

        [[150.194],
         [189.961],
         [138.459],
         [119.780],
         [171.855],
         [182.048],
         [174.242],
         [147.351],
         [189.649],
         [153.328]],

        [[164.679],
         [140.623],
         [178.504],
         [165.631],
         [182.745],
         [167.146],
         [167.930],
         [171.582],
         [176.241],
         [181.760]],

        [[160.150],
         [177.356],
         [125.401],
         [173.985],
         [133.112],
         [169.410],
         [153.255],
         [149.976],
         [181.099],
         [164.547]],

        [[183.824],
         [161.985],
         [169.114],
         [166.800],
         [192.995],
         [190.706],
         [166.846],
         [161.921],
         [158.567],
         [150.661]],

        [[147.913],
         [161.549],
         [171.136],
         [135.703],
         [172.437],
         [157.255],
         [166.054],
         [192.871],
         [180.911],
         [160.317]],

        [[144.525],
         [152.889],
         [157.136],
         [184.242],
         [165.407],
         [150.031],
         [166.643],
         [154.475],
         [186.791],
         [171.972]],

        [[147.304],
         [152.072],
         [172.360],
         [162.956],
         [158.372],
         [159.603],
         [179.004],
         [132.446],
         [127.629],
         [179.501]],

        [[168.340],
         [168.374],
         [149.810],
         [173.635],
         [157.439],
         [182.324],
         [186.009],
         [164.676],
         [160.079],
         [184.560]],

        [[142.884],
         [169.004],
         [158.024],
         [184.659],
         [175.944],
         [155.808],
         [177.868],
         [157.937],
         [166.109],
         [157.723]],

        [[175.827],
         [167.804],
         [171.955],
         [176.010],
         [152.946],
         [145.542],
         [194.214],
         [180.003],
         [187.370],
         [138.688]],

        [[184.653],
         [157.937],
         [181.924],
         [152.024],
         [171.611],
         [165.895],
         [170.135],
         [152.032],
         [157.738],
         [ 72.287]],

        [[170.295],
         [164.169],
         [180.928],
         [159.902],
         [168.287],
         [158.135],
         [168.501],
         [143.181],
         [167.719],
         [162.020]],

        [[176.798],
         [189.245],
         [154.167],
         [186.822],
         [160.462],
         [159.312],
         [167.493],
         [146.622],
         [150.399],
         [149.322]],

        [[152.635],
         [158.807],
         [173.373],
         [172.203],
         [173.793],
         [163.169],
         [165.996],
         [121.855],
         [136.569],
         [169.943]]], device='cuda:0')
clipping threshold 0.25961975651177677
a after update for 1 param tensor([[-0.003],
        [-0.014],
        [ 0.006],
        [ 0.027],
        [-0.030],
        [ 0.039],
        [ 0.033],
        [ 0.021],
        [-0.074],
        [ 0.020],
        [-0.025],
        [-0.015],
        [ 0.006],
        [ 0.015],
        [ 0.008],
        [-0.024],
        [-0.049],
        [-0.002],
        [ 0.007],
        [ 0.016],
        [-0.035],
        [ 0.057],
        [ 0.051],
        [-0.021],
        [ 0.052],
        [ 0.060],
        [-0.075],
        [ 0.018],
        [ 0.018],
        [ 0.029]], device='cuda:0')
s after update for 1 param tensor([[1.239],
        [1.458],
        [1.547],
        [1.888],
        [1.820],
        [1.715],
        [1.998],
        [1.269],
        [1.921],
        [1.567],
        [1.422],
        [2.121],
        [1.509],
        [1.248],
        [1.843],
        [1.999],
        [1.649],
        [1.659],
        [1.186],
        [1.074],
        [1.935],
        [1.459],
        [1.939],
        [1.754],
        [1.943],
        [1.770],
        [2.209],
        [1.137],
        [1.385],
        [2.096]], device='cuda:0')
b after update for 1 param tensor([[144.509],
        [156.794],
        [161.489],
        [178.417],
        [175.167],
        [170.053],
        [183.555],
        [146.243],
        [179.987],
        [162.519],
        [154.816],
        [189.092],
        [159.481],
        [145.041],
        [176.284],
        [183.603],
        [166.717],
        [167.242],
        [141.388],
        [134.533],
        [180.615],
        [156.851],
        [180.795],
        [171.976],
        [181.002],
        [172.738],
        [192.990],
        [138.433],
        [152.787],
        [187.985]], device='cuda:0')
clipping threshold 0.25961975651177677
||w||^2 0.04813418170420366
exp ma of ||w||^2 0.06473190743231823
||w|| 0.2193950357328161
exp ma of ||w|| 0.24531366219568126
||w||^2 0.04300443264430871
exp ma of ||w||^2 0.04407725930712481
||w|| 0.2073751013123531
exp ma of ||w|| 0.20553501938400529
cuda
Objective function 30.56 = squared loss an data 29.37 + 0.5*rho*h**2 0.404182 + alpha*h 0.343957 + L2reg 0.22 + L1reg 0.22 ; SHD = 59 ; DAG True
Proportion of microbatches that were clipped  0.7018023817186997
iteration 2 in inner loop, alpha 12.097631376180843 rho 1000.0 h 0.028431756667927743
9630
cuda
Objective function 34.19 = squared loss an data 29.37 + 0.5*rho*h**2 4.041824 + alpha*h 0.343957 + L2reg 0.22 + L1reg 0.22 ; SHD = 59 ; DAG True
||w||^2 0.48043155794827663
exp ma of ||w||^2 177.88686926084785
||w|| 0.693131703176443
exp ma of ||w|| 1.221511712739217
||w||^2 0.027372764359637494
exp ma of ||w||^2 0.03564326520343881
||w|| 0.16544716485826372
exp ma of ||w|| 0.18744327549397258
||w||^2 0.021996834293303083
exp ma of ||w||^2 0.032429937605407924
||w|| 0.148313297762888
exp ma of ||w|| 0.17756827905295078
||w||^2 0.07064941186535778
exp ma of ||w||^2 0.037693643612860296
||w|| 0.2657995708524711
exp ma of ||w|| 0.19037243073105267
||w||^2 0.026271351511831013
exp ma of ||w||^2 0.03914614852856066
||w|| 0.16208439626266008
exp ma of ||w|| 0.19395995011366393
cuda
Objective function 30.46 = squared loss an data 29.51 + 0.5*rho*h**2 0.435767 + alpha*h 0.112939 + L2reg 0.22 + L1reg 0.18 ; SHD = 56 ; DAG True
Proportion of microbatches that were clipped  0.7001131038940055
iteration 3 in inner loop, alpha 12.097631376180843 rho 10000.0 h 0.009335601991210751
iteration 3 in outer loop, alpha = 105.45365128828836, rho = 10000.0, h = 0.009335601991210751
cuda
9630
cuda
Objective function 31.33 = squared loss an data 29.51 + 0.5*rho*h**2 0.435767 + alpha*h 0.984473 + L2reg 0.22 + L1reg 0.18 ; SHD = 56 ; DAG True
||w||^2 0.2643212485599255
exp ma of ||w||^2 56.20394084694518
||w|| 0.514121822684007
exp ma of ||w|| 0.7080600821207007
||w||^2 0.2921408333101699
exp ma of ||w||^2 13.7427242799161
||w|| 0.540500539602108
exp ma of ||w|| 0.6088144880944025
||w||^2 0.20295888675986457
exp ma of ||w||^2 1.449614061971015
||w|| 0.4505095856470366
exp ma of ||w|| 0.47346933480930004
cuda
Objective function 30.66 = squared loss an data 29.40 + 0.5*rho*h**2 0.199340 + alpha*h 0.665847 + L2reg 0.23 + L1reg 0.16 ; SHD = 53 ; DAG True
Proportion of microbatches that were clipped  0.7063315303849825
iteration 1 in inner loop, alpha 105.45365128828836 rho 10000.0 h 0.006314117364752292
9630
cuda
Objective function 32.46 = squared loss an data 29.40 + 0.5*rho*h**2 1.993404 + alpha*h 0.665847 + L2reg 0.23 + L1reg 0.16 ; SHD = 53 ; DAG True
||w||^2 2651774169984.345
exp ma of ||w||^2 1331596789474.3586
||w|| 1628426.9004116657
exp ma of ||w|| 852501.2865701995
||w||^2 5.608763733307904
exp ma of ||w||^2 219371.08062129706
||w|| 2.368282865982842
exp ma of ||w|| 4.941820177134963
||w||^2 0.1296541196502822
exp ma of ||w||^2 0.13068587250575212
||w|| 0.36007515833542614
exp ma of ||w|| 0.33456000524986984
cuda
Objective function 30.63 = squared loss an data 29.61 + 0.5*rho*h**2 0.366606 + alpha*h 0.285546 + L2reg 0.23 + L1reg 0.14 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.6977346278317152
iteration 2 in inner loop, alpha 105.45365128828836 rho 100000.0 h 0.002707788185809079
iteration 4 in outer loop, alpha = 2813.2418370973674, rho = 1000000.0, h = 0.002707788185809079
Threshold 0.3
[[0.002 0.041 0.031 0.118 0.055 0.026 0.045 0.021 0.024 0.018 0.033 0.027
  0.035 0.031 0.074 0.016 0.034 0.023 0.097 0.036 0.079 0.02  0.013 0.017
  0.036 0.014 0.031 0.022 0.082 0.032]
 [0.051 0.002 0.021 0.036 0.083 0.017 0.065 0.024 0.043 0.026 0.02  0.038
  0.038 0.029 0.033 0.062 0.021 0.025 0.077 0.033 0.069 0.079 0.106 0.032
  0.064 0.026 0.036 0.133 0.029 0.031]
 [0.086 0.098 0.002 0.069 0.136 0.026 0.055 0.066 0.026 0.046 0.033 0.449
  0.035 0.056 0.059 0.12  0.106 0.075 0.125 0.049 0.059 0.116 0.149 0.039
  0.132 0.05  0.105 0.055 0.055 0.054]
 [0.027 0.042 0.03  0.002 0.039 0.021 0.032 0.007 0.063 0.028 0.015 0.066
  0.02  0.029 0.053 0.055 0.043 0.024 0.051 0.022 0.037 0.036 0.03  0.016
  0.035 0.02  0.031 0.036 0.018 0.014]
 [0.053 0.029 0.015 0.06  0.002 0.026 0.019 0.025 0.018 0.031 0.017 0.036
  0.05  0.025 0.016 0.017 0.067 0.009 0.082 0.017 0.053 0.029 0.033 0.013
  0.038 0.017 0.03  0.041 0.085 0.013]
 [0.118 0.148 0.115 0.154 0.11  0.003 0.09  0.075 0.126 0.038 0.063 0.18
  0.068 0.027 0.111 0.062 0.085 0.059 0.046 0.068 0.243 0.083 0.045 0.049
  0.046 0.032 0.038 0.052 0.045 0.121]
 [0.058 0.038 0.039 0.08  0.11  0.027 0.002 0.043 0.076 0.093 0.016 0.094
  0.043 0.028 0.053 0.14  0.088 0.024 0.128 0.054 0.144 0.021 0.085 0.037
  0.144 0.009 0.037 0.064 0.019 0.105]
 [0.082 0.089 0.033 0.294 0.089 0.032 0.065 0.001 0.072 0.032 0.02  0.052
  0.048 0.103 0.091 0.061 0.067 0.033 0.148 0.073 0.086 0.042 0.035 0.015
  0.046 0.008 0.096 0.03  0.082 0.05 ]
 [0.078 0.051 0.07  0.031 0.137 0.018 0.033 0.03  0.002 0.06  0.013 0.071
  0.023 0.039 0.029 0.103 0.082 0.034 0.198 0.01  0.245 0.06  0.079 0.023
  0.023 0.031 0.068 0.048 0.051 0.08 ]
 [0.092 0.084 0.058 0.073 0.081 0.058 0.02  0.057 0.036 0.002 0.042 0.052
  0.121 0.059 0.078 0.124 0.097 0.031 0.136 0.042 0.141 0.183 0.049 0.035
  0.027 0.014 0.03  0.046 0.098 0.103]
 [0.079 0.105 0.09  0.162 0.181 0.035 0.16  0.123 0.101 0.053 0.002 0.112
  0.164 0.09  0.063 0.062 0.04  0.085 0.186 0.097 0.075 0.061 0.083 0.038
  0.043 0.047 0.036 0.415 0.036 0.149]
 [0.057 0.064 0.004 0.042 0.042 0.013 0.016 0.039 0.024 0.035 0.018 0.002
  0.043 0.035 0.021 0.112 0.072 0.03  0.052 0.023 0.047 0.024 0.033 0.021
  0.055 0.016 0.013 0.02  0.044 0.015]
 [0.042 0.065 0.065 0.133 0.04  0.035 0.059 0.038 0.071 0.017 0.021 0.061
  0.001 0.025 0.103 0.033 0.016 0.031 0.106 0.075 0.034 0.132 0.031 0.012
  0.044 0.04  0.098 0.04  0.025 0.097]
 [0.081 0.073 0.047 0.089 0.111 0.102 0.086 0.018 0.072 0.049 0.025 0.076
  0.102 0.002 0.084 0.061 0.041 0.039 0.096 0.092 0.093 0.065 0.065 0.038
  0.037 0.015 0.063 0.034 0.102 0.078]
 [0.051 0.056 0.024 0.066 0.078 0.021 0.027 0.021 0.065 0.041 0.039 0.066
  0.024 0.032 0.001 0.116 0.093 0.049 0.054 0.048 0.154 0.028 0.113 0.073
  0.04  0.006 0.028 0.114 0.016 0.021]
 [0.12  0.027 0.028 0.05  0.099 0.036 0.016 0.035 0.024 0.018 0.033 0.024
  0.06  0.028 0.019 0.002 0.045 0.039 0.031 0.072 0.035 0.028 0.076 0.041
  0.061 0.009 0.037 0.038 0.047 0.05 ]
 [0.075 0.091 0.018 0.046 0.041 0.02  0.014 0.032 0.042 0.031 0.037 0.02
  0.125 0.047 0.019 0.052 0.002 0.024 0.079 0.044 0.071 0.036 0.05  0.027
  0.035 0.021 0.035 0.061 0.067 0.032]
 [0.122 0.105 0.048 0.109 0.325 0.028 0.11  0.077 0.065 0.071 0.03  0.085
  0.066 0.063 0.063 0.067 0.105 0.002 0.051 0.064 0.033 0.081 0.088 0.038
  0.037 0.008 0.029 0.049 0.056 0.106]
 [0.027 0.024 0.027 0.044 0.035 0.031 0.019 0.025 0.012 0.009 0.014 0.043
  0.017 0.02  0.034 0.062 0.024 0.033 0.002 0.021 0.084 0.078 0.079 0.005
  0.034 0.023 0.026 0.084 0.043 0.021]
 [0.052 0.056 0.043 0.097 0.17  0.053 0.059 0.029 0.228 0.052 0.027 0.069
  0.032 0.035 0.052 0.026 0.047 0.041 0.091 0.002 0.258 0.09  0.122 0.043
  0.067 0.029 0.037 0.061 0.033 0.05 ]
 [0.034 0.03  0.028 0.06  0.046 0.008 0.017 0.036 0.011 0.019 0.021 0.034
  0.06  0.024 0.016 0.043 0.038 0.049 0.025 0.014 0.001 0.022 0.03  0.013
  0.028 0.008 0.035 0.038 0.014 0.021]
 [0.139 0.028 0.024 0.083 0.073 0.021 0.082 0.054 0.044 0.02  0.034 0.099
  0.02  0.034 0.068 0.054 0.053 0.033 0.037 0.02  0.1   0.002 0.049 0.019
  0.063 0.018 0.047 0.071 0.039 0.064]
 [0.125 0.008 0.019 0.071 0.095 0.031 0.042 0.038 0.038 0.043 0.021 0.05
  0.077 0.038 0.021 0.042 0.052 0.037 0.038 0.017 0.086 0.079 0.003 0.008
  0.036 0.009 0.016 0.018 0.014 0.039]
 [0.124 0.067 0.049 0.15  0.105 0.041 0.052 0.186 0.098 0.078 0.07  0.106
  0.127 0.076 0.06  0.052 0.071 0.047 0.367 0.041 0.238 0.083 0.456 0.002
  0.134 0.007 0.065 0.075 0.027 0.139]
 [0.089 0.04  0.022 0.062 0.034 0.055 0.018 0.052 0.084 0.098 0.032 0.047
  0.058 0.056 0.068 0.033 0.059 0.038 0.057 0.03  0.101 0.045 0.069 0.018
  0.002 0.036 0.086 0.046 0.061 0.026]
 [0.166 0.074 0.043 0.109 0.137 0.066 0.169 0.297 0.061 0.137 0.06  0.111
  0.063 0.083 0.401 0.205 0.139 0.321 0.087 0.092 0.306 0.114 0.214 0.316
  0.062 0.002 0.057 0.244 0.407 0.099]
 [0.061 0.053 0.022 0.065 0.077 0.061 0.058 0.029 0.032 0.062 0.068 0.226
  0.017 0.039 0.115 0.126 0.069 0.125 0.077 0.058 0.046 0.052 0.119 0.032
  0.023 0.028 0.002 0.117 0.034 0.03 ]
 [0.085 0.022 0.04  0.076 0.041 0.041 0.026 0.078 0.059 0.039 0.005 0.096
  0.055 0.081 0.018 0.033 0.038 0.049 0.023 0.034 0.069 0.046 0.146 0.042
  0.062 0.011 0.015 0.002 0.032 0.031]
 [0.037 0.091 0.054 0.161 0.048 0.052 0.064 0.035 0.043 0.017 0.068 0.051
  0.071 0.027 0.11  0.036 0.036 0.05  0.039 0.063 0.196 0.072 0.085 0.055
  0.048 0.005 0.076 0.094 0.003 0.108]
 [0.073 0.059 0.04  0.152 0.183 0.023 0.029 0.04  0.021 0.022 0.017 0.175
  0.021 0.034 0.145 0.031 0.059 0.028 0.118 0.06  0.095 0.039 0.062 0.019
  0.069 0.018 0.066 0.068 0.025 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.449
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.415 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.325 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.367 0.    0.    0.    0.456 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.401 0.    0.    0.321 0.    0.    0.306 0.    0.    0.316
  0.    0.    0.    0.    0.407 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.3, 'tpr': 0.11666666666666667, 'fpr': 0.008, 'f1': 0.2, 'shd': 54, 'npred': 10, 'ntrue': 60}
[0.041 0.031 0.118 0.055 0.026 0.045 0.021 0.024 0.018 0.033 0.027 0.035
 0.031 0.074 0.016 0.034 0.023 0.097 0.036 0.079 0.02  0.013 0.017 0.036
 0.014 0.031 0.022 0.082 0.032 0.051 0.021 0.036 0.083 0.017 0.065 0.024
 0.043 0.026 0.02  0.038 0.038 0.029 0.033 0.062 0.021 0.025 0.077 0.033
 0.069 0.079 0.106 0.032 0.064 0.026 0.036 0.133 0.029 0.031 0.086 0.098
 0.069 0.136 0.026 0.055 0.066 0.026 0.046 0.033 0.449 0.035 0.056 0.059
 0.12  0.106 0.075 0.125 0.049 0.059 0.116 0.149 0.039 0.132 0.05  0.105
 0.055 0.055 0.054 0.027 0.042 0.03  0.039 0.021 0.032 0.007 0.063 0.028
 0.015 0.066 0.02  0.029 0.053 0.055 0.043 0.024 0.051 0.022 0.037 0.036
 0.03  0.016 0.035 0.02  0.031 0.036 0.018 0.014 0.053 0.029 0.015 0.06
 0.026 0.019 0.025 0.018 0.031 0.017 0.036 0.05  0.025 0.016 0.017 0.067
 0.009 0.082 0.017 0.053 0.029 0.033 0.013 0.038 0.017 0.03  0.041 0.085
 0.013 0.118 0.148 0.115 0.154 0.11  0.09  0.075 0.126 0.038 0.063 0.18
 0.068 0.027 0.111 0.062 0.085 0.059 0.046 0.068 0.243 0.083 0.045 0.049
 0.046 0.032 0.038 0.052 0.045 0.121 0.058 0.038 0.039 0.08  0.11  0.027
 0.043 0.076 0.093 0.016 0.094 0.043 0.028 0.053 0.14  0.088 0.024 0.128
 0.054 0.144 0.021 0.085 0.037 0.144 0.009 0.037 0.064 0.019 0.105 0.082
 0.089 0.033 0.294 0.089 0.032 0.065 0.072 0.032 0.02  0.052 0.048 0.103
 0.091 0.061 0.067 0.033 0.148 0.073 0.086 0.042 0.035 0.015 0.046 0.008
 0.096 0.03  0.082 0.05  0.078 0.051 0.07  0.031 0.137 0.018 0.033 0.03
 0.06  0.013 0.071 0.023 0.039 0.029 0.103 0.082 0.034 0.198 0.01  0.245
 0.06  0.079 0.023 0.023 0.031 0.068 0.048 0.051 0.08  0.092 0.084 0.058
 0.073 0.081 0.058 0.02  0.057 0.036 0.042 0.052 0.121 0.059 0.078 0.124
 0.097 0.031 0.136 0.042 0.141 0.183 0.049 0.035 0.027 0.014 0.03  0.046
 0.098 0.103 0.079 0.105 0.09  0.162 0.181 0.035 0.16  0.123 0.101 0.053
 0.112 0.164 0.09  0.063 0.062 0.04  0.085 0.186 0.097 0.075 0.061 0.083
 0.038 0.043 0.047 0.036 0.415 0.036 0.149 0.057 0.064 0.004 0.042 0.042
 0.013 0.016 0.039 0.024 0.035 0.018 0.043 0.035 0.021 0.112 0.072 0.03
 0.052 0.023 0.047 0.024 0.033 0.021 0.055 0.016 0.013 0.02  0.044 0.015
 0.042 0.065 0.065 0.133 0.04  0.035 0.059 0.038 0.071 0.017 0.021 0.061
 0.025 0.103 0.033 0.016 0.031 0.106 0.075 0.034 0.132 0.031 0.012 0.044
 0.04  0.098 0.04  0.025 0.097 0.081 0.073 0.047 0.089 0.111 0.102 0.086
 0.018 0.072 0.049 0.025 0.076 0.102 0.084 0.061 0.041 0.039 0.096 0.092
 0.093 0.065 0.065 0.038 0.037 0.015 0.063 0.034 0.102 0.078 0.051 0.056
 0.024 0.066 0.078 0.021 0.027 0.021 0.065 0.041 0.039 0.066 0.024 0.032
 0.116 0.093 0.049 0.054 0.048 0.154 0.028 0.113 0.073 0.04  0.006 0.028
 0.114 0.016 0.021 0.12  0.027 0.028 0.05  0.099 0.036 0.016 0.035 0.024
 0.018 0.033 0.024 0.06  0.028 0.019 0.045 0.039 0.031 0.072 0.035 0.028
 0.076 0.041 0.061 0.009 0.037 0.038 0.047 0.05  0.075 0.091 0.018 0.046
 0.041 0.02  0.014 0.032 0.042 0.031 0.037 0.02  0.125 0.047 0.019 0.052
 0.024 0.079 0.044 0.071 0.036 0.05  0.027 0.035 0.021 0.035 0.061 0.067
 0.032 0.122 0.105 0.048 0.109 0.325 0.028 0.11  0.077 0.065 0.071 0.03
 0.085 0.066 0.063 0.063 0.067 0.105 0.051 0.064 0.033 0.081 0.088 0.038
 0.037 0.008 0.029 0.049 0.056 0.106 0.027 0.024 0.027 0.044 0.035 0.031
 0.019 0.025 0.012 0.009 0.014 0.043 0.017 0.02  0.034 0.062 0.024 0.033
 0.021 0.084 0.078 0.079 0.005 0.034 0.023 0.026 0.084 0.043 0.021 0.052
 0.056 0.043 0.097 0.17  0.053 0.059 0.029 0.228 0.052 0.027 0.069 0.032
 0.035 0.052 0.026 0.047 0.041 0.091 0.258 0.09  0.122 0.043 0.067 0.029
 0.037 0.061 0.033 0.05  0.034 0.03  0.028 0.06  0.046 0.008 0.017 0.036
 0.011 0.019 0.021 0.034 0.06  0.024 0.016 0.043 0.038 0.049 0.025 0.014
 0.022 0.03  0.013 0.028 0.008 0.035 0.038 0.014 0.021 0.139 0.028 0.024
 0.083 0.073 0.021 0.082 0.054 0.044 0.02  0.034 0.099 0.02  0.034 0.068
 0.054 0.053 0.033 0.037 0.02  0.1   0.049 0.019 0.063 0.018 0.047 0.071
 0.039 0.064 0.125 0.008 0.019 0.071 0.095 0.031 0.042 0.038 0.038 0.043
 0.021 0.05  0.077 0.038 0.021 0.042 0.052 0.037 0.038 0.017 0.086 0.079
 0.008 0.036 0.009 0.016 0.018 0.014 0.039 0.124 0.067 0.049 0.15  0.105
 0.041 0.052 0.186 0.098 0.078 0.07  0.106 0.127 0.076 0.06  0.052 0.071
 0.047 0.367 0.041 0.238 0.083 0.456 0.134 0.007 0.065 0.075 0.027 0.139
 0.089 0.04  0.022 0.062 0.034 0.055 0.018 0.052 0.084 0.098 0.032 0.047
 0.058 0.056 0.068 0.033 0.059 0.038 0.057 0.03  0.101 0.045 0.069 0.018
 0.036 0.086 0.046 0.061 0.026 0.166 0.074 0.043 0.109 0.137 0.066 0.169
 0.297 0.061 0.137 0.06  0.111 0.063 0.083 0.401 0.205 0.139 0.321 0.087
 0.092 0.306 0.114 0.214 0.316 0.062 0.057 0.244 0.407 0.099 0.061 0.053
 0.022 0.065 0.077 0.061 0.058 0.029 0.032 0.062 0.068 0.226 0.017 0.039
 0.115 0.126 0.069 0.125 0.077 0.058 0.046 0.052 0.119 0.032 0.023 0.028
 0.117 0.034 0.03  0.085 0.022 0.04  0.076 0.041 0.041 0.026 0.078 0.059
 0.039 0.005 0.096 0.055 0.081 0.018 0.033 0.038 0.049 0.023 0.034 0.069
 0.046 0.146 0.042 0.062 0.011 0.015 0.032 0.031 0.037 0.091 0.054 0.161
 0.048 0.052 0.064 0.035 0.043 0.017 0.068 0.051 0.071 0.027 0.11  0.036
 0.036 0.05  0.039 0.063 0.196 0.072 0.085 0.055 0.048 0.005 0.076 0.094
 0.108 0.073 0.059 0.04  0.152 0.183 0.023 0.029 0.04  0.021 0.022 0.017
 0.175 0.021 0.034 0.145 0.031 0.059 0.028 0.118 0.06  0.095 0.039 0.062
 0.019 0.069 0.018 0.066 0.068 0.025]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.5900205761316873, 0.20334310337343078)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
