samples  5000  graph  30 120 ER mlp  minibatch size  100  noise  0.6  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.407192360868244
iteration 1 in outer loop, alpha = 2.407192360868244, rho = 1.0, h = 2.407192360868244
cuda
iteration 1 in inner loop,alpha 2.407192360868244 rho 1.0 h 1.519847643277938
iteration 2 in inner loop,alpha 2.407192360868244 rho 10.0 h 0.6736861819345883
iteration 3 in inner loop,alpha 2.407192360868244 rho 100.0 h 0.216490139944284
iteration 2 in outer loop, alpha = 24.056206355296645, rho = 100.0, h = 0.216490139944284
cuda
iteration 1 in inner loop,alpha 24.056206355296645 rho 100.0 h 0.12144845719680575
iteration 2 in inner loop,alpha 24.056206355296645 rho 1000.0 h 0.043869644319229906
iteration 3 in outer loop, alpha = 67.92585067452654, rho = 1000.0, h = 0.043869644319229906
cuda
iteration 1 in inner loop,alpha 67.92585067452654 rho 1000.0 h 0.021663943251340356
iteration 2 in inner loop,alpha 67.92585067452654 rho 10000.0 h 0.007499864311192539
iteration 4 in outer loop, alpha = 142.92449378645193, rho = 10000.0, h = 0.007499864311192539
cuda
iteration 1 in inner loop,alpha 142.92449378645193 rho 10000.0 h 0.003466679437860165
iteration 2 in inner loop,alpha 142.92449378645193 rho 100000.0 h 0.0011236062511201794
iteration 5 in outer loop, alpha = 255.28511889846988, rho = 100000.0, h = 0.0011236062511201794
cuda
iteration 1 in inner loop,alpha 255.28511889846988 rho 100000.0 h 0.0004888399768780971
iteration 6 in outer loop, alpha = 744.1250957765669, rho = 1000000.0, h = 0.0004888399768780971
Threshold 0.3
[[0.002 0.    0.001 1.951 0.002 0.179 0.21  0.    0.    0.    0.    0.002
  0.    0.    0.    0.949 0.    0.    0.738 0.196 0.    0.    0.215 0.
  0.    0.    0.18  0.    0.266 0.   ]
 [0.424 0.003 0.004 0.105 0.204 1.574 0.175 0.    1.183 0.    0.    0.137
  0.    0.    0.    0.847 0.    1.225 0.329 0.725 0.    0.002 1.093 0.001
  0.    0.    0.185 0.    0.243 0.118]
 [0.166 0.097 0.002 0.489 0.034 0.45  0.241 0.004 0.095 0.    0.    0.173
  0.    0.    0.    0.752 0.    0.294 0.12  0.078 0.    0.    0.183 0.
  0.    0.002 0.19  0.    0.231 0.005]
 [0.    0.    0.    0.002 0.001 0.001 0.157 0.    0.    0.    0.    0.
  0.    0.    0.    0.945 0.    0.    0.215 0.673 0.    0.    0.124 0.
  0.    0.    0.    0.    0.277 0.   ]
 [0.084 0.005 0.029 0.21  0.003 0.095 0.255 0.    0.243 0.    0.    0.224
  0.    0.    0.    0.196 0.    0.001 0.723 0.187 0.    0.001 1.48  0.
  0.    0.002 0.11  0.    0.252 0.006]
 [0.001 0.    0.    0.184 0.008 0.002 1.719 0.    0.002 0.    0.    0.
  0.    0.    0.    0.166 0.    0.    0.156 0.124 0.    0.    0.284 0.
  0.    0.    0.    0.    0.862 0.   ]
 [0.    0.    0.    0.004 0.    0.    0.004 0.    0.001 0.    0.    0.
  0.    0.    0.    1.01  0.    0.    0.014 0.004 0.    0.    0.001 0.
  0.    0.    0.    0.    0.22  0.   ]
 [0.427 0.848 0.121 0.176 2.208 0.206 0.209 0.002 1.369 0.    0.    0.158
  0.    0.    0.    0.558 0.    0.312 0.196 0.814 0.    0.002 0.313 0.001
  0.    0.005 0.159 0.    0.505 0.271]
 [1.22  0.    0.001 0.254 0.001 0.11  0.135 0.    0.002 0.    0.    0.
  0.    0.    0.    0.131 0.    0.    0.252 0.937 0.    0.    0.139 0.
  0.    0.    0.135 0.    0.171 0.   ]
 [0.312 0.964 0.176 0.237 0.299 0.371 0.257 0.309 1.181 0.001 0.    0.218
  2.249 0.139 0.006 0.34  0.    0.4   0.287 0.11  0.002 0.483 0.475 0.086
  0.    0.17  0.255 0.002 0.348 1.874]
 [1.673 1.835 0.314 0.181 0.11  0.338 0.287 1.975 0.322 1.283 0.002 0.074
  0.174 0.3   0.005 1.056 0.001 0.286 0.186 0.154 0.027 0.091 0.276 0.02
  0.    0.21  0.079 0.016 1.107 0.402]
 [0.127 0.002 0.003 0.144 0.004 0.144 0.995 0.001 0.931 0.    0.    0.001
  0.    0.    0.    0.352 0.    0.001 0.228 0.128 0.    0.    0.138 0.
  0.    0.002 0.999 0.    0.159 0.   ]
 [0.147 1.727 0.098 0.185 2.362 0.37  0.276 1.383 0.793 0.    0.    1.756
  0.001 0.    0.    0.283 0.    0.18  0.589 0.094 0.001 2.582 0.506 0.004
  0.    0.031 1.099 0.    0.315 0.262]
 [0.216 0.232 2.761 1.163 0.132 1.724 0.345 0.156 0.411 0.002 0.001 0.477
  0.3   0.002 0.    0.517 0.    1.75  0.404 0.217 0.    0.123 0.304 2.64
  0.    0.529 0.374 0.007 0.223 2.426]
 [0.227 0.876 0.074 0.279 0.24  0.256 0.445 0.283 0.238 0.027 0.02  0.687
  2.52  0.083 0.002 0.272 0.    0.172 0.167 0.171 0.374 0.625 1.002 3.6
  0.005 0.102 0.444 0.003 0.517 0.267]
 [0.    0.    0.    0.    0.001 0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    0.006 0.    0.    0.003 0.001 0.    0.    0.    0.
  0.    0.    0.    0.    0.382 0.   ]
 [0.207 0.245 0.151 0.064 0.372 1.116 0.543 0.321 0.549 0.133 0.077 0.122
  1.093 0.097 3.081 0.468 0.001 0.032 0.247 0.081 0.743 1.377 0.206 0.324
  0.005 0.053 0.137 0.005 0.273 0.151]
 [0.172 0.    0.001 0.245 0.173 0.279 0.162 0.    1.12  0.    0.    0.202
  0.    0.    0.    0.228 0.    0.001 0.721 0.331 0.    0.    0.387 0.
  0.    0.    0.821 0.    0.171 0.001]
 [0.001 0.    0.001 0.001 0.    0.002 0.115 0.    0.    0.    0.    0.
  0.    0.    0.    0.172 0.    0.    0.011 0.    0.    0.    0.01  0.
  0.    0.    0.001 0.    0.034 0.   ]
 [0.    0.    0.    0.002 0.001 0.002 0.194 0.    0.    0.    0.    0.001
  0.    0.    0.    0.281 0.    0.    0.822 0.003 0.    0.    0.163 0.
  0.    0.    0.    0.    0.536 0.   ]
 [0.189 0.509 0.203 0.398 1.914 1.07  0.229 1.113 0.185 0.084 0.021 0.223
  0.225 3.745 0.    0.186 0.001 0.214 0.25  0.064 0.001 1.737 0.701 0.073
  0.001 2.438 0.263 0.    0.241 0.144]
 [0.085 0.232 2.476 1.104 1.613 0.244 0.656 0.31  0.473 0.    0.    0.378
  0.    0.    0.    0.266 0.    2.305 0.386 0.256 0.    0.001 0.455 0.
  0.    0.004 0.3   0.    0.648 2.13 ]
 [0.001 0.001 0.001 0.003 0.    0.001 0.874 0.    0.001 0.    0.    0.
  0.    0.    0.    0.23  0.    0.    0.499 0.004 0.    0.    0.005 0.
  0.    0.    0.001 0.    0.152 0.   ]
 [2.089 0.291 0.106 0.239 0.205 0.178 0.932 0.166 0.358 0.001 0.    2.189
  0.261 0.    0.    0.806 0.    0.087 0.808 0.879 0.    0.274 0.547 0.002
  0.    0.015 1.593 0.002 0.189 0.124]
 [0.226 0.458 0.716 0.137 0.176 0.548 0.534 0.275 0.116 1.16  4.384 0.159
  0.181 2.169 0.044 0.15  0.01  0.4   0.183 0.085 0.034 0.117 0.91  0.1
  0.001 2.796 0.112 0.004 0.227 0.932]
 [0.222 1.767 0.169 0.074 0.179 0.222 0.139 0.122 0.217 0.003 0.004 0.115
  0.017 0.    0.    0.111 0.    0.116 0.342 0.163 0.    0.124 1.339 0.035
  0.    0.003 0.063 0.    0.119 0.116]
 [0.001 0.    0.001 1.59  0.008 1.829 0.397 0.    0.002 0.    0.    0.
  0.    0.    0.    0.739 0.    0.    0.975 0.231 0.    0.    1.392 0.
  0.    0.    0.002 0.    0.879 0.   ]
 [2.097 0.335 0.175 0.242 0.139 0.209 0.147 0.097 0.23  0.013 0.007 0.985
  0.006 0.015 0.021 0.2   0.017 0.119 0.202 0.076 0.001 0.052 0.943 0.029
  0.003 2.999 0.139 0.    0.157 0.098]
 [0.001 0.    0.    0.002 0.003 0.001 0.002 0.001 0.    0.    0.    0.
  0.    0.    0.    0.011 0.    0.    0.045 0.007 0.    0.    0.006 0.
  0.    0.    0.001 0.    0.008 0.   ]
 [0.246 0.007 0.237 0.425 0.125 0.266 0.285 0.002 0.385 0.    0.    1.99
  0.    0.    0.    0.216 0.    0.324 0.703 0.184 0.    0.    0.187 0.
  0.    0.001 0.781 0.    0.11  0.002]]
[[0.    0.    0.    1.951 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.949 0.    0.    0.738 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.424 0.    0.    0.    0.    1.574 0.    0.    1.183 0.    0.    0.
  0.    0.    0.    0.847 0.    1.225 0.329 0.725 0.    0.    1.093 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.489 0.    0.45  0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.752 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.945 0.    0.    0.    0.673 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.723 0.    0.    0.    1.48  0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.719 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.862 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.01  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.427 0.848 0.    0.    2.208 0.    0.    0.    1.369 0.    0.    0.
  0.    0.    0.    0.558 0.    0.312 0.    0.814 0.    0.    0.313 0.
  0.    0.    0.    0.    0.505 0.   ]
 [1.22  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.937 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.312 0.964 0.    0.    0.    0.371 0.    0.309 1.181 0.    0.    0.
  2.249 0.    0.    0.34  0.    0.4   0.    0.    0.    0.483 0.475 0.
  0.    0.    0.    0.    0.348 1.874]
 [1.673 1.835 0.314 0.    0.    0.338 0.    1.975 0.322 1.283 0.    0.
  0.    0.    0.    1.056 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.107 0.402]
 [0.    0.    0.    0.    0.    0.    0.995 0.    0.931 0.    0.    0.
  0.    0.    0.    0.352 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.999 0.    0.    0.   ]
 [0.    1.727 0.    0.    2.362 0.37  0.    1.383 0.793 0.    0.    1.756
  0.    0.    0.    0.    0.    0.    0.589 0.    0.    2.582 0.506 0.
  0.    0.    1.099 0.    0.315 0.   ]
 [0.    0.    2.761 1.163 0.    1.724 0.345 0.    0.411 0.    0.    0.477
  0.    0.    0.    0.517 0.    1.75  0.404 0.    0.    0.    0.304 2.64
  0.    0.529 0.374 0.    0.    2.426]
 [0.    0.876 0.    0.    0.    0.    0.445 0.    0.    0.    0.    0.687
  2.52  0.    0.    0.    0.    0.    0.    0.    0.374 0.625 1.002 3.6
  0.    0.    0.444 0.    0.517 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.382 0.   ]
 [0.    0.    0.    0.    0.372 1.116 0.543 0.321 0.549 0.    0.    0.
  1.093 0.    3.081 0.468 0.    0.    0.    0.    0.743 1.377 0.    0.324
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.12  0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.721 0.331 0.    0.    0.387 0.
  0.    0.    0.821 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.822 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.536 0.   ]
 [0.    0.509 0.    0.398 1.914 1.07  0.    1.113 0.    0.    0.    0.
  0.    3.745 0.    0.    0.    0.    0.    0.    0.    1.737 0.701 0.
  0.    2.438 0.    0.    0.    0.   ]
 [0.    0.    2.476 1.104 1.613 0.    0.656 0.31  0.473 0.    0.    0.378
  0.    0.    0.    0.    0.    2.305 0.386 0.    0.    0.    0.455 0.
  0.    0.    0.    0.    0.648 2.13 ]
 [0.    0.    0.    0.    0.    0.    0.874 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [2.089 0.    0.    0.    0.    0.    0.932 0.    0.358 0.    0.    2.189
  0.    0.    0.    0.806 0.    0.    0.808 0.879 0.    0.    0.547 0.
  0.    0.    1.593 0.    0.    0.   ]
 [0.    0.458 0.716 0.    0.    0.548 0.534 0.    0.    1.16  4.384 0.
  0.    2.169 0.    0.    0.    0.4   0.    0.    0.    0.    0.91  0.
  0.    2.796 0.    0.    0.    0.932]
 [0.    1.767 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.342 0.    0.    0.    1.339 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.59  0.    1.829 0.397 0.    0.    0.    0.    0.
  0.    0.    0.    0.739 0.    0.    0.975 0.    0.    0.    1.392 0.
  0.    0.    0.    0.    0.879 0.   ]
 [2.097 0.335 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.985
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.943 0.
  0.    2.999 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.425 0.    0.    0.    0.    0.385 0.    0.    1.99
  0.    0.    0.    0.    0.    0.324 0.703 0.    0.    0.    0.    0.
  0.    0.    0.781 0.    0.    0.   ]]
{'fdr': 0.3352272727272727, 'tpr': 0.975, 'fpr': 0.1873015873015873, 'f1': 0.7905405405405407, 'shd': 62, 'npred': 176, 'ntrue': 120}
[2.395e-04 9.372e-04 1.951e+00 1.919e-03 1.795e-01 2.097e-01 5.311e-05
 3.240e-04 9.755e-06 1.996e-05 2.039e-03 5.553e-05 6.472e-05 3.431e-05
 9.485e-01 2.036e-05 6.017e-05 7.379e-01 1.957e-01 7.094e-07 6.012e-05
 2.151e-01 1.094e-04 2.036e-06 1.089e-04 1.804e-01 8.946e-06 2.665e-01
 3.443e-04 4.241e-01 4.344e-03 1.045e-01 2.041e-01 1.574e+00 1.745e-01
 3.704e-04 1.183e+00 7.709e-05 1.377e-04 1.371e-01 1.493e-04 1.173e-04
 1.067e-04 8.473e-01 4.261e-05 1.225e+00 3.289e-01 7.246e-01 1.114e-05
 1.594e-03 1.093e+00 7.986e-04 2.578e-07 4.690e-04 1.850e-01 2.224e-05
 2.430e-01 1.182e-01 1.660e-01 9.720e-02 4.893e-01 3.361e-02 4.504e-01
 2.412e-01 3.565e-03 9.485e-02 4.271e-06 9.640e-05 1.734e-01 1.277e-05
 2.377e-04 3.487e-06 7.522e-01 5.517e-06 2.941e-01 1.198e-01 7.801e-02
 1.856e-05 5.228e-05 1.835e-01 3.377e-04 1.751e-05 2.175e-03 1.898e-01
 1.172e-04 2.314e-01 5.241e-03 1.099e-04 3.684e-05 1.263e-04 5.302e-04
 1.050e-03 1.569e-01 4.821e-05 3.934e-05 1.237e-05 7.142e-05 6.011e-05
 1.533e-05 1.726e-05 1.768e-06 9.454e-01 5.068e-06 2.730e-05 2.151e-01
 6.726e-01 5.738e-06 1.016e-05 1.238e-01 1.428e-05 9.719e-06 3.957e-05
 1.556e-04 2.582e-05 2.771e-01 2.869e-05 8.355e-02 4.872e-03 2.880e-02
 2.102e-01 9.478e-02 2.552e-01 3.171e-04 2.433e-01 2.653e-05 1.298e-04
 2.243e-01 2.870e-05 1.370e-04 1.503e-05 1.957e-01 1.305e-06 1.083e-03
 7.225e-01 1.866e-01 4.562e-05 6.259e-04 1.480e+00 3.215e-04 7.827e-07
 1.703e-03 1.103e-01 2.174e-04 2.518e-01 5.917e-03 6.106e-04 3.922e-04
 9.452e-05 1.839e-01 7.610e-03 1.719e+00 2.734e-04 1.606e-03 1.424e-05
 1.690e-05 4.560e-05 5.749e-05 7.008e-05 5.583e-06 1.660e-01 2.420e-06
 1.687e-04 1.565e-01 1.242e-01 2.506e-06 7.874e-05 2.844e-01 7.236e-05
 1.463e-06 1.092e-04 9.782e-05 1.208e-05 8.623e-01 4.528e-05 3.535e-04
 1.894e-04 1.478e-04 4.200e-03 1.070e-04 3.260e-04 1.055e-04 9.897e-04
 1.573e-05 5.068e-05 5.779e-05 1.398e-05 1.593e-06 1.353e-05 1.010e+00
 9.363e-06 9.746e-05 1.362e-02 3.587e-03 1.810e-06 1.438e-05 6.047e-04
 1.978e-04 3.421e-06 1.509e-04 1.018e-04 4.979e-05 2.201e-01 6.828e-05
 4.269e-01 8.480e-01 1.211e-01 1.755e-01 2.208e+00 2.063e-01 2.089e-01
 1.369e+00 4.581e-05 7.043e-05 1.583e-01 1.422e-04 1.255e-04 1.939e-05
 5.579e-01 9.650e-06 3.125e-01 1.963e-01 8.137e-01 3.684e-05 2.223e-03
 3.127e-01 1.470e-03 5.850e-06 4.877e-03 1.591e-01 1.319e-04 5.049e-01
 2.709e-01 1.220e+00 1.764e-04 1.265e-03 2.541e-01 8.395e-04 1.101e-01
 1.351e-01 7.575e-05 3.768e-06 1.526e-04 1.070e-04 1.835e-05 6.043e-05
 6.036e-06 1.307e-01 7.502e-06 1.706e-04 2.516e-01 9.375e-01 2.811e-06
 1.250e-04 1.390e-01 6.174e-05 3.780e-07 2.274e-04 1.348e-01 3.806e-05
 1.713e-01 6.255e-05 3.120e-01 9.644e-01 1.765e-01 2.368e-01 2.991e-01
 3.705e-01 2.569e-01 3.093e-01 1.181e+00 9.179e-05 2.183e-01 2.249e+00
 1.390e-01 6.485e-03 3.398e-01 1.919e-04 3.995e-01 2.868e-01 1.103e-01
 2.475e-03 4.830e-01 4.746e-01 8.617e-02 8.617e-05 1.701e-01 2.548e-01
 1.723e-03 3.479e-01 1.874e+00 1.673e+00 1.835e+00 3.138e-01 1.807e-01
 1.105e-01 3.380e-01 2.867e-01 1.975e+00 3.221e-01 1.283e+00 7.389e-02
 1.744e-01 2.996e-01 4.513e-03 1.056e+00 1.114e-03 2.858e-01 1.858e-01
 1.542e-01 2.727e-02 9.135e-02 2.756e-01 2.017e-02 2.771e-04 2.102e-01
 7.879e-02 1.611e-02 1.107e+00 4.021e-01 1.267e-01 1.872e-03 3.277e-03
 1.438e-01 4.396e-03 1.439e-01 9.952e-01 5.403e-04 9.312e-01 2.384e-05
 1.085e-04 2.585e-05 5.301e-05 3.011e-06 3.518e-01 2.719e-05 6.805e-04
 2.279e-01 1.281e-01 2.298e-06 3.995e-05 1.380e-01 2.802e-04 4.326e-06
 1.760e-03 9.985e-01 6.522e-05 1.594e-01 2.385e-04 1.474e-01 1.727e+00
 9.846e-02 1.851e-01 2.362e+00 3.697e-01 2.762e-01 1.383e+00 7.930e-01
 1.855e-04 4.019e-05 1.756e+00 1.409e-04 8.346e-05 2.825e-01 2.688e-05
 1.804e-01 5.893e-01 9.369e-02 8.102e-04 2.582e+00 5.057e-01 4.163e-03
 7.034e-06 3.095e-02 1.099e+00 4.974e-04 3.149e-01 2.617e-01 2.157e-01
 2.322e-01 2.761e+00 1.163e+00 1.324e-01 1.724e+00 3.454e-01 1.562e-01
 4.114e-01 2.215e-03 1.381e-03 4.765e-01 2.998e-01 7.131e-05 5.169e-01
 3.174e-04 1.750e+00 4.037e-01 2.166e-01 1.386e-04 1.232e-01 3.037e-01
 2.640e+00 3.103e-04 5.289e-01 3.742e-01 7.304e-03 2.229e-01 2.426e+00
 2.273e-01 8.758e-01 7.430e-02 2.793e-01 2.400e-01 2.556e-01 4.452e-01
 2.830e-01 2.383e-01 2.662e-02 2.030e-02 6.867e-01 2.520e+00 8.350e-02
 2.723e-01 2.102e-04 1.725e-01 1.672e-01 1.711e-01 3.744e-01 6.248e-01
 1.002e+00 3.600e+00 5.211e-03 1.017e-01 4.443e-01 2.744e-03 5.167e-01
 2.668e-01 5.980e-05 2.838e-04 2.724e-04 1.345e-04 5.069e-04 1.688e-04
 1.221e-03 9.040e-05 1.519e-04 5.726e-05 3.774e-05 7.536e-05 9.967e-05
 4.014e-05 4.358e-06 7.114e-06 9.330e-05 3.468e-03 7.691e-04 2.891e-06
 8.625e-06 2.726e-04 2.262e-05 2.462e-06 1.294e-04 1.476e-04 1.579e-05
 3.817e-01 4.249e-05 2.068e-01 2.454e-01 1.512e-01 6.389e-02 3.715e-01
 1.116e+00 5.434e-01 3.213e-01 5.492e-01 1.327e-01 7.672e-02 1.215e-01
 1.093e+00 9.672e-02 3.081e+00 4.676e-01 3.154e-02 2.473e-01 8.119e-02
 7.433e-01 1.377e+00 2.060e-01 3.240e-01 4.838e-03 5.258e-02 1.367e-01
 4.853e-03 2.729e-01 1.512e-01 1.719e-01 2.978e-04 5.660e-04 2.453e-01
 1.732e-01 2.787e-01 1.622e-01 2.957e-04 1.120e+00 7.648e-06 5.343e-06
 2.024e-01 4.671e-05 2.037e-05 4.693e-06 2.283e-01 3.270e-07 7.210e-01
 3.314e-01 2.015e-05 2.805e-04 3.865e-01 8.903e-05 6.462e-06 1.302e-04
 8.214e-01 9.622e-06 1.709e-01 9.332e-04 8.533e-04 3.006e-04 1.271e-03
 8.864e-04 2.970e-04 1.694e-03 1.153e-01 4.242e-05 1.490e-04 4.065e-05
 1.142e-05 3.179e-04 9.263e-05 9.634e-05 5.464e-05 1.716e-01 4.137e-05
 1.678e-04 2.856e-04 3.879e-06 5.552e-05 9.654e-03 3.525e-04 8.651e-07
 4.347e-04 7.655e-04 1.521e-04 3.430e-02 8.268e-05 2.680e-04 5.719e-05
 4.148e-04 2.459e-03 7.040e-04 2.320e-03 1.939e-01 4.511e-04 1.540e-04
 5.249e-06 2.332e-05 5.084e-04 1.963e-05 5.261e-05 8.060e-05 2.811e-01
 2.280e-05 4.673e-05 8.217e-01 1.334e-05 7.112e-06 1.633e-01 2.316e-04
 1.325e-05 1.911e-04 4.238e-04 3.150e-05 5.357e-01 5.917e-05 1.887e-01
 5.089e-01 2.031e-01 3.983e-01 1.914e+00 1.070e+00 2.292e-01 1.113e+00
 1.854e-01 8.420e-02 2.065e-02 2.229e-01 2.249e-01 3.745e+00 3.807e-04
 1.857e-01 6.123e-04 2.144e-01 2.498e-01 6.410e-02 1.737e+00 7.008e-01
 7.307e-02 9.131e-04 2.438e+00 2.629e-01 4.788e-04 2.410e-01 1.435e-01
 8.528e-02 2.319e-01 2.476e+00 1.104e+00 1.613e+00 2.438e-01 6.560e-01
 3.099e-01 4.729e-01 4.774e-05 8.842e-06 3.782e-01 3.001e-05 5.814e-05
 1.263e-05 2.663e-01 5.544e-06 2.305e+00 3.860e-01 2.564e-01 4.725e-05
 4.548e-01 2.525e-04 1.239e-06 3.690e-03 2.998e-01 7.760e-05 6.480e-01
 2.130e+00 6.285e-04 8.447e-04 7.174e-04 3.300e-03 4.976e-04 7.285e-04
 8.739e-01 6.772e-05 9.831e-04 5.465e-06 6.351e-05 2.025e-04 9.537e-05
 4.055e-05 2.985e-06 2.298e-01 2.411e-06 8.967e-05 4.993e-01 4.488e-03
 2.592e-06 3.749e-05 1.267e-04 6.791e-06 2.200e-04 8.838e-04 1.738e-05
 1.524e-01 6.587e-05 2.089e+00 2.912e-01 1.056e-01 2.391e-01 2.053e-01
 1.775e-01 9.324e-01 1.656e-01 3.577e-01 1.493e-03 5.609e-05 2.189e+00
 2.605e-01 5.977e-05 1.140e-04 8.063e-01 5.985e-05 8.715e-02 8.084e-01
 8.787e-01 6.157e-06 2.744e-01 5.475e-01 4.921e-05 1.536e-02 1.593e+00
 1.526e-03 1.889e-01 1.239e-01 2.261e-01 4.577e-01 7.163e-01 1.370e-01
 1.756e-01 5.478e-01 5.335e-01 2.754e-01 1.156e-01 1.160e+00 4.384e+00
 1.591e-01 1.806e-01 2.169e+00 4.447e-02 1.495e-01 1.033e-02 4.005e-01
 1.829e-01 8.467e-02 3.379e-02 1.174e-01 9.100e-01 1.001e-01 2.796e+00
 1.122e-01 3.968e-03 2.272e-01 9.323e-01 2.224e-01 1.767e+00 1.690e-01
 7.385e-02 1.790e-01 2.216e-01 1.389e-01 1.224e-01 2.173e-01 3.240e-03
 3.955e-03 1.145e-01 1.729e-02 4.049e-04 9.079e-05 1.112e-01 7.995e-05
 1.156e-01 3.424e-01 1.633e-01 2.638e-05 1.238e-01 1.339e+00 3.532e-02
 4.039e-04 6.275e-02 8.965e-05 1.195e-01 1.165e-01 1.402e-03 2.818e-04
 7.634e-04 1.590e+00 7.578e-03 1.829e+00 3.969e-01 2.788e-04 2.014e-03
 1.521e-05 3.763e-05 3.074e-04 2.935e-05 5.122e-05 3.925e-06 7.392e-01
 5.835e-06 1.366e-04 9.752e-01 2.313e-01 3.354e-06 4.664e-05 1.392e+00
 1.994e-04 1.134e-06 3.427e-04 7.987e-06 8.789e-01 1.299e-04 2.097e+00
 3.352e-01 1.753e-01 2.423e-01 1.394e-01 2.095e-01 1.466e-01 9.657e-02
 2.301e-01 1.339e-02 7.005e-03 9.846e-01 6.237e-03 1.540e-02 2.148e-02
 1.997e-01 1.696e-02 1.188e-01 2.018e-01 7.564e-02 1.451e-03 5.242e-02
 9.426e-01 2.886e-02 3.063e-03 2.999e+00 1.385e-01 1.572e-01 9.786e-02
 7.352e-04 1.906e-04 4.244e-04 2.260e-03 2.782e-03 9.599e-04 2.248e-03
 6.248e-04 3.467e-04 9.641e-06 3.364e-05 2.553e-04 3.480e-05 3.096e-05
 3.210e-05 1.118e-02 3.264e-05 2.018e-04 4.515e-02 7.064e-03 1.993e-05
 8.689e-05 5.567e-03 1.558e-04 5.355e-05 2.560e-04 9.968e-04 3.936e-06
 2.620e-04 2.463e-01 6.702e-03 2.371e-01 4.249e-01 1.255e-01 2.655e-01
 2.851e-01 2.105e-03 3.846e-01 3.463e-05 4.175e-05 1.990e+00 3.188e-06
 1.061e-04 1.710e-05 2.157e-01 4.753e-06 3.238e-01 7.034e-01 1.844e-01
 2.065e-05 7.377e-05 1.868e-01 3.285e-04 2.297e-05 1.041e-03 7.806e-01
 5.672e-05 1.096e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.9931111111111111, 0.973642239941634)
cuda
9630
cuda
Objective function 1220.30 = squared loss an data 964.29 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 397 ; DAG False
||w||^2 0.31103898583316303
exp ma of ||w||^2 6.462598366373561
||w|| 0.5577086926282959
exp ma of ||w|| 0.541727602442282
||w||^2 0.2603096053055529
exp ma of ||w||^2 0.22876049879612798
||w|| 0.5102054540139226
exp ma of ||w|| 0.4728778340664582
||w||^2 0.1822233053760141
exp ma of ||w||^2 0.22536463092014924
||w|| 0.42687621786182245
exp ma of ||w|| 0.4666222075433303
||w||^2 0.2254037085028778
exp ma of ||w||^2 0.22916086417043235
||w|| 0.4747670044378377
exp ma of ||w|| 0.4716409323765238
||w||^2 0.17172138710360849
exp ma of ||w||^2 0.2386636873333379
||w|| 0.4143927932573255
exp ma of ||w|| 0.4783514914424045
||w||^2 0.1809872909773723
exp ma of ||w||^2 0.24612530798090917
||w|| 0.4254260111668917
exp ma of ||w|| 0.4824299512861923
||w||^2 0.16428481912078355
exp ma of ||w||^2 0.2415800437502932
||w|| 0.405320637422749
exp ma of ||w|| 0.4789914097894596
cuda
Objective function 362.94 = squared loss an data 358.48 + 0.5*rho*h**2 3.333668 + alpha*h 0.000000 + L2reg 0.61 + L1reg 0.51 ; SHD = 159 ; DAG False
Proportion of microbatches that were clipped  0.7359479199745951
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.582118548767653
iteration 1 in outer loop, alpha = 2.582118548767653, rho = 1.0, h = 2.582118548767653
cuda
9630
cuda
Objective function 369.61 = squared loss an data 358.48 + 0.5*rho*h**2 3.333668 + alpha*h 6.667336 + L2reg 0.61 + L1reg 0.51 ; SHD = 159 ; DAG False
||w||^2 395500.83061953454
exp ma of ||w||^2 6718811.897693749
||w|| 628.8885677284446
exp ma of ||w|| 1473.617092343754
||w||^2 133964.41746992094
exp ma of ||w||^2 3950986.8805173035
||w|| 366.0114990952073
exp ma of ||w|| 1028.6195699304444
||w||^2 481.0973275936878
exp ma of ||w||^2 177321.81040524706
||w|| 21.933930965371616
exp ma of ||w|| 132.97663757934063
||w||^2 0.6194823977275931
exp ma of ||w||^2 0.5170321681260651
||w|| 0.7870720410023425
exp ma of ||w|| 0.6941690107284546
||w||^2 0.2457417510300135
exp ma of ||w||^2 0.5374897430780851
||w|| 0.49572346225492847
exp ma of ||w|| 0.706974817946633
cuda
Objective function 145.36 = squared loss an data 135.32 + 0.5*rho*h**2 2.556548 + alpha*h 5.838727 + L2reg 1.14 + L1reg 0.51 ; SHD = 161 ; DAG False
Proportion of microbatches that were clipped  0.7499594353399318
iteration 1 in inner loop, alpha 2.582118548767653 rho 1.0 h 2.2612155394674645
9630
cuda
Objective function 168.37 = squared loss an data 135.32 + 0.5*rho*h**2 25.565479 + alpha*h 5.838727 + L2reg 1.14 + L1reg 0.51 ; SHD = 161 ; DAG False
||w||^2 0.7488001288251989
exp ma of ||w||^2 1.1536671214515988
||w|| 0.8653323805481907
exp ma of ||w|| 1.0468493026003662
||w||^2 1.0708719568853065
exp ma of ||w||^2 1.238568610072802
||w|| 1.034829433716159
exp ma of ||w|| 1.087663335844091
||w||^2 0.6747478267081324
exp ma of ||w||^2 1.218950870880784
||w|| 0.8214303541433884
exp ma of ||w|| 1.0620304938987168
cuda
Objective function 93.97 = squared loss an data 82.67 + 0.5*rho*h**2 6.388012 + alpha*h 2.918596 + L2reg 1.54 + L1reg 0.45 ; SHD = 131 ; DAG False
Proportion of microbatches that were clipped  0.7707651550568801
iteration 2 in inner loop, alpha 2.582118548767653 rho 10.0 h 1.130310759601791
9630
cuda
Objective function 151.46 = squared loss an data 82.67 + 0.5*rho*h**2 63.880121 + alpha*h 2.918596 + L2reg 1.54 + L1reg 0.45 ; SHD = 131 ; DAG False
||w||^2 756687925345.7583
exp ma of ||w||^2 250042536495.1936
||w|| 869878.1094761256
exp ma of ||w|| 289043.2353168955
||w||^2 380466024777.9403
exp ma of ||w||^2 294841350119.4825
||w|| 616819.2804849248
exp ma of ||w|| 351233.02141637285
||w||^2 6831617399.422354
exp ma of ||w||^2 50348417423.84952
||w|| 82653.59882922433
exp ma of ||w|| 196369.2847510283
||w||^2 8355752.510000453
exp ma of ||w||^2 101746519.65549646
||w|| 2890.631853073036
exp ma of ||w|| 6329.438185219084
||w||^2 93196.79307101482
exp ma of ||w||^2 2914593.110917377
||w|| 305.2814980817128
exp ma of ||w|| 681.892932335004
||w||^2 5.839042003040985
exp ma of ||w||^2 1061.5448967213033
||w|| 2.416410975608451
exp ma of ||w|| 7.366279891417243
||w||^2 4.820331409869134
exp ma of ||w||^2 173.49528587805665
||w|| 2.195525315242148
exp ma of ||w|| 3.602365015670885
||w||^2 1.5072891899686744
exp ma of ||w||^2 38.76689571877085
||w|| 1.2277170642980713
exp ma of ||w|| 2.220348449231401
||w||^2 1.0741887455402916
exp ma of ||w||^2 2.5492762429663007
||w|| 1.0364307721890023
exp ma of ||w|| 1.5461773450152685
cuda
Objective function 89.77 = squared loss an data 76.91 + 0.5*rho*h**2 9.563041 + alpha*h 1.129248 + L2reg 1.78 + L1reg 0.38 ; SHD = 125 ; DAG True
Proportion of microbatches that were clipped  0.782547391508695
iteration 3 in inner loop, alpha 2.582118548767653 rho 100.0 h 0.43733377125953155
iteration 2 in outer loop, alpha = 46.31549567472081, rho = 100.0, h = 0.43733377125953155
cuda
9630
cuda
Objective function 108.89 = squared loss an data 76.91 + 0.5*rho*h**2 9.563041 + alpha*h 20.255330 + L2reg 1.78 + L1reg 0.38 ; SHD = 125 ; DAG True
||w||^2 13926643227.96355
exp ma of ||w||^2 46239603823.8736
||w|| 118011.1995870034
exp ma of ||w|| 181688.87153746595
||w||^2 647761.0709922737
exp ma of ||w||^2 29489447.893659882
||w|| 804.8360522443522
exp ma of ||w|| 2604.311524847027
||w||^2 10.444152993519145
exp ma of ||w||^2 2225.3199396952923
||w|| 3.2317414799948256
exp ma of ||w|| 9.075148536842537
||w||^2 9.687461830236854
exp ma of ||w||^2 1666.45817603132
||w|| 3.1124687677528353
exp ma of ||w|| 7.719785036000932
||w||^2 7.867898094635123
exp ma of ||w||^2 583.1545433923193
||w|| 2.8049773786316217
exp ma of ||w|| 4.926799465226247
||w||^2 2.9824088662931687
exp ma of ||w||^2 57.74586568706786
||w|| 1.726965218611298
exp ma of ||w|| 2.4837670834483316
||w||^2 2.5698089498320473
exp ma of ||w||^2 4.847661065919864
||w|| 1.6030623661704642
exp ma of ||w|| 1.913999367987265
||w||^2 3.2099850069616482
exp ma of ||w||^2 3.336103088435571
||w|| 1.791643102563021
exp ma of ||w|| 1.7634014767544641
||w||^2 2.626212043991617
exp ma of ||w||^2 3.2478913868570403
||w|| 1.620559176331311
exp ma of ||w|| 1.7353746301746376
cuda
Objective function 97.16 = squared loss an data 75.77 + 0.5*rho*h**2 4.770753 + alpha*h 14.306546 + L2reg 1.95 + L1reg 0.36 ; SHD = 115 ; DAG True
Proportion of microbatches that were clipped  0.780380673499268
iteration 1 in inner loop, alpha 46.31549567472081 rho 100.0 h 0.3088932842686489
9630
cuda
Objective function 140.10 = squared loss an data 75.77 + 0.5*rho*h**2 47.707531 + alpha*h 14.306546 + L2reg 1.95 + L1reg 0.36 ; SHD = 115 ; DAG True
||w||^2 3992579.179034834
exp ma of ||w||^2 84556643.85650869
||w|| 1998.1439335130074
exp ma of ||w|| 4928.559244780842
||w||^2 1518.9808339770211
exp ma of ||w||^2 161030.90409131366
||w|| 38.97410465908128
exp ma of ||w|| 76.14528922682115
||w||^2 15.381424769046253
exp ma of ||w||^2 21.072052624180557
||w|| 3.921915956397619
exp ma of ||w|| 2.5173932320912704
v before min max tensor([[ 4.515e+02, -1.426e+02, -2.404e+02,  ..., -2.519e+01, -2.497e+02,
         -4.002e+02],
        [-1.687e+02, -1.674e+02, -3.458e+02,  ...,  3.458e+02, -1.553e+02,
          2.596e+03],
        [ 3.855e+03, -3.312e+02,  5.118e+00,  ..., -2.601e+02,  6.360e+01,
          1.376e+03],
        ...,
        [ 2.396e+02, -2.102e+02, -3.313e+02,  ...,  5.905e+02,  4.124e+02,
          9.957e+03],
        [-3.275e+02, -3.259e+02, -1.134e+02,  ...,  1.279e+02, -3.291e+02,
         -3.852e+02],
        [-4.046e+02, -3.782e+02, -3.506e+01,  ...,  6.881e+02,  1.003e+03,
         -1.932e+02]], device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 5.118e+00,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        ...,
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e-12]], device='cuda:0')
v before min max tensor([-1.548e+02, -6.720e+01,  1.625e+03, -1.297e+02, -2.978e+02, -2.129e+02,
         4.508e+02, -3.479e+01,  7.034e+02, -2.005e+02,  1.915e+03, -3.789e+02,
        -3.331e+02, -3.440e+00,  1.339e+02,  2.652e+03, -2.554e+02, -3.552e+02,
         1.876e+02, -3.558e+02, -1.064e+02, -2.062e+02, -3.359e+02, -6.564e+01,
        -3.655e+02, -1.841e+02,  1.441e+02,  2.448e+01, -1.385e+02, -1.156e+02,
        -3.280e+00,  1.023e+02, -3.300e+02, -8.188e+01, -3.553e+02,  7.909e+02,
         1.718e+03,  3.589e+02, -1.949e+02,  3.650e+02,  3.910e+01, -3.817e+02,
         2.854e+01, -1.627e+01,  2.743e+00, -1.906e+02,  8.784e+01, -3.212e+02,
         4.817e+03, -9.887e+01,  1.247e+03, -3.748e+02, -3.130e+02, -2.069e+02,
         1.075e+03, -8.355e+01,  3.746e+02, -1.493e+02, -3.002e+02, -3.504e+02,
        -2.735e+02, -2.068e+02,  2.554e+03,  4.830e+02, -1.498e+02, -4.038e+01,
         2.099e+02, -1.254e+01,  5.034e+02,  4.977e+02, -6.219e+01, -2.684e+02,
        -1.854e+02,  3.761e+02, -1.634e+02,  5.355e+02, -7.636e-01, -1.861e+02,
        -3.235e+02, -2.126e+02,  3.356e+01, -2.656e+02, -1.456e+02, -1.754e+02,
        -1.164e+02, -2.742e+02, -3.064e+02, -3.268e+02,  3.962e+02, -9.895e+01,
        -3.513e+02,  1.277e+03,  1.490e+02, -3.019e+02,  7.770e+01, -4.144e+02,
        -4.171e+02, -6.872e+01,  3.460e+02, -3.122e+02,  1.724e+03,  1.894e+03,
         6.161e+02, -1.810e+02,  1.672e+02,  1.348e+03, -1.354e+02, -2.521e+02,
         6.086e+01,  3.629e+02,  3.228e+02, -2.566e+02, -3.557e+02, -3.452e+02,
         8.564e+02, -3.458e+02,  9.244e+02,  1.190e+02,  2.419e+02,  4.490e+02,
        -1.728e+02, -3.585e+02,  5.346e+01,  1.341e+02, -1.994e+02, -3.345e+02,
         1.917e+01, -2.642e+02,  4.458e+01,  1.556e+02, -1.418e+02, -2.715e+02,
        -3.461e+02,  5.528e+02, -3.226e+02, -2.815e+02,  1.260e+03, -4.262e+02,
         8.633e+02, -3.258e+01, -2.867e+01, -3.288e+02,  3.480e+02,  3.780e+02,
        -2.337e+02, -1.574e+02, -4.015e+02, -6.833e+00,  8.248e+02, -1.255e+02,
        -3.359e+02,  2.950e+02,  1.468e+02,  2.545e+03,  1.992e+01,  2.256e+02,
        -1.915e+02, -2.743e+02, -3.508e+02,  1.615e+02,  2.395e+02,  4.474e+00,
        -3.347e+02, -3.020e+02, -1.216e+02,  2.879e+02, -1.223e+02,  1.504e+02,
        -2.491e+02,  3.554e+02, -3.095e+02,  8.934e+02, -3.613e+02, -1.774e+02,
         1.394e+03,  6.993e+02, -2.073e+02, -1.867e+02,  2.654e+02, -2.331e+02,
        -2.452e+02, -3.145e+02,  3.304e+02,  1.412e+03,  1.022e+03, -2.736e+02,
        -3.964e+02,  5.293e+02, -1.622e+02,  2.112e+02, -3.240e+02, -1.984e+02,
        -1.462e+02, -2.607e+02,  4.766e+02,  1.678e+02,  1.425e+03, -1.461e+02,
         1.043e+02, -4.082e+02, -2.257e+02, -1.048e+02, -2.175e+02, -3.240e+02,
        -1.791e+02,  6.665e+02, -3.013e+02, -3.679e+02, -9.564e+01, -1.881e+02,
         1.284e+02, -3.203e+02, -3.364e+02,  6.536e+02, -1.651e+01,  2.674e+03,
        -1.978e+02,  3.204e+02, -1.546e+02, -1.680e+02,  7.651e+01, -2.966e+02,
        -2.585e+02, -4.404e+02, -3.012e+02, -2.436e+02,  1.623e+03,  1.292e+03,
        -1.006e+02, -3.036e+02,  5.299e+02, -2.772e+02,  9.579e+00, -1.432e+02,
        -3.629e+02,  2.511e+02, -1.068e+02,  2.128e+02, -2.704e+02, -3.225e+02,
        -4.602e+01, -2.249e+02,  1.251e+03, -2.630e+02, -2.931e+02, -2.014e+02,
         6.615e+02, -1.523e+02, -1.994e+02, -2.592e+02, -1.314e+02, -1.829e+02,
         2.228e+02,  3.459e+02, -1.842e+02, -2.970e+01,  8.520e+01, -2.800e+02,
        -2.367e+02,  3.663e+02, -2.851e+02,  1.733e+01, -3.644e+02, -3.015e+02,
        -3.215e+02, -7.145e+01, -3.968e+02, -2.996e+02, -2.286e+02, -3.380e+02,
         3.916e+03, -6.891e+01, -3.784e+02,  4.669e+02, -1.133e+02, -2.021e+02,
        -3.293e+02, -9.582e+01,  1.322e+02,  5.696e+01, -8.002e+01, -9.638e+01,
        -1.254e+02, -1.444e+02, -1.687e+02,  5.277e+02,  3.600e+02,  1.706e+03,
        -3.579e+02, -2.000e+02, -4.090e+02, -2.876e+02, -2.045e+02, -3.967e+02,
        -3.278e+02,  2.986e+02,  4.904e+02, -1.033e+02,  1.179e+03, -1.323e+02],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 2.743e+00, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 4.474e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 9.579e+00, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 1.354e+03],
         [-3.019e+02],
         [ 7.327e+02],
         [-2.793e+02],
         [ 8.058e+01],
         [ 5.686e+02],
         [-4.312e+02],
         [ 3.269e+02],
         [-3.298e+02],
         [ 2.690e+01]],

        [[-2.433e+02],
         [-4.350e+02],
         [-2.146e+02],
         [-3.851e+02],
         [ 4.766e+01],
         [ 9.649e+01],
         [ 2.949e+02],
         [-1.461e+02],
         [ 3.619e+02],
         [ 6.771e+02]],

        [[-2.597e+02],
         [-1.433e+02],
         [-1.842e+02],
         [-3.498e+02],
         [-3.129e+02],
         [-1.965e+02],
         [-1.366e+00],
         [-3.441e+02],
         [ 4.139e+02],
         [-1.002e+02]],

        [[-4.262e+02],
         [ 2.961e+02],
         [ 4.100e+02],
         [ 9.689e+02],
         [-3.602e+00],
         [ 1.884e+03],
         [-1.819e+02],
         [-3.012e+02],
         [ 7.200e+02],
         [ 1.172e+03]],

        [[ 1.101e+02],
         [ 7.314e+01],
         [ 8.189e+01],
         [ 4.018e+02],
         [ 5.456e+02],
         [-4.558e+02],
         [-2.822e+02],
         [-3.977e+02],
         [-2.826e+02],
         [-2.711e+02]],

        [[ 6.683e+02],
         [-3.023e+02],
         [-2.096e+02],
         [-1.438e+02],
         [-3.086e+02],
         [-1.963e+02],
         [ 1.762e+02],
         [-2.476e+02],
         [-3.586e+02],
         [ 1.860e+03]],

        [[ 1.628e+03],
         [-2.396e+02],
         [-3.946e+02],
         [ 3.651e+02],
         [-8.672e+00],
         [-7.453e+01],
         [-9.853e+01],
         [-2.828e+02],
         [-3.282e+02],
         [ 3.729e+02]],

        [[-2.225e+02],
         [-3.561e+02],
         [ 1.050e+03],
         [-1.728e+02],
         [ 1.057e+02],
         [ 8.110e+02],
         [-4.624e+01],
         [-1.149e+02],
         [-7.814e+01],
         [ 2.664e+02]],

        [[-3.699e+01],
         [-3.090e+02],
         [ 1.438e+02],
         [-2.705e+02],
         [-2.241e+02],
         [-1.973e+02],
         [ 1.486e+02],
         [-5.037e+01],
         [ 2.570e+03],
         [ 3.946e+01]],

        [[-3.467e+02],
         [-1.758e+02],
         [ 2.206e+02],
         [-3.705e+02],
         [-2.518e+02],
         [-1.189e+02],
         [ 1.734e+02],
         [-2.158e+02],
         [-1.292e+02],
         [ 6.059e+02]],

        [[-2.811e+02],
         [-2.396e+02],
         [-1.857e+02],
         [-2.665e+02],
         [ 3.147e+01],
         [ 1.861e+02],
         [ 1.711e+01],
         [ 2.069e+02],
         [-2.657e+02],
         [ 1.580e+02]],

        [[-1.436e+02],
         [-3.034e+02],
         [-2.313e+02],
         [ 2.423e+02],
         [ 3.002e+01],
         [-1.837e+02],
         [ 2.408e+02],
         [ 4.764e+01],
         [-2.645e+02],
         [-3.429e+02]],

        [[ 3.305e+02],
         [-1.830e+02],
         [ 1.174e+02],
         [-9.468e+01],
         [-2.887e+02],
         [ 1.162e+03],
         [-2.824e+01],
         [ 3.151e+02],
         [-3.073e+02],
         [ 3.662e+02]],

        [[-2.844e+02],
         [ 1.334e+03],
         [-2.149e+02],
         [-2.755e+02],
         [-2.319e+02],
         [-2.849e+02],
         [-2.907e+02],
         [-2.312e+02],
         [-3.252e+02],
         [-2.254e+02]],

        [[-3.148e+02],
         [-1.275e+02],
         [ 9.165e+02],
         [-1.385e+02],
         [-2.348e+01],
         [-1.863e+02],
         [ 4.756e+01],
         [-2.345e+02],
         [ 1.573e+02],
         [ 4.648e+01]],

        [[-3.162e+02],
         [-3.857e+02],
         [-4.114e+02],
         [ 1.284e+03],
         [-3.440e+02],
         [-1.347e+02],
         [ 1.703e+02],
         [ 6.102e+02],
         [-3.012e+02],
         [-3.256e+02]],

        [[-2.603e+02],
         [-3.812e+02],
         [-1.128e+02],
         [ 1.380e+02],
         [-3.265e+02],
         [-3.822e+02],
         [ 2.155e+02],
         [-1.857e+02],
         [ 1.442e+02],
         [-3.156e+02]],

        [[-3.468e+02],
         [-2.407e+02],
         [ 2.549e+02],
         [-2.258e+02],
         [ 1.755e+03],
         [-2.081e+02],
         [-2.141e+02],
         [-3.363e+02],
         [ 1.504e+02],
         [-3.797e+02]],

        [[ 6.487e+02],
         [-3.186e+02],
         [ 3.241e+02],
         [ 1.307e+02],
         [-2.524e+02],
         [-2.449e+02],
         [-7.141e+01],
         [-2.801e+02],
         [ 9.014e+02],
         [ 1.291e+03]],

        [[ 8.303e+02],
         [-2.341e+02],
         [-3.069e+02],
         [-2.620e+02],
         [ 9.359e+02],
         [-3.698e+02],
         [ 1.684e+02],
         [-2.074e+02],
         [ 2.158e+02],
         [-1.343e+02]],

        [[-2.865e+02],
         [-1.785e+02],
         [ 2.513e+02],
         [-3.649e+01],
         [ 1.973e+02],
         [-2.156e+02],
         [ 3.631e+02],
         [-4.055e+02],
         [-3.191e+02],
         [ 5.288e+01]],

        [[-2.172e+02],
         [-2.121e+02],
         [ 1.918e+02],
         [ 3.575e+02],
         [-2.954e+02],
         [-2.848e+02],
         [-2.891e+02],
         [-7.504e+01],
         [-3.233e+02],
         [-2.945e+02]],

        [[-2.441e+02],
         [-1.078e+02],
         [ 3.772e+02],
         [-3.105e+02],
         [-2.560e+02],
         [-2.676e+02],
         [ 3.823e+01],
         [-9.494e+01],
         [-1.087e+01],
         [-4.538e+02]],

        [[-2.609e+02],
         [-3.250e+02],
         [-3.252e+02],
         [-1.491e+02],
         [-2.727e+02],
         [-2.507e+02],
         [-2.925e+02],
         [-2.526e+02],
         [-2.264e+02],
         [ 1.279e+03]],

        [[-2.199e+02],
         [-2.823e+02],
         [-9.662e+01],
         [ 5.813e+02],
         [-3.929e+02],
         [-7.622e+01],
         [-3.960e+02],
         [-3.158e+02],
         [ 5.013e+02],
         [ 3.415e+02]],

        [[-2.454e+02],
         [-2.567e+02],
         [ 4.776e+03],
         [-4.178e+02],
         [-2.282e+02],
         [-1.577e+02],
         [-4.327e+02],
         [-2.883e+02],
         [-7.702e+01],
         [-1.845e+02]],

        [[ 8.930e+02],
         [-8.977e+01],
         [ 6.039e+02],
         [-3.230e+02],
         [ 1.612e+02],
         [-3.505e+02],
         [ 1.952e+02],
         [-1.764e+02],
         [-2.952e+02],
         [ 8.473e+01]],

        [[-1.929e+02],
         [ 3.118e+02],
         [-2.006e+02],
         [-6.807e-02],
         [ 1.643e+03],
         [-2.045e+02],
         [-1.023e+02],
         [-1.092e+02],
         [-2.596e+02],
         [ 9.421e+02]],

        [[-1.636e+02],
         [ 9.549e+01],
         [-1.886e+02],
         [ 8.977e+00],
         [-3.848e+02],
         [ 1.461e+02],
         [-2.881e+01],
         [-3.260e+01],
         [-2.711e+02],
         [ 8.079e+01]],

        [[ 1.507e+02],
         [-2.512e+02],
         [ 1.127e+02],
         [-3.144e+02],
         [-3.479e+02],
         [-3.214e+02],
         [-2.187e+02],
         [-1.795e+02],
         [-1.988e+02],
         [-3.608e+02]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [8.977e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-195.908],
        [-329.741],
        [-199.188],
        [-342.401],
        [-236.666],
        [-331.333],
        [ -56.635],
        [-179.932],
        [-118.464],
        [ -79.059],
        [-204.502],
        [ -93.643],
        [-221.927],
        [ -60.668],
        [-213.809],
        [-296.906],
        [ 175.252],
        [-314.017],
        [  12.518],
        [-165.079],
        [  84.746],
        [-247.162],
        [-283.160],
        [-342.357],
        [ 659.282],
        [-244.058],
        [ 999.731],
        [-209.106],
        [-177.090],
        [-313.999]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[ 0.011, -0.087,  0.040,  ...,  0.135, -0.018,  0.064],
        [-0.024, -0.111, -0.062,  ...,  0.148, -0.075,  0.112],
        [ 0.046, -0.061,  0.039,  ..., -0.046, -0.169, -0.031],
        ...,
        [ 0.102, -0.038, -0.043,  ..., -0.211,  0.013,  0.197],
        [ 0.042, -0.129, -0.096,  ..., -0.000,  0.046, -0.054],
        [-0.109, -0.176, -0.011,  ..., -0.031, -0.006, -0.083]],
       device='cuda:0')
s after update for 1 param tensor([[2.150, 1.625, 1.153,  ..., 1.788, 1.578, 1.830],
        [1.497, 1.353, 1.617,  ..., 1.806, 1.578, 2.123],
        [1.552, 1.760, 2.102,  ..., 1.192, 2.143, 1.737],
        ...,
        [1.882, 1.024, 1.768,  ..., 1.873, 1.847, 2.468],
        [1.651, 1.780, 1.560,  ..., 2.084, 1.701, 2.048],
        [2.045, 1.744, 1.948,  ..., 2.136, 1.640, 1.949]], device='cuda:0')
b after update for 1 param tensor([[184.056, 159.991, 134.798,  ..., 167.847, 157.702, 169.807],
        [153.562, 146.031, 159.599,  ..., 168.670, 157.667, 182.879],
        [156.371, 166.505, 181.973,  ..., 137.064, 183.744, 165.455],
        ...,
        [172.179, 127.035, 166.897,  ..., 171.775, 170.577, 197.180],
        [161.272, 167.488, 156.767,  ..., 181.213, 163.696, 179.650],
        [179.501, 165.786, 175.191,  ..., 183.437, 160.770, 175.234]],
       device='cuda:0')
clipping threshold 1.9337532558832238
a after update for 1 param tensor([ 1.986e-02, -1.372e-01, -4.810e-02,  9.700e-02, -3.371e-02,  7.418e-02,
         9.094e-02,  7.717e-02,  4.014e-02, -3.801e-02, -6.208e-02, -6.754e-02,
        -6.838e-03,  7.957e-02, -6.852e-02,  4.095e-02,  1.632e-02, -7.411e-02,
         1.367e-01,  1.442e-01,  1.125e-01, -3.214e-03,  5.172e-02, -8.220e-02,
        -5.948e-02, -1.494e-03,  2.364e-02, -1.096e-01, -1.795e-02, -3.540e-02,
        -9.225e-02, -7.397e-02, -1.115e-01, -1.313e-01,  3.959e-02, -5.722e-03,
         1.082e-01,  1.158e-02,  2.735e-02, -1.201e-03, -5.701e-02,  8.109e-03,
        -6.826e-02, -1.036e-01,  1.043e-02,  6.948e-02, -1.106e-01,  1.530e-01,
         3.521e-02, -6.335e-03, -2.101e-03, -3.614e-02, -5.775e-04, -6.095e-02,
        -1.737e-02, -1.627e-02, -1.293e-02,  4.950e-02,  1.621e-01,  3.495e-02,
        -1.794e-02, -1.008e-01, -1.915e-01,  3.356e-02,  2.557e-02, -5.906e-02,
         2.564e-01,  1.760e-01,  1.439e-02, -1.142e-01,  1.234e-02,  1.411e-02,
         5.544e-02, -1.821e-01, -1.432e-01, -1.007e-01, -1.050e-01, -1.627e-02,
        -2.348e-01, -3.332e-02, -1.892e-03, -2.371e-02,  9.950e-03, -5.180e-03,
         1.489e-02,  7.647e-02, -2.279e-01, -3.649e-02,  1.877e-01,  6.235e-02,
         9.293e-03,  1.145e-01, -1.539e-02, -4.835e-02,  3.307e-02, -2.022e-02,
         9.599e-02, -3.893e-02, -1.936e-01,  2.969e-02,  7.977e-02, -3.020e-01,
         1.971e-02,  6.133e-03, -3.400e-02,  1.185e-01, -2.147e-03, -1.875e-02,
        -1.548e-02, -3.367e-02,  6.630e-02, -2.504e-02,  1.897e-01, -2.541e-03,
        -1.653e-01,  1.454e-02, -2.240e-02, -1.570e-02, -2.181e-01,  5.533e-02,
        -6.649e-02, -1.892e-01,  4.519e-02, -2.460e-02, -4.140e-02,  1.353e-01,
        -1.302e-02,  2.209e-01,  2.015e-01,  4.976e-02,  2.428e-02,  1.939e-02,
         9.916e-02, -1.758e-01,  1.344e-01,  3.855e-02, -6.173e-02, -2.022e-03,
         1.701e-03,  2.343e-01,  2.460e-02, -5.681e-02, -4.418e-02,  1.339e-01,
        -1.502e-01, -1.215e-02,  2.541e-02,  7.179e-03, -6.254e-03,  9.529e-02,
        -7.004e-02,  6.648e-02, -9.294e-02, -3.291e-04,  4.140e-02,  7.676e-02,
         1.001e-01, -2.158e-02, -1.237e-01,  1.675e-03,  3.737e-02, -1.878e-02,
         1.806e-01,  2.537e-01,  2.487e-02, -6.421e-02,  4.952e-02, -2.691e-02,
        -1.592e-02,  1.303e-01, -1.907e-02,  6.579e-02, -2.209e-02,  1.630e-01,
        -1.843e-01,  4.359e-02, -7.596e-02,  1.663e-01, -9.731e-02,  3.626e-02,
        -1.636e-01,  8.277e-02,  4.955e-02,  4.629e-02,  4.863e-02, -6.343e-02,
        -1.986e-01, -1.468e-01,  7.566e-02,  3.049e-02, -7.946e-02, -6.893e-02,
        -5.392e-02,  5.193e-03,  7.382e-02, -3.214e-02, -1.151e-01, -1.315e-02,
        -1.644e-02,  5.749e-02,  3.879e-03,  1.184e-01, -2.111e-02, -1.053e-01,
        -6.281e-02, -3.247e-02, -1.464e-01, -3.171e-02,  1.224e-01,  8.044e-03,
        -1.926e-02, -3.748e-02,  6.697e-02, -6.770e-02,  1.135e-02,  3.939e-03,
         6.524e-02, -1.013e-01,  2.085e-03,  6.460e-02,  8.143e-02, -4.267e-03,
        -2.529e-02, -2.705e-02,  6.288e-03,  1.380e-01, -4.776e-02, -3.124e-01,
        -1.124e-03, -9.237e-04, -4.433e-02,  2.053e-03,  3.244e-02, -6.117e-02,
         6.283e-02, -8.953e-03, -3.134e-02,  1.867e-01, -6.423e-02,  3.292e-04,
        -9.394e-03, -1.879e-02,  8.308e-02,  1.387e-02,  1.047e-01, -1.746e-02,
        -1.078e-01, -2.286e-01, -5.562e-02,  1.983e-02,  1.283e-02, -1.886e-01,
         1.742e-02,  1.256e-01, -4.600e-02,  1.703e-04, -1.135e-01, -3.339e-02,
         2.525e-03, -1.440e-01, -7.624e-02, -2.496e-01,  6.809e-02, -2.670e-02,
        -1.154e-01, -3.383e-02,  3.532e-03, -3.391e-02,  8.738e-02, -1.396e-01,
         6.989e-02, -4.077e-02, -3.367e-02,  2.164e-01,  6.294e-02, -3.671e-03,
         9.234e-02, -1.019e-01,  1.016e-02,  1.641e-01,  6.537e-02, -2.450e-01,
         2.925e-02, -6.752e-02, -5.887e-02, -8.673e-02, -9.096e-02,  4.240e-02,
         1.697e-02, -5.354e-02,  2.983e-01,  1.064e-01, -1.340e-01, -3.130e-01,
        -5.450e-02, -6.762e-02, -1.231e-01, -8.239e-02, -2.172e-01, -4.418e-02],
       device='cuda:0')
s after update for 1 param tensor([1.641, 1.927, 2.284, 1.578, 1.439, 1.504, 1.912, 1.649, 2.095, 1.563,
        1.827, 1.754, 1.736, 1.204, 2.017, 1.821, 1.331, 1.680, 1.683, 1.923,
        1.593, 0.978, 1.597, 1.568, 1.685, 0.841, 2.102, 2.260, 1.726, 1.407,
        2.053, 1.553, 1.598, 1.868, 1.626, 1.976, 1.848, 1.319, 1.011, 1.988,
        1.800, 1.905, 1.937, 1.843, 1.132, 0.970, 1.908, 1.613, 2.026, 0.743,
        2.245, 1.718, 1.630, 1.216, 2.200, 1.722, 1.961, 1.727, 1.908, 1.724,
        1.666, 1.761, 2.065, 1.806, 1.320, 1.561, 2.089, 1.608, 2.025, 1.386,
        0.294, 1.278, 1.584, 1.659, 1.207, 2.082, 1.810, 1.250, 1.556, 1.077,
        1.553, 1.220, 0.737, 0.805, 1.624, 1.316, 1.762, 1.678, 2.052, 1.794,
        1.607, 1.727, 2.196, 1.459, 1.561, 1.908, 2.026, 1.689, 2.106, 1.701,
        1.700, 2.177, 1.821, 0.855, 1.637, 1.873, 1.395, 1.154, 1.345, 1.238,
        1.178, 1.326, 1.786, 1.578, 2.145, 1.637, 1.792, 1.377, 2.199, 1.359,
        1.881, 1.827, 1.583, 1.497, 0.957, 1.894, 1.062, 1.996, 1.638, 1.898,
        0.848, 1.241, 1.709, 1.643, 1.482, 1.287, 1.826, 1.974, 1.325, 1.961,
        0.172, 1.503, 2.174, 1.787, 1.790, 1.733, 1.881, 0.103, 1.627, 1.315,
        1.541, 1.981, 1.911, 2.109, 1.784, 1.907, 1.169, 1.263, 1.740, 1.596,
        1.588, 1.760, 1.974, 1.382, 1.608, 1.523, 1.573, 1.496, 1.673, 1.993,
        1.494, 1.821, 1.756, 1.175, 1.569, 1.817, 1.140, 1.341, 1.970, 1.070,
        1.218, 1.773, 1.774, 1.682, 1.264, 1.515, 1.837, 1.697, 1.332, 2.022,
        1.761, 1.734, 1.460, 1.325, 1.852, 1.627, 1.487, 1.583, 1.483, 1.890,
        1.388, 1.385, 1.166, 1.573, 1.107, 1.783, 1.406, 1.692, 1.682, 1.761,
        1.166, 1.624, 1.765, 1.603, 0.113, 1.847, 0.904, 1.933, 0.838, 1.632,
        1.815, 2.046, 1.184, 2.067, 1.530, 1.272, 2.119, 2.102, 1.693, 1.389,
        1.765, 1.497, 1.659, 1.119, 1.664, 1.647, 1.284, 1.966, 1.277, 1.602,
        1.276, 1.059, 1.929, 1.213, 1.365, 0.955, 1.748, 1.562, 1.723, 1.255,
        1.016, 1.854, 2.101, 1.669, 1.638, 1.700, 1.589, 1.854, 1.200, 1.753,
        1.353, 2.345, 1.980, 1.743, 1.523, 0.616, 1.829, 1.382, 1.810, 1.578,
        1.605, 1.703, 1.782, 2.051, 1.289, 0.956, 1.512, 1.125, 1.552, 1.941,
        1.570, 2.146, 0.868, 1.566, 1.492, 1.929, 1.670, 1.401, 1.917, 1.440,
        1.901, 1.552, 0.981, 1.903, 1.498, 2.061, 1.859, 1.881, 1.729, 1.620],
       device='cuda:0')
b after update for 1 param tensor([160.806, 174.227, 189.707, 157.669, 150.570, 153.952, 173.572, 161.192,
        181.691, 156.918, 169.648, 166.242, 165.392, 137.727, 178.280, 169.375,
        144.830, 162.703, 162.832, 174.075, 158.451, 124.151, 158.650, 157.200,
        162.961, 115.135, 181.979, 188.723, 164.911, 148.877, 179.857, 156.426,
        158.674, 171.572, 160.037, 176.456, 170.623, 144.135, 126.221, 176.980,
        168.423, 173.245, 174.686, 170.384, 133.536, 123.597, 173.369, 159.423,
        178.648, 108.163, 188.069, 164.511, 160.256, 138.427, 186.186, 164.728,
        175.778, 164.942, 173.364, 164.799, 162.006, 166.580, 180.368, 168.676,
        144.190, 156.833, 181.402, 159.190, 178.622, 147.764,  68.113, 141.877,
        157.977, 161.655, 137.904, 181.128, 168.861, 140.364, 156.578, 130.241,
        156.437, 138.644, 107.772, 112.599, 159.983, 144.012, 166.601, 162.622,
        179.794, 168.138, 159.104, 164.933, 186.030, 151.618, 156.835, 173.377,
        178.645, 163.108, 182.153, 163.690, 163.678, 185.218, 169.391, 116.064,
        160.601, 171.801, 148.264, 134.829, 145.600, 139.639, 136.243, 144.530,
        167.740, 157.688, 183.838, 160.579, 168.014, 147.298, 186.147, 146.338,
        172.154, 169.645, 157.922, 153.571, 122.764, 172.741, 129.381, 177.329,
        160.655, 172.908, 115.586, 139.860, 164.100, 160.908, 152.783, 142.422,
        169.603, 176.345, 144.461, 175.763,  51.987, 153.892, 185.061, 167.793,
        167.930, 165.235, 172.148,  40.239, 160.120, 143.916, 155.806, 176.689,
        173.516, 182.285, 167.645, 173.338, 135.729, 141.086, 165.576, 158.574,
        158.176, 166.529, 176.354, 147.589, 159.173, 154.907, 157.442, 153.537,
        162.353, 177.206, 153.409, 169.383, 166.340, 136.061, 157.217, 169.191,
        134.036, 145.349, 176.171, 129.839, 138.526, 167.117, 167.182, 162.802,
        141.100, 154.506, 170.115, 163.522, 144.849, 178.486, 166.586, 165.284,
        151.667, 144.512, 170.817, 160.130, 153.065, 157.928, 152.841, 172.586,
        147.885, 147.749, 135.536, 157.424, 132.052, 167.602, 148.851, 163.299,
        162.803, 166.578, 135.549, 159.982, 166.779, 158.910,  42.217, 170.574,
        119.372, 174.515, 114.898, 160.338, 169.103, 179.547, 136.589, 180.469,
        155.261, 141.570, 182.702, 181.983, 163.312, 147.956, 166.750, 153.602,
        161.667, 132.787, 161.910, 161.073, 142.247, 176.013, 141.851, 158.871,
        141.788, 129.151, 174.356, 138.240, 146.641, 122.669, 165.935, 156.883,
        164.749, 140.624, 126.546, 170.936, 181.928, 162.183, 160.660, 163.681,
        158.240, 170.907, 137.519, 166.171, 146.031, 192.199, 176.608, 165.722,
        154.932,  98.546, 169.774, 147.575, 168.897, 157.697, 159.002, 163.783,
        167.570, 179.787, 142.537, 122.739, 154.323, 133.161, 156.384, 174.886,
        157.269, 183.885, 116.971, 157.064, 153.343, 174.341, 162.190, 148.559,
        173.780, 150.636, 173.047, 156.397, 124.310, 173.176, 153.651, 180.196,
        171.130, 172.145, 165.073, 159.741], device='cuda:0')
clipping threshold 1.9337532558832238
a after update for 1 param tensor([[[-1.123e-01],
         [ 1.253e-01],
         [-1.988e-01],
         [ 1.465e-01],
         [-1.663e-02],
         [ 1.608e-02],
         [-4.809e-02],
         [ 9.361e-02],
         [-3.390e-02],
         [-1.577e-01]],

        [[ 1.019e-03],
         [ 2.316e-01],
         [ 6.306e-02],
         [-5.556e-02],
         [ 2.248e-01],
         [-1.143e-01],
         [ 5.276e-02],
         [ 1.375e-02],
         [ 3.109e-02],
         [ 7.094e-02]],

        [[ 1.172e-01],
         [-7.646e-02],
         [ 7.532e-02],
         [-9.020e-02],
         [ 7.105e-02],
         [-1.358e-01],
         [-7.376e-02],
         [ 2.513e-02],
         [ 1.143e-02],
         [ 2.115e-02]],

        [[ 3.719e-02],
         [ 4.118e-02],
         [ 9.737e-02],
         [ 6.775e-02],
         [-8.681e-02],
         [-3.029e-02],
         [-3.774e-02],
         [-3.898e-02],
         [ 2.969e-02],
         [-1.802e-02]],

        [[ 8.066e-02],
         [ 3.886e-02],
         [ 3.770e-02],
         [-3.797e-02],
         [-4.976e-02],
         [-8.087e-02],
         [ 6.262e-03],
         [ 1.736e-02],
         [-1.862e-01],
         [-7.374e-02]],

        [[-9.853e-02],
         [ 4.922e-02],
         [-1.002e-01],
         [-9.457e-02],
         [ 6.055e-02],
         [ 1.020e-01],
         [ 6.642e-02],
         [-6.274e-02],
         [-2.485e-01],
         [ 1.318e-01]],

        [[-1.190e-01],
         [ 1.358e-01],
         [ 2.402e-01],
         [-3.754e-04],
         [-8.658e-02],
         [-1.459e-01],
         [ 4.342e-02],
         [ 1.783e-02],
         [-6.363e-02],
         [ 4.659e-02]],

        [[ 5.048e-02],
         [-6.637e-03],
         [ 5.804e-02],
         [-8.176e-02],
         [ 1.138e-01],
         [-7.429e-02],
         [ 2.690e-02],
         [-2.207e-01],
         [-4.624e-02],
         [-1.179e-02]],

        [[-3.461e-03],
         [-4.431e-02],
         [ 1.586e-01],
         [-5.998e-04],
         [-1.137e-01],
         [ 1.178e-01],
         [-1.163e-02],
         [-2.099e-02],
         [-1.862e-02],
         [ 1.329e-02]],

        [[ 2.256e-01],
         [-3.009e-02],
         [ 6.572e-02],
         [ 9.235e-03],
         [ 3.714e-03],
         [ 1.215e-01],
         [ 1.075e-01],
         [-1.142e-01],
         [-2.256e-02],
         [-3.015e-03]],

        [[ 1.974e-01],
         [-1.847e-01],
         [ 8.907e-02],
         [-1.601e-02],
         [-5.636e-02],
         [ 9.770e-02],
         [-2.678e-01],
         [-5.047e-02],
         [-1.260e-01],
         [ 7.908e-03]],

        [[-3.738e-02],
         [-1.296e-01],
         [-1.802e-02],
         [-4.412e-02],
         [ 2.238e-02],
         [ 4.194e-02],
         [ 7.290e-02],
         [-7.181e-02],
         [-2.094e-02],
         [-1.864e-01]],

        [[-9.678e-02],
         [ 1.108e-01],
         [ 1.896e-01],
         [ 5.661e-02],
         [ 1.136e-01],
         [ 1.405e-01],
         [-6.221e-03],
         [-9.686e-04],
         [ 2.094e-02],
         [-1.717e-01]],

        [[-2.321e-02],
         [-1.182e-01],
         [-2.740e-02],
         [ 1.046e-01],
         [ 2.310e-03],
         [-3.734e-02],
         [ 5.628e-02],
         [ 1.691e-02],
         [-8.863e-02],
         [ 1.888e-02]],

        [[ 2.080e-05],
         [ 4.299e-02],
         [-2.240e-02],
         [ 1.392e-02],
         [-3.266e-02],
         [-4.229e-02],
         [ 1.944e-02],
         [ 2.072e-02],
         [ 1.266e-02],
         [-6.438e-02]],

        [[-1.908e-01],
         [ 1.307e-02],
         [ 5.284e-03],
         [ 6.723e-02],
         [ 6.637e-02],
         [-1.215e-01],
         [-5.565e-02],
         [-4.557e-02],
         [-1.383e-01],
         [ 4.001e-02]],

        [[-9.911e-03],
         [ 1.153e-02],
         [-4.906e-03],
         [-1.016e-02],
         [ 5.859e-02],
         [ 7.140e-02],
         [-1.273e-01],
         [ 1.517e-02],
         [ 1.661e-01],
         [ 9.197e-02]],

        [[ 6.670e-02],
         [ 6.970e-02],
         [ 6.547e-02],
         [ 2.448e-03],
         [-1.273e-02],
         [ 1.020e-01],
         [-2.123e-02],
         [ 3.519e-02],
         [-1.731e-01],
         [ 8.337e-03]],

        [[ 4.021e-02],
         [ 1.741e-01],
         [ 4.452e-02],
         [ 1.411e-01],
         [ 2.479e-02],
         [-6.802e-02],
         [ 2.182e-02],
         [-4.976e-02],
         [ 4.715e-02],
         [-1.541e-02]],

        [[ 2.432e-01],
         [ 3.274e-02],
         [ 1.893e-01],
         [-9.281e-02],
         [-4.953e-03],
         [-1.263e-01],
         [ 1.345e-01],
         [-1.235e-01],
         [-2.764e-02],
         [-5.082e-02]],

        [[ 3.477e-02],
         [ 1.094e-01],
         [ 3.631e-02],
         [-4.654e-03],
         [-5.179e-02],
         [-8.535e-03],
         [ 7.248e-02],
         [ 1.263e-02],
         [ 7.186e-02],
         [ 1.379e-01]],

        [[ 5.862e-03],
         [-6.655e-04],
         [ 3.038e-02],
         [ 2.000e-01],
         [ 1.005e-02],
         [-2.127e-02],
         [-1.015e-01],
         [-9.827e-03],
         [ 8.162e-02],
         [-2.436e-01]],

        [[-1.860e-01],
         [ 1.633e-02],
         [-5.914e-02],
         [ 2.152e-01],
         [-5.383e-02],
         [ 1.399e-01],
         [-1.127e-01],
         [-2.248e-02],
         [ 2.489e-02],
         [-4.569e-02]],

        [[ 1.630e-01],
         [-9.202e-02],
         [-8.332e-02],
         [-1.605e-02],
         [ 1.092e-01],
         [ 8.971e-02],
         [ 1.634e-01],
         [-4.278e-02],
         [-8.233e-02],
         [ 1.123e-02]],

        [[-9.413e-02],
         [-5.898e-03],
         [-5.809e-02],
         [ 2.633e-02],
         [ 2.559e-02],
         [-2.428e-02],
         [-1.389e-01],
         [-1.475e-01],
         [ 3.771e-02],
         [ 5.005e-02]],

        [[-5.025e-02],
         [-1.602e-02],
         [-2.611e-01],
         [-8.922e-02],
         [-2.326e-03],
         [-7.903e-02],
         [ 1.459e-02],
         [-8.025e-02],
         [-1.198e-01],
         [-4.821e-02]],

        [[ 1.307e-01],
         [ 6.862e-02],
         [-4.832e-02],
         [ 6.250e-03],
         [ 1.902e-02],
         [ 2.416e-02],
         [ 7.872e-02],
         [-1.800e-02],
         [ 1.362e-01],
         [ 8.457e-02]],

        [[-1.104e-03],
         [-5.481e-02],
         [ 7.723e-03],
         [ 5.620e-02],
         [-9.800e-02],
         [ 9.568e-03],
         [ 7.910e-02],
         [-2.804e-02],
         [ 1.024e-01],
         [-1.189e-02]],

        [[ 7.238e-02],
         [ 1.579e-01],
         [-4.401e-02],
         [-8.276e-02],
         [-3.170e-02],
         [-6.700e-02],
         [ 2.524e-02],
         [ 8.835e-02],
         [ 2.147e-01],
         [ 1.089e-03]],

        [[ 4.269e-02],
         [-2.221e-01],
         [ 1.149e-01],
         [ 3.820e-02],
         [-1.716e-01],
         [ 2.957e-02],
         [ 4.078e-03],
         [ 4.006e-02],
         [-1.348e-01],
         [ 1.119e-01]]], device='cuda:0')
s after update for 1 param tensor([[[1.937],
         [1.592],
         [2.102],
         [1.724],
         [1.451],
         [1.261],
         [1.972],
         [2.423],
         [1.738],
         [2.072]],

        [[1.383],
         [2.029],
         [1.370],
         [1.881],
         [1.521],
         [1.803],
         [1.894],
         [1.158],
         [1.560],
         [1.683]],

        [[1.584],
         [1.846],
         [1.200],
         [1.794],
         [1.511],
         [1.872],
         [0.919],
         [1.988],
         [1.673],
         [0.737]],

        [[1.949],
         [1.930],
         [1.717],
         [1.587],
         [1.143],
         [1.723],
         [0.837],
         [1.424],
         [1.691],
         [2.119]],

        [[2.015],
         [2.207],
         [1.725],
         [1.726],
         [1.682],
         [2.090],
         [1.334],
         [1.969],
         [2.246],
         [1.592]],

        [[2.078],
         [1.440],
         [1.251],
         [1.188],
         [1.426],
         [1.234],
         [2.048],
         [1.151],
         [1.980],
         [1.919]],

        [[2.009],
         [1.201],
         [2.010],
         [1.719],
         [0.940],
         [1.706],
         [1.396],
         [1.370],
         [1.539],
         [1.560]],

        [[1.132],
         [1.636],
         [1.956],
         [1.307],
         [1.648],
         [1.996],
         [0.931],
         [1.713],
         [1.273],
         [1.869]],

        [[0.312],
         [1.458],
         [1.489],
         [1.466],
         [1.542],
         [1.671],
         [1.653],
         [1.421],
         [2.183],
         [1.523]],

        [[1.807],
         [0.880],
         [1.969],
         [1.701],
         [1.215],
         [1.641],
         [1.237],
         [1.331],
         [1.245],
         [1.213]],

        [[1.446],
         [1.147],
         [1.394],
         [1.720],
         [1.726],
         [1.620],
         [2.138],
         [2.062],
         [1.729],
         [1.814]],

        [[1.469],
         [1.431],
         [1.225],
         [1.660],
         [1.286],
         [1.023],
         [2.027],
         [1.922],
         [1.435],
         [1.574]],

        [[2.023],
         [1.493],
         [1.720],
         [1.184],
         [1.434],
         [1.729],
         [1.164],
         [1.688],
         [1.434],
         [2.088]],

        [[1.537],
         [1.813],
         [1.095],
         [1.807],
         [1.333],
         [1.322],
         [2.005],
         [1.065],
         [1.497],
         [1.704]],

        [[1.471],
         [1.447],
         [1.646],
         [1.712],
         [0.113],
         [1.320],
         [1.197],
         [1.198],
         [1.915],
         [1.774]],

        [[1.493],
         [1.763],
         [1.914],
         [1.895],
         [1.636],
         [2.005],
         [1.668],
         [1.984],
         [1.472],
         [1.490]],

        [[1.339],
         [2.041],
         [1.170],
         [1.133],
         [1.549],
         [1.863],
         [2.060],
         [1.048],
         [2.310],
         [1.555]],

        [[1.593],
         [1.149],
         [1.824],
         [1.621],
         [1.971],
         [0.959],
         [1.214],
         [1.679],
         [1.911],
         [1.760]],

        [[1.604],
         [1.790],
         [1.979],
         [1.719],
         [1.241],
         [1.851],
         [1.679],
         [1.324],
         [1.702],
         [1.637]],

        [[1.993],
         [1.430],
         [1.418],
         [1.565],
         [2.095],
         [1.691],
         [1.934],
         [0.969],
         [1.510],
         [1.086]],

        [[1.610],
         [1.595],
         [1.871],
         [1.241],
         [1.589],
         [1.204],
         [1.418],
         [2.104],
         [1.584],
         [1.801]],

        [[1.229],
         [1.680],
         [1.162],
         [1.907],
         [1.638],
         [1.517],
         [2.037],
         [1.315],
         [1.814],
         [1.567]],

        [[1.239],
         [0.497],
         [1.849],
         [1.426],
         [1.357],
         [1.473],
         [2.169],
         [1.118],
         [0.603],
         [2.095]],

        [[1.698],
         [1.493],
         [1.488],
         [1.481],
         [1.269],
         [1.778],
         [1.921],
         [1.741],
         [1.050],
         [1.794]],

        [[1.108],
         [1.724],
         [0.589],
         [1.716],
         [1.865],
         [1.209],
         [2.129],
         [1.462],
         [1.791],
         [1.413]],

        [[1.498],
         [1.649],
         [1.734],
         [2.039],
         [1.499],
         [1.549],
         [1.986],
         [1.819],
         [2.122],
         [1.216]],

        [[2.181],
         [1.096],
         [1.796],
         [1.529],
         [1.571],
         [1.626],
         [1.567],
         [0.827],
         [1.466],
         [1.711]],

        [[1.587],
         [1.353],
         [1.881],
         [1.407],
         [1.607],
         [1.276],
         [1.650],
         [0.877],
         [1.672],
         [1.505]],

        [[1.574],
         [2.095],
         [1.218],
         [2.174],
         [1.797],
         [1.781],
         [1.550],
         [1.389],
         [1.276],
         [1.634]],

        [[1.273],
         [1.700],
         [1.910],
         [1.518],
         [1.790],
         [1.500],
         [1.301],
         [1.336],
         [1.395],
         [1.756]]], device='cuda:0')
b after update for 1 param tensor([[[174.686],
         [158.372],
         [181.987],
         [164.792],
         [151.194],
         [140.969],
         [176.259],
         [195.407],
         [165.458],
         [180.680]],

        [[147.596],
         [178.787],
         [146.918],
         [172.164],
         [154.782],
         [168.542],
         [172.771],
         [135.093],
         [156.759],
         [162.860]],

        [[157.990],
         [170.549],
         [137.510],
         [168.140],
         [154.303],
         [171.755],
         [120.331],
         [176.976],
         [162.341],
         [107.793]],

        [[175.247],
         [174.401],
         [164.473],
         [158.139],
         [134.176],
         [164.766],
         [114.819],
         [149.808],
         [163.240],
         [182.727]],

        [[178.175],
         [186.476],
         [164.861],
         [164.899],
         [162.802],
         [181.477],
         [145.004],
         [176.125],
         [188.113],
         [158.371]],

        [[180.934],
         [150.645],
         [140.381],
         [136.827],
         [149.907],
         [139.424],
         [179.636],
         [134.656],
         [176.637],
         [173.862]],

        [[177.921],
         [137.578],
         [177.950],
         [164.578],
         [121.689],
         [163.947],
         [148.286],
         [146.922],
         [155.700],
         [156.774]],

        [[133.564],
         [160.552],
         [175.551],
         [143.511],
         [161.154],
         [177.335],
         [121.143],
         [164.266],
         [141.611],
         [171.624]],

        [[ 70.142],
         [151.589],
         [153.155],
         [151.967],
         [155.874],
         [162.249],
         [161.406],
         [149.607],
         [185.453],
         [154.907]],

        [[168.732],
         [117.780],
         [176.120],
         [163.692],
         [138.383],
         [160.810],
         [139.607],
         [144.824],
         [140.052],
         [138.231]],

        [[150.926],
         [134.428],
         [148.207],
         [164.610],
         [164.893],
         [159.783],
         [183.523],
         [180.258],
         [165.046],
         [169.049]],

        [[152.117],
         [150.163],
         [138.902],
         [161.747],
         [142.334],
         [126.944],
         [178.694],
         [174.000],
         [150.371],
         [157.474]],

        [[178.545],
         [153.361],
         [164.612],
         [136.592],
         [150.334],
         [165.056],
         [135.445],
         [163.098],
         [150.300],
         [181.389]],

        [[155.614],
         [169.027],
         [131.364],
         [168.735],
         [144.918],
         [144.333],
         [177.756],
         [129.508],
         [153.584],
         [163.844]],

        [[152.247],
         [150.995],
         [161.054],
         [164.253],
         [ 42.130],
         [144.214],
         [137.352],
         [137.363],
         [173.692],
         [167.199]],

        [[153.398],
         [166.684],
         [173.649],
         [172.773],
         [160.575],
         [177.732],
         [162.138],
         [176.827],
         [152.292],
         [153.208]],

        [[145.236],
         [179.313],
         [135.752],
         [133.603],
         [156.235],
         [171.339],
         [180.176],
         [128.483],
         [190.761],
         [156.548]],

        [[158.419],
         [134.569],
         [169.506],
         [159.826],
         [176.224],
         [122.908],
         [138.304],
         [162.646],
         [173.515],
         [166.504]],

        [[158.992],
         [167.960],
         [176.591],
         [164.571],
         [139.830],
         [170.777],
         [162.646],
         [144.424],
         [163.758],
         [160.582]],

        [[177.225],
         [150.126],
         [149.459],
         [157.028],
         [181.684],
         [163.205],
         [174.547],
         [123.573],
         [154.250],
         [130.800]],

        [[159.255],
         [158.542],
         [171.694],
         [139.815],
         [158.221],
         [137.738],
         [149.455],
         [182.073],
         [157.970],
         [168.448]],

        [[139.176],
         [162.702],
         [135.308],
         [173.358],
         [160.662],
         [154.579],
         [179.172],
         [143.962],
         [169.072],
         [157.120]],

        [[139.709],
         [ 88.457],
         [170.672],
         [149.911],
         [146.206],
         [152.338],
         [184.852],
         [132.704],
         [ 97.455],
         [181.691]],

        [[163.586],
         [153.352],
         [153.125],
         [152.739],
         [141.376],
         [167.353],
         [173.956],
         [165.643],
         [128.615],
         [168.106]],

        [[132.105],
         [164.816],
         [ 96.312],
         [164.428],
         [171.411],
         [138.042],
         [183.158],
         [151.763],
         [168.000],
         [149.203]],

        [[153.613],
         [161.180],
         [165.286],
         [179.234],
         [153.681],
         [156.227],
         [176.873],
         [169.276],
         [182.857],
         [138.438]],

        [[185.386],
         [131.431],
         [168.213],
         [155.203],
         [157.326],
         [160.079],
         [157.146],
         [114.118],
         [152.005],
         [164.171]],

        [[158.107],
         [146.013],
         [172.170],
         [148.914],
         [159.145],
         [141.779],
         [161.220],
         [117.521],
         [162.327],
         [154.012]],

        [[157.495],
         [181.663],
         [138.547],
         [185.066],
         [168.272],
         [167.512],
         [156.266],
         [147.935],
         [141.776],
         [160.439]],

        [[141.635],
         [163.666],
         [173.477],
         [154.663],
         [167.938],
         [153.750],
         [143.147],
         [145.084],
         [148.235],
         [166.344]]], device='cuda:0')
clipping threshold 1.9337532558832238
a after update for 1 param tensor([[-0.046],
        [ 0.051],
        [ 0.100],
        [ 0.110],
        [ 0.017],
        [ 0.186],
        [ 0.080],
        [ 0.072],
        [-0.106],
        [ 0.020],
        [-0.140],
        [ 0.060],
        [-0.002],
        [ 0.049],
        [ 0.028],
        [ 0.092],
        [-0.018],
        [ 0.124],
        [ 0.097],
        [ 0.019],
        [-0.243],
        [ 0.029],
        [ 0.070],
        [ 0.059],
        [ 0.097],
        [ 0.100],
        [-0.059],
        [ 0.031],
        [ 0.111],
        [ 0.073]], device='cuda:0')
s after update for 1 param tensor([[1.350],
        [1.514],
        [1.659],
        [1.815],
        [1.756],
        [1.636],
        [1.891],
        [1.561],
        [1.334],
        [1.539],
        [1.669],
        [1.914],
        [1.139],
        [1.610],
        [1.407],
        [1.669],
        [1.486],
        [1.552],
        [1.198],
        [0.962],
        [2.039],
        [1.180],
        [1.444],
        [1.613],
        [1.903],
        [1.668],
        [2.146],
        [1.490],
        [1.362],
        [1.680]], device='cuda:0')
b after update for 1 param tensor([[145.848],
        [154.451],
        [161.697],
        [169.115],
        [166.355],
        [160.558],
        [172.629],
        [156.837],
        [144.978],
        [155.735],
        [162.167],
        [173.666],
        [133.936],
        [159.274],
        [148.887],
        [162.166],
        [153.002],
        [156.390],
        [137.395],
        [123.106],
        [179.252],
        [136.339],
        [150.843],
        [159.432],
        [173.142],
        [162.116],
        [183.863],
        [153.217],
        [146.470],
        [162.676]], device='cuda:0')
clipping threshold 1.9337532558832238
||w||^2 6.407184054144468
exp ma of ||w||^2 7.562089334630855
||w|| 2.531241603273869
exp ma of ||w|| 2.277240597665887
||w||^2 5.589463342939745
exp ma of ||w||^2 3.696414335484723
||w|| 2.3642045899075117
exp ma of ||w|| 1.8741148153265477
cuda
Objective function 96.63 = squared loss an data 77.79 + 0.5*rho*h**2 9.909553 + alpha*h 6.520312 + L2reg 2.09 + L1reg 0.32 ; SHD = 121 ; DAG True
Proportion of microbatches that were clipped  0.7888638558094625
iteration 2 in inner loop, alpha 46.31549567472081 rho 1000.0 h 0.1407803478861105
9630
cuda
Objective function 185.82 = squared loss an data 77.79 + 0.5*rho*h**2 99.095532 + alpha*h 6.520312 + L2reg 2.09 + L1reg 0.32 ; SHD = 121 ; DAG True
||w||^2 208.30575758745374
exp ma of ||w||^2 59577.660074773754
||w|| 14.432801446270012
exp ma of ||w|| 40.060267327344675
||w||^2 3.377705769312732
exp ma of ||w||^2 5.8265926704487905
||w|| 1.8378535766792554
exp ma of ||w|| 2.3425043804674766
||w||^2 3.786462653117609
exp ma of ||w||^2 4.953489376740804
||w|| 1.9458835147864348
exp ma of ||w|| 2.150540675565891
||w||^2 7.5302880040270175
exp ma of ||w||^2 4.880675877727373
||w|| 2.7441370235516698
exp ma of ||w|| 2.1308064344988167
||w||^2 5.941270506416847
exp ma of ||w||^2 4.931031933209433
||w|| 2.4374721550033853
exp ma of ||w|| 2.1462595976188505
cuda
Objective function 97.54 = squared loss an data 80.28 + 0.5*rho*h**2 12.487042 + alpha*h 2.314574 + L2reg 2.18 + L1reg 0.28 ; SHD = 119 ; DAG True
Proportion of microbatches that were clipped  0.7938277589271288
iteration 3 in inner loop, alpha 46.31549567472081 rho 10000.0 h 0.0499740765654515
iteration 3 in outer loop, alpha = 546.0562613292358, rho = 10000.0, h = 0.0499740765654515
cuda
9630
cuda
Objective function 122.52 = squared loss an data 80.28 + 0.5*rho*h**2 12.487042 + alpha*h 27.288657 + L2reg 2.18 + L1reg 0.28 ; SHD = 119 ; DAG True
||w||^2 72.22018157693559
exp ma of ||w||^2 13077.340859378388
||w|| 8.498245794099837
exp ma of ||w|| 16.904020303446305
||w||^2 29.451896416896947
exp ma of ||w||^2 3171.0827561614915
||w|| 5.426960145136221
exp ma of ||w|| 8.787370881949522
||w||^2 4.715853458207146
exp ma of ||w||^2 304.30438932944554
||w|| 2.17160158827699
exp ma of ||w|| 4.256876445915096
cuda
Objective function 106.28 = squared loss an data 79.37 + 0.5*rho*h**2 5.785997 + alpha*h 18.575542 + L2reg 2.28 + L1reg 0.27 ; SHD = 118 ; DAG True
Proportion of microbatches that were clipped  0.7989182309895004
iteration 1 in inner loop, alpha 546.0562613292358 rho 10000.0 h 0.034017634302678346
9630
cuda
Objective function 158.35 = squared loss an data 79.37 + 0.5*rho*h**2 57.859972 + alpha*h 18.575542 + L2reg 2.28 + L1reg 0.27 ; SHD = 118 ; DAG True
||w||^2 9612916782211.871
exp ma of ||w||^2 5651425786383.909
||w|| 3100470.4130521663
exp ma of ||w|| 2078765.1295893667
||w||^2 3449.867938499954
exp ma of ||w||^2 1796283.908223429
||w|| 58.73557642945163
exp ma of ||w|| 178.73538016990088
||w||^2 7.660533519695013
exp ma of ||w||^2 8.544022193787546
||w|| 2.7677668831921185
exp ma of ||w|| 2.836791142233107
cuda
Objective function 100.98 = squared loss an data 80.01 + 0.5*rho*h**2 10.454531 + alpha*h 7.895955 + L2reg 2.38 + L1reg 0.24 ; SHD = 117 ; DAG True
Proportion of microbatches that were clipped  0.7948220064724919
iteration 2 in inner loop, alpha 546.0562613292358 rho 100000.0 h 0.014459966281890502
iteration 4 in outer loop, alpha = 15006.022543219737, rho = 1000000.0, h = 0.014459966281890502
Threshold 0.3
[[0.004 0.083 0.028 0.021 0.094 0.018 0.037 0.042 0.051 0.026 0.085 0.054
  0.022 0.145 0.006 0.031 0.101 0.069 0.381 0.025 0.066 0.039 0.035 0.015
  0.025 0.042 0.016 0.023 0.094 0.056]
 [0.061 0.005 0.055 0.059 0.136 0.038 0.111 0.018 0.12  0.046 0.114 0.09
  0.011 0.048 0.012 0.24  0.057 0.043 0.142 0.055 0.122 0.048 0.141 0.1
  0.059 0.133 0.146 0.111 0.071 0.049]
 [0.203 0.05  0.004 0.062 0.126 0.039 0.092 0.035 0.088 0.069 0.111 0.117
  0.028 0.502 0.031 0.108 0.168 0.067 0.14  0.044 0.069 0.02  0.136 0.06
  0.081 0.07  0.075 0.064 0.106 0.04 ]
 [0.204 0.055 0.104 0.005 0.18  0.036 0.067 0.056 0.145 0.078 0.088 0.195
  0.03  0.344 0.007 0.226 0.105 0.096 0.207 0.105 0.084 0.07  0.087 0.208
  0.12  0.067 0.071 0.123 0.136 0.242]
 [0.052 0.036 0.05  0.027 0.005 0.022 0.026 0.006 0.017 0.015 0.064 0.035
  0.005 0.038 0.014 0.042 0.017 0.03  0.087 0.015 0.023 0.006 0.05  0.022
  0.048 0.042 0.052 0.062 0.085 0.027]
 [0.212 0.144 0.189 0.178 0.217 0.005 0.277 0.064 0.343 0.118 0.184 0.29
  0.06  0.205 0.066 0.208 0.24  0.22  0.145 0.118 0.117 0.079 0.066 0.15
  0.081 0.115 0.056 0.091 0.518 0.1  ]
 [0.145 0.067 0.062 0.059 0.181 0.017 0.006 0.119 0.134 0.128 0.168 0.17
  0.043 0.307 0.012 0.306 0.05  0.091 0.301 0.041 0.174 0.032 0.145 0.028
  0.105 0.11  0.033 0.083 0.103 0.164]
 [0.126 0.243 0.114 0.076 1.106 0.09  0.055 0.002 0.227 0.06  0.156 0.078
  0.084 0.294 0.045 0.083 0.25  0.106 0.271 0.11  0.096 0.136 0.172 0.089
  0.088 0.107 0.105 0.088 0.169 0.199]
 [0.138 0.067 0.083 0.032 0.349 0.019 0.032 0.016 0.002 0.055 0.12  0.072
  0.021 0.098 0.009 0.099 0.229 0.064 0.218 0.024 0.037 0.086 0.068 0.039
  0.048 0.09  0.04  0.076 0.033 0.137]
 [0.232 0.157 0.072 0.069 0.244 0.032 0.035 0.088 0.082 0.003 0.282 0.16
  0.032 0.199 0.017 0.147 0.085 0.122 0.202 0.059 0.213 0.127 0.118 0.15
  0.054 0.158 0.046 0.049 0.111 0.112]
 [0.066 0.033 0.059 0.049 0.088 0.023 0.037 0.031 0.055 0.024 0.005 0.084
  0.025 0.124 0.022 0.072 0.051 0.068 0.112 0.05  0.036 0.046 0.108 0.033
  0.007 0.008 0.024 0.019 0.332 0.057]
 [0.063 0.056 0.049 0.035 0.107 0.014 0.027 0.051 0.046 0.027 0.047 0.005
  0.01  0.128 0.013 0.139 0.068 0.041 0.08  0.021 0.041 0.027 0.073 0.013
  0.028 0.056 0.034 0.023 0.039 0.028]
 [0.253 0.444 0.158 0.187 0.999 0.093 0.155 0.077 0.188 0.158 0.094 0.592
  0.004 0.274 0.112 0.25  0.126 0.164 0.369 0.152 0.252 0.194 0.305 0.194
  0.063 0.115 0.269 0.102 0.244 0.245]
 [0.035 0.057 0.015 0.013 0.209 0.026 0.009 0.017 0.069 0.019 0.021 0.08
  0.022 0.003 0.009 0.065 0.054 0.029 0.159 0.058 0.043 0.032 0.083 0.042
  0.033 0.066 0.04  0.035 0.114 0.039]
 [0.597 0.383 0.158 0.537 0.202 0.064 0.402 0.136 0.362 0.211 0.184 0.229
  0.048 0.308 0.003 0.411 0.313 0.259 0.697 0.479 0.344 0.122 0.294 1.344
  0.143 0.126 0.124 0.374 0.459 0.222]
 [0.128 0.033 0.059 0.023 0.146 0.022 0.017 0.05  0.057 0.044 0.069 0.034
  0.016 0.049 0.011 0.004 0.057 0.074 0.107 0.02  0.049 0.045 0.058 0.068
  0.03  0.029 0.045 0.057 0.084 0.065]
 [0.056 0.077 0.037 0.082 0.304 0.023 0.065 0.019 0.019 0.067 0.1   0.086
  0.03  0.095 0.018 0.106 0.003 0.075 0.213 0.057 0.097 0.03  0.05  0.041
  0.049 0.05  0.037 0.06  0.088 0.048]
 [0.094 0.151 0.092 0.066 0.153 0.024 0.057 0.048 0.091 0.05  0.067 0.163
  0.029 0.18  0.023 0.072 0.076 0.004 0.252 0.071 0.046 0.023 0.082 0.074
  0.036 0.055 0.067 0.068 0.05  0.275]
 [0.016 0.039 0.039 0.01  0.045 0.022 0.018 0.024 0.02  0.023 0.038 0.065
  0.013 0.042 0.006 0.048 0.02  0.024 0.004 0.011 0.074 0.028 0.014 0.01
  0.033 0.058 0.02  0.02  0.078 0.029]
 [0.277 0.118 0.142 0.064 0.265 0.056 0.121 0.034 0.242 0.073 0.093 0.268
  0.034 0.067 0.011 0.263 0.054 0.088 0.267 0.003 0.332 0.106 0.19  0.531
  0.06  0.074 0.097 0.12  0.767 0.104]
 [0.058 0.053 0.069 0.055 0.162 0.029 0.021 0.055 0.136 0.023 0.119 0.159
  0.016 0.159 0.007 0.093 0.073 0.081 0.079 0.021 0.004 0.043 0.044 0.058
  0.031 0.026 0.042 0.031 0.078 0.08 ]
 [0.183 0.059 0.254 0.077 0.729 0.057 0.108 0.051 0.039 0.068 0.144 0.249
  0.033 0.159 0.033 0.098 0.12  0.328 0.204 0.038 0.094 0.004 0.151 0.109
  0.043 0.109 0.062 0.108 0.244 0.071]
 [0.099 0.03  0.032 0.056 0.111 0.059 0.037 0.031 0.067 0.043 0.064 0.048
  0.016 0.066 0.02  0.096 0.087 0.078 0.368 0.029 0.12  0.046 0.005 0.08
  0.037 0.056 0.016 0.035 0.108 0.029]
 [0.514 0.073 0.043 0.024 0.127 0.028 0.173 0.061 0.126 0.028 0.123 0.185
  0.031 0.119 0.003 0.056 0.118 0.057 0.551 0.011 0.149 0.048 0.056 0.004
  0.078 0.075 0.046 0.063 0.059 0.055]
 [0.198 0.102 0.063 0.054 0.127 0.073 0.057 0.059 0.15  0.105 0.474 0.121
  0.058 0.208 0.027 0.129 0.075 0.107 0.105 0.072 0.165 0.068 0.089 0.078
  0.004 0.703 0.097 0.146 0.223 0.093]
 [0.155 0.07  0.105 0.084 0.102 0.058 0.055 0.049 0.066 0.035 0.656 0.067
  0.04  0.09  0.036 0.172 0.086 0.056 0.064 0.04  0.125 0.066 0.079 0.061
  0.007 0.003 0.042 0.035 0.258 0.045]
 [0.237 0.048 0.075 0.094 0.089 0.084 0.122 0.044 0.14  0.128 0.258 0.193
  0.028 0.121 0.036 0.141 0.149 0.095 0.224 0.056 0.108 0.066 0.273 0.135
  0.045 0.05  0.005 0.118 0.534 0.098]
 [0.211 0.048 0.079 0.053 0.065 0.078 0.066 0.037 0.1   0.098 0.134 0.228
  0.033 0.17  0.012 0.076 0.09  0.078 0.193 0.057 0.167 0.051 0.153 0.092
  0.054 0.265 0.041 0.003 0.064 0.044]
 [0.057 0.066 0.046 0.029 0.087 0.012 0.039 0.032 0.058 0.035 0.013 0.148
  0.013 0.056 0.007 0.03  0.032 0.092 0.068 0.008 0.07  0.018 0.06  0.086
  0.018 0.011 0.009 0.048 0.003 0.051]
 [0.09  0.085 0.108 0.017 0.191 0.054 0.034 0.035 0.039 0.06  0.115 0.206
  0.022 0.244 0.02  0.082 0.087 0.021 0.133 0.07  0.103 0.089 0.123 0.067
  0.044 0.124 0.074 0.113 0.124 0.007]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.381 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.502 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.344 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.343 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.518 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.307 0.    0.306 0.    0.    0.301 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.106 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.349 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.332 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.444 0.    0.    0.999 0.    0.    0.    0.    0.    0.    0.592
  0.    0.    0.    0.    0.    0.    0.369 0.    0.    0.    0.305 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.597 0.383 0.    0.537 0.    0.    0.402 0.    0.362 0.    0.    0.
  0.    0.308 0.    0.411 0.313 0.    0.697 0.479 0.344 0.    0.    1.344
  0.    0.    0.    0.374 0.459 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.304 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.332 0.    0.    0.531
  0.    0.    0.    0.    0.767 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.729 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.328 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.368 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.514 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.551 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.474 0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.703 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.656 0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.534 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5116279069767442, 'tpr': 0.175, 'fpr': 0.06984126984126984, 'f1': 0.2576687116564417, 'shd': 117, 'npred': 43, 'ntrue': 120}
[0.083 0.028 0.021 0.094 0.018 0.037 0.042 0.051 0.026 0.085 0.054 0.022
 0.145 0.006 0.031 0.101 0.069 0.381 0.025 0.066 0.039 0.035 0.015 0.025
 0.042 0.016 0.023 0.094 0.056 0.061 0.055 0.059 0.136 0.038 0.111 0.018
 0.12  0.046 0.114 0.09  0.011 0.048 0.012 0.24  0.057 0.043 0.142 0.055
 0.122 0.048 0.141 0.1   0.059 0.133 0.146 0.111 0.071 0.049 0.203 0.05
 0.062 0.126 0.039 0.092 0.035 0.088 0.069 0.111 0.117 0.028 0.502 0.031
 0.108 0.168 0.067 0.14  0.044 0.069 0.02  0.136 0.06  0.081 0.07  0.075
 0.064 0.106 0.04  0.204 0.055 0.104 0.18  0.036 0.067 0.056 0.145 0.078
 0.088 0.195 0.03  0.344 0.007 0.226 0.105 0.096 0.207 0.105 0.084 0.07
 0.087 0.208 0.12  0.067 0.071 0.123 0.136 0.242 0.052 0.036 0.05  0.027
 0.022 0.026 0.006 0.017 0.015 0.064 0.035 0.005 0.038 0.014 0.042 0.017
 0.03  0.087 0.015 0.023 0.006 0.05  0.022 0.048 0.042 0.052 0.062 0.085
 0.027 0.212 0.144 0.189 0.178 0.217 0.277 0.064 0.343 0.118 0.184 0.29
 0.06  0.205 0.066 0.208 0.24  0.22  0.145 0.118 0.117 0.079 0.066 0.15
 0.081 0.115 0.056 0.091 0.518 0.1   0.145 0.067 0.062 0.059 0.181 0.017
 0.119 0.134 0.128 0.168 0.17  0.043 0.307 0.012 0.306 0.05  0.091 0.301
 0.041 0.174 0.032 0.145 0.028 0.105 0.11  0.033 0.083 0.103 0.164 0.126
 0.243 0.114 0.076 1.106 0.09  0.055 0.227 0.06  0.156 0.078 0.084 0.294
 0.045 0.083 0.25  0.106 0.271 0.11  0.096 0.136 0.172 0.089 0.088 0.107
 0.105 0.088 0.169 0.199 0.138 0.067 0.083 0.032 0.349 0.019 0.032 0.016
 0.055 0.12  0.072 0.021 0.098 0.009 0.099 0.229 0.064 0.218 0.024 0.037
 0.086 0.068 0.039 0.048 0.09  0.04  0.076 0.033 0.137 0.232 0.157 0.072
 0.069 0.244 0.032 0.035 0.088 0.082 0.282 0.16  0.032 0.199 0.017 0.147
 0.085 0.122 0.202 0.059 0.213 0.127 0.118 0.15  0.054 0.158 0.046 0.049
 0.111 0.112 0.066 0.033 0.059 0.049 0.088 0.023 0.037 0.031 0.055 0.024
 0.084 0.025 0.124 0.022 0.072 0.051 0.068 0.112 0.05  0.036 0.046 0.108
 0.033 0.007 0.008 0.024 0.019 0.332 0.057 0.063 0.056 0.049 0.035 0.107
 0.014 0.027 0.051 0.046 0.027 0.047 0.01  0.128 0.013 0.139 0.068 0.041
 0.08  0.021 0.041 0.027 0.073 0.013 0.028 0.056 0.034 0.023 0.039 0.028
 0.253 0.444 0.158 0.187 0.999 0.093 0.155 0.077 0.188 0.158 0.094 0.592
 0.274 0.112 0.25  0.126 0.164 0.369 0.152 0.252 0.194 0.305 0.194 0.063
 0.115 0.269 0.102 0.244 0.245 0.035 0.057 0.015 0.013 0.209 0.026 0.009
 0.017 0.069 0.019 0.021 0.08  0.022 0.009 0.065 0.054 0.029 0.159 0.058
 0.043 0.032 0.083 0.042 0.033 0.066 0.04  0.035 0.114 0.039 0.597 0.383
 0.158 0.537 0.202 0.064 0.402 0.136 0.362 0.211 0.184 0.229 0.048 0.308
 0.411 0.313 0.259 0.697 0.479 0.344 0.122 0.294 1.344 0.143 0.126 0.124
 0.374 0.459 0.222 0.128 0.033 0.059 0.023 0.146 0.022 0.017 0.05  0.057
 0.044 0.069 0.034 0.016 0.049 0.011 0.057 0.074 0.107 0.02  0.049 0.045
 0.058 0.068 0.03  0.029 0.045 0.057 0.084 0.065 0.056 0.077 0.037 0.082
 0.304 0.023 0.065 0.019 0.019 0.067 0.1   0.086 0.03  0.095 0.018 0.106
 0.075 0.213 0.057 0.097 0.03  0.05  0.041 0.049 0.05  0.037 0.06  0.088
 0.048 0.094 0.151 0.092 0.066 0.153 0.024 0.057 0.048 0.091 0.05  0.067
 0.163 0.029 0.18  0.023 0.072 0.076 0.252 0.071 0.046 0.023 0.082 0.074
 0.036 0.055 0.067 0.068 0.05  0.275 0.016 0.039 0.039 0.01  0.045 0.022
 0.018 0.024 0.02  0.023 0.038 0.065 0.013 0.042 0.006 0.048 0.02  0.024
 0.011 0.074 0.028 0.014 0.01  0.033 0.058 0.02  0.02  0.078 0.029 0.277
 0.118 0.142 0.064 0.265 0.056 0.121 0.034 0.242 0.073 0.093 0.268 0.034
 0.067 0.011 0.263 0.054 0.088 0.267 0.332 0.106 0.19  0.531 0.06  0.074
 0.097 0.12  0.767 0.104 0.058 0.053 0.069 0.055 0.162 0.029 0.021 0.055
 0.136 0.023 0.119 0.159 0.016 0.159 0.007 0.093 0.073 0.081 0.079 0.021
 0.043 0.044 0.058 0.031 0.026 0.042 0.031 0.078 0.08  0.183 0.059 0.254
 0.077 0.729 0.057 0.108 0.051 0.039 0.068 0.144 0.249 0.033 0.159 0.033
 0.098 0.12  0.328 0.204 0.038 0.094 0.151 0.109 0.043 0.109 0.062 0.108
 0.244 0.071 0.099 0.03  0.032 0.056 0.111 0.059 0.037 0.031 0.067 0.043
 0.064 0.048 0.016 0.066 0.02  0.096 0.087 0.078 0.368 0.029 0.12  0.046
 0.08  0.037 0.056 0.016 0.035 0.108 0.029 0.514 0.073 0.043 0.024 0.127
 0.028 0.173 0.061 0.126 0.028 0.123 0.185 0.031 0.119 0.003 0.056 0.118
 0.057 0.551 0.011 0.149 0.048 0.056 0.078 0.075 0.046 0.063 0.059 0.055
 0.198 0.102 0.063 0.054 0.127 0.073 0.057 0.059 0.15  0.105 0.474 0.121
 0.058 0.208 0.027 0.129 0.075 0.107 0.105 0.072 0.165 0.068 0.089 0.078
 0.703 0.097 0.146 0.223 0.093 0.155 0.07  0.105 0.084 0.102 0.058 0.055
 0.049 0.066 0.035 0.656 0.067 0.04  0.09  0.036 0.172 0.086 0.056 0.064
 0.04  0.125 0.066 0.079 0.061 0.007 0.042 0.035 0.258 0.045 0.237 0.048
 0.075 0.094 0.089 0.084 0.122 0.044 0.14  0.128 0.258 0.193 0.028 0.121
 0.036 0.141 0.149 0.095 0.224 0.056 0.108 0.066 0.273 0.135 0.045 0.05
 0.118 0.534 0.098 0.211 0.048 0.079 0.053 0.065 0.078 0.066 0.037 0.1
 0.098 0.134 0.228 0.033 0.17  0.012 0.076 0.09  0.078 0.193 0.057 0.167
 0.051 0.153 0.092 0.054 0.265 0.041 0.064 0.044 0.057 0.066 0.046 0.029
 0.087 0.012 0.039 0.032 0.058 0.035 0.013 0.148 0.013 0.056 0.007 0.03
 0.032 0.092 0.068 0.008 0.07  0.018 0.06  0.086 0.018 0.011 0.009 0.048
 0.051 0.09  0.085 0.108 0.017 0.191 0.054 0.034 0.035 0.039 0.06  0.115
 0.206 0.022 0.244 0.02  0.082 0.087 0.021 0.133 0.07  0.103 0.089 0.123
 0.067 0.044 0.124 0.074 0.113 0.124]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.6312, 0.32179212705409244)
Iterations 567
Achieves (13.598573656938171, 1e-05)-DP
