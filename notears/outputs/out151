samples  5000  graph  30 120 ER mlp  minibatch size  100  noise  0.8  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.407192360868244
iteration 1 in outer loop, alpha = 2.407192360868244, rho = 1.0, h = 2.407192360868244
cuda
iteration 1 in inner loop,alpha 2.407192360868244 rho 1.0 h 1.519847643277938
iteration 2 in inner loop,alpha 2.407192360868244 rho 10.0 h 0.6683469298187887
iteration 3 in inner loop,alpha 2.407192360868244 rho 100.0 h 0.21559932738688303
iteration 2 in outer loop, alpha = 23.967125099556547, rho = 100.0, h = 0.21559932738688303
cuda
iteration 1 in inner loop,alpha 23.967125099556547 rho 100.0 h 0.1215159319801522
iteration 2 in inner loop,alpha 23.967125099556547 rho 1000.0 h 0.04405852855256143
iteration 3 in outer loop, alpha = 68.02565365211797, rho = 1000.0, h = 0.04405852855256143
cuda
iteration 1 in inner loop,alpha 68.02565365211797 rho 1000.0 h 0.02085655661729291
iteration 2 in inner loop,alpha 68.02565365211797 rho 10000.0 h 0.0075768395846402825
iteration 4 in outer loop, alpha = 143.7940494985208, rho = 10000.0, h = 0.0075768395846402825
cuda
iteration 1 in inner loop,alpha 143.7940494985208 rho 10000.0 h 0.0033524644048874563
iteration 2 in inner loop,alpha 143.7940494985208 rho 100000.0 h 0.0010487403167083187
iteration 5 in outer loop, alpha = 248.66808116935266, rho = 100000.0, h = 0.0010487403167083187
cuda
iteration 1 in inner loop,alpha 248.66808116935266 rho 100000.0 h 0.0005433208452814142
iteration 6 in outer loop, alpha = 791.9889264507669, rho = 1000000.0, h = 0.0005433208452814142
Threshold 0.3
[[0.002 0.    0.002 1.931 0.001 0.173 0.206 0.    0.    0.    0.    0.
  0.    0.    0.    0.93  0.    0.    0.731 0.17  0.    0.    0.147 0.
  0.    0.    0.167 0.    0.221 0.   ]
 [0.423 0.003 0.008 0.116 0.154 1.562 0.197 0.    1.184 0.    0.    0.136
  0.    0.    0.    0.864 0.    1.204 0.366 0.677 0.    0.002 1.014 0.001
  0.    0.    0.181 0.    0.233 0.132]
 [0.169 0.066 0.002 0.489 0.015 0.446 0.194 0.002 0.091 0.    0.    0.165
  0.    0.    0.    0.778 0.    0.258 0.058 0.068 0.    0.    0.171 0.
  0.    0.003 0.189 0.    0.22  0.005]
 [0.    0.    0.    0.002 0.    0.001 0.16  0.    0.    0.    0.    0.
  0.    0.    0.    0.901 0.    0.    0.174 0.631 0.    0.    0.16  0.
  0.    0.    0.    0.    0.253 0.   ]
 [0.099 0.009 0.073 0.217 0.003 0.107 0.333 0.    0.263 0.    0.    0.263
  0.    0.    0.    0.168 0.    0.002 0.738 0.192 0.    0.001 1.432 0.
  0.    0.001 0.122 0.    0.243 0.004]
 [0.001 0.    0.    0.186 0.004 0.002 1.703 0.    0.002 0.    0.    0.
  0.    0.    0.    0.238 0.    0.    0.191 0.117 0.    0.    0.289 0.
  0.    0.    0.    0.    0.806 0.   ]
 [0.    0.    0.    0.001 0.    0.    0.004 0.    0.001 0.    0.    0.
  0.    0.    0.    1.032 0.    0.    0.01  0.002 0.    0.    0.    0.
  0.    0.    0.    0.    0.141 0.   ]
 [0.425 0.82  0.11  0.168 2.237 0.2   0.196 0.002 1.343 0.    0.    0.153
  0.    0.    0.    0.593 0.    0.309 0.166 0.752 0.    0.003 0.251 0.001
  0.    0.005 0.143 0.001 0.488 0.271]
 [1.226 0.    0.003 0.255 0.001 0.112 0.16  0.    0.002 0.    0.    0.
  0.    0.    0.    0.057 0.    0.    0.295 0.903 0.    0.    0.165 0.
  0.    0.    0.122 0.    0.161 0.   ]
 [0.294 0.968 0.178 0.236 0.245 0.368 0.276 0.309 1.189 0.001 0.    0.219
  2.246 0.139 0.007 0.343 0.001 0.404 0.226 0.098 0.002 0.484 0.459 0.086
  0.    0.169 0.237 0.001 0.329 1.871]
 [1.688 1.816 0.321 0.184 0.231 0.339 0.274 1.969 0.324 1.278 0.002 0.079
  0.163 0.3   0.004 1.1   0.001 0.285 0.216 0.155 0.026 0.091 0.257 0.019
  0.    0.21  0.087 0.015 1.062 0.413]
 [0.142 0.003 0.004 0.147 0.003 0.142 0.985 0.    0.966 0.    0.    0.001
  0.    0.    0.    0.346 0.    0.002 0.247 0.129 0.    0.    0.187 0.
  0.    0.001 0.969 0.    0.159 0.   ]
 [0.146 1.674 0.105 0.189 2.434 0.354 0.304 1.378 0.799 0.    0.    1.74
  0.001 0.    0.    0.234 0.    0.182 0.659 0.098 0.    2.578 0.534 0.004
  0.    0.034 1.067 0.001 0.308 0.258]
 [0.214 0.197 2.758 1.15  0.111 1.711 0.349 0.161 0.423 0.002 0.002 0.477
  0.316 0.002 0.    0.501 0.    1.754 0.408 0.214 0.    0.129 0.351 2.637
  0.    0.528 0.37  0.007 0.211 2.429]
 [0.199 0.863 0.07  0.278 0.22  0.248 0.442 0.282 0.232 0.026 0.02  0.686
  2.511 0.084 0.002 0.238 0.    0.174 0.21  0.174 0.371 0.623 1.059 3.6
  0.006 0.103 0.446 0.002 0.476 0.272]
 [0.    0.    0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    0.005 0.    0.    0.007 0.001 0.    0.    0.    0.
  0.    0.    0.    0.    0.29  0.   ]
 [0.209 0.241 0.149 0.064 0.415 1.101 0.547 0.315 0.552 0.132 0.076 0.122
  1.095 0.098 3.073 0.49  0.001 0.029 0.222 0.074 0.739 1.384 0.179 0.324
  0.    0.053 0.133 0.004 0.271 0.151]
 [0.143 0.    0.003 0.248 0.128 0.271 0.181 0.    1.118 0.    0.    0.201
  0.    0.    0.    0.219 0.    0.001 0.75  0.319 0.    0.    0.363 0.
  0.    0.    0.73  0.    0.193 0.001]
 [0.001 0.    0.002 0.001 0.    0.003 0.149 0.    0.    0.    0.    0.
  0.    0.    0.    0.121 0.    0.    0.012 0.    0.    0.    0.009 0.
  0.    0.    0.001 0.    0.013 0.   ]
 [0.    0.    0.001 0.002 0.001 0.003 0.208 0.    0.    0.    0.    0.
  0.    0.    0.    0.281 0.    0.    0.738 0.004 0.    0.    0.228 0.
  0.    0.    0.    0.    0.469 0.   ]
 [0.185 0.523 0.209 0.392 1.887 1.048 0.249 1.113 0.191 0.084 0.022 0.223
  0.228 3.741 0.001 0.211 0.001 0.215 0.236 0.061 0.001 1.745 0.636 0.072
  0.001 2.438 0.247 0.001 0.228 0.159]
 [0.088 0.232 2.468 1.101 1.636 0.245 0.65  0.297 0.485 0.    0.    0.386
  0.    0.    0.    0.226 0.    2.303 0.339 0.25  0.    0.001 0.534 0.
  0.    0.004 0.284 0.001 0.549 2.114]
 [0.001 0.001 0.001 0.005 0.    0.001 0.976 0.    0.001 0.    0.    0.
  0.    0.    0.    0.221 0.    0.    0.502 0.003 0.    0.    0.004 0.
  0.    0.    0.001 0.    0.123 0.   ]
 [2.075 0.283 0.127 0.266 0.158 0.163 0.961 0.161 0.338 0.002 0.    2.188
  0.262 0.    0.    0.828 0.    0.086 0.796 0.855 0.    0.276 0.416 0.002
  0.    0.016 1.558 0.004 0.147 0.125]
 [0.233 0.455 0.695 0.141 0.186 0.546 0.559 0.275 0.117 1.156 4.378 0.158
  0.187 2.165 0.044 0.131 0.011 0.398 0.182 0.083 0.033 0.112 0.914 0.101
  0.001 2.793 0.108 0.001 0.229 0.931]
 [0.222 1.743 0.175 0.077 0.161 0.229 0.1   0.124 0.22  0.003 0.004 0.117
  0.014 0.    0.    0.099 0.    0.117 0.383 0.151 0.    0.122 1.358 0.035
  0.    0.003 0.058 0.    0.085 0.125]
 [0.002 0.    0.001 1.581 0.005 1.809 0.402 0.    0.002 0.    0.    0.
  0.    0.    0.    0.834 0.    0.    0.882 0.207 0.    0.    1.323 0.
  0.    0.    0.003 0.    0.813 0.   ]
 [2.101 0.342 0.189 0.251 0.154 0.212 0.133 0.1   0.23  0.013 0.007 0.98
  0.006 0.016 0.022 0.179 0.018 0.122 0.173 0.065 0.001 0.053 0.908 0.029
  0.003 2.995 0.138 0.    0.149 0.096]
 [0.001 0.    0.001 0.003 0.002 0.001 0.006 0.001 0.    0.    0.    0.001
  0.    0.    0.    0.016 0.    0.    0.104 0.01  0.    0.    0.002 0.
  0.    0.    0.001 0.    0.009 0.   ]
 [0.244 0.006 0.245 0.429 0.155 0.259 0.317 0.002 0.373 0.    0.    1.985
  0.    0.    0.    0.22  0.    0.316 0.674 0.186 0.    0.    0.192 0.
  0.    0.002 0.73  0.    0.096 0.002]]
[[0.    0.    0.    1.931 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.93  0.    0.    0.731 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.423 0.    0.    0.    0.    1.562 0.    0.    1.184 0.    0.    0.
  0.    0.    0.    0.864 0.    1.204 0.366 0.677 0.    0.    1.014 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.489 0.    0.446 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.778 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.901 0.    0.    0.    0.631 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.333 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.738 0.    0.    0.    1.432 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.703 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.806 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.032 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.425 0.82  0.    0.    2.237 0.    0.    0.    1.343 0.    0.    0.
  0.    0.    0.    0.593 0.    0.309 0.    0.752 0.    0.    0.    0.
  0.    0.    0.    0.    0.488 0.   ]
 [1.226 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.903 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.968 0.    0.    0.    0.368 0.    0.309 1.189 0.    0.    0.
  2.246 0.    0.    0.343 0.    0.404 0.    0.    0.    0.484 0.459 0.
  0.    0.    0.    0.    0.329 1.871]
 [1.688 1.816 0.321 0.    0.    0.339 0.    1.969 0.324 1.278 0.    0.
  0.    0.3   0.    1.1   0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.062 0.413]
 [0.    0.    0.    0.    0.    0.    0.985 0.    0.966 0.    0.    0.
  0.    0.    0.    0.346 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.969 0.    0.    0.   ]
 [0.    1.674 0.    0.    2.434 0.354 0.304 1.378 0.799 0.    0.    1.74
  0.    0.    0.    0.    0.    0.    0.659 0.    0.    2.578 0.534 0.
  0.    0.    1.067 0.    0.308 0.   ]
 [0.    0.    2.758 1.15  0.    1.711 0.349 0.    0.423 0.    0.    0.477
  0.316 0.    0.    0.501 0.    1.754 0.408 0.    0.    0.    0.351 2.637
  0.    0.528 0.37  0.    0.    2.429]
 [0.    0.863 0.    0.    0.    0.    0.442 0.    0.    0.    0.    0.686
  2.511 0.    0.    0.    0.    0.    0.    0.    0.371 0.623 1.059 3.6
  0.    0.    0.446 0.    0.476 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.415 1.101 0.547 0.315 0.552 0.    0.    0.
  1.095 0.    3.073 0.49  0.    0.    0.    0.    0.739 1.384 0.    0.324
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.118 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.75  0.319 0.    0.    0.363 0.
  0.    0.    0.73  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.738 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.469 0.   ]
 [0.    0.523 0.    0.392 1.887 1.048 0.    1.113 0.    0.    0.    0.
  0.    3.741 0.    0.    0.    0.    0.    0.    0.    1.745 0.636 0.
  0.    2.438 0.    0.    0.    0.   ]
 [0.    0.    2.468 1.101 1.636 0.    0.65  0.    0.485 0.    0.    0.386
  0.    0.    0.    0.    0.    2.303 0.339 0.    0.    0.    0.534 0.
  0.    0.    0.    0.    0.549 2.114]
 [0.    0.    0.    0.    0.    0.    0.976 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.502 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [2.075 0.    0.    0.    0.    0.    0.961 0.    0.338 0.    0.    2.188
  0.    0.    0.    0.828 0.    0.    0.796 0.855 0.    0.    0.416 0.
  0.    0.    1.558 0.    0.    0.   ]
 [0.    0.455 0.695 0.    0.    0.546 0.559 0.    0.    1.156 4.378 0.
  0.    2.165 0.    0.    0.    0.398 0.    0.    0.    0.    0.914 0.
  0.    2.793 0.    0.    0.    0.931]
 [0.    1.743 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.383 0.    0.    0.    1.358 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.581 0.    1.809 0.402 0.    0.    0.    0.    0.
  0.    0.    0.    0.834 0.    0.    0.882 0.    0.    0.    1.323 0.
  0.    0.    0.    0.    0.813 0.   ]
 [2.101 0.342 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.98
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.908 0.
  0.    2.995 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.429 0.    0.    0.317 0.    0.373 0.    0.    1.985
  0.    0.    0.    0.    0.    0.316 0.674 0.    0.    0.    0.    0.
  0.    0.    0.73  0.    0.    0.   ]]
{'fdr': 0.3446327683615819, 'tpr': 0.9666666666666667, 'fpr': 0.19365079365079366, 'f1': 0.7811447811447813, 'shd': 65, 'npred': 177, 'ntrue': 120}
[1.358e-04 1.885e-03 1.931e+00 1.300e-03 1.731e-01 2.062e-01 5.247e-05
 2.267e-04 2.351e-05 9.161e-06 1.429e-04 1.113e-05 2.455e-05 3.492e-05
 9.301e-01 2.110e-05 7.392e-05 7.312e-01 1.700e-01 7.238e-07 2.873e-05
 1.466e-01 1.713e-04 2.087e-06 1.111e-04 1.672e-01 2.866e-05 2.213e-01
 3.410e-05 4.226e-01 8.236e-03 1.157e-01 1.544e-01 1.562e+00 1.967e-01
 3.589e-04 1.184e+00 8.895e-05 5.478e-05 1.360e-01 2.336e-04 1.021e-04
 1.010e-04 8.643e-01 4.637e-05 1.204e+00 3.659e-01 6.766e-01 1.112e-05
 2.098e-03 1.014e+00 7.229e-04 1.021e-07 3.919e-04 1.813e-01 2.011e-05
 2.334e-01 1.319e-01 1.689e-01 6.648e-02 4.892e-01 1.486e-02 4.461e-01
 1.937e-01 2.478e-03 9.076e-02 9.263e-07 1.253e-05 1.649e-01 1.691e-05
 1.986e-04 4.339e-06 7.777e-01 5.654e-06 2.582e-01 5.841e-02 6.845e-02
 1.869e-05 1.081e-04 1.705e-01 3.229e-04 1.820e-05 2.510e-03 1.886e-01
 4.032e-05 2.204e-01 4.974e-03 1.110e-04 8.043e-05 1.321e-04 3.137e-04
 1.352e-03 1.600e-01 4.666e-05 1.202e-04 3.712e-06 1.586e-05 3.509e-05
 5.567e-06 1.887e-05 1.923e-06 9.008e-01 8.691e-06 2.773e-05 1.742e-01
 6.305e-01 6.069e-06 1.124e-05 1.605e-01 1.879e-05 1.038e-05 6.570e-05
 6.530e-05 1.130e-05 2.534e-01 3.048e-05 9.915e-02 9.474e-03 7.264e-02
 2.165e-01 1.074e-01 3.327e-01 3.041e-04 2.633e-01 2.650e-05 1.932e-05
 2.630e-01 7.907e-05 3.345e-05 1.507e-05 1.677e-01 1.320e-06 1.601e-03
 7.385e-01 1.921e-01 5.298e-06 5.705e-04 1.432e+00 2.896e-04 7.940e-07
 7.510e-04 1.217e-01 3.095e-04 2.433e-01 4.100e-03 7.904e-04 3.645e-04
 7.571e-05 1.857e-01 4.002e-03 1.703e+00 2.677e-04 1.885e-03 9.942e-06
 1.836e-05 4.537e-05 3.124e-05 1.845e-05 6.414e-06 2.383e-01 2.736e-06
 1.551e-04 1.913e-01 1.167e-01 2.717e-06 1.388e-05 2.886e-01 2.892e-05
 1.604e-06 4.761e-05 1.082e-04 6.512e-06 8.057e-01 3.909e-05 2.902e-04
 1.494e-04 1.160e-04 9.082e-04 9.171e-05 2.931e-04 9.235e-05 7.536e-04
 3.032e-06 5.970e-06 6.615e-05 2.437e-05 6.616e-06 1.356e-05 1.032e+00
 9.759e-06 2.448e-05 9.704e-03 1.783e-03 1.874e-06 1.454e-05 4.905e-04
 9.638e-05 3.532e-06 1.456e-05 3.197e-05 7.526e-06 1.410e-01 1.785e-05
 4.250e-01 8.199e-01 1.104e-01 1.676e-01 2.237e+00 2.001e-01 1.961e-01
 1.343e+00 3.633e-05 2.365e-05 1.531e-01 2.029e-04 1.031e-04 1.995e-05
 5.935e-01 5.647e-06 3.089e-01 1.662e-01 7.521e-01 5.610e-06 2.529e-03
 2.509e-01 1.368e-03 6.052e-06 4.878e-03 1.429e-01 7.172e-04 4.881e-01
 2.715e-01 1.226e+00 2.879e-04 3.067e-03 2.548e-01 1.055e-03 1.119e-01
 1.600e-01 5.982e-05 2.591e-06 5.917e-06 9.832e-05 3.425e-05 4.658e-06
 6.881e-06 5.670e-02 7.576e-06 1.793e-04 2.954e-01 9.033e-01 2.813e-06
 3.721e-05 1.652e-01 5.263e-05 4.156e-07 1.696e-04 1.218e-01 1.738e-05
 1.614e-01 3.234e-05 2.939e-01 9.675e-01 1.784e-01 2.359e-01 2.454e-01
 3.679e-01 2.757e-01 3.085e-01 1.189e+00 9.584e-05 2.192e-01 2.246e+00
 1.386e-01 6.780e-03 3.428e-01 1.044e-03 4.043e-01 2.259e-01 9.808e-02
 2.383e-03 4.844e-01 4.594e-01 8.593e-02 4.666e-05 1.694e-01 2.368e-01
 8.884e-04 3.294e-01 1.871e+00 1.688e+00 1.816e+00 3.211e-01 1.845e-01
 2.312e-01 3.386e-01 2.736e-01 1.969e+00 3.238e-01 1.278e+00 7.890e-02
 1.625e-01 3.004e-01 4.346e-03 1.100e+00 1.149e-03 2.852e-01 2.157e-01
 1.547e-01 2.604e-02 9.138e-02 2.574e-01 1.861e-02 2.406e-04 2.103e-01
 8.734e-02 1.451e-02 1.062e+00 4.131e-01 1.424e-01 2.507e-03 3.578e-03
 1.467e-01 3.402e-03 1.420e-01 9.849e-01 4.376e-04 9.662e-01 2.682e-05
 9.440e-06 3.999e-05 1.023e-05 3.571e-06 3.463e-01 3.125e-05 2.312e-03
 2.471e-01 1.286e-01 2.340e-06 3.658e-05 1.869e-01 5.186e-05 4.473e-06
 5.305e-04 9.688e-01 1.049e-04 1.590e-01 2.492e-04 1.458e-01 1.674e+00
 1.053e-01 1.890e-01 2.434e+00 3.543e-01 3.038e-01 1.378e+00 7.988e-01
 1.741e-04 3.473e-05 1.740e+00 1.406e-04 7.834e-05 2.339e-01 2.359e-05
 1.823e-01 6.585e-01 9.831e-02 3.672e-05 2.578e+00 5.336e-01 3.625e-03
 7.578e-06 3.394e-02 1.067e+00 1.299e-03 3.081e-01 2.579e-01 2.140e-01
 1.972e-01 2.758e+00 1.150e+00 1.106e-01 1.711e+00 3.493e-01 1.612e-01
 4.231e-01 2.222e-03 1.530e-03 4.774e-01 3.163e-01 7.682e-05 5.014e-01
 1.458e-04 1.754e+00 4.077e-01 2.143e-01 8.404e-05 1.293e-01 3.512e-01
 2.637e+00 2.698e-04 5.281e-01 3.702e-01 7.050e-03 2.106e-01 2.429e+00
 1.989e-01 8.626e-01 6.961e-02 2.784e-01 2.204e-01 2.482e-01 4.422e-01
 2.818e-01 2.324e-01 2.591e-02 1.996e-02 6.860e-01 2.511e+00 8.417e-02
 2.378e-01 2.610e-04 1.741e-01 2.100e-01 1.735e-01 3.714e-01 6.234e-01
 1.059e+00 3.600e+00 6.123e-03 1.026e-01 4.458e-01 1.959e-03 4.757e-01
 2.717e-01 3.851e-05 3.234e-04 2.658e-04 1.350e-04 4.982e-04 1.657e-04
 1.176e-03 3.072e-05 3.878e-05 1.886e-05 1.451e-05 4.450e-05 3.313e-05
 1.393e-05 4.472e-06 7.342e-06 1.000e-04 7.369e-03 7.442e-04 2.931e-06
 1.130e-05 2.318e-04 2.908e-05 2.544e-06 1.281e-04 1.417e-04 1.293e-05
 2.899e-01 2.119e-05 2.090e-01 2.405e-01 1.486e-01 6.411e-02 4.146e-01
 1.101e+00 5.469e-01 3.150e-01 5.517e-01 1.324e-01 7.570e-02 1.216e-01
 1.095e+00 9.773e-02 3.073e+00 4.898e-01 2.902e-02 2.217e-01 7.399e-02
 7.394e-01 1.384e+00 1.794e-01 3.242e-01 4.458e-04 5.322e-02 1.330e-01
 4.228e-03 2.713e-01 1.512e-01 1.427e-01 4.157e-04 3.113e-03 2.479e-01
 1.282e-01 2.705e-01 1.812e-01 9.408e-05 1.118e+00 5.664e-06 1.487e-05
 2.006e-01 8.018e-06 1.173e-04 4.934e-06 2.194e-01 3.424e-07 7.499e-01
 3.193e-01 2.071e-05 1.241e-04 3.626e-01 9.451e-05 6.873e-06 1.353e-04
 7.299e-01 1.054e-05 1.931e-01 1.060e-03 8.902e-04 4.068e-04 1.718e-03
 7.961e-04 3.421e-04 2.589e-03 1.487e-01 5.015e-05 1.626e-04 1.707e-05
 9.320e-06 3.071e-04 5.118e-05 3.130e-05 6.012e-05 1.209e-01 4.545e-05
 2.903e-04 4.222e-04 4.299e-06 6.015e-05 9.217e-03 2.713e-04 9.136e-07
 4.292e-04 1.098e-03 1.690e-05 1.264e-02 1.045e-04 3.015e-04 2.647e-04
 9.509e-04 2.387e-03 7.200e-04 2.666e-03 2.082e-01 3.262e-04 1.373e-04
 6.974e-06 1.107e-05 1.984e-04 2.253e-05 4.871e-05 4.367e-05 2.809e-01
 2.586e-05 5.271e-05 7.381e-01 1.483e-05 4.779e-06 2.284e-01 1.884e-04
 1.542e-05 1.280e-04 2.969e-04 2.277e-05 4.688e-01 7.158e-05 1.852e-01
 5.227e-01 2.090e-01 3.918e-01 1.887e+00 1.048e+00 2.493e-01 1.113e+00
 1.913e-01 8.407e-02 2.185e-02 2.226e-01 2.277e-01 3.741e+00 6.903e-04
 2.109e-01 6.222e-04 2.149e-01 2.355e-01 6.147e-02 1.745e+00 6.357e-01
 7.249e-02 8.633e-04 2.438e+00 2.475e-01 9.898e-04 2.284e-01 1.585e-01
 8.799e-02 2.320e-01 2.468e+00 1.101e+00 1.636e+00 2.445e-01 6.499e-01
 2.971e-01 4.845e-01 2.308e-05 4.979e-06 3.863e-01 3.244e-05 1.463e-05
 2.329e-05 2.261e-01 5.755e-06 2.303e+00 3.393e-01 2.502e-01 5.560e-05
 5.342e-01 2.476e-04 1.266e-06 3.932e-03 2.839e-01 7.796e-04 5.485e-01
 2.114e+00 6.880e-04 8.803e-04 7.008e-04 4.542e-03 2.798e-04 8.040e-04
 9.756e-01 4.197e-05 1.005e-03 7.199e-06 1.589e-05 2.480e-04 2.164e-05
 1.615e-05 3.642e-06 2.205e-01 2.669e-06 1.143e-04 5.020e-01 2.905e-03
 2.961e-06 5.048e-05 2.352e-05 7.656e-06 1.621e-04 8.649e-04 2.065e-06
 1.228e-01 7.331e-05 2.075e+00 2.833e-01 1.265e-01 2.665e-01 1.577e-01
 1.627e-01 9.606e-01 1.609e-01 3.377e-01 1.767e-03 3.783e-05 2.188e+00
 2.622e-01 9.715e-05 1.800e-04 8.284e-01 6.146e-05 8.602e-02 7.964e-01
 8.550e-01 6.256e-06 2.762e-01 4.160e-01 4.015e-05 1.608e-02 1.558e+00
 4.479e-03 1.473e-01 1.246e-01 2.333e-01 4.550e-01 6.953e-01 1.412e-01
 1.858e-01 5.460e-01 5.594e-01 2.750e-01 1.168e-01 1.156e+00 4.378e+00
 1.575e-01 1.874e-01 2.165e+00 4.357e-02 1.313e-01 1.055e-02 3.985e-01
 1.823e-01 8.316e-02 3.336e-02 1.124e-01 9.143e-01 1.009e-01 2.793e+00
 1.084e-01 8.790e-04 2.291e-01 9.310e-01 2.221e-01 1.743e+00 1.746e-01
 7.657e-02 1.610e-01 2.290e-01 1.000e-01 1.245e-01 2.197e-01 3.032e-03
 4.183e-03 1.173e-01 1.444e-02 4.464e-04 1.363e-04 9.865e-02 6.668e-05
 1.170e-01 3.834e-01 1.510e-01 4.165e-05 1.220e-01 1.358e+00 3.523e-02
 4.351e-04 5.814e-02 9.453e-05 8.510e-02 1.254e-01 1.729e-03 2.132e-04
 8.988e-04 1.581e+00 4.622e-03 1.809e+00 4.018e-01 2.563e-04 2.228e-03
 9.323e-06 1.196e-05 3.223e-04 7.230e-05 1.597e-05 4.980e-06 8.337e-01
 6.422e-06 1.585e-04 8.820e-01 2.070e-01 3.655e-06 4.728e-05 1.323e+00
 3.484e-05 1.259e-06 2.428e-04 2.430e-05 8.130e-01 7.369e-05 2.101e+00
 3.417e-01 1.891e-01 2.509e-01 1.545e-01 2.115e-01 1.334e-01 9.976e-02
 2.295e-01 1.288e-02 6.523e-03 9.801e-01 6.188e-03 1.566e-02 2.151e-02
 1.786e-01 1.778e-02 1.218e-01 1.728e-01 6.518e-02 8.965e-04 5.254e-02
 9.079e-01 2.860e-02 3.307e-03 2.995e+00 1.379e-01 1.492e-01 9.556e-02
 1.017e-03 4.849e-04 5.441e-04 3.311e-03 1.865e-03 1.463e-03 5.524e-03
 1.370e-03 4.665e-04 1.409e-05 1.456e-04 5.927e-04 5.023e-05 2.970e-05
 3.564e-05 1.621e-02 4.251e-05 3.114e-04 1.041e-01 9.864e-03 1.477e-05
 1.136e-04 2.010e-03 1.488e-04 6.722e-05 4.434e-05 1.121e-03 2.568e-06
 2.039e-04 2.440e-01 5.703e-03 2.446e-01 4.287e-01 1.549e-01 2.595e-01
 3.172e-01 1.996e-03 3.732e-01 3.867e-05 1.362e-05 1.985e+00 1.408e-05
 1.275e-04 8.781e-07 2.204e-01 5.111e-06 3.156e-01 6.738e-01 1.864e-01
 2.093e-05 1.135e-04 1.921e-01 3.296e-04 2.387e-05 2.065e-03 7.296e-01
 2.509e-04 9.645e-02]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.9922777777777778, 0.9710876060293828)
cuda
9630
cuda
Objective function 1220.30 = squared loss an data 964.29 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 397 ; DAG False
||w||^2 0.3041144257717001
exp ma of ||w||^2 6.782473717578225
||w|| 0.5514657067957174
exp ma of ||w|| 0.5257498457476637
||w||^2 0.2601037588730923
exp ma of ||w||^2 0.22197851838175378
||w|| 0.5100036851563842
exp ma of ||w|| 0.46553709000305893
||w||^2 0.20479176018042758
exp ma of ||w||^2 0.2223559876892752
||w|| 0.4525392360673576
exp ma of ||w|| 0.4634129473023371
||w||^2 0.19394374721466112
exp ma of ||w||^2 0.21650582543505567
||w|| 0.44039044859608517
exp ma of ||w|| 0.458737537650633
||w||^2 0.1858183796366829
exp ma of ||w||^2 0.22334968927178483
||w|| 0.4310665605642392
exp ma of ||w|| 0.4622303118815184
||w||^2 0.16350589353778897
exp ma of ||w||^2 0.21703441394021825
||w|| 0.40435861996226685
exp ma of ||w|| 0.45289620297952643
||w||^2 0.1380676253255945
exp ma of ||w||^2 0.2177655625741431
||w|| 0.37157452190051254
exp ma of ||w|| 0.45406044651798405
cuda
Objective function 381.77 = squared loss an data 377.11 + 0.5*rho*h**2 3.545246 + alpha*h 0.000000 + L2reg 0.59 + L1reg 0.53 ; SHD = 163 ; DAG False
Proportion of microbatches that were clipped  0.7353127977135598
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.662797832807364
iteration 1 in outer loop, alpha = 2.662797832807364, rho = 1.0, h = 2.662797832807364
cuda
9630
cuda
Objective function 388.86 = squared loss an data 377.11 + 0.5*rho*h**2 3.545246 + alpha*h 7.090492 + L2reg 0.59 + L1reg 0.53 ; SHD = 163 ; DAG False
||w||^2 50919.826518668575
exp ma of ||w||^2 2750820.827080848
||w|| 225.6542189250371
exp ma of ||w|| 683.7246387722251
||w||^2 18517.79531241087
exp ma of ||w||^2 1597741.4760322305
||w|| 136.08010623309664
exp ma of ||w|| 466.2496557615462
||w||^2 162.99895953682625
exp ma of ||w||^2 67693.51860209319
||w|| 12.767104587055996
exp ma of ||w|| 53.12524868911437
||w||^2 0.518937546460544
exp ma of ||w||^2 0.4685689504840241
||w|| 0.7203731994324497
exp ma of ||w|| 0.6625113996824056
||w||^2 0.19665765589404033
exp ma of ||w||^2 0.48948287867789314
||w|| 0.4434609970381165
exp ma of ||w|| 0.6751146761382602
cuda
Objective function 165.20 = squared loss an data 154.21 + 0.5*rho*h**2 2.951105 + alpha*h 6.469125 + L2reg 1.05 + L1reg 0.52 ; SHD = 177 ; DAG False
Proportion of microbatches that were clipped  0.7476878143761155
iteration 1 in inner loop, alpha 2.662797832807364 rho 1.0 h 2.429446409808044
9630
cuda
Objective function 191.76 = squared loss an data 154.21 + 0.5*rho*h**2 29.511049 + alpha*h 6.469125 + L2reg 1.05 + L1reg 0.52 ; SHD = 177 ; DAG False
||w||^2 0.5032115492459575
exp ma of ||w||^2 0.9012527529407897
||w|| 0.7093740545339655
exp ma of ||w|| 0.9219930044945687
||w||^2 0.8514978639211818
exp ma of ||w||^2 1.0340955599909163
||w|| 0.9227664189388243
exp ma of ||w|| 0.9911404407752501
||w||^2 0.6191485863427342
exp ma of ||w||^2 1.0439846185979336
||w|| 0.7868599534496175
exp ma of ||w|| 0.9826852253606169
cuda
Objective function 108.12 = squared loss an data 96.62 + 0.5*rho*h**2 6.609733 + alpha*h 3.061577 + L2reg 1.38 + L1reg 0.45 ; SHD = 135 ; DAG False
Proportion of microbatches that were clipped  0.7676484338475923
iteration 2 in inner loop, alpha 2.662797832807364 rho 10.0 h 1.1497593646718833
9630
cuda
Objective function 167.61 = squared loss an data 96.62 + 0.5*rho*h**2 66.097330 + alpha*h 3.061577 + L2reg 1.38 + L1reg 0.45 ; SHD = 135 ; DAG False
||w||^2 686422514261.4592
exp ma of ||w||^2 209068752680.8247
||w|| 828506.1944617308
exp ma of ||w|| 264408.0685742107
||w||^2 315918406326.3097
exp ma of ||w||^2 248837774204.0846
||w|| 562066.1939009583
exp ma of ||w|| 323178.11281626637
||w||^2 2488043925.854025
exp ma of ||w||^2 34307457652.907578
||w|| 49880.29596798745
exp ma of ||w|| 155734.14376425435
||w||^2 1348221.8563447113
exp ma of ||w||^2 38736680.80290433
||w|| 1161.1295605334967
exp ma of ||w|| 3284.815836291381
||w||^2 13912.638339768018
exp ma of ||w||^2 1001939.7719427834
||w|| 117.95184754707329
exp ma of ||w|| 290.3348299711258
||w||^2 2.5303770716697
exp ma of ||w||^2 355.5859490963007
||w|| 1.5907158991063426
exp ma of ||w|| 4.130427137366131
||w||^2 2.689024434943564
exp ma of ||w||^2 58.787274968708154
||w|| 1.6398245134597678
exp ma of ||w|| 2.2612399531132015
||w||^2 1.4593100269720796
exp ma of ||w||^2 14.164921991539764
||w|| 1.2080190507488198
exp ma of ||w|| 1.6943619918658026
||w||^2 0.9547732343389288
exp ma of ||w||^2 2.2801780147766726
||w|| 0.9771249839907528
exp ma of ||w|| 1.4621235524136675
cuda
Objective function 101.89 = squared loss an data 87.71 + 0.5*rho*h**2 10.964746 + alpha*h 1.246960 + L2reg 1.58 + L1reg 0.38 ; SHD = 126 ; DAG True
Proportion of microbatches that were clipped  0.779100736330879
iteration 3 in inner loop, alpha 2.662797832807364 rho 100.0 h 0.46828934797242994
iteration 2 in outer loop, alpha = 49.49173263005036, rho = 100.0, h = 0.46828934797242994
cuda
9630
cuda
Objective function 123.82 = squared loss an data 87.71 + 0.5*rho*h**2 10.964746 + alpha*h 23.176451 + L2reg 1.58 + L1reg 0.38 ; SHD = 126 ; DAG True
||w||^2 7065033904.654563
exp ma of ||w||^2 28113440566.696854
||w|| 84053.7560413249
exp ma of ||w|| 136113.1910694792
||w||^2 90279.159131498
exp ma of ||w||^2 11237288.750388542
||w|| 300.46490499141163
exp ma of ||w|| 1153.0062998162368
||w||^2 5.542189640960708
exp ma of ||w||^2 810.664337119598
||w|| 2.354185557886359
exp ma of ||w|| 5.5453203322523725
||w||^2 4.608245199853939
exp ma of ||w||^2 607.9934379134492
||w|| 2.146682370508953
exp ma of ||w|| 4.866418100173732
||w||^2 5.06511497806044
exp ma of ||w||^2 215.12784696496954
||w|| 2.2505810312140375
exp ma of ||w|| 3.4684754445617028
||w||^2 2.352832621565136
exp ma of ||w||^2 23.152303545366973
||w|| 1.5338945927165712
exp ma of ||w|| 2.083111445208962
||w||^2 2.3633145267722027
exp ma of ||w||^2 3.7781853430865464
||w|| 1.5373075576384196
exp ma of ||w|| 1.794000411724027
||w||^2 2.234853506732579
exp ma of ||w||^2 3.013502322179325
||w|| 1.4949426432919022
exp ma of ||w|| 1.6772423157781147
||w||^2 2.1211648032835186
exp ma of ||w||^2 3.0235811008426525
||w|| 1.456421918018099
exp ma of ||w|| 1.6671525115620267
cuda
Objective function 109.78 = squared loss an data 85.68 + 0.5*rho*h**2 5.538010 + alpha*h 16.471172 + L2reg 1.73 + L1reg 0.36 ; SHD = 118 ; DAG True
Proportion of microbatches that were clipped  0.7785911826907435
iteration 1 in inner loop, alpha 49.49173263005036 rho 100.0 h 0.33280654252072495
9630
cuda
Objective function 159.62 = squared loss an data 85.68 + 0.5*rho*h**2 55.380097 + alpha*h 16.471172 + L2reg 1.73 + L1reg 0.36 ; SHD = 118 ; DAG True
||w||^2 665159.1942305955
exp ma of ||w||^2 36291341.05831339
||w|| 815.5729239194957
exp ma of ||w|| 2299.0408532641727
||w||^2 254.95523556914304
exp ma of ||w||^2 63792.89587639653
||w|| 15.967317732454097
exp ma of ||w|| 36.01031893512569
||w||^2 11.727981072343633
exp ma of ||w||^2 11.325186619354294
||w|| 3.4246140034087977
exp ma of ||w|| 2.3100132411175753
v before min max tensor([[  735.067,  -381.501,  -503.375,  ...,  -124.633,  -379.569,
          -664.922],
        [  463.622,  -290.102,  -597.277,  ...,   578.804,  -270.018,
          4448.239],
        [ 4661.371,  -527.531,   -36.348,  ...,  -531.167,   138.428,
          1867.881],
        ...,
        [  -22.951,  -353.204,  -549.798,  ...,   706.903,   483.212,
         18107.651],
        [ -619.898,  -456.828,  -229.883,  ...,   101.583,  -527.832,
          1653.180],
        [ -734.538,  -620.268,  -295.330,  ...,  1175.103,  1034.972,
          1960.760]], device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        ...,
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01]], device='cuda:0')
v before min max tensor([-2.892e+02, -1.758e+02,  2.125e+03, -2.425e+02, -5.222e+02, -3.768e+02,
         5.084e+02, -1.043e+02,  9.792e+02, -3.484e+02,  2.810e+03, -6.468e+02,
        -5.439e+02, -2.752e+01,  4.135e+01,  3.277e+03, -4.311e+02, -5.773e+02,
         1.724e+02, -6.083e+02, -2.239e+02, -3.175e+02, -4.650e+02, -1.229e+02,
        -5.935e+02, -3.362e+02,  2.441e+02, -4.557e+01, -2.951e+02, -2.258e+02,
        -3.058e+01,  1.309e+02, -5.669e+02, -1.733e+02, -5.927e+02,  1.157e+03,
         2.592e+03,  3.804e+02, -3.772e+02,  5.372e+02,  1.309e+01, -6.457e+02,
         7.912e+00, -6.453e+01, -2.504e+01, -4.027e+02,  7.457e+01, -5.301e+02,
         7.516e+03, -1.973e+02,  1.750e+03, -5.712e+02, -5.268e+02, -3.802e+02,
         1.570e+03, -1.524e+02,  5.492e+02, -2.611e+02, -5.145e+02, -5.745e+02,
        -3.987e+02, -3.587e+02,  3.819e+03,  6.788e+02, -3.349e+02, -1.053e+02,
         2.350e+02, -6.629e+01,  7.224e+02,  7.097e+02, -3.998e+02, -4.756e+02,
        -3.143e+02,  5.238e+02, -2.993e+02,  8.687e+02, -2.673e+01, -3.386e+02,
        -5.572e+02, -1.927e+02,  3.111e+01, -4.707e+02, -2.019e+02, -3.694e+02,
        -2.201e+02, -4.680e+02, -4.333e+02, -4.717e+02,  4.703e+02, -2.016e+02,
        -6.488e+02,  1.920e+03,  1.860e+02, -4.915e+02,  9.190e+01, -6.930e+02,
        -6.853e+02, -1.452e+02,  5.021e+02, -5.289e+02,  2.517e+03,  2.303e+03,
         9.086e+02, -3.205e+02,  2.825e+02,  1.976e+03, -1.956e+02, -4.183e+02,
         1.425e+02,  4.192e+02,  4.497e+02, -4.255e+02, -5.816e+02, -5.361e+02,
         1.370e+03, -5.649e+02,  1.373e+03,  1.703e+02,  3.266e+02,  7.535e+02,
        -3.296e+02, -6.365e+02,  1.927e+01,  1.660e+02, -4.342e+02, -5.758e+02,
         6.509e+01, -4.529e+02,  3.944e+01,  1.994e+02, -2.695e+02, -4.520e+02,
        -5.512e+02,  8.551e+02, -5.477e+02, -4.980e+02,  1.857e+03, -6.952e+02,
         1.770e+03, -1.120e+02, -2.530e+02, -5.384e+02,  5.407e+02,  5.961e+02,
        -3.909e+02, -2.869e+02, -6.598e+02, -7.188e+01,  1.350e+03, -2.447e+02,
        -5.721e+02,  3.703e+02,  1.186e+02,  3.765e+03, -5.023e+01,  2.874e+02,
        -3.484e+02, -4.493e+02, -5.061e+02,  1.973e+02,  2.402e+02, -4.496e+01,
        -5.297e+02, -5.345e+02, -2.584e+02,  4.145e+02, -2.020e+02,  1.636e+02,
        -4.286e+02,  4.765e+02, -4.105e+02,  1.392e+03, -6.061e+02, -3.048e+02,
         2.286e+03,  1.002e+03, -4.344e+02, -3.286e+02,  3.255e+02, -4.117e+02,
        -4.024e+02, -5.467e+02,  4.548e+02,  2.164e+03,  1.921e+03, -4.879e+02,
        -6.601e+02,  7.551e+02, -1.610e+02,  2.399e+02, -5.367e+02, -3.689e+02,
        -2.850e+02, -5.339e+02,  7.559e+02,  1.622e+02,  2.318e+03, -2.621e+02,
         8.379e+01, -6.617e+02, -3.790e+02, -1.816e+02, -4.628e+02, -6.023e+02,
        -3.179e+02,  1.043e+03, -4.887e+02, -6.237e+02, -2.249e+02, -3.593e+02,
         2.143e+02, -5.342e+02, -5.606e+02,  9.728e+02, -5.638e+01,  4.060e+03,
        -4.118e+02,  5.033e+02, -3.094e+02, -2.921e+02,  8.352e+01, -4.896e+02,
        -4.272e+02, -7.128e+02, -5.165e+02, -3.896e+02,  2.207e+03,  1.880e+03,
        -2.093e+02, -5.191e+02,  7.768e+02, -4.702e+02, -4.686e+01, -2.905e+02,
        -6.054e+02,  3.507e+02, -1.716e+02,  1.831e+02, -4.771e+02, -5.443e+02,
        -9.712e+01, -4.887e+02,  2.056e+03, -4.491e+02, -4.971e+02, -3.705e+02,
         9.476e+02, -3.179e+02, -3.387e+02, -4.646e+02, -3.172e+02, -4.315e+02,
         3.190e+02,  6.169e+02, -3.244e+02, -9.755e+01,  1.484e+02, -5.598e+02,
        -4.451e+02,  6.700e+02, -4.637e+02, -4.565e+00, -6.326e+02, -5.167e+02,
        -4.957e+02, -1.303e+02, -6.642e+02, -5.121e+02, -4.211e+02, -5.964e+02,
         6.231e+03, -1.343e+02, -6.754e+02,  6.865e+02, -1.640e+02, -3.150e+02,
        -5.675e+02, -1.712e+02,  1.725e+02,  4.004e+01, -1.805e+02, -1.390e+02,
        -2.858e+02, -3.409e+02, -2.514e+02,  8.228e+02,  6.060e+02,  2.601e+03,
        -6.403e+02, -2.811e+02, -6.872e+02, -4.941e+02, -3.996e+02, -6.010e+02,
        -4.389e+02,  4.123e+02,  7.289e+02, -2.079e+02,  1.730e+03, -2.515e+02],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        7.912e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 2.353e+03],
         [-4.871e+02],
         [ 1.153e+03],
         [-4.931e+02],
         [ 5.325e+01],
         [ 1.015e+03],
         [-6.214e+02],
         [ 3.632e+02],
         [-5.711e+02],
         [-1.911e+01]],

        [[-3.179e+02],
         [-6.861e+02],
         [-4.865e+02],
         [-6.093e+02],
         [ 1.945e+02],
         [ 3.365e+02],
         [ 6.531e+02],
         [-2.874e+02],
         [ 3.401e+02],
         [ 1.393e+03]],

        [[-5.249e+02],
         [-1.826e+02],
         [-2.780e+02],
         [-6.366e+02],
         [-4.877e+02],
         [-4.327e+02],
         [-1.051e+02],
         [-5.056e+02],
         [ 6.470e+02],
         [-4.008e+02]],

        [[-6.906e+02],
         [ 4.684e+02],
         [ 6.098e+02],
         [ 1.629e+03],
         [-3.624e+01],
         [ 2.865e+03],
         [-3.346e+02],
         [-5.125e+02],
         [ 9.303e+02],
         [ 1.590e+03]],

        [[ 7.456e+01],
         [ 1.809e+01],
         [ 1.103e+02],
         [ 6.460e+02],
         [ 6.663e+02],
         [-7.470e+02],
         [-4.937e+02],
         [-6.653e+02],
         [-4.529e+02],
         [-4.494e+02]],

        [[ 9.625e+02],
         [-5.261e+02],
         [-2.913e+02],
         [-2.649e+02],
         [-5.444e+02],
         [-3.863e+02],
         [ 2.519e+02],
         [-4.977e+02],
         [-6.042e+02],
         [ 2.680e+03]],

        [[ 2.599e+03],
         [-4.087e+02],
         [-6.452e+02],
         [ 4.973e+02],
         [-6.551e+01],
         [-1.281e+02],
         [-2.272e+02],
         [-5.082e+02],
         [-5.400e+02],
         [ 4.711e+02]],

        [[-4.022e+02],
         [-4.748e+02],
         [ 1.474e+03],
         [-3.088e+02],
         [ 2.538e+02],
         [ 1.162e+03],
         [-1.047e+02],
         [-2.741e+02],
         [-2.182e+02],
         [ 4.242e+02]],

        [[-1.513e+02],
         [-5.253e+02],
         [ 1.573e+02],
         [-4.642e+02],
         [-4.054e+02],
         [-3.651e+02],
         [ 2.106e+02],
         [-1.053e+02],
         [ 3.773e+03],
         [ 3.075e+01]],

        [[-6.225e+02],
         [-3.727e+02],
         [ 2.418e+02],
         [-5.373e+02],
         [-3.211e+02],
         [-2.462e+02],
         [ 2.525e+02],
         [-4.108e+02],
         [-2.689e+02],
         [ 1.245e+03]],

        [[-4.816e+02],
         [-4.841e+02],
         [-3.026e+02],
         [-5.356e+02],
         [ 2.346e+02],
         [ 1.430e+02],
         [ 1.990e+02],
         [ 2.777e+02],
         [-5.228e+02],
         [ 5.846e+01]],

        [[-1.937e+02],
         [-6.167e+02],
         [-4.082e+02],
         [ 5.091e+02],
         [-4.014e+01],
         [-3.553e+02],
         [ 1.761e+02],
         [ 3.246e+00],
         [-3.030e+02],
         [-5.785e+02]],

        [[ 4.773e+02],
         [-3.543e+02],
         [ 1.938e+02],
         [-1.200e+02],
         [-4.145e+02],
         [ 1.739e+03],
         [-6.568e+01],
         [ 4.744e+02],
         [-5.003e+02],
         [ 5.362e+02]],

        [[-5.384e+02],
         [ 2.032e+03],
         [-3.462e+02],
         [-4.318e+02],
         [-3.526e+02],
         [-4.793e+02],
         [-5.202e+02],
         [-3.778e+02],
         [-6.024e+02],
         [-3.293e+02]],

        [[-5.677e+02],
         [-1.986e+02],
         [ 1.435e+03],
         [-2.117e+02],
         [-2.453e+01],
         [-2.856e+02],
         [ 1.664e+01],
         [-3.746e+02],
         [ 1.957e+02],
         [ 1.444e+02]],

        [[-5.162e+02],
         [-6.225e+02],
         [-6.779e+02],
         [ 1.438e+03],
         [-5.486e+02],
         [-2.538e+02],
         [ 2.417e+01],
         [ 6.761e+02],
         [-4.798e+02],
         [-5.168e+02]],

        [[-4.423e+02],
         [-5.556e+02],
         [-2.174e+02],
         [ 2.030e+02],
         [-5.620e+02],
         [-6.425e+02],
         [ 2.742e+02],
         [-3.071e+02],
         [ 1.626e+02],
         [-5.151e+02]],

        [[-6.382e+02],
         [-3.875e+02],
         [ 3.887e+02],
         [-3.583e+02],
         [ 2.592e+03],
         [-4.622e+02],
         [-3.635e+02],
         [-5.631e+02],
         [ 2.134e+02],
         [-6.783e+02]],

        [[ 1.071e+03],
         [-5.601e+02],
         [ 3.620e+02],
         [ 1.779e+02],
         [-3.470e+02],
         [-4.173e+02],
         [-1.767e+02],
         [-5.261e+02],
         [ 1.355e+03],
         [ 3.070e+02]],

        [[ 1.188e+03],
         [-4.435e+02],
         [-5.245e+02],
         [-4.661e+02],
         [ 1.364e+03],
         [-7.064e+02],
         [ 2.036e+02],
         [-3.240e+02],
         [ 2.204e+02],
         [-1.777e+02]],

        [[-2.123e+02],
         [-3.238e+02],
         [ 2.932e+02],
         [-1.601e+02],
         [ 2.887e+02],
         [-3.830e+02],
         [ 4.414e+02],
         [-6.457e+02],
         [-5.448e+02],
         [ 2.712e+01]],

        [[-3.113e+02],
         [-3.836e+02],
         [ 2.874e+02],
         [ 4.599e+02],
         [-4.916e+02],
         [-4.016e+02],
         [-5.238e+02],
         [-2.067e+02],
         [-5.539e+02],
         [-5.371e+02]],

        [[-4.125e+02],
         [-3.827e+02],
         [ 4.941e+02],
         [-5.105e+02],
         [-4.599e+02],
         [-4.664e+02],
         [ 9.816e+01],
         [-2.064e+02],
         [ 1.337e+01],
         [-7.777e+02]],

        [[-4.595e+02],
         [-5.197e+02],
         [-4.714e+02],
         [-2.706e+02],
         [-4.438e+02],
         [-4.533e+02],
         [-4.842e+02],
         [-4.525e+02],
         [-3.939e+02],
         [ 1.814e+03]],

        [[-3.886e+02],
         [-4.794e+02],
         [-3.427e+02],
         [ 8.822e+02],
         [-6.731e+02],
         [-1.536e+02],
         [-6.566e+02],
         [-5.119e+02],
         [ 5.806e+02],
         [ 7.077e+02]],

        [[-2.934e+02],
         [-3.678e+02],
         [ 6.205e+03],
         [-6.470e+02],
         [-3.056e+02],
         [-3.832e+02],
         [-6.890e+02],
         [-4.667e+02],
         [-3.336e+02],
         [-1.868e+02]],

        [[ 1.311e+03],
         [-1.717e+02],
         [ 7.638e+02],
         [-5.527e+02],
         [ 2.884e+02],
         [-5.704e+02],
         [ 2.482e+02],
         [-3.442e+02],
         [-4.942e+02],
         [ 8.384e+01]],

        [[-3.267e+02],
         [ 4.393e+02],
         [-3.698e+02],
         [-5.607e+01],
         [ 2.571e+03],
         [-3.543e+02],
         [-2.310e+02],
         [-2.766e+02],
         [-4.572e+02],
         [ 1.378e+03]],

        [[-3.293e+02],
         [-1.221e+01],
         [-4.285e+02],
         [ 1.432e+01],
         [-5.269e+02],
         [ 8.718e+01],
         [-6.720e+01],
         [-1.676e+02],
         [-4.578e+02],
         [ 1.427e+02]],

        [[ 2.341e+02],
         [-4.165e+02],
         [ 4.346e+01],
         [-5.377e+02],
         [-5.800e+02],
         [-5.448e+02],
         [-3.949e+02],
         [-3.395e+02],
         [-2.830e+02],
         [-5.906e+02]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [3.246e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-313.608],
        [-514.292],
        [-253.196],
        [-577.433],
        [-474.604],
        [-557.999],
        [-190.968],
        [-299.314],
        [-240.630],
        [-167.855],
        [ -95.534],
        [-314.095],
        [-405.211],
        [-223.337],
        [-472.052],
        [-466.599],
        [ 234.042],
        [-536.492],
        [  14.759],
        [-243.275],
        [ -38.117],
        [-479.345],
        [-464.269],
        [-569.477],
        [ 907.761],
        [-189.239],
        [1564.408],
        [-287.000],
        [-201.271],
        [-588.890]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.093, -0.069,  0.027,  ...,  0.080, -0.087,  0.083],
        [-0.009, -0.138, -0.042,  ...,  0.216, -0.096,  0.101],
        [ 0.094,  0.033,  0.051,  ..., -0.046, -0.168, -0.030],
        ...,
        [ 0.136, -0.026, -0.054,  ..., -0.204,  0.027,  0.176],
        [-0.008, -0.166, -0.098,  ..., -0.070,  0.097,  0.007],
        [-0.104, -0.281, -0.030,  ..., -0.007, -0.036, -0.079]],
       device='cuda:0')
s after update for 1 param tensor([[1.867, 1.650, 1.420,  ..., 1.882, 1.446, 1.890],
        [1.667, 1.419, 1.693,  ..., 1.855, 1.582, 2.194],
        [1.915, 1.600, 2.033,  ..., 1.503, 2.174, 1.812],
        ...,
        [1.850, 1.042, 1.830,  ..., 1.874, 1.847, 2.480],
        [1.794, 1.582, 1.763,  ..., 2.160, 1.754, 2.132],
        [2.094, 1.793, 2.014,  ..., 2.136, 1.640, 2.044]], device='cuda:0')
b after update for 1 param tensor([[173.535, 163.147, 151.379,  ..., 174.276, 152.746, 174.639],
        [164.020, 151.321, 165.279,  ..., 172.979, 159.761, 188.143],
        [175.797, 160.675, 181.089,  ..., 155.742, 187.305, 170.988],
        ...,
        [172.766, 129.667, 171.828,  ..., 173.859, 172.616, 200.046],
        [170.144, 159.741, 168.640,  ..., 186.686, 168.202, 185.462],
        [183.798, 170.065, 180.266,  ..., 185.630, 162.688, 181.596]],
       device='cuda:0')
clipping threshold 1.8138702218772897
a after update for 1 param tensor([ 0.042, -0.214, -0.072,  0.114, -0.135,  0.095,  0.105,  0.094,  0.030,
        -0.007, -0.086, -0.115,  0.028,  0.103, -0.052,  0.007, -0.020, -0.121,
         0.273,  0.173,  0.135,  0.004,  0.026, -0.030, -0.105, -0.005,  0.054,
        -0.166, -0.015, -0.086, -0.081, -0.141, -0.126, -0.161,  0.113, -0.014,
         0.194,  0.024,  0.047,  0.034, -0.081,  0.044, -0.068, -0.151,  0.014,
         0.096, -0.110,  0.178,  0.018, -0.007,  0.030, -0.050, -0.038, -0.111,
        -0.010, -0.040, -0.070,  0.029,  0.151,  0.081, -0.061, -0.163, -0.339,
         0.091,  0.085, -0.054,  0.302,  0.316,  0.016, -0.156,  0.044,  0.017,
         0.063, -0.320, -0.231, -0.083, -0.080, -0.030, -0.370, -0.065,  0.019,
        -0.003,  0.005,  0.037,  0.020,  0.106, -0.295, -0.047,  0.219,  0.089,
         0.016,  0.169, -0.021, -0.061,  0.046, -0.050,  0.152, -0.061, -0.307,
         0.071,  0.113, -0.378,  0.024,  0.036,  0.025,  0.191, -0.003, -0.042,
        -0.036, -0.037,  0.120,  0.042,  0.185,  0.009, -0.196, -0.038,  0.014,
        -0.017, -0.349,  0.074, -0.090, -0.289,  0.121, -0.032, -0.134,  0.196,
        -0.038,  0.273,  0.272,  0.112,  0.042,  0.034,  0.125, -0.257,  0.146,
         0.065, -0.109,  0.030,  0.014,  0.288,  0.063, -0.059, -0.078,  0.202,
        -0.223, -0.024,  0.001,  0.021,  0.016,  0.147, -0.030,  0.075, -0.099,
        -0.053,  0.067,  0.041,  0.149, -0.027, -0.132, -0.017,  0.045, -0.040,
         0.222,  0.413,  0.019, -0.049,  0.065, -0.064, -0.046,  0.118,  0.003,
         0.126, -0.100,  0.307, -0.218,  0.048, -0.117,  0.219, -0.189,  0.035,
        -0.207,  0.064,  0.060,  0.021,  0.073, -0.161, -0.263, -0.195,  0.016,
         0.018, -0.034, -0.127, -0.053,  0.025,  0.082, -0.047, -0.185, -0.005,
        -0.010,  0.012,  0.018,  0.104, -0.028, -0.145, -0.109, -0.060, -0.160,
         0.008,  0.201,  0.082, -0.025, -0.124,  0.099, -0.083,  0.018, -0.033,
         0.112, -0.103, -0.008,  0.065,  0.108,  0.017, -0.029, -0.076, -0.062,
         0.155, -0.093, -0.442,  0.091,  0.005, -0.078, -0.014,  0.053, -0.035,
         0.066, -0.045, -0.053,  0.281, -0.128, -0.133, -0.006, -0.044,  0.107,
         0.004,  0.068, -0.066, -0.162, -0.349, -0.113,  0.005,  0.014, -0.285,
        -0.038,  0.139, -0.089,  0.036, -0.110, -0.116, -0.015, -0.108, -0.080,
        -0.334,  0.085, -0.012, -0.226, -0.031,  0.005,  0.013,  0.116, -0.170,
         0.079, -0.099,  0.005,  0.253,  0.071, -0.002,  0.138, -0.103,  0.024,
         0.212,  0.087, -0.363,  0.024, -0.161, -0.122, -0.128, -0.080,  0.067,
        -0.022, -0.074,  0.319,  0.149, -0.203, -0.441, -0.119, -0.111, -0.110,
        -0.161, -0.306, -0.044], device='cuda:0')
s after update for 1 param tensor([1.716, 2.002, 2.100, 1.587, 1.582, 1.592, 1.659, 1.705, 2.059, 1.640,
        1.908, 1.832, 1.879, 1.287, 1.991, 1.635, 1.421, 1.694, 1.762, 2.055,
        1.638, 0.921, 1.365, 1.587, 1.687, 0.949, 2.151, 1.858, 1.731, 1.438,
        2.004, 1.800, 1.693, 1.884, 1.673, 2.002, 1.901, 1.184, 1.222, 2.184,
        1.499, 1.998, 1.882, 1.945, 1.057, 1.255, 1.986, 1.639, 2.091, 0.919,
        2.246, 1.615, 1.672, 1.345, 2.217, 1.758, 1.970, 1.731, 1.997, 1.731,
        1.488, 1.805, 2.136, 1.824, 1.652, 1.715, 2.090, 1.773, 2.069, 1.389,
        1.224, 1.390, 1.631, 1.738, 1.309, 2.261, 1.991, 1.354, 1.631, 0.602,
        1.792, 1.335, 0.630, 1.044, 1.625, 1.379, 1.487, 1.481, 1.844, 1.794,
        1.832, 1.743, 2.371, 1.460, 1.647, 1.971, 2.035, 1.692, 2.114, 1.775,
        1.811, 1.983, 1.894, 0.904, 1.722, 1.744, 1.448, 1.181, 1.350, 1.238,
        1.186, 1.396, 1.790, 1.513, 2.212, 1.639, 1.844, 1.460, 2.251, 1.470,
        1.940, 1.969, 1.759, 1.497, 1.309, 1.964, 1.708, 1.996, 1.819, 2.083,
        0.991, 1.276, 1.662, 1.716, 1.552, 1.406, 1.832, 1.974, 1.561, 1.756,
        0.876, 1.521, 2.236, 1.876, 1.800, 1.783, 1.881, 0.505, 1.702, 1.422,
        1.615, 1.980, 1.484, 2.139, 1.472, 1.961, 1.284, 1.271, 1.563, 1.635,
        1.359, 1.631, 1.885, 1.508, 1.806, 1.525, 1.416, 1.401, 1.701, 2.072,
        1.217, 1.940, 1.832, 1.278, 1.667, 1.818, 1.445, 1.397, 2.009, 1.168,
        1.222, 1.874, 1.775, 1.751, 1.410, 1.579, 1.874, 1.713, 0.900, 2.066,
        1.804, 1.800, 1.598, 1.668, 2.053, 1.631, 1.571, 1.572, 1.388, 1.895,
        1.391, 1.390, 1.527, 1.793, 1.165, 1.870, 1.410, 1.764, 1.886, 1.824,
        1.239, 1.631, 1.790, 1.630, 0.238, 1.897, 1.162, 2.123, 1.026, 1.641,
        1.816, 2.046, 1.207, 2.068, 1.631, 1.274, 2.005, 2.164, 1.762, 1.466,
        1.794, 1.523, 1.397, 1.333, 1.708, 1.839, 1.292, 1.759, 1.379, 1.722,
        1.278, 1.413, 2.111, 1.276, 1.438, 1.088, 1.765, 1.765, 1.732, 1.386,
        1.368, 2.053, 2.187, 1.691, 1.729, 1.732, 1.627, 1.933, 1.305, 1.710,
        1.353, 2.056, 2.097, 1.821, 1.463, 0.623, 1.891, 1.453, 1.902, 1.717,
        1.672, 1.711, 1.949, 2.105, 1.038, 0.920, 1.607, 1.151, 1.618, 1.963,
        1.567, 2.306, 1.243, 1.681, 1.511, 1.937, 1.743, 1.402, 2.006, 1.125,
        1.959, 1.610, 1.200, 1.768, 1.239, 2.061, 1.862, 1.705, 1.749, 1.651],
       device='cuda:0')
b after update for 1 param tensor([166.370, 179.742, 184.083, 160.013, 159.737, 160.248, 163.619, 165.845,
        182.244, 162.688, 175.458, 171.929, 174.099, 144.087, 179.229, 162.402,
        151.393, 165.312, 168.621, 182.104, 162.570, 121.918, 148.386, 160.030,
        164.970, 123.724, 186.277, 173.142, 167.108, 152.312, 179.833, 170.396,
        165.278, 174.342, 164.277, 179.713, 175.146, 138.188, 140.396, 187.702,
        155.499, 179.537, 174.259, 177.151, 130.567, 142.296, 179.020, 162.604,
        183.677, 121.799, 190.355, 161.409, 164.222, 147.308, 189.139, 168.420,
        178.291, 167.125, 179.511, 167.114, 154.933, 170.671, 185.624, 171.553,
        163.246, 166.362, 183.642, 169.129, 182.707, 149.675, 140.553, 149.752,
        162.229, 167.462, 145.309, 190.987, 179.216, 147.801, 162.220,  98.536,
        170.035, 146.755, 100.808, 129.786, 161.915, 149.143, 154.900, 154.595,
        172.473, 170.154, 171.906, 167.683, 195.584, 153.464, 163.022, 178.346,
        181.204, 165.210, 184.673, 169.244, 170.914, 178.852, 174.818, 120.796,
        166.705, 167.763, 152.869, 138.020, 147.580, 141.330, 138.356, 150.094,
        169.957, 156.219, 188.917, 162.592, 172.473, 153.485, 190.558, 154.006,
        176.929, 178.218, 168.444, 155.405, 145.334, 177.992, 166.014, 179.449,
        171.290, 183.337, 126.430, 143.478, 163.747, 166.412, 158.219, 150.630,
        171.916, 178.465, 158.706, 168.321, 118.893, 156.627, 189.935, 173.995,
        170.421, 169.630, 174.215,  90.267, 165.710, 151.443, 161.402, 178.746,
        154.731, 185.750, 154.114, 177.856, 143.914, 143.224, 158.823, 162.407,
        148.097, 162.239, 174.373, 155.993, 170.710, 156.859, 151.129, 150.347,
        165.638, 182.844, 140.120, 176.933, 171.924, 143.584, 163.986, 171.253,
        152.662, 150.156, 180.047, 137.253, 140.388, 173.865, 169.251, 168.071,
        150.831, 159.619, 173.898, 166.262, 120.505, 182.570, 170.595, 170.399,
        160.586, 164.052, 181.976, 162.211, 159.195, 159.253, 149.662, 174.839,
        149.803, 149.776, 156.938, 170.099, 137.114, 173.677, 150.821, 168.696,
        174.457, 171.524, 141.405, 162.227, 169.948, 162.161,  62.010, 174.934,
        136.924, 185.080, 128.644, 162.713, 171.150, 181.693, 139.541, 182.681,
        162.210, 143.345, 179.873, 186.848, 168.585, 153.800, 170.133, 156.746,
        150.126, 146.676, 166.012, 172.248, 144.373, 168.467, 149.179, 166.670,
        143.601, 150.973, 184.543, 143.454, 152.343, 132.479, 168.753, 168.764,
        167.185, 149.538, 148.579, 181.998, 187.858, 165.170, 167.000, 167.159,
        162.005, 176.577, 145.120, 166.102, 147.769, 182.116, 183.939, 171.393,
        153.653, 100.271, 174.653, 153.130, 175.177, 166.455, 164.226, 166.159,
        177.324, 184.284, 129.409, 121.823, 161.006, 136.251, 161.556, 177.957,
        159.016, 192.867, 141.639, 164.679, 156.146, 176.798, 167.679, 150.378,
        179.888, 134.720, 177.781, 161.173, 139.152, 168.886, 141.371, 182.369,
        173.332, 165.871, 167.993, 163.224], device='cuda:0')
clipping threshold 1.8138702218772897
a after update for 1 param tensor([[[ 0.001],
         [ 0.136],
         [-0.277],
         [ 0.156],
         [ 0.016],
         [ 0.058],
         [-0.092],
         [ 0.141],
         [-0.100],
         [-0.250]],

        [[ 0.019],
         [ 0.358],
         [ 0.142],
         [-0.016],
         [ 0.430],
         [-0.100],
         [ 0.027],
         [-0.017],
         [ 0.084],
         [ 0.051]],

        [[ 0.192],
         [-0.014],
         [ 0.050],
         [-0.065],
         [ 0.168],
         [-0.183],
         [-0.103],
         [-0.071],
         [ 0.035],
         [ 0.013]],

        [[ 0.015],
         [ 0.067],
         [ 0.109],
         [ 0.130],
         [-0.126],
         [-0.064],
         [-0.069],
         [-0.101],
         [ 0.008],
         [ 0.076]],

        [[ 0.165],
         [ 0.026],
         [ 0.049],
         [ 0.025],
         [-0.104],
         [-0.061],
         [-0.035],
         [-0.043],
         [-0.170],
         [-0.089]],

        [[-0.110],
         [-0.032],
         [-0.108],
         [-0.156],
         [ 0.076],
         [ 0.161],
         [ 0.095],
         [-0.103],
         [-0.279],
         [ 0.145]],

        [[-0.183],
         [ 0.159],
         [ 0.282],
         [-0.029],
         [-0.071],
         [-0.218],
         [ 0.078],
         [ 0.032],
         [-0.053],
         [ 0.061]],

        [[ 0.146],
         [-0.026],
         [ 0.049],
         [-0.127],
         [ 0.178],
         [-0.066],
         [ 0.002],
         [-0.292],
         [-0.132],
         [-0.068]],

        [[-0.005],
         [-0.038],
         [ 0.230],
         [ 0.022],
         [-0.159],
         [ 0.179],
         [ 0.004],
         [ 0.001],
         [ 0.019],
         [ 0.024]],

        [[ 0.228],
         [-0.044],
         [ 0.064],
         [-0.034],
         [-0.004],
         [ 0.182],
         [ 0.183],
         [-0.181],
         [-0.025],
         [ 0.014]],

        [[ 0.254],
         [-0.262],
         [ 0.097],
         [ 0.021],
         [-0.074],
         [ 0.140],
         [-0.312],
         [-0.113],
         [-0.157],
         [-0.044]],

        [[-0.028],
         [-0.160],
         [-0.049],
         [-0.054],
         [-0.005],
         [ 0.027],
         [ 0.109],
         [-0.094],
         [ 0.018],
         [-0.181]],

        [[-0.157],
         [ 0.155],
         [ 0.272],
         [ 0.039],
         [ 0.180],
         [ 0.202],
         [ 0.003],
         [ 0.037],
         [ 0.043],
         [-0.321]],

        [[-0.073],
         [-0.116],
         [-0.034],
         [ 0.168],
         [-0.018],
         [-0.067],
         [ 0.117],
         [ 0.061],
         [-0.116],
         [-0.001]],

        [[ 0.034],
         [-0.015],
         [-0.045],
         [-0.009],
         [-0.035],
         [-0.032],
         [ 0.052],
         [ 0.096],
         [-0.064],
         [-0.074]],

        [[-0.201],
         [-0.005],
         [ 0.057],
         [ 0.062],
         [ 0.076],
         [-0.148],
         [-0.106],
         [-0.025],
         [-0.187],
         [ 0.042]],

        [[-0.032],
         [-0.007],
         [-0.028],
         [-0.025],
         [ 0.010],
         [ 0.094],
         [-0.146],
         [ 0.010],
         [ 0.213],
         [ 0.097]],

        [[ 0.056],
         [ 0.052],
         [ 0.074],
         [ 0.025],
         [-0.011],
         [ 0.119],
         [-0.016],
         [ 0.015],
         [-0.227],
         [ 0.040]],

        [[ 0.145],
         [ 0.237],
         [ 0.043],
         [ 0.253],
         [-0.024],
         [-0.052],
         [-0.016],
         [-0.048],
         [ 0.045],
         [-0.031]],

        [[ 0.339],
         [ 0.039],
         [ 0.264],
         [-0.076],
         [ 0.009],
         [-0.311],
         [ 0.147],
         [-0.129],
         [-0.038],
         [-0.061]],

        [[ 0.022],
         [ 0.201],
         [ 0.066],
         [ 0.006],
         [-0.031],
         [-0.087],
         [ 0.068],
         [ 0.017],
         [ 0.080],
         [ 0.171]],

        [[ 0.021],
         [-0.037],
         [ 0.014],
         [ 0.298],
         [-0.037],
         [-0.023],
         [-0.080],
         [ 0.051],
         [ 0.130],
         [-0.357]],

        [[-0.190],
         [ 0.038],
         [-0.071],
         [ 0.208],
         [-0.015],
         [ 0.215],
         [-0.084],
         [-0.020],
         [ 0.055],
         [-0.124]],

        [[ 0.172],
         [-0.072],
         [-0.087],
         [-0.026],
         [ 0.118],
         [ 0.130],
         [ 0.168],
         [-0.115],
         [-0.090],
         [ 0.032]],

        [[-0.116],
         [-0.006],
         [-0.091],
         [ 0.046],
         [ 0.013],
         [-0.035],
         [-0.272],
         [-0.158],
         [ 0.064],
         [ 0.101]],

        [[-0.076],
         [-0.044],
         [-0.361],
         [-0.063],
         [-0.004],
         [-0.140],
         [ 0.007],
         [-0.078],
         [-0.096],
         [-0.058]],

        [[ 0.127],
         [ 0.139],
         [-0.072],
         [-0.001],
         [ 0.070],
         [-0.003],
         [ 0.057],
         [-0.019],
         [ 0.233],
         [ 0.117]],

        [[-0.006],
         [-0.120],
         [ 0.061],
         [ 0.095],
         [-0.095],
         [ 0.061],
         [ 0.128],
         [-0.045],
         [ 0.141],
         [ 0.002]],

        [[ 0.120],
         [ 0.187],
         [-0.032],
         [-0.116],
         [-0.012],
         [-0.101],
         [ 0.061],
         [ 0.150],
         [ 0.278],
         [-0.010]],

        [[ 0.083],
         [-0.250],
         [ 0.139],
         [ 0.107],
         [-0.164],
         [-0.027],
         [ 0.056],
         [ 0.051],
         [-0.174],
         [ 0.259]]], device='cuda:0')
s after update for 1 param tensor([[[2.058],
         [1.592],
         [2.132],
         [1.827],
         [1.537],
         [1.441],
         [1.754],
         [2.442],
         [1.795],
         [1.675]],

        [[1.378],
         [2.088],
         [1.618],
         [1.797],
         [1.824],
         [2.008],
         [1.850],
         [1.161],
         [1.699],
         [1.736]],

        [[1.795],
         [1.854],
         [1.410],
         [1.996],
         [1.526],
         [1.938],
         [1.258],
         [2.056],
         [1.673],
         [1.458]],

        [[1.952],
         [1.993],
         [1.731],
         [1.649],
         [1.300],
         [1.723],
         [0.962],
         [1.498],
         [1.613],
         [2.189]],

        [[1.797],
         [2.079],
         [1.501],
         [1.812],
         [1.754],
         [2.118],
         [1.424],
         [1.992],
         [2.246],
         [1.592]],

        [[2.078],
         [1.542],
         [1.044],
         [1.287],
         [1.575],
         [1.365],
         [2.128],
         [1.407],
         [1.970],
         [1.921]],

        [[2.067],
         [1.224],
         [2.037],
         [1.792],
         [0.922],
         [1.711],
         [1.444],
         [1.479],
         [1.579],
         [1.564]],

        [[1.258],
         [1.345],
         [1.942],
         [1.432],
         [1.840],
         [2.008],
         [0.919],
         [1.846],
         [1.402],
         [1.874]],

        [[0.683],
         [1.516],
         [1.547],
         [1.483],
         [1.606],
         [1.685],
         [1.655],
         [1.424],
         [2.199],
         [1.523]],

        [[1.949],
         [1.137],
         [1.794],
         [1.520],
         [0.953],
         [1.720],
         [1.290],
         [1.506],
         [1.462],
         [1.375]],

        [[1.627],
         [1.371],
         [1.470],
         [1.801],
         [1.948],
         [1.639],
         [2.144],
         [2.120],
         [1.751],
         [1.890]],

        [[1.478],
         [1.840],
         [1.233],
         [1.814],
         [1.002],
         [1.095],
         [2.027],
         [1.733],
         [1.077],
         [1.632]],

        [[2.164],
         [1.680],
         [1.775],
         [0.774],
         [1.239],
         [1.753],
         [1.114],
         [1.804],
         [1.434],
         [2.345]],

        [[1.670],
         [1.913],
         [1.085],
         [1.845],
         [1.388],
         [1.416],
         [2.191],
         [1.081],
         [1.714],
         [1.572]],

        [[1.659],
         [1.481],
         [1.648],
         [1.804],
         [0.082],
         [1.353],
         [1.212],
         [1.298],
         [1.993],
         [1.844]],

        [[1.515],
         [1.771],
         [1.986],
         [1.757],
         [1.636],
         [1.942],
         [1.649],
         [1.987],
         [1.528],
         [1.491]],

        [[1.394],
         [1.813],
         [1.228],
         [1.161],
         [1.637],
         [1.921],
         [2.212],
         [1.057],
         [2.388],
         [1.560]],

        [[1.843],
         [1.169],
         [1.955],
         [1.552],
         [1.975],
         [1.312],
         [1.249],
         [1.695],
         [1.915],
         [1.958]],

        [[1.712],
         [1.865],
         [1.786],
         [1.919],
         [1.065],
         [1.864],
         [1.532],
         [1.540],
         [1.770],
         [1.011]],

        [[2.035],
         [1.490],
         [1.488],
         [1.591],
         [2.095],
         [1.995],
         [1.911],
         [0.973],
         [1.505],
         [0.803]],

        [[0.769],
         [1.754],
         [1.873],
         [1.259],
         [1.622],
         [1.311],
         [1.418],
         [2.105],
         [1.662],
         [1.823]],

        [[1.061],
         [1.762],
         [1.190],
         [1.984],
         [1.680],
         [1.296],
         [2.061],
         [1.415],
         [1.916],
         [1.754]],

        [[1.265],
         [1.105],
         [1.923],
         [1.473],
         [1.454],
         [1.627],
         [2.179],
         [1.126],
         [1.461],
         [2.200]],

        [[1.845],
         [1.467],
         [1.331],
         [1.656],
         [1.265],
         [1.785],
         [1.904],
         [1.835],
         [1.117],
         [1.825]],

        [[1.206],
         [1.806],
         [1.151],
         [1.866],
         [1.971],
         [1.278],
         [2.207],
         [1.474],
         [1.602],
         [1.752]],

        [[1.576],
         [1.516],
         [1.525],
         [2.088],
         [1.255],
         [1.633],
         [1.968],
         [1.980],
         [2.122],
         [0.981]],

        [[2.223],
         [1.166],
         [1.634],
         [1.609],
         [1.655],
         [1.636],
         [1.574],
         [1.002],
         [1.475],
         [1.837]],

        [[1.589],
         [1.428],
         [1.936],
         [1.499],
         [1.680],
         [1.338],
         [1.672],
         [1.248],
         [1.824],
         [1.504]],

        [[1.381],
         [1.841],
         [1.308],
         [2.243],
         [1.531],
         [1.826],
         [1.598],
         [1.401],
         [1.306],
         [1.507]],

        [[1.419],
         [1.496],
         [1.902],
         [1.566],
         [1.790],
         [1.592],
         [1.320],
         [1.497],
         [1.409],
         [1.762]]], device='cuda:0')
b after update for 1 param tensor([[[182.241],
         [160.290],
         [185.477],
         [171.673],
         [157.457],
         [152.488],
         [168.215],
         [198.477],
         [170.173],
         [164.399]],

        [[149.111],
         [183.560],
         [161.576],
         [170.270],
         [171.566],
         [179.999],
         [172.771],
         [136.836],
         [165.573],
         [167.358]],

        [[170.157],
         [172.933],
         [150.816],
         [179.465],
         [156.923],
         [176.842],
         [142.492],
         [182.150],
         [164.286],
         [153.367]],

        [[177.442],
         [179.315],
         [167.137],
         [163.112],
         [144.845],
         [166.744],
         [124.602],
         [155.464],
         [161.328],
         [187.947]],

        [[170.260],
         [183.127],
         [155.618],
         [170.973],
         [168.212],
         [184.839],
         [151.556],
         [179.252],
         [190.359],
         [160.271]],

        [[183.097],
         [157.720],
         [129.813],
         [144.087],
         [159.387],
         [148.376],
         [185.291],
         [150.680],
         [178.283],
         [176.065]],

        [[182.623],
         [140.526],
         [181.300],
         [170.053],
         [121.932],
         [166.125],
         [152.623],
         [154.452],
         [159.636],
         [158.842]],

        [[142.484],
         [147.334],
         [177.013],
         [152.013],
         [172.278],
         [179.972],
         [121.777],
         [172.577],
         [150.420],
         [173.865]],

        [[104.950],
         [156.378],
         [157.998],
         [154.676],
         [160.948],
         [164.872],
         [163.386],
         [151.574],
         [188.349],
         [156.760]],

        [[177.330],
         [135.470],
         [170.148],
         [156.581],
         [123.982],
         [166.566],
         [144.266],
         [155.881],
         [153.587],
         [148.963]],

        [[162.014],
         [148.752],
         [154.011],
         [170.447],
         [177.293],
         [162.616],
         [185.967],
         [184.933],
         [168.101],
         [174.633]],

        [[154.426],
         [172.319],
         [141.061],
         [171.060],
         [127.134],
         [132.930],
         [180.851],
         [167.205],
         [131.841],
         [162.278]],

        [[186.869],
         [164.624],
         [169.210],
         [111.730],
         [141.411],
         [168.180],
         [134.042],
         [170.582],
         [152.123],
         [194.528]],

        [[164.122],
         [175.661],
         [132.333],
         [172.515],
         [149.623],
         [151.174],
         [187.994],
         [132.059],
         [166.290],
         [159.274]],

        [[163.615],
         [154.571],
         [163.040],
         [170.597],
         [ 36.440],
         [147.728],
         [139.837],
         [144.740],
         [179.305],
         [172.477]],

        [[156.363],
         [169.022],
         [179.002],
         [168.376],
         [162.489],
         [176.990],
         [163.118],
         [179.044],
         [157.033],
         [155.089]],

        [[149.976],
         [171.023],
         [140.773],
         [136.892],
         [162.498],
         [176.059],
         [188.919],
         [130.566],
         [196.292],
         [158.642]],

        [[172.439],
         [137.362],
         [177.593],
         [158.266],
         [178.498],
         [145.519],
         [141.973],
         [165.361],
         [175.773],
         [177.740]],

        [[166.219],
         [173.477],
         [169.731],
         [175.971],
         [131.089],
         [173.438],
         [157.217],
         [157.642],
         [169.000],
         [127.689]],

        [[181.203],
         [155.037],
         [154.967],
         [160.215],
         [183.867],
         [179.415],
         [175.591],
         [125.266],
         [155.825],
         [113.820]],

        [[111.381],
         [168.214],
         [173.856],
         [142.548],
         [161.788],
         [145.428],
         [151.270],
         [184.292],
         [163.757],
         [171.521]],

        [[130.819],
         [168.584],
         [138.554],
         [178.918],
         [164.626],
         [144.584],
         [182.343],
         [151.095],
         [175.799],
         [168.225]],

        [[142.866],
         [133.493],
         [176.121],
         [154.180],
         [153.159],
         [162.025],
         [187.480],
         [134.760],
         [153.530],
         [188.410]],

        [[172.526],
         [153.837],
         [146.549],
         [163.456],
         [142.862],
         [169.707],
         [175.285],
         [172.050],
         [134.218],
         [171.596]],

        [[139.516],
         [170.685],
         [136.277],
         [173.496],
         [178.331],
         [143.605],
         [188.694],
         [154.238],
         [160.776],
         [168.132]],

        [[159.462],
         [156.406],
         [156.878],
         [183.522],
         [142.305],
         [162.335],
         [178.211],
         [178.720],
         [185.046],
         [125.792]],

        [[189.392],
         [137.159],
         [162.376],
         [161.128],
         [163.431],
         [162.485],
         [159.353],
         [127.173],
         [154.269],
         [172.144]],

        [[160.102],
         [151.798],
         [176.726],
         [155.497],
         [164.649],
         [146.916],
         [164.253],
         [141.917],
         [171.562],
         [155.753]],

        [[149.288],
         [172.339],
         [145.269],
         [190.251],
         [157.183],
         [171.655],
         [160.562],
         [150.356],
         [145.151],
         [155.920]],

        [[151.297],
         [155.383],
         [175.186],
         [158.959],
         [169.963],
         [160.248],
         [145.916],
         [155.419],
         [150.791],
         [168.629]]], device='cuda:0')
clipping threshold 1.8138702218772897
a after update for 1 param tensor([[-0.037],
        [ 0.074],
        [ 0.191],
        [ 0.123],
        [-0.024],
        [ 0.223],
        [ 0.128],
        [ 0.081],
        [-0.170],
        [ 0.070],
        [-0.143],
        [ 0.086],
        [-0.023],
        [ 0.079],
        [ 0.038],
        [ 0.165],
        [-0.050],
        [ 0.126],
        [ 0.161],
        [ 0.064],
        [-0.285],
        [ 0.185],
        [ 0.068],
        [ 0.022],
        [ 0.207],
        [ 0.108],
        [-0.154],
        [ 0.036],
        [ 0.146],
        [ 0.100]], device='cuda:0')
s after update for 1 param tensor([[1.434],
        [1.558],
        [1.622],
        [1.815],
        [1.819],
        [1.715],
        [1.901],
        [1.644],
        [1.574],
        [1.774],
        [1.438],
        [2.114],
        [1.272],
        [1.610],
        [1.636],
        [1.818],
        [1.487],
        [1.660],
        [1.322],
        [1.019],
        [1.839],
        [1.455],
        [1.399],
        [1.650],
        [1.976],
        [1.459],
        [2.303],
        [1.228],
        [1.372],
        [1.817]], device='cuda:0')
b after update for 1 param tensor([[152.130],
        [158.568],
        [161.790],
        [171.132],
        [171.330],
        [166.321],
        [175.145],
        [162.868],
        [159.383],
        [169.181],
        [152.297],
        [184.663],
        [143.273],
        [161.185],
        [162.486],
        [171.268],
        [154.868],
        [163.640],
        [146.036],
        [128.209],
        [172.262],
        [153.240],
        [150.214],
        [163.144],
        [178.562],
        [153.407],
        [192.775],
        [140.772],
        [148.806],
        [171.232]], device='cuda:0')
clipping threshold 1.8138702218772897
||w||^2 4.633603008247858
exp ma of ||w||^2 5.555709942199769
||w|| 2.15258054628575
exp ma of ||w|| 2.117724736151952
||w||^2 3.989644189164525
exp ma of ||w||^2 3.4051158988553927
||w|| 1.997409369449469
exp ma of ||w|| 1.8038059025946214
cuda
Objective function 108.97 = squared loss an data 87.27 + 0.5*rho*h**2 11.902334 + alpha*h 7.635961 + L2reg 1.84 + L1reg 0.32 ; SHD = 119 ; DAG True
Proportion of microbatches that were clipped  0.7872545864177664
iteration 2 in inner loop, alpha 49.49173263005036 rho 1000.0 h 0.15428761519276435
9630
cuda
Objective function 216.09 = squared loss an data 87.27 + 0.5*rho*h**2 119.023341 + alpha*h 7.635961 + L2reg 1.84 + L1reg 0.32 ; SHD = 119 ; DAG True
||w||^2 139.73668164109094
exp ma of ||w||^2 27885.861052500473
||w|| 11.821027097553365
exp ma of ||w|| 24.861572504473514
||w||^2 3.483962964336862
exp ma of ||w||^2 5.328965103197261
||w|| 1.8665376943252074
exp ma of ||w|| 2.243975633477926
||w||^2 4.243072180567679
exp ma of ||w||^2 4.8241995694318405
||w|| 2.059871884503422
exp ma of ||w|| 2.126887190134162
||w||^2 7.07133431047509
exp ma of ||w||^2 4.78008566415755
||w|| 2.6591980577751424
exp ma of ||w|| 2.102461879521161
||w||^2 5.921780285744536
exp ma of ||w||^2 5.072906283195042
||w|| 2.4334708310856197
exp ma of ||w|| 2.1684888366493777
cuda
Objective function 111.90 = squared loss an data 90.85 + 0.5*rho*h**2 16.058799 + alpha*h 2.804815 + L2reg 1.91 + L1reg 0.28 ; SHD = 118 ; DAG True
Proportion of microbatches that were clipped  0.7936661819356924
iteration 3 in inner loop, alpha 49.49173263005036 rho 10000.0 h 0.056672389894696806
iteration 3 in outer loop, alpha = 616.2156315770185, rho = 10000.0, h = 0.056672389894696806
cuda
9630
cuda
Objective function 144.02 = squared loss an data 90.85 + 0.5*rho*h**2 16.058799 + alpha*h 34.922413 + L2reg 1.91 + L1reg 0.28 ; SHD = 118 ; DAG True
||w||^2 46.3507608058012
exp ma of ||w||^2 7170.765690911153
||w|| 6.808139305698819
exp ma of ||w|| 11.890772119659609
||w||^2 17.39910155797409
exp ma of ||w||^2 1740.6114909584303
||w|| 4.171223029037658
exp ma of ||w|| 6.567352308063214
||w||^2 5.303925604777286
exp ma of ||w||^2 171.16988700845903
||w|| 2.30302531570482
exp ma of ||w|| 3.79985435490195
cuda
Objective function 123.03 = squared loss an data 89.56 + 0.5*rho*h**2 7.442939 + alpha*h 23.774968 + L2reg 1.99 + L1reg 0.26 ; SHD = 117 ; DAG True
Proportion of microbatches that were clipped  0.7989182309895004
iteration 1 in inner loop, alpha 616.2156315770185 rho 10000.0 h 0.03858222163056624
9630
cuda
Objective function 190.02 = squared loss an data 89.56 + 0.5*rho*h**2 74.429391 + alpha*h 23.774968 + L2reg 1.99 + L1reg 0.26 ; SHD = 117 ; DAG True
||w||^2 9124238594508.578
exp ma of ||w||^2 5663136037185.453
||w|| 3020635.4620358576
exp ma of ||w|| 2096861.3358247364
||w||^2 2055.2303916928972
exp ma of ||w||^2 1451101.7601273537
||w|| 45.334648908896355
exp ma of ||w|| 112.24883616588438
||w||^2 7.79784335160204
exp ma of ||w||^2 8.795836013583004
||w|| 2.792461880062473
exp ma of ||w|| 2.8918720294994835
cuda
Objective function 118.58 = squared loss an data 90.99 + 0.5*rho*h**2 14.723093 + alpha*h 10.574193 + L2reg 2.07 + L1reg 0.23 ; SHD = 119 ; DAG True
Proportion of microbatches that were clipped  0.7970873786407767
iteration 2 in inner loop, alpha 616.2156315770185 rho 100000.0 h 0.017159891236460112
iteration 4 in outer loop, alpha = 17776.10686803713, rho = 1000000.0, h = 0.017159891236460112
Threshold 0.3
[[0.003 0.084 0.034 0.029 0.141 0.032 0.021 0.049 0.051 0.028 0.075 0.059
  0.032 0.132 0.006 0.031 0.099 0.065 0.396 0.028 0.086 0.044 0.029 0.019
  0.044 0.033 0.048 0.029 0.042 0.054]
 [0.067 0.006 0.056 0.063 0.142 0.04  0.108 0.025 0.107 0.05  0.08  0.071
  0.015 0.125 0.022 0.143 0.069 0.044 0.101 0.056 0.137 0.095 0.083 0.151
  0.055 0.071 0.144 0.128 0.075 0.075]
 [0.162 0.066 0.004 0.082 0.113 0.041 0.095 0.032 0.09  0.064 0.129 0.136
  0.044 0.408 0.051 0.153 0.199 0.077 0.138 0.053 0.095 0.054 0.137 0.056
  0.14  0.056 0.1   0.074 0.106 0.075]
 [0.166 0.054 0.085 0.004 0.158 0.035 0.046 0.062 0.157 0.081 0.066 0.165
  0.044 0.266 0.021 0.189 0.109 0.069 0.185 0.057 0.077 0.072 0.055 0.235
  0.146 0.047 0.068 0.12  0.078 0.136]
 [0.037 0.029 0.053 0.033 0.004 0.029 0.025 0.009 0.016 0.017 0.049 0.054
  0.007 0.04  0.024 0.044 0.025 0.033 0.071 0.014 0.037 0.008 0.047 0.03
  0.053 0.033 0.05  0.054 0.089 0.02 ]
 [0.196 0.133 0.184 0.203 0.24  0.007 0.228 0.062 0.304 0.108 0.198 0.327
  0.093 0.19  0.096 0.18  0.251 0.171 0.151 0.092 0.143 0.107 0.062 0.185
  0.099 0.088 0.057 0.1   0.408 0.084]
 [0.195 0.068 0.074 0.11  0.201 0.025 0.007 0.129 0.162 0.233 0.162 0.2
  0.047 0.326 0.014 0.24  0.058 0.086 0.372 0.045 0.188 0.059 0.132 0.189
  0.22  0.097 0.058 0.145 0.082 0.214]
 [0.12  0.238 0.125 0.076 0.794 0.083 0.053 0.003 0.158 0.058 0.114 0.085
  0.079 0.228 0.036 0.094 0.213 0.094 0.258 0.088 0.104 0.144 0.121 0.097
  0.092 0.05  0.091 0.061 0.162 0.087]
 [0.154 0.075 0.09  0.035 0.357 0.018 0.032 0.029 0.004 0.062 0.075 0.092
  0.026 0.095 0.011 0.103 0.244 0.066 0.213 0.026 0.041 0.118 0.068 0.05
  0.069 0.053 0.064 0.091 0.026 0.098]
 [0.216 0.152 0.081 0.067 0.241 0.042 0.027 0.098 0.069 0.004 0.164 0.132
  0.039 0.117 0.025 0.136 0.111 0.09  0.209 0.057 0.252 0.173 0.075 0.162
  0.065 0.033 0.044 0.058 0.099 0.068]
 [0.077 0.056 0.055 0.063 0.125 0.024 0.04  0.058 0.075 0.038 0.004 0.087
  0.029 0.153 0.021 0.049 0.065 0.078 0.13  0.062 0.056 0.055 0.102 0.061
  0.026 0.012 0.031 0.058 0.247 0.042]
 [0.059 0.073 0.046 0.041 0.101 0.013 0.029 0.062 0.042 0.043 0.046 0.004
  0.011 0.136 0.026 0.126 0.074 0.037 0.084 0.032 0.044 0.022 0.062 0.025
  0.032 0.031 0.058 0.031 0.035 0.02 ]
 [0.2   0.363 0.14  0.17  0.793 0.071 0.163 0.078 0.121 0.128 0.121 0.538
  0.005 0.211 0.127 0.153 0.098 0.107 0.319 0.136 0.241 0.203 0.151 0.19
  0.103 0.075 0.233 0.11  0.167 0.179]
 [0.044 0.037 0.021 0.03  0.182 0.029 0.011 0.023 0.082 0.061 0.019 0.078
  0.041 0.004 0.011 0.069 0.053 0.03  0.146 0.067 0.084 0.054 0.068 0.049
  0.041 0.062 0.054 0.04  0.103 0.05 ]
 [0.516 0.284 0.077 0.383 0.166 0.049 0.417 0.138 0.268 0.203 0.158 0.188
  0.047 0.23  0.004 0.222 0.292 0.142 0.673 0.311 0.264 0.128 0.251 1.122
  0.138 0.074 0.115 0.351 0.439 0.179]
 [0.144 0.058 0.05  0.026 0.166 0.043 0.026 0.063 0.061 0.049 0.125 0.043
  0.032 0.056 0.03  0.004 0.072 0.04  0.14  0.019 0.058 0.044 0.038 0.071
  0.036 0.023 0.041 0.084 0.09  0.074]
 [0.06  0.06  0.03  0.086 0.252 0.026 0.054 0.025 0.02  0.055 0.082 0.084
  0.033 0.093 0.015 0.06  0.005 0.059 0.245 0.058 0.097 0.037 0.051 0.043
  0.064 0.042 0.048 0.056 0.071 0.033]
 [0.111 0.177 0.089 0.099 0.145 0.035 0.067 0.076 0.104 0.09  0.07  0.17
  0.061 0.162 0.054 0.112 0.101 0.004 0.24  0.076 0.069 0.05  0.067 0.16
  0.05  0.044 0.111 0.118 0.061 0.262]
 [0.018 0.055 0.041 0.018 0.073 0.026 0.015 0.023 0.026 0.024 0.039 0.067
  0.024 0.045 0.005 0.045 0.02  0.029 0.003 0.015 0.086 0.029 0.014 0.017
  0.036 0.043 0.03  0.037 0.053 0.04 ]
 [0.238 0.114 0.14  0.07  0.269 0.075 0.129 0.048 0.267 0.075 0.094 0.246
  0.041 0.07  0.018 0.229 0.058 0.108 0.258 0.006 0.372 0.14  0.162 0.513
  0.076 0.061 0.109 0.113 0.732 0.097]
 [0.059 0.048 0.061 0.064 0.139 0.022 0.02  0.054 0.133 0.017 0.113 0.149
  0.016 0.082 0.019 0.091 0.075 0.059 0.065 0.022 0.003 0.048 0.042 0.066
  0.035 0.029 0.049 0.042 0.073 0.065]
 [0.189 0.03  0.102 0.081 0.521 0.053 0.061 0.05  0.035 0.057 0.111 0.215
  0.038 0.108 0.034 0.114 0.117 0.121 0.18  0.036 0.089 0.004 0.099 0.106
  0.06  0.079 0.047 0.057 0.159 0.064]
 [0.136 0.039 0.047 0.066 0.142 0.062 0.053 0.048 0.057 0.086 0.07  0.052
  0.042 0.091 0.017 0.126 0.1   0.103 0.294 0.045 0.132 0.087 0.006 0.127
  0.073 0.044 0.038 0.055 0.093 0.043]
 [0.415 0.059 0.048 0.025 0.133 0.024 0.032 0.063 0.118 0.025 0.089 0.146
  0.031 0.116 0.005 0.064 0.103 0.028 0.432 0.014 0.131 0.056 0.034 0.004
  0.096 0.035 0.044 0.052 0.042 0.047]
 [0.146 0.11  0.041 0.049 0.119 0.061 0.026 0.06  0.101 0.102 0.198 0.102
  0.049 0.169 0.022 0.086 0.076 0.073 0.085 0.057 0.163 0.069 0.058 0.07
  0.004 0.229 0.097 0.137 0.171 0.071]
 [0.19  0.096 0.16  0.145 0.102 0.072 0.064 0.123 0.096 0.144 0.71  0.122
  0.073 0.097 0.046 0.221 0.134 0.107 0.104 0.054 0.187 0.102 0.095 0.119
  0.033 0.006 0.066 0.185 0.281 0.079]
 [0.085 0.049 0.06  0.097 0.123 0.08  0.085 0.054 0.115 0.11  0.266 0.101
  0.025 0.112 0.043 0.115 0.13  0.059 0.236 0.053 0.111 0.098 0.136 0.141
  0.047 0.038 0.005 0.135 0.328 0.083]
 [0.168 0.044 0.074 0.06  0.092 0.08  0.038 0.067 0.094 0.085 0.088 0.186
  0.032 0.158 0.012 0.063 0.108 0.036 0.15  0.063 0.163 0.131 0.118 0.098
  0.057 0.035 0.041 0.003 0.056 0.045]
 [0.101 0.08  0.055 0.051 0.08  0.022 0.045 0.044 0.051 0.046 0.027 0.153
  0.023 0.055 0.01  0.03  0.049 0.093 0.103 0.009 0.081 0.044 0.07  0.169
  0.029 0.018 0.016 0.082 0.006 0.062]
 [0.11  0.055 0.081 0.042 0.28  0.069 0.017 0.091 0.059 0.107 0.167 0.24
  0.032 0.187 0.03  0.08  0.122 0.028 0.113 0.069 0.137 0.106 0.107 0.089
  0.08  0.086 0.092 0.115 0.114 0.007]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.396 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.408 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.304 0.    0.    0.327
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.408 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.326 0.    0.    0.    0.    0.372 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.794 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.357 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.363 0.    0.    0.793 0.    0.    0.    0.    0.    0.    0.538
  0.    0.    0.    0.    0.    0.    0.319 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.516 0.    0.    0.383 0.    0.    0.417 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.673 0.311 0.    0.    0.    1.122
  0.    0.    0.    0.351 0.439 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.372 0.    0.    0.513
  0.    0.    0.    0.    0.732 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.521 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.415 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.432 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.71  0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.328 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5172413793103449, 'tpr': 0.11666666666666667, 'fpr': 0.047619047619047616, 'f1': 0.18791946308724833, 'shd': 119, 'npred': 29, 'ntrue': 120}
[0.084 0.034 0.029 0.141 0.032 0.021 0.049 0.051 0.028 0.075 0.059 0.032
 0.132 0.006 0.031 0.099 0.065 0.396 0.028 0.086 0.044 0.029 0.019 0.044
 0.033 0.048 0.029 0.042 0.054 0.067 0.056 0.063 0.142 0.04  0.108 0.025
 0.107 0.05  0.08  0.071 0.015 0.125 0.022 0.143 0.069 0.044 0.101 0.056
 0.137 0.095 0.083 0.151 0.055 0.071 0.144 0.128 0.075 0.075 0.162 0.066
 0.082 0.113 0.041 0.095 0.032 0.09  0.064 0.129 0.136 0.044 0.408 0.051
 0.153 0.199 0.077 0.138 0.053 0.095 0.054 0.137 0.056 0.14  0.056 0.1
 0.074 0.106 0.075 0.166 0.054 0.085 0.158 0.035 0.046 0.062 0.157 0.081
 0.066 0.165 0.044 0.266 0.021 0.189 0.109 0.069 0.185 0.057 0.077 0.072
 0.055 0.235 0.146 0.047 0.068 0.12  0.078 0.136 0.037 0.029 0.053 0.033
 0.029 0.025 0.009 0.016 0.017 0.049 0.054 0.007 0.04  0.024 0.044 0.025
 0.033 0.071 0.014 0.037 0.008 0.047 0.03  0.053 0.033 0.05  0.054 0.089
 0.02  0.196 0.133 0.184 0.203 0.24  0.228 0.062 0.304 0.108 0.198 0.327
 0.093 0.19  0.096 0.18  0.251 0.171 0.151 0.092 0.143 0.107 0.062 0.185
 0.099 0.088 0.057 0.1   0.408 0.084 0.195 0.068 0.074 0.11  0.201 0.025
 0.129 0.162 0.233 0.162 0.2   0.047 0.326 0.014 0.24  0.058 0.086 0.372
 0.045 0.188 0.059 0.132 0.189 0.22  0.097 0.058 0.145 0.082 0.214 0.12
 0.238 0.125 0.076 0.794 0.083 0.053 0.158 0.058 0.114 0.085 0.079 0.228
 0.036 0.094 0.213 0.094 0.258 0.088 0.104 0.144 0.121 0.097 0.092 0.05
 0.091 0.061 0.162 0.087 0.154 0.075 0.09  0.035 0.357 0.018 0.032 0.029
 0.062 0.075 0.092 0.026 0.095 0.011 0.103 0.244 0.066 0.213 0.026 0.041
 0.118 0.068 0.05  0.069 0.053 0.064 0.091 0.026 0.098 0.216 0.152 0.081
 0.067 0.241 0.042 0.027 0.098 0.069 0.164 0.132 0.039 0.117 0.025 0.136
 0.111 0.09  0.209 0.057 0.252 0.173 0.075 0.162 0.065 0.033 0.044 0.058
 0.099 0.068 0.077 0.056 0.055 0.063 0.125 0.024 0.04  0.058 0.075 0.038
 0.087 0.029 0.153 0.021 0.049 0.065 0.078 0.13  0.062 0.056 0.055 0.102
 0.061 0.026 0.012 0.031 0.058 0.247 0.042 0.059 0.073 0.046 0.041 0.101
 0.013 0.029 0.062 0.042 0.043 0.046 0.011 0.136 0.026 0.126 0.074 0.037
 0.084 0.032 0.044 0.022 0.062 0.025 0.032 0.031 0.058 0.031 0.035 0.02
 0.2   0.363 0.14  0.17  0.793 0.071 0.163 0.078 0.121 0.128 0.121 0.538
 0.211 0.127 0.153 0.098 0.107 0.319 0.136 0.241 0.203 0.151 0.19  0.103
 0.075 0.233 0.11  0.167 0.179 0.044 0.037 0.021 0.03  0.182 0.029 0.011
 0.023 0.082 0.061 0.019 0.078 0.041 0.011 0.069 0.053 0.03  0.146 0.067
 0.084 0.054 0.068 0.049 0.041 0.062 0.054 0.04  0.103 0.05  0.516 0.284
 0.077 0.383 0.166 0.049 0.417 0.138 0.268 0.203 0.158 0.188 0.047 0.23
 0.222 0.292 0.142 0.673 0.311 0.264 0.128 0.251 1.122 0.138 0.074 0.115
 0.351 0.439 0.179 0.144 0.058 0.05  0.026 0.166 0.043 0.026 0.063 0.061
 0.049 0.125 0.043 0.032 0.056 0.03  0.072 0.04  0.14  0.019 0.058 0.044
 0.038 0.071 0.036 0.023 0.041 0.084 0.09  0.074 0.06  0.06  0.03  0.086
 0.252 0.026 0.054 0.025 0.02  0.055 0.082 0.084 0.033 0.093 0.015 0.06
 0.059 0.245 0.058 0.097 0.037 0.051 0.043 0.064 0.042 0.048 0.056 0.071
 0.033 0.111 0.177 0.089 0.099 0.145 0.035 0.067 0.076 0.104 0.09  0.07
 0.17  0.061 0.162 0.054 0.112 0.101 0.24  0.076 0.069 0.05  0.067 0.16
 0.05  0.044 0.111 0.118 0.061 0.262 0.018 0.055 0.041 0.018 0.073 0.026
 0.015 0.023 0.026 0.024 0.039 0.067 0.024 0.045 0.005 0.045 0.02  0.029
 0.015 0.086 0.029 0.014 0.017 0.036 0.043 0.03  0.037 0.053 0.04  0.238
 0.114 0.14  0.07  0.269 0.075 0.129 0.048 0.267 0.075 0.094 0.246 0.041
 0.07  0.018 0.229 0.058 0.108 0.258 0.372 0.14  0.162 0.513 0.076 0.061
 0.109 0.113 0.732 0.097 0.059 0.048 0.061 0.064 0.139 0.022 0.02  0.054
 0.133 0.017 0.113 0.149 0.016 0.082 0.019 0.091 0.075 0.059 0.065 0.022
 0.048 0.042 0.066 0.035 0.029 0.049 0.042 0.073 0.065 0.189 0.03  0.102
 0.081 0.521 0.053 0.061 0.05  0.035 0.057 0.111 0.215 0.038 0.108 0.034
 0.114 0.117 0.121 0.18  0.036 0.089 0.099 0.106 0.06  0.079 0.047 0.057
 0.159 0.064 0.136 0.039 0.047 0.066 0.142 0.062 0.053 0.048 0.057 0.086
 0.07  0.052 0.042 0.091 0.017 0.126 0.1   0.103 0.294 0.045 0.132 0.087
 0.127 0.073 0.044 0.038 0.055 0.093 0.043 0.415 0.059 0.048 0.025 0.133
 0.024 0.032 0.063 0.118 0.025 0.089 0.146 0.031 0.116 0.005 0.064 0.103
 0.028 0.432 0.014 0.131 0.056 0.034 0.096 0.035 0.044 0.052 0.042 0.047
 0.146 0.11  0.041 0.049 0.119 0.061 0.026 0.06  0.101 0.102 0.198 0.102
 0.049 0.169 0.022 0.086 0.076 0.073 0.085 0.057 0.163 0.069 0.058 0.07
 0.229 0.097 0.137 0.171 0.071 0.19  0.096 0.16  0.145 0.102 0.072 0.064
 0.123 0.096 0.144 0.71  0.122 0.073 0.097 0.046 0.221 0.134 0.107 0.104
 0.054 0.187 0.102 0.095 0.119 0.033 0.066 0.185 0.281 0.079 0.085 0.049
 0.06  0.097 0.123 0.08  0.085 0.054 0.115 0.11  0.266 0.101 0.025 0.112
 0.043 0.115 0.13  0.059 0.236 0.053 0.111 0.098 0.136 0.141 0.047 0.038
 0.135 0.328 0.083 0.168 0.044 0.074 0.06  0.092 0.08  0.038 0.067 0.094
 0.085 0.088 0.186 0.032 0.158 0.012 0.063 0.108 0.036 0.15  0.063 0.163
 0.131 0.118 0.098 0.057 0.035 0.041 0.056 0.045 0.101 0.08  0.055 0.051
 0.08  0.022 0.045 0.044 0.051 0.046 0.027 0.153 0.023 0.055 0.01  0.03
 0.049 0.093 0.103 0.009 0.081 0.044 0.07  0.169 0.029 0.018 0.016 0.082
 0.062 0.11  0.055 0.081 0.042 0.28  0.069 0.017 0.091 0.059 0.107 0.167
 0.24  0.032 0.187 0.03  0.08  0.122 0.028 0.113 0.069 0.137 0.106 0.107
 0.089 0.08  0.086 0.092 0.115 0.114]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.5767222222222221, 0.265670793823929)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
