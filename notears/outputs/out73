samples  5000  graph  15 30 ER mlp  minibatch size  50  noise  0.5  minibatches per NN training  125 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006583550827196305
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0011537970975332712
iteration 4 in outer loop, alpha = 156.00431709949223, rho = 100000.0, h = 0.0011537970975332712
cuda
iteration 1 in inner loop,alpha 156.00431709949223 rho 100000.0 h 0.00033575511004180214
iteration 5 in outer loop, alpha = 491.7594271412944, rho = 1000000.0, h = 0.00033575511004180214
Threshold 0.3
[[0.002 0.    0.    0.005 0.194 0.    0.    0.    0.    2.734 0.179 0.001
  0.001 0.    0.006]
 [2.52  0.001 0.002 0.262 0.215 0.01  0.01  0.001 0.002 2.189 0.199 1.342
  0.001 0.001 0.098]
 [0.027 0.068 0.001 0.147 0.166 0.035 0.137 0.001 0.067 0.125 0.135 0.162
  0.001 0.001 0.006]
 [0.153 0.    0.001 0.001 0.204 0.    0.    0.    0.    0.214 0.335 0.001
  0.    0.    1.022]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    0.001]
 [2.646 0.03  0.001 0.968 0.376 0.001 1.377 0.    0.055 1.746 0.956 1.472
  0.012 0.002 1.923]
 [0.414 0.044 0.    0.336 0.408 0.    0.001 0.    0.059 0.139 1.162 0.096
  0.001 0.    0.163]
 [2.69  0.002 0.    2.092 0.137 0.612 0.046 0.    0.003 0.313 0.139 0.061
  0.014 0.022 0.118]
 [0.103 0.383 0.002 0.638 0.415 0.002 0.001 0.001 0.003 2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.001 0.    0.    0.002 0.402 0.001
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.057 0.    0.001 0.    0.001 0.001 0.015 0.
  0.    0.    0.005]
 [0.37  0.    0.    0.18  0.153 0.    0.004 0.    0.002 0.427 0.58  0.001
  0.    0.001 0.032]
 [0.041 0.078 0.    0.112 3.047 0.017 0.008 0.    1.212 0.107 1.585 0.031
  0.001 0.016 0.021]
 [0.126 1.398 0.    1.127 1.231 0.003 0.006 0.    3.368 0.386 0.434 0.158
  0.011 0.002 0.562]
 [0.089 0.    0.    0.    0.111 0.    0.    0.    0.    0.106 0.081 0.002
  0.004 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.734 0.    0.
  0.    0.    0.   ]
 [2.52  0.    0.    0.    0.    0.    0.    0.    0.    2.189 0.    1.342
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.335 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.968 0.376 0.    1.377 0.    0.    1.746 0.956 1.472
  0.    0.    1.923]
 [0.414 0.    0.    0.336 0.408 0.    0.    0.    0.    0.    1.162 0.
  0.    0.    0.   ]
 [2.69  0.    0.    2.092 0.    0.612 0.    0.    0.    0.313 0.    0.
  0.    0.    0.   ]
 [0.    0.383 0.    0.638 0.415 0.    0.    0.    0.    2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.    0.    0.    0.    0.402 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.057 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.427 0.58  0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.047 0.    0.    0.    1.212 0.    1.585 0.
  0.    0.    0.   ]
 [0.    1.398 0.    1.127 1.231 0.    0.    0.    3.368 0.386 0.434 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[7.166e-05 3.031e-04 4.737e-03 1.942e-01 6.655e-05 2.207e-04 4.993e-05
 2.937e-04 2.734e+00 1.793e-01 5.105e-04 1.050e-03 1.242e-04 5.740e-03
 2.520e+00 1.954e-03 2.620e-01 2.147e-01 9.724e-03 9.801e-03 9.328e-04
 1.620e-03 2.189e+00 1.994e-01 1.342e+00 5.488e-04 9.886e-04 9.781e-02
 2.679e-02 6.849e-02 1.472e-01 1.660e-01 3.508e-02 1.368e-01 5.716e-04
 6.687e-02 1.254e-01 1.352e-01 1.619e-01 8.477e-04 1.272e-03 6.092e-03
 1.532e-01 1.509e-04 9.882e-04 2.036e-01 4.044e-05 3.897e-04 2.489e-04
 6.858e-05 2.135e-01 3.353e-01 1.155e-03 4.106e-04 1.293e-04 1.022e+00
 6.064e-05 4.791e-06 5.172e-05 2.471e-04 1.136e-05 1.928e-04 4.191e-06
 2.163e-04 3.383e-04 1.833e-03 1.007e-04 2.631e-05 7.614e-05 1.086e-03
 2.646e+00 3.009e-02 1.478e-03 9.684e-01 3.765e-01 1.377e+00 1.229e-04
 5.513e-02 1.746e+00 9.560e-01 1.472e+00 1.180e-02 1.838e-03 1.923e+00
 4.141e-01 4.437e-02 3.959e-04 3.360e-01 4.079e-01 4.254e-04 5.935e-05
 5.872e-02 1.392e-01 1.162e+00 9.588e-02 6.731e-04 4.063e-04 1.632e-01
 2.690e+00 2.126e-03 4.852e-04 2.092e+00 1.369e-01 6.121e-01 4.646e-02
 3.330e-03 3.125e-01 1.385e-01 6.062e-02 1.448e-02 2.181e-02 1.178e-01
 1.034e-01 3.829e-01 2.017e-03 6.383e-01 4.147e-01 2.074e-03 1.394e-03
 1.328e-03 2.308e+00 1.722e+00 1.030e+00 5.492e-05 3.972e-04 3.674e-01
 1.834e-04 4.607e-06 8.949e-05 4.394e-04 2.722e+00 6.113e-06 8.992e-04
 1.147e-05 4.538e-04 4.020e-01 1.020e-03 1.265e-04 1.194e-04 2.179e-03
 6.634e-04 5.288e-05 3.430e-04 1.176e-03 3.057e+00 1.252e-04 7.885e-04
 1.415e-04 9.900e-04 8.589e-04 1.728e-04 2.153e-04 3.103e-04 5.194e-03
 3.696e-01 4.210e-04 8.175e-05 1.805e-01 1.529e-01 2.371e-04 3.593e-03
 6.136e-05 1.729e-03 4.267e-01 5.799e-01 2.072e-04 6.430e-04 3.169e-02
 4.077e-02 7.764e-02 2.020e-04 1.125e-01 3.047e+00 1.651e-02 8.033e-03
 3.454e-04 1.212e+00 1.069e-01 1.585e+00 3.144e-02 1.628e-02 2.119e-02
 1.264e-01 1.398e+00 2.136e-04 1.127e+00 1.231e+00 3.196e-03 6.293e-03
 3.254e-04 3.368e+00 3.864e-01 4.339e-01 1.576e-01 1.130e-02 5.624e-01
 8.923e-02 3.157e-04 3.306e-04 7.760e-05 1.109e-01 7.500e-05 1.536e-04
 2.248e-04 2.530e-04 1.060e-01 8.126e-02 2.152e-03 4.216e-03 1.418e-04]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9805555555555556, 0.9709424613206593)
cuda
2565
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
||w||^2 2665199887.8523684
exp ma of ||w||^2 549743435320.9347
||w|| 51625.57397116635
exp ma of ||w|| 373433.15245411685
||w||^2 0.21041360219733735
exp ma of ||w||^2 16225314.140499998
||w|| 0.4587086245072544
exp ma of ||w|| 13.663513610944262
||w||^2 0.16373536230981336
exp ma of ||w||^2 4873.154888873428
||w|| 0.4046422646113643
exp ma of ||w|| 0.4033641462689814
||w||^2 0.2154600094661321
exp ma of ||w||^2 137.66816042913172
||w|| 0.46417670069288497
exp ma of ||w|| 0.40520938822128644
||w||^2 0.13645635999697822
exp ma of ||w||^2 69.60141019137106
||w|| 0.3693999999959099
exp ma of ||w|| 0.4160141044105001
||w||^2 0.1751413954130898
exp ma of ||w||^2 22.502871259252636
||w|| 0.41849897898691435
exp ma of ||w|| 0.4377165581559299
||w||^2 0.13078557065898735
exp ma of ||w||^2 1.9276475429662716
||w|| 0.36164287724077654
exp ma of ||w|| 0.44976302465730045
||w||^2 0.21558601857680296
exp ma of ||w||^2 0.2881588526234804
||w|| 0.4643124148424237
exp ma of ||w|| 0.4952257234266427
||w||^2 0.33240551690541364
exp ma of ||w||^2 0.2856817184451344
||w|| 0.5765461966793413
exp ma of ||w|| 0.496299440506345
||w||^2 0.12063906835852961
exp ma of ||w||^2 0.3227219046152766
||w|| 0.3473313523978646
exp ma of ||w|| 0.5207987014781817
||w||^2 0.4651865197794963
exp ma of ||w||^2 0.4619551574024655
||w|| 0.682045834075318
exp ma of ||w|| 0.5998839930018847
||w||^2 0.13434970272858845
exp ma of ||w||^2 0.49350515627249525
||w|| 0.366537450649437
exp ma of ||w|| 0.6255549145437109
cuda
Objective function 22.02 = squared loss an data 20.22 + 0.5*rho*h**2 1.210849 + alpha*h 0.000000 + L2reg 0.45 + L1reg 0.15 ; SHD = 51 ; DAG False
Proportion of microbatches that were clipped  0.7543687734770538
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.5561806284050093
iteration 1 in outer loop, alpha = 1.5561806284050093, rho = 1.0, h = 1.5561806284050093
cuda
2565
cuda
Objective function 24.45 = squared loss an data 20.22 + 0.5*rho*h**2 1.210849 + alpha*h 2.421698 + L2reg 0.45 + L1reg 0.15 ; SHD = 51 ; DAG False
||w||^2 18031518.104857713
exp ma of ||w||^2 773142663.0242168
||w|| 4246.35350681708
exp ma of ||w|| 16264.885490768966
||w||^2 0.41037939583637856
exp ma of ||w||^2 213361.71742144492
||w|| 0.6406086136139434
exp ma of ||w|| 10.923470851485376
||w||^2 0.8923730118099295
exp ma of ||w||^2 1.104172872232286
||w|| 0.9446549697164195
exp ma of ||w|| 0.6168865565059297
||w||^2 3.635247965567562
exp ma of ||w||^2 0.4847317533914615
||w|| 1.9066326246992529
exp ma of ||w|| 0.6355805530521695
||w||^2 0.6885043352836655
exp ma of ||w||^2 0.5013078355120141
||w|| 0.8297616135274429
exp ma of ||w|| 0.6590226300397324
v before min max tensor([[-0.093, 11.834, -2.807,  ..., -0.396,  5.606, -4.057],
        [ 6.366, -3.832, -1.995,  ..., -0.178,  1.274, -1.926],
        [25.262,  1.979, -2.646,  ..., -4.325,  1.012, 13.947],
        ...,
        [-1.738, -2.461, -0.576,  ..., -1.745, -2.551, -0.807],
        [ 0.078, 16.457, -2.307,  ...,  3.092,  1.504,  3.437],
        [-2.773, -0.108, -3.616,  ..., -2.869, -4.841, -1.799]],
       device='cuda:0')
v tensor([[1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 5.606e+00,
         1.000e-12],
        [6.366e+00, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.274e+00,
         1.000e-12],
        [1.000e+01, 1.979e+00, 1.000e-12,  ..., 1.000e-12, 1.012e+00,
         1.000e+01],
        ...,
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [7.760e-02, 1.000e+01, 1.000e-12,  ..., 3.092e+00, 1.504e+00,
         3.437e+00],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12]], device='cuda:0')
v before min max tensor([16.744, -2.536, -2.396, -1.617, -1.666,  7.560,  1.803, -1.071, -2.238,
        -1.994, -1.187, -3.734,  4.252, -2.262, -2.353, -2.141, -1.456, -0.118,
        -4.850, -2.676, -1.902, -2.210,  4.541, 14.818, -2.685, -4.641,  1.112,
         0.814, -0.157, -1.283,  3.716,  2.899, -3.410, -4.052, -4.408,  5.277,
        -2.195, -0.296, -2.725, -2.689,  1.379, -4.274, -5.038,  7.494, -3.351,
        -3.046, -1.372, -3.657, 30.652, -3.883,  1.092, -2.359, -3.251, -3.101,
        -2.858, -2.080, -2.161,  1.404,  1.661, -3.737, -1.780, -1.698,  3.004,
        -1.345, -3.310,  0.733,  4.255, -3.463, -1.761, -3.396, -3.496, -2.624,
        11.469, -0.414,  9.054,  3.291, -0.036,  6.943, -1.328, -3.112,  2.651,
         1.482,  2.890, -0.234, -3.324, -1.512, -4.155, -2.719, -2.800, -2.402,
         4.817, -3.022,  2.281, 10.648,  2.742, -4.473, -0.226, -0.113, -0.274,
        -2.159, -1.828, -3.028, -1.171,  1.255,  0.910, -3.706, -3.464, -1.539,
         8.919, -0.299, -1.503,  1.858, -4.086,  2.700, -0.326, 20.053, -1.843,
        -4.467,  2.630, -2.530, -3.542, -1.736, -3.417,  0.664, -2.220,  3.016,
         9.795,  2.499, -0.965, -0.624, -4.289,  1.909,  6.912,  1.450, -2.184,
        -1.995,  0.347, -3.996, 22.541, 10.690, -4.543, -2.824, -0.516,  4.614,
        -2.268, -3.829, -2.950, -4.065, -3.961, -2.732], device='cuda:0')
v tensor([1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 7.560e+00,
        1.803e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        4.252e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.541e+00, 1.000e+01,
        1.000e-12, 1.000e-12, 1.112e+00, 8.141e-01, 1.000e-12, 1.000e-12,
        3.716e+00, 2.899e+00, 1.000e-12, 1.000e-12, 1.000e-12, 5.277e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.379e+00, 1.000e-12,
        1.000e-12, 7.494e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.092e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.404e+00, 1.661e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 3.004e+00, 1.000e-12, 1.000e-12, 7.332e-01,
        4.255e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 9.054e+00, 3.291e+00, 1.000e-12, 6.943e+00,
        1.000e-12, 1.000e-12, 2.651e+00, 1.482e+00, 2.890e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        4.817e+00, 1.000e-12, 2.281e+00, 1.000e+01, 2.742e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.255e+00, 9.102e-01, 1.000e-12, 1.000e-12, 1.000e-12,
        8.919e+00, 1.000e-12, 1.000e-12, 1.858e+00, 1.000e-12, 2.700e+00,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 2.630e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 6.639e-01, 1.000e-12, 3.016e+00,
        9.795e+00, 2.499e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.909e+00,
        6.912e+00, 1.450e+00, 1.000e-12, 1.000e-12, 3.468e-01, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 4.614e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 1.315e+00],
         [-3.513e+00],
         [-2.711e+00],
         [-3.303e+00],
         [-3.132e+00],
         [-5.647e-01],
         [-1.265e+00],
         [-1.706e-01],
         [-2.635e+00],
         [-2.573e+00]],

        [[-1.766e+00],
         [ 2.039e-02],
         [-2.346e+00],
         [-4.797e+00],
         [-2.059e+00],
         [-2.943e+00],
         [-3.791e+00],
         [ 3.864e+00],
         [-4.073e+00],
         [ 8.728e+00]],

        [[-3.702e+00],
         [-2.824e+00],
         [-1.807e+00],
         [-4.730e-01],
         [-2.316e+00],
         [ 2.575e+00],
         [ 1.132e+01],
         [-1.633e+00],
         [-3.858e+00],
         [ 1.816e+00]],

        [[-4.633e+00],
         [ 3.715e+00],
         [ 1.051e+01],
         [-4.128e+00],
         [-4.046e-01],
         [-2.405e+00],
         [-5.076e-01],
         [-1.769e+00],
         [-2.715e+00],
         [-5.611e+00]],

        [[-3.557e+00],
         [-5.981e+00],
         [-3.798e+00],
         [-4.065e+00],
         [-4.101e+00],
         [ 5.678e+01],
         [-4.629e+00],
         [-2.969e+00],
         [ 2.289e+01],
         [-3.166e+00]],

        [[-3.768e+00],
         [-4.062e+00],
         [ 1.070e+01],
         [-2.249e+00],
         [ 3.292e+00],
         [-1.574e+00],
         [-2.553e+00],
         [ 6.793e+00],
         [-2.789e+00],
         [-3.231e+00]],

        [[-2.958e+00],
         [-2.173e+00],
         [-3.125e+00],
         [ 7.869e+00],
         [-2.092e+00],
         [ 1.522e+01],
         [-4.579e+00],
         [-2.566e+00],
         [-2.657e+00],
         [-2.706e+00]],

        [[ 2.313e+00],
         [-3.331e+00],
         [-2.179e+00],
         [ 3.038e+00],
         [-2.449e+00],
         [-1.247e-01],
         [ 2.510e+00],
         [-4.284e+00],
         [-2.192e+00],
         [ 2.158e+00]],

        [[ 9.585e-01],
         [-9.413e-01],
         [-4.596e+00],
         [-4.937e+00],
         [-8.554e-01],
         [ 1.959e+00],
         [ 8.523e+00],
         [-9.637e-01],
         [-3.899e+00],
         [ 5.495e+00]],

        [[ 5.537e+00],
         [ 6.833e+00],
         [-3.613e+00],
         [ 1.023e+01],
         [ 8.247e+00],
         [-5.188e+00],
         [ 6.041e+00],
         [-2.605e+00],
         [-3.896e+00],
         [-5.249e+00]],

        [[-4.470e+00],
         [-4.033e+00],
         [-1.111e+00],
         [ 4.358e+00],
         [-4.339e+00],
         [ 2.652e+00],
         [-4.223e+00],
         [ 1.174e+00],
         [-4.308e+00],
         [-2.823e+00]],

        [[ 4.719e+00],
         [-3.560e+00],
         [-2.696e+00],
         [-4.188e+00],
         [-1.827e+00],
         [-2.562e+00],
         [ 2.223e+00],
         [ 1.465e+00],
         [-4.212e+00],
         [-3.231e+00]],

        [[-4.876e+00],
         [-8.664e-01],
         [-5.008e+00],
         [-2.954e+00],
         [-3.294e+00],
         [-3.446e+00],
         [-3.759e+00],
         [-3.107e-01],
         [-1.744e+00],
         [-3.303e+00]],

        [[-7.052e-01],
         [-1.036e+00],
         [-1.897e+00],
         [ 7.398e-01],
         [ 7.533e+00],
         [ 1.908e-01],
         [-2.304e+00],
         [-1.761e+00],
         [ 1.046e+01],
         [-3.171e+00]],

        [[-1.932e+00],
         [ 1.567e+01],
         [ 4.169e+00],
         [-3.102e+00],
         [-2.682e+00],
         [ 2.180e+01],
         [ 7.451e-01],
         [-4.011e+00],
         [-3.364e+00],
         [-2.030e+00]]], device='cuda:0')
v tensor([[[1.315e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [2.039e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.864e+00],
         [1.000e-12],
         [8.728e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.575e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.816e+00]],

        [[1.000e-12],
         [3.715e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [3.292e+00],
         [1.000e-12],
         [1.000e-12],
         [6.793e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.869e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[2.313e+00],
         [1.000e-12],
         [1.000e-12],
         [3.038e+00],
         [1.000e-12],
         [1.000e-12],
         [2.510e+00],
         [1.000e-12],
         [1.000e-12],
         [2.158e+00]],

        [[9.585e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.959e+00],
         [8.523e+00],
         [1.000e-12],
         [1.000e-12],
         [5.495e+00]],

        [[5.537e+00],
         [6.833e+00],
         [1.000e-12],
         [1.000e+01],
         [8.247e+00],
         [1.000e-12],
         [6.041e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.358e+00],
         [1.000e-12],
         [2.652e+00],
         [1.000e-12],
         [1.174e+00],
         [1.000e-12],
         [1.000e-12]],

        [[4.719e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.223e+00],
         [1.465e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.398e-01],
         [7.533e+00],
         [1.908e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [4.169e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [7.451e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-1.977],
        [ 1.399],
        [-0.104],
        [26.780],
        [-3.625],
        [-0.073],
        [ 1.170],
        [-2.519],
        [-3.967],
        [-3.489],
        [29.159],
        [ 5.005],
        [12.158],
        [ 6.069],
        [-0.842]], device='cuda:0')
v tensor([[1.000e-12],
        [1.399e+00],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.170e+00],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [5.005e+00],
        [1.000e+01],
        [6.069e+00],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.030,  0.019, -0.032,  ...,  0.019,  0.000,  0.021],
        [-0.001, -0.017,  0.005,  ...,  0.023,  0.031,  0.014],
        [-0.029,  0.021, -0.003,  ..., -0.014,  0.009,  0.005],
        ...,
        [ 0.005,  0.005,  0.017,  ..., -0.018,  0.027, -0.011],
        [-0.068,  0.012,  0.019,  ..., -0.021, -0.014,  0.037],
        [-0.082,  0.002, -0.027,  ..., -0.018, -0.004, -0.017]],
       device='cuda:0')
s after update for 1 param tensor([[1.021, 1.759, 1.290,  ..., 1.452, 1.728, 1.532],
        [1.090, 1.232, 1.573,  ..., 1.164, 0.993, 0.814],
        [1.903, 1.610, 1.269,  ..., 1.513, 1.108, 1.992],
        ...,
        [0.619, 0.853, 1.114,  ..., 1.030, 1.150, 1.222],
        [1.054, 1.272, 0.926,  ..., 1.186, 1.542, 1.499],
        [1.188, 1.054, 1.315,  ..., 0.887, 1.526, 1.175]], device='cuda:0')
b after update for 1 param tensor([[56.889, 74.656, 63.935,  ..., 67.830, 74.008, 69.678],
        [58.777, 62.476, 70.595,  ..., 60.722, 56.092, 50.801],
        [77.656, 71.435, 63.414,  ..., 69.252, 59.249, 79.461],
        ...,
        [44.292, 51.990, 59.407,  ..., 57.119, 60.356, 62.224],
        [57.784, 63.478, 54.156,  ..., 61.305, 69.892, 68.926],
        [61.363, 57.806, 64.562,  ..., 53.019, 69.547, 61.026]],
       device='cuda:0')
clipping threshold 0.4378185288545258
a after update for 1 param tensor([ 0.043, -0.017, -0.019,  0.022, -0.022, -0.028, -0.029, -0.003, -0.027,
        -0.005, -0.004,  0.013,  0.040,  0.029,  0.003, -0.013,  0.020, -0.024,
        -0.021,  0.038,  0.014,  0.067,  0.004, -0.045,  0.012, -0.009, -0.013,
         0.010, -0.066,  0.025,  0.019, -0.001, -0.053,  0.013, -0.038,  0.015,
         0.049, -0.066, -0.020, -0.010,  0.028, -0.089, -0.008, -0.030, -0.041,
        -0.016,  0.025, -0.003, -0.013, -0.060, -0.026, -0.002,  0.037, -0.038,
         0.005, -0.003,  0.040, -0.066,  0.001,  0.003, -0.007,  0.004, -0.007,
         0.020,  0.002, -0.023,  0.018,  0.067, -0.069, -0.023,  0.035,  0.015,
        -0.028, -0.042, -0.049, -0.027, -0.015, -0.017, -0.023, -0.002,  0.020,
         0.093, -0.005, -0.008,  0.041, -0.021, -0.050, -0.040,  0.020,  0.069,
         0.009,  0.004, -0.038,  0.014,  0.009,  0.036, -0.005,  0.044,  0.023,
        -0.043, -0.035,  0.011,  0.047,  0.007, -0.008,  0.035, -0.021, -0.055,
         0.001,  0.099, -0.001,  0.040,  0.005, -0.023,  0.021, -0.026, -0.011,
        -0.005, -0.008,  0.000,  0.025,  0.010, -0.040,  0.012, -0.018, -0.018,
        -0.031,  0.015, -0.054, -0.027,  0.042, -0.021, -0.030, -0.011, -0.025,
        -0.044,  0.097, -0.006,  0.012,  0.009, -0.022,  0.057,  0.035,  0.026,
        -0.063, -0.025, -0.024,  0.026,  0.001, -0.030], device='cuda:0')
s after update for 1 param tensor([1.704, 0.785, 1.289, 0.521, 0.932, 1.107, 1.164, 0.414, 0.970, 0.640,
        1.278, 1.256, 1.555, 0.747, 0.888, 0.677, 0.638, 1.276, 1.500, 0.853,
        0.600, 1.338, 1.001, 1.868, 1.188, 1.557, 0.777, 0.494, 1.359, 1.205,
        1.614, 1.216, 1.105, 1.255, 1.373, 1.140, 1.406, 0.923, 1.249, 0.881,
        0.747, 1.360, 1.583, 1.316, 1.053, 1.206, 1.725, 1.185, 1.486, 1.532,
        0.846, 0.796, 1.006, 1.000, 1.042, 1.652, 1.153, 0.634, 1.143, 1.170,
        0.805, 1.040, 1.123, 0.638, 1.025, 1.102, 1.279, 1.328, 0.566, 1.287,
        1.087, 0.882, 1.544, 0.564, 1.445, 0.917, 0.522, 1.093, 0.594, 1.036,
        0.728, 0.965, 0.902, 0.629, 1.044, 0.626, 1.497, 1.176, 0.924, 0.762,
        1.649, 1.360, 1.556, 1.523, 1.492, 1.562, 0.513, 1.151, 1.283, 0.765,
        1.177, 1.074, 1.361, 1.063, 1.278, 1.155, 1.195, 0.580, 1.536, 1.518,
        0.842, 1.003, 1.428, 1.381, 0.793, 1.499, 0.596, 1.392, 1.512, 1.124,
        1.430, 0.540, 1.147, 0.818, 0.754, 1.012, 2.026, 1.560, 1.122, 1.297,
        1.431, 1.036, 1.400, 0.893, 1.005, 1.257, 0.828, 1.238, 1.770, 1.623,
        1.605, 0.877, 0.565, 0.851, 0.777, 1.243, 0.956, 1.316, 1.288, 0.850],
       device='cuda:0')
b after update for 1 param tensor([73.489, 49.870, 63.910, 40.629, 54.340, 59.223, 60.725, 36.236, 55.444,
        45.018, 63.645, 63.101, 70.190, 48.640, 53.056, 46.314, 44.977, 63.592,
        68.950, 51.998, 43.620, 65.126, 56.320, 76.945, 61.366, 70.239, 49.634,
        39.556, 65.626, 61.789, 71.525, 62.071, 59.175, 63.069, 65.967, 60.108,
        66.742, 54.094, 62.921, 52.846, 48.656, 65.654, 70.824, 64.574, 57.767,
        61.825, 73.929, 61.277, 68.624, 69.669, 51.783, 50.232, 56.463, 56.307,
        57.462, 72.359, 60.458, 44.837, 60.184, 60.884, 50.502, 57.409, 59.642,
        44.955, 56.989, 59.103, 63.665, 64.871, 42.336, 63.859, 58.700, 52.863,
        69.958, 42.262, 67.679, 53.913, 40.668, 58.853, 43.369, 57.291, 48.040,
        55.288, 53.460, 44.647, 57.521, 44.537, 68.882, 61.049, 54.104, 49.128,
        72.295, 65.659, 70.227, 69.476, 68.768, 70.357, 40.301, 60.400, 63.758,
        49.239, 61.072, 58.351, 65.681, 58.050, 63.645, 60.507, 61.543, 42.875,
        69.770, 69.353, 51.664, 56.370, 67.267, 66.154, 50.141, 68.933, 43.451,
        66.428, 69.210, 59.670, 67.318, 41.364, 60.295, 50.911, 48.882, 56.640,
        80.119, 70.311, 59.626, 64.121, 67.350, 57.296, 66.607, 53.206, 56.432,
        63.108, 51.230, 62.634, 74.886, 71.716, 71.323, 52.704, 42.331, 51.938,
        49.607, 62.759, 55.051, 64.582, 63.886, 51.911], device='cuda:0')
clipping threshold 0.4378185288545258
a after update for 1 param tensor([[[-2.765e-02],
         [-2.727e-02],
         [-5.608e-02],
         [-1.461e-05],
         [ 5.636e-02],
         [-4.465e-02],
         [ 6.007e-02],
         [-4.786e-02],
         [-4.943e-02],
         [ 3.787e-03]],

        [[ 2.919e-03],
         [ 4.057e-02],
         [ 7.350e-02],
         [ 2.907e-02],
         [ 5.160e-02],
         [ 1.453e-02],
         [-4.069e-02],
         [ 3.381e-02],
         [ 8.979e-02],
         [ 2.713e-02]],

        [[ 4.005e-02],
         [-1.825e-02],
         [-6.284e-04],
         [-3.005e-03],
         [ 1.537e-02],
         [-5.176e-02],
         [-4.958e-02],
         [-2.336e-02],
         [ 1.068e-02],
         [ 2.954e-02]],

        [[-3.897e-02],
         [ 3.108e-02],
         [-5.223e-02],
         [-1.103e-01],
         [ 6.331e-03],
         [-1.221e-02],
         [-8.843e-02],
         [ 3.573e-03],
         [-2.718e-02],
         [ 2.722e-02]],

        [[ 1.958e-01],
         [-2.629e-01],
         [ 1.241e-01],
         [ 1.524e-01],
         [-2.847e-01],
         [ 1.750e-01],
         [-2.677e-01],
         [ 1.967e-01],
         [-2.225e-01],
         [ 3.183e-01]],

        [[-1.161e-02],
         [ 3.982e-02],
         [-2.666e-02],
         [ 7.058e-03],
         [-3.413e-02],
         [ 5.246e-02],
         [ 1.403e-02],
         [ 1.421e-02],
         [-2.114e-02],
         [ 3.358e-02]],

        [[-6.825e-03],
         [ 4.840e-02],
         [-3.480e-02],
         [ 1.578e-03],
         [-1.741e-02],
         [ 1.694e-02],
         [-4.301e-02],
         [ 3.286e-02],
         [-1.072e-03],
         [ 1.405e-02]],

        [[ 2.312e-02],
         [ 2.073e-02],
         [ 4.296e-02],
         [ 6.173e-02],
         [ 1.410e-02],
         [ 2.660e-02],
         [ 3.590e-02],
         [-9.871e-03],
         [ 1.163e-02],
         [-9.284e-03]],

        [[-5.594e-02],
         [ 1.302e-01],
         [-1.803e-02],
         [-5.094e-02],
         [-9.607e-02],
         [-1.216e-01],
         [-4.099e-02],
         [-5.043e-03],
         [-9.465e-03],
         [ 6.941e-02]],

        [[ 4.939e-02],
         [ 8.835e-02],
         [-6.485e-02],
         [ 9.743e-02],
         [ 1.530e-01],
         [ 7.482e-02],
         [-3.245e-02],
         [ 4.573e-02],
         [ 9.433e-02],
         [ 4.979e-02]],

        [[-1.439e-01],
         [ 1.038e-01],
         [ 4.244e-02],
         [ 5.861e-02],
         [ 7.933e-02],
         [ 8.590e-02],
         [ 4.825e-02],
         [-1.079e-01],
         [-1.005e-01],
         [ 4.757e-04]],

        [[-2.385e-04],
         [ 5.415e-02],
         [ 7.584e-02],
         [-1.320e-02],
         [ 9.041e-02],
         [ 5.158e-02],
         [ 9.164e-02],
         [ 5.090e-02],
         [ 1.611e-02],
         [ 3.896e-03]],

        [[ 4.759e-04],
         [-9.293e-03],
         [-1.201e-03],
         [ 2.659e-02],
         [-3.566e-02],
         [-1.349e-02],
         [ 4.832e-02],
         [ 3.299e-02],
         [-3.893e-02],
         [-5.608e-03]],

        [[-1.140e-02],
         [ 1.770e-02],
         [ 2.475e-02],
         [ 1.076e-02],
         [-3.744e-02],
         [ 1.047e-02],
         [-3.175e-02],
         [-2.370e-02],
         [-2.674e-02],
         [ 6.239e-03]],

        [[-3.350e-02],
         [-9.490e-02],
         [-1.857e-02],
         [-4.176e-03],
         [ 2.435e-02],
         [-5.455e-03],
         [ 1.766e-03],
         [ 6.948e-02],
         [-7.549e-03],
         [-5.303e-03]]], device='cuda:0')
s after update for 1 param tensor([[[0.937],
         [1.092],
         [0.952],
         [1.231],
         [1.003],
         [1.548],
         [1.035],
         [1.664],
         [1.538],
         [1.119]],

        [[0.900],
         [0.884],
         [0.746],
         [1.585],
         [1.297],
         [1.266],
         [1.172],
         [1.596],
         [1.527],
         [1.536]],

        [[1.168],
         [1.072],
         [1.184],
         [1.484],
         [1.239],
         [0.872],
         [1.438],
         [0.899],
         [1.477],
         [1.467]],

        [[1.437],
         [1.228],
         [1.551],
         [1.364],
         [1.558],
         [1.608],
         [1.607],
         [0.827],
         [1.209],
         [1.749]],

        [[1.510],
         [1.851],
         [1.197],
         [1.619],
         [1.500],
         [1.888],
         [1.653],
         [1.444],
         [2.524],
         [0.993]],

        [[1.312],
         [1.558],
         [2.152],
         [0.699],
         [1.128],
         [0.888],
         [0.945],
         [1.737],
         [1.301],
         [1.224]],

        [[0.931],
         [0.773],
         [1.402],
         [1.510],
         [0.835],
         [1.276],
         [1.501],
         [0.964],
         [1.309],
         [1.186]],

        [[0.889],
         [1.165],
         [0.984],
         [1.526],
         [1.270],
         [0.931],
         [1.060],
         [1.438],
         [1.257],
         [1.443]],

        [[1.344],
         [1.705],
         [1.420],
         [1.768],
         [1.037],
         [1.069],
         [1.652],
         [1.094],
         [1.365],
         [1.443]],

        [[1.599],
         [1.705],
         [1.437],
         [1.983],
         [1.958],
         [1.638],
         [1.220],
         [1.027],
         [1.221],
         [1.625]],

        [[1.427],
         [1.268],
         [1.260],
         [1.589],
         [1.579],
         [1.414],
         [1.365],
         [1.006],
         [1.409],
         [0.876]],

        [[1.433],
         [1.101],
         [0.874],
         [1.415],
         [1.243],
         [0.793],
         [1.545],
         [1.332],
         [1.334],
         [1.161]],

        [[1.569],
         [0.809],
         [1.553],
         [1.036],
         [1.149],
         [1.259],
         [1.257],
         [0.600],
         [1.026],
         [1.021]],

        [[0.574],
         [0.788],
         [0.699],
         [1.056],
         [1.764],
         [0.740],
         [0.883],
         [0.821],
         [1.415],
         [1.155]],

        [[1.213],
         [1.598],
         [1.731],
         [0.966],
         [1.186],
         [1.625],
         [1.264],
         [1.240],
         [1.111],
         [1.115]]], device='cuda:0')
b after update for 1 param tensor([[[54.477],
         [58.833],
         [54.914],
         [62.467],
         [56.379],
         [70.045],
         [57.262],
         [72.611],
         [69.805],
         [59.553]],

        [[53.405],
         [52.920],
         [48.637],
         [70.880],
         [64.102],
         [63.333],
         [60.939],
         [71.124],
         [69.564],
         [69.756]],

        [[60.850],
         [58.283],
         [61.266],
         [68.579],
         [62.661],
         [52.565],
         [67.496],
         [53.383],
         [68.404],
         [68.190]],

        [[67.473],
         [62.375],
         [70.117],
         [65.753],
         [70.273],
         [71.377],
         [71.356],
         [51.183],
         [61.904],
         [74.442]],

        [[69.171],
         [76.592],
         [61.598],
         [71.628],
         [68.953],
         [77.345],
         [72.385],
         [67.637],
         [89.432],
         [56.082]],

        [[64.470],
         [70.273],
         [82.589],
         [47.079],
         [59.785],
         [53.046],
         [54.736],
         [74.194],
         [64.219],
         [62.287]],

        [[54.316],
         [49.485],
         [66.644],
         [69.182],
         [51.438],
         [63.592],
         [68.971],
         [55.272],
         [64.413],
         [61.300]],

        [[53.086],
         [60.761],
         [55.835],
         [69.538],
         [63.445],
         [54.311],
         [57.966],
         [67.505],
         [63.122],
         [67.619]],

        [[65.263],
         [73.498],
         [67.091],
         [74.850],
         [57.322],
         [58.204],
         [72.348],
         [58.883],
         [65.762],
         [67.611]],

        [[71.176],
         [73.509],
         [67.474],
         [79.270],
         [78.768],
         [72.054],
         [62.175],
         [57.056],
         [62.197],
         [71.762]],

        [[67.245],
         [63.401],
         [63.183],
         [70.950],
         [70.739],
         [66.937],
         [65.776],
         [56.462],
         [66.832],
         [52.694]],

        [[67.398],
         [59.062],
         [52.623],
         [66.954],
         [62.760],
         [50.123],
         [69.972],
         [64.981],
         [65.029],
         [60.651]],

        [[70.510],
         [50.624],
         [70.153],
         [57.311],
         [60.333],
         [63.174],
         [63.123],
         [43.611],
         [57.031],
         [56.877]],

        [[42.665],
         [49.959],
         [47.057],
         [57.835],
         [74.770],
         [48.422],
         [52.885],
         [51.001],
         [66.958],
         [60.510]],

        [[61.997],
         [71.163],
         [74.073],
         [55.317],
         [61.294],
         [71.769],
         [63.289],
         [62.674],
         [59.328],
         [59.445]]], device='cuda:0')
clipping threshold 0.4378185288545258
a after update for 1 param tensor([[-0.012],
        [ 0.013],
        [ 0.027],
        [-0.029],
        [-0.048],
        [ 0.022],
        [ 0.011],
        [-0.078],
        [ 0.030],
        [ 0.017],
        [-0.065],
        [-0.018],
        [ 0.059],
        [ 0.027],
        [-0.055]], device='cuda:0')
s after update for 1 param tensor([[1.883],
        [1.081],
        [1.288],
        [1.330],
        [1.273],
        [0.749],
        [1.453],
        [1.013],
        [1.542],
        [1.477],
        [2.018],
        [1.391],
        [1.584],
        [1.210],
        [1.611]], device='cuda:0')
b after update for 1 param tensor([[77.239],
        [58.534],
        [63.895],
        [64.921],
        [63.514],
        [48.713],
        [67.863],
        [56.665],
        [69.914],
        [68.403],
        [79.973],
        [66.381],
        [70.840],
        [61.910],
        [71.458]], device='cuda:0')
clipping threshold 0.4378185288545258
cuda
Objective function 15.50 = squared loss an data 12.47 + 0.5*rho*h**2 0.549143 + alpha*h 1.630864 + L2reg 0.71 + L1reg 0.14 ; SHD = 40 ; DAG False
Proportion of microbatches that were clipped  0.7611845247572042
iteration 1 in inner loop, alpha 1.5561806284050093 rho 1.0 h 1.047991237352889
2565
cuda
Objective function 20.44 = squared loss an data 12.47 + 0.5*rho*h**2 5.491428 + alpha*h 1.630864 + L2reg 0.71 + L1reg 0.14 ; SHD = 40 ; DAG False
||w||^2 0.5147238619216932
exp ma of ||w||^2 322.0770771240133
||w|| 0.7174425844077651
exp ma of ||w|| 0.7505104920243709
||w||^2 1.1426410315329987
exp ma of ||w||^2 3.768499719146042
||w|| 1.068943886054361
exp ma of ||w|| 0.7490462257945089
||w||^2 0.2710599379227729
exp ma of ||w||^2 0.616282828852123
||w|| 0.5206341689927515
exp ma of ||w|| 0.7294369545999225
||w||^2 0.2870152903060257
exp ma of ||w||^2 0.6475922495124025
||w|| 0.535738079947679
exp ma of ||w|| 0.746854392679955
||w||^2 0.16961849074392338
exp ma of ||w||^2 0.6764530778719835
||w|| 0.41184765477531055
exp ma of ||w|| 0.761610597569789
cuda
Objective function 15.55 = squared loss an data 12.81 + 0.5*rho*h**2 1.052361 + alpha*h 0.713933 + L2reg 0.85 + L1reg 0.13 ; SHD = 38 ; DAG True
Proportion of microbatches that were clipped  0.7557301899148657
iteration 2 in inner loop, alpha 1.5561806284050093 rho 10.0 h 0.4587725504191411
2565
cuda
Objective function 25.02 = squared loss an data 12.81 + 0.5*rho*h**2 10.523613 + alpha*h 0.713933 + L2reg 0.85 + L1reg 0.13 ; SHD = 38 ; DAG True
||w||^2 12.029280541012655
exp ma of ||w||^2 4247740.158290762
||w|| 3.4683253222575092
exp ma of ||w|| 106.94844048180015
||w||^2 0.5534356103673973
exp ma of ||w||^2 1.1001675905907673
||w|| 0.7439325307898541
exp ma of ||w|| 0.8061395955887304
||w||^2 3.758783779384122
exp ma of ||w||^2 0.8705800195347706
||w|| 1.9387583086563736
exp ma of ||w|| 0.8656872036679057
||w||^2 0.3043088939708422
exp ma of ||w||^2 0.8599501024190769
||w|| 0.5516419980121549
exp ma of ||w|| 0.8636681159738244
||w||^2 1.6913388519196024
exp ma of ||w||^2 1.0869856192906628
||w|| 1.300514841099325
exp ma of ||w|| 0.9426239308635067
||w||^2 2.2423389035707264
exp ma of ||w||^2 1.0479842015551302
||w|| 1.497444123689003
exp ma of ||w|| 0.932251728443297
||w||^2 0.9961249493930656
exp ma of ||w||^2 1.025047864286701
||w|| 0.9980605940488111
exp ma of ||w|| 0.9333019097074675
||w||^2 1.2280516191676552
exp ma of ||w||^2 0.9107689734883186
||w|| 1.1081749045920752
exp ma of ||w|| 0.8811035261360735
cuda
Objective function 16.93 = squared loss an data 14.53 + 0.5*rho*h**2 1.093869 + alpha*h 0.230175 + L2reg 0.96 + L1reg 0.12 ; SHD = 33 ; DAG True
Proportion of microbatches that were clipped  0.7638215324927256
iteration 3 in inner loop, alpha 1.5561806284050093 rho 100.0 h 0.14791005782579347
iteration 2 in outer loop, alpha = 16.347186410984357, rho = 100.0, h = 0.14791005782579347
cuda
2565
cuda
Objective function 19.12 = squared loss an data 14.53 + 0.5*rho*h**2 1.093869 + alpha*h 2.417913 + L2reg 0.96 + L1reg 0.12 ; SHD = 33 ; DAG True
||w||^2 1.0436426578963485
exp ma of ||w||^2 413273.11187051475
||w|| 1.0215883015659235
exp ma of ||w|| 10.672680699640194
||w||^2 0.5479215897896521
exp ma of ||w||^2 0.9980085220678583
||w|| 0.7402172585056715
exp ma of ||w|| 0.9327261208082223
||w||^2 0.392583583258457
exp ma of ||w||^2 0.8602854061295551
||w|| 0.6265649074584827
exp ma of ||w|| 0.8639383919307563
||w||^2 1.0140050694152356
exp ma of ||w||^2 1.0470377681728629
||w|| 1.0069781871596006
exp ma of ||w|| 0.9439153230841713
||w||^2 0.15325963082007815
exp ma of ||w||^2 1.0550508285103928
||w|| 0.3914838832188091
exp ma of ||w|| 0.9462459978685142
cuda
Objective function 17.49 = squared loss an data 14.03 + 0.5*rho*h**2 0.556588 + alpha*h 1.724745 + L2reg 1.06 + L1reg 0.12 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.7632850241545893
iteration 1 in inner loop, alpha 16.347186410984357 rho 100.0 h 0.10550714868930022
2565
cuda
Objective function 22.50 = squared loss an data 14.03 + 0.5*rho*h**2 5.565879 + alpha*h 1.724745 + L2reg 1.06 + L1reg 0.12 ; SHD = 40 ; DAG True
||w||^2 32580012345.721714
exp ma of ||w||^2 69151375696.22871
||w|| 180499.34167669897
exp ma of ||w|| 217183.25097386958
||w||^2 1.082242615226228
exp ma of ||w||^2 1.359113933685874
||w|| 1.0403089037522595
exp ma of ||w|| 1.019511646552875
||w||^2 1.6686413232641746
exp ma of ||w||^2 1.2647113598463635
||w|| 1.2917590035545232
exp ma of ||w|| 1.0432236634972496
||w||^2 0.8228034617110175
exp ma of ||w||^2 1.2813799292594956
||w|| 0.9070851457889814
exp ma of ||w|| 1.040478592922299
||w||^2 0.7080909207101037
exp ma of ||w||^2 1.1524210320662376
||w|| 0.8414813846485872
exp ma of ||w|| 0.9869524205665372
cuda
Objective function 17.47 = squared loss an data 14.71 + 0.5*rho*h**2 0.821805 + alpha*h 0.662739 + L2reg 1.15 + L1reg 0.12 ; SHD = 38 ; DAG True
Proportion of microbatches that were clipped  0.7672774027643845
iteration 2 in inner loop, alpha 16.347186410984357 rho 1000.0 h 0.04054146287003846
2565
cuda
Objective function 24.86 = squared loss an data 14.71 + 0.5*rho*h**2 8.218051 + alpha*h 0.662739 + L2reg 1.15 + L1reg 0.12 ; SHD = 38 ; DAG True
cuda
Objective function 17.61 = squared loss an data 15.06 + 0.5*rho*h**2 0.966563 + alpha*h 0.227286 + L2reg 1.24 + L1reg 0.12 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.7644466209598433
iteration 3 in inner loop, alpha 16.347186410984357 rho 10000.0 h 0.01390369268699665
iteration 3 in outer loop, alpha = 155.38411328095086, rho = 10000.0, h = 0.01390369268699665
cuda
2565
cuda
Objective function 19.54 = squared loss an data 15.06 + 0.5*rho*h**2 0.966563 + alpha*h 2.160413 + L2reg 1.24 + L1reg 0.12 ; SHD = 43 ; DAG True
||w||^2 1427390252.691651
exp ma of ||w||^2 79000035228.74638
||w|| 37780.81858154546
exp ma of ||w|| 160190.7607443006
||w||^2 0.338861614398921
exp ma of ||w||^2 84.39742449728008
||w|| 0.5821182134231164
exp ma of ||w|| 0.9953457804214731
||w||^2 0.3259866012082664
exp ma of ||w||^2 1.2952355044266017
||w|| 0.5709523633441466
exp ma of ||w|| 0.9747009104110521
||w||^2 0.46962503125289207
exp ma of ||w||^2 1.1236597385895217
||w|| 0.6852919314079892
exp ma of ||w|| 0.9942836994722637
cuda
Objective function 17.35 = squared loss an data 14.55 + 0.5*rho*h**2 0.261064 + alpha*h 1.122782 + L2reg 1.29 + L1reg 0.12 ; SHD = 41 ; DAG True
Proportion of microbatches that were clipped  0.764269590454692
iteration 1 in inner loop, alpha 155.38411328095086 rho 10000.0 h 0.007225847905491634
2565
cuda
Objective function 19.70 = squared loss an data 14.55 + 0.5*rho*h**2 2.610644 + alpha*h 1.122782 + L2reg 1.29 + L1reg 0.12 ; SHD = 41 ; DAG True
||w||^2 23.213603699966704
exp ma of ||w||^2 6230631407.644293
||w|| 4.818049781806608
exp ma of ||w|| 518.8142603185244
||w||^2 1.900181228113156
exp ma of ||w||^2 3537.144781375591
||w|| 1.3784706119874868
exp ma of ||w|| 1.0956938125161686
||w||^2 1.096582148490023
exp ma of ||w||^2 3432.155377555832
||w|| 1.0471781837347562
exp ma of ||w|| 1.10509602903569
||w||^2 0.6197251493552062
exp ma of ||w||^2 18.425683226857203
||w|| 0.7872262377202669
exp ma of ||w|| 0.9793561964544166
cuda
Objective function 16.86 = squared loss an data 14.50 + 0.5*rho*h**2 0.422722 + alpha*h 0.451803 + L2reg 1.36 + L1reg 0.12 ; SHD = 41 ; DAG True
Proportion of microbatches that were clipped  0.7670988654781199
iteration 2 in inner loop, alpha 155.38411328095086 rho 100000.0 h 0.002907652841695807
iteration 4 in outer loop, alpha = 446.1493974505315, rho = 100000.0, h = 0.002907652841695807
cuda
2565
cuda
Objective function 17.71 = squared loss an data 14.50 + 0.5*rho*h**2 0.422722 + alpha*h 1.297248 + L2reg 1.36 + L1reg 0.12 ; SHD = 41 ; DAG True
||w||^2 5.503938980037006
exp ma of ||w||^2 559487411.0913028
||w|| 2.3460475229707103
exp ma of ||w|| 33.129734994927354
||w||^2 2.036708088175401
exp ma of ||w||^2 3291357.7234328543
||w|| 1.4271328207897824
exp ma of ||w|| 1.7317959408368855
||w||^2 0.7869063938007202
exp ma of ||w||^2 55069.51632311183
||w|| 0.8870774452102366
exp ma of ||w|| 1.2093259740307065
cuda
Objective function 17.18 = squared loss an data 14.50 + 0.5*rho*h**2 0.231637 + alpha*h 0.960282 + L2reg 1.37 + L1reg 0.12 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.7630600032346757
iteration 1 in inner loop, alpha 446.1493974505315 rho 100000.0 h 0.002152378151961898
iteration 5 in outer loop, alpha = 2598.5275494124294, rho = 1000000.0, h = 0.002152378151961898
Threshold 0.3
[[0.003 0.052 0.299 0.269 0.141 0.035 0.013 0.003 0.015 0.881 0.061 0.077
  0.02  0.006 0.085]
 [0.095 0.003 0.429 0.101 0.418 0.249 0.035 0.029 0.164 0.302 0.378 0.691
  0.069 0.001 0.146]
 [0.02  0.009 0.002 0.016 0.082 0.037 0.016 0.01  0.027 0.036 0.024 0.063
  0.008 0.004 0.054]
 [0.015 0.065 0.254 0.002 0.55  0.068 0.013 0.005 0.055 0.297 0.184 0.145
  0.022 0.008 0.239]
 [0.004 0.007 0.048 0.005 0.003 0.008 0.003 0.003 0.003 0.002 0.003 0.008
  0.002 0.001 0.011]
 [0.095 0.019 0.105 0.047 0.274 0.004 0.032 0.019 0.052 0.078 0.151 0.23
  0.02  0.004 0.18 ]
 [0.289 0.105 0.343 0.316 0.61  0.175 0.003 0.131 0.156 0.312 1.059 0.398
  0.106 0.023 0.298]
 [1.249 0.148 0.395 0.541 0.958 0.18  0.034 0.002 0.428 0.412 0.24  0.359
  0.087 0.015 0.209]
 [0.172 0.03  0.091 0.082 0.461 0.103 0.018 0.012 0.004 0.965 0.841 0.641
  0.011 0.002 0.078]
 [0.005 0.022 0.095 0.019 1.844 0.037 0.014 0.003 0.004 0.004 0.042 0.033
  0.009 0.002 0.023]
 [0.067 0.01  0.178 0.013 1.852 0.025 0.002 0.013 0.004 0.099 0.006 0.018
  0.002 0.001 0.025]
 [0.067 0.006 0.078 0.029 0.291 0.024 0.01  0.011 0.009 0.112 0.285 0.004
  0.016 0.004 0.023]
 [0.174 0.063 0.508 0.227 1.105 0.323 0.045 0.054 0.27  0.29  1.335 0.2
  0.003 0.042 0.237]
 [0.524 1.271 0.718 0.47  0.459 0.737 0.147 0.253 2.504 1.094 0.625 0.72
  0.097 0.003 0.188]
 [0.031 0.036 0.074 0.015 0.442 0.034 0.023 0.02  0.066 0.193 0.127 0.098
  0.021 0.021 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.881 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.429 0.    0.418 0.    0.    0.    0.    0.302 0.378 0.691
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.55  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.343 0.316 0.61  0.    0.    0.    0.    0.312 1.059 0.398
  0.    0.    0.   ]
 [1.249 0.    0.395 0.541 0.958 0.    0.    0.    0.428 0.412 0.    0.359
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.461 0.    0.    0.    0.    0.965 0.841 0.641
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.844 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.852 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.508 0.    1.105 0.323 0.    0.    0.    0.    1.335 0.
  0.    0.    0.   ]
 [0.524 1.271 0.718 0.47  0.459 0.737 0.    0.    2.504 1.094 0.625 0.72
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.442 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.5853658536585366, 'tpr': 0.5666666666666667, 'fpr': 0.32, 'f1': 0.47887323943661975, 'shd': 37, 'npred': 41, 'ntrue': 30}
[5.178e-02 2.994e-01 2.695e-01 1.412e-01 3.543e-02 1.264e-02 2.881e-03
 1.506e-02 8.807e-01 6.125e-02 7.732e-02 2.046e-02 5.863e-03 8.518e-02
 9.499e-02 4.285e-01 1.015e-01 4.179e-01 2.489e-01 3.495e-02 2.902e-02
 1.636e-01 3.021e-01 3.779e-01 6.909e-01 6.915e-02 1.075e-03 1.458e-01
 1.982e-02 9.112e-03 1.557e-02 8.242e-02 3.691e-02 1.581e-02 1.038e-02
 2.679e-02 3.633e-02 2.431e-02 6.304e-02 8.423e-03 4.197e-03 5.434e-02
 1.532e-02 6.518e-02 2.545e-01 5.502e-01 6.811e-02 1.342e-02 5.056e-03
 5.462e-02 2.971e-01 1.837e-01 1.454e-01 2.211e-02 8.293e-03 2.391e-01
 4.019e-03 7.391e-03 4.826e-02 4.658e-03 8.031e-03 2.695e-03 2.972e-03
 3.137e-03 1.584e-03 3.039e-03 8.333e-03 1.827e-03 9.820e-04 1.073e-02
 9.527e-02 1.913e-02 1.047e-01 4.741e-02 2.744e-01 3.207e-02 1.891e-02
 5.205e-02 7.797e-02 1.514e-01 2.296e-01 1.988e-02 3.883e-03 1.803e-01
 2.894e-01 1.049e-01 3.432e-01 3.155e-01 6.099e-01 1.752e-01 1.312e-01
 1.557e-01 3.116e-01 1.059e+00 3.982e-01 1.058e-01 2.275e-02 2.982e-01
 1.249e+00 1.480e-01 3.955e-01 5.408e-01 9.584e-01 1.804e-01 3.440e-02
 4.276e-01 4.115e-01 2.402e-01 3.590e-01 8.696e-02 1.517e-02 2.089e-01
 1.717e-01 2.967e-02 9.098e-02 8.217e-02 4.605e-01 1.030e-01 1.839e-02
 1.187e-02 9.647e-01 8.408e-01 6.410e-01 1.096e-02 1.653e-03 7.837e-02
 4.992e-03 2.216e-02 9.473e-02 1.886e-02 1.844e+00 3.741e-02 1.370e-02
 3.405e-03 4.348e-03 4.236e-02 3.280e-02 8.855e-03 1.894e-03 2.329e-02
 6.712e-02 1.024e-02 1.782e-01 1.302e-02 1.852e+00 2.522e-02 2.386e-03
 1.340e-02 3.682e-03 9.933e-02 1.835e-02 2.179e-03 1.176e-03 2.514e-02
 6.678e-02 5.871e-03 7.779e-02 2.892e-02 2.914e-01 2.369e-02 1.042e-02
 1.142e-02 8.557e-03 1.124e-01 2.847e-01 1.579e-02 3.548e-03 2.292e-02
 1.741e-01 6.302e-02 5.077e-01 2.270e-01 1.105e+00 3.227e-01 4.474e-02
 5.376e-02 2.698e-01 2.896e-01 1.335e+00 2.000e-01 4.217e-02 2.372e-01
 5.239e-01 1.271e+00 7.184e-01 4.700e-01 4.590e-01 7.371e-01 1.471e-01
 2.534e-01 2.504e+00 1.094e+00 6.252e-01 7.200e-01 9.738e-02 1.877e-01
 3.083e-02 3.638e-02 7.361e-02 1.454e-02 4.421e-01 3.428e-02 2.306e-02
 2.005e-02 6.641e-02 1.925e-01 1.265e-01 9.826e-02 2.146e-02 2.113e-02]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8575925925925926, 0.6112337630289353)
Iterations 1250
Achieves (18.188967874884245, 1e-05)-DP
