samples  5000  graph  20 40 ER mlp  minibatch size  100  noise  1.0  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617290165629
iteration 1 in outer loop, alpha = 1.5634617290165629, rho = 1.0, h = 1.5634617290165629
cuda
iteration 1 in inner loop,alpha 1.5634617290165629 rho 1.0 h 1.0348993344619615
iteration 2 in inner loop,alpha 1.5634617290165629 rho 10.0 h 0.44327485256017596
iteration 3 in inner loop,alpha 1.5634617290165629 rho 100.0 h 0.13934255419852803
iteration 2 in outer loop, alpha = 15.497717148869366, rho = 100.0, h = 0.13934255419852803
cuda
iteration 1 in inner loop,alpha 15.497717148869366 rho 100.0 h 0.07460289572677681
iteration 2 in inner loop,alpha 15.497717148869366 rho 1000.0 h 0.024764933367762865
iteration 3 in outer loop, alpha = 40.262650516632235, rho = 1000.0, h = 0.024764933367762865
cuda
iteration 1 in inner loop,alpha 40.262650516632235 rho 1000.0 h 0.01022908734000083
iteration 2 in inner loop,alpha 40.262650516632235 rho 10000.0 h 0.0032630688940749053
iteration 4 in outer loop, alpha = 72.89333945738129, rho = 10000.0, h = 0.0032630688940749053
cuda
iteration 1 in inner loop,alpha 72.89333945738129 rho 10000.0 h 0.0012492397787227105
iteration 2 in inner loop,alpha 72.89333945738129 rho 100000.0 h 0.0004957166567010063
iteration 5 in outer loop, alpha = 122.46500512748192, rho = 100000.0, h = 0.0004957166567010063
cuda
iteration 1 in inner loop,alpha 122.46500512748192 rho 100000.0 h 0.00026513258484328617
iteration 6 in outer loop, alpha = 387.59758997076807, rho = 1000000.0, h = 0.00026513258484328617
Threshold 0.3
[[0.    0.012 0.017 2.244 0.034 0.037 0.041 0.006 1.983 0.014 1.814 0.002
  0.11  0.003 0.219 0.776 0.002 0.003 0.034 0.221]
 [0.025 0.005 0.154 0.059 0.343 0.131 0.037 0.002 3.042 0.056 0.021 0.007
  0.611 0.005 1.762 1.604 0.283 0.    0.227 0.297]
 [0.002 0.005 0.003 0.067 0.125 1.022 0.001 0.    2.284 0.077 0.064 0.
  2.219 0.001 1.274 0.788 0.001 0.    3.743 2.089]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.001 0.    0.    0.
  0.    0.    0.136 1.509 0.    0.    0.    0.001]
 [0.    0.    0.    1.646 0.002 0.136 0.001 0.    0.001 0.04  0.022 0.
  0.398 0.009 0.102 0.358 0.001 0.    0.041 0.172]
 [0.    0.    0.    0.125 0.005 0.001 0.    0.    0.005 0.051 0.06  0.
  0.287 0.    1.859 0.433 0.    0.    0.001 0.099]
 [0.002 0.019 0.438 0.052 0.174 0.11  0.005 0.    0.033 0.004 0.015 0.
  0.025 0.002 0.059 1.6   0.01  0.002 0.049 0.023]
 [0.005 0.014 0.373 0.049 0.181 0.108 4.441 0.001 0.137 0.008 0.004 0.037
  0.017 0.006 0.132 0.286 0.067 0.008 0.06  0.04 ]
 [0.    0.    0.    0.065 0.941 0.121 0.001 0.    0.002 0.054 0.047 0.
  0.238 0.    0.281 0.35  0.001 0.    0.017 0.224]
 [0.    0.    0.    0.106 0.    0.    0.    0.    0.    0.001 0.809 0.
  0.    0.    0.173 0.244 0.    0.    0.    0.168]
 [0.    0.    0.    0.116 0.001 0.    0.001 0.    0.    0.    0.002 0.
  0.    0.    1.26  0.254 0.    0.    0.    2.247]
 [0.004 0.004 2.456 0.065 0.015 0.188 3.558 0.009 0.224 0.012 0.012 0.002
  0.239 0.004 0.552 0.325 0.008 0.041 0.213 0.158]
 [0.    0.    0.    0.135 0.    0.001 0.    0.    0.001 3.175 0.14  0.
  0.002 0.    0.332 0.58  0.001 0.    0.    0.399]
 [0.002 0.008 0.019 0.047 0.024 0.104 0.004 0.014 0.093 0.021 0.076 0.02
  0.27  0.    0.166 0.188 0.008 0.013 2.62  0.253]
 [0.    0.    0.    0.006 0.001 0.    0.    0.    0.001 0.    0.    0.
  0.    0.    0.006 0.24  0.    0.    0.    0.005]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.009 0.006 0.    0.    0.    0.   ]
 [0.005 0.006 0.235 0.068 0.24  0.225 0.036 0.001 0.272 0.061 0.051 0.007
  0.41  0.002 1.778 1.434 0.003 0.001 0.326 0.427]
 [0.004 4.261 0.015 0.056 0.254 0.012 0.014 0.005 0.585 0.002 0.004 0.004
  1.331 0.003 0.177 0.15  2.98  0.001 1.412 0.942]
 [0.002 0.001 0.    0.062 0.012 1.171 0.    0.    0.081 0.062 0.016 0.
  0.945 0.    0.368 1.36  0.    0.    0.003 0.482]
 [0.    0.    0.    2.074 0.    0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.215 0.322 0.    0.    0.    0.004]]
[[0.    0.    0.    2.244 0.    0.    0.    0.    1.983 0.    1.814 0.
  0.    0.    0.    0.776 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.343 0.    0.    0.    3.042 0.    0.    0.
  0.611 0.    1.762 1.604 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.022 0.    0.    2.284 0.    0.    0.
  2.219 0.    1.274 0.788 0.    0.    3.743 2.089]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.509 0.    0.    0.    0.   ]
 [0.    0.    0.    1.646 0.    0.    0.    0.    0.    0.    0.    0.
  0.398 0.    0.    0.358 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.859 0.433 0.    0.    0.    0.   ]
 [0.    0.    0.438 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.6   0.    0.    0.    0.   ]
 [0.    0.    0.373 0.    0.    0.    4.441 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.941 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.35  0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.809 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.26  0.    0.    0.    0.    2.247]
 [0.    0.    2.456 0.    0.    0.    3.558 0.    0.    0.    0.    0.
  0.    0.    0.552 0.325 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.175 0.    0.
  0.    0.    0.332 0.58  0.    0.    0.    0.399]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.62  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.41  0.    1.778 1.434 0.    0.    0.326 0.427]
 [0.    4.261 0.    0.    0.    0.    0.    0.    0.585 0.    0.    0.
  1.331 0.    0.    0.    2.98  0.    1.412 0.942]
 [0.    0.    0.    0.    0.    1.171 0.    0.    0.    0.    0.    0.
  0.945 0.    0.368 1.36  0.    0.    0.    0.482]
 [0.    0.    0.    2.074 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.322 0.    0.    0.    0.   ]]
{'fdr': 0.3103448275862069, 'tpr': 1.0, 'fpr': 0.12, 'f1': 0.8163265306122449, 'shd': 18, 'npred': 58, 'ntrue': 40}
[1.230e-02 1.726e-02 2.244e+00 3.422e-02 3.651e-02 4.142e-02 6.367e-03
 1.983e+00 1.419e-02 1.814e+00 1.720e-03 1.097e-01 2.997e-03 2.192e-01
 7.764e-01 1.913e-03 2.632e-03 3.398e-02 2.209e-01 2.498e-02 1.538e-01
 5.882e-02 3.427e-01 1.305e-01 3.742e-02 1.523e-03 3.042e+00 5.647e-02
 2.090e-02 7.295e-03 6.108e-01 5.424e-03 1.762e+00 1.604e+00 2.830e-01
 4.672e-04 2.266e-01 2.967e-01 2.072e-03 5.117e-03 6.707e-02 1.254e-01
 1.022e+00 5.658e-04 1.457e-04 2.284e+00 7.743e-02 6.440e-02 4.253e-04
 2.219e+00 1.226e-03 1.274e+00 7.881e-01 7.273e-04 1.124e-04 3.743e+00
 2.089e+00 1.854e-05 1.601e-04 2.730e-04 7.930e-04 1.064e-03 3.955e-04
 1.084e-04 5.776e-04 2.702e-05 6.629e-05 1.620e-04 7.296e-05 1.312e-05
 1.357e-01 1.509e+00 3.970e-04 1.361e-04 3.258e-04 1.273e-03 1.008e-05
 1.418e-04 2.139e-04 1.646e+00 1.360e-01 1.437e-03 2.341e-05 7.529e-04
 4.023e-02 2.172e-02 1.050e-04 3.979e-01 9.183e-03 1.022e-01 3.578e-01
 1.206e-03 4.630e-05 4.065e-02 1.725e-01 1.681e-04 1.914e-04 1.354e-04
 1.253e-01 4.994e-03 3.469e-04 5.983e-06 4.943e-03 5.097e-02 6.030e-02
 5.699e-05 2.871e-01 1.284e-05 1.859e+00 4.330e-01 3.831e-04 3.472e-05
 7.742e-04 9.880e-02 2.092e-03 1.909e-02 4.384e-01 5.250e-02 1.736e-01
 1.097e-01 2.288e-04 3.281e-02 3.849e-03 1.519e-02 2.965e-05 2.492e-02
 1.571e-03 5.940e-02 1.600e+00 1.006e-02 2.434e-03 4.933e-02 2.298e-02
 5.225e-03 1.400e-02 3.734e-01 4.906e-02 1.807e-01 1.080e-01 4.441e+00
 1.365e-01 8.477e-03 3.609e-03 3.736e-02 1.676e-02 5.846e-03 1.325e-01
 2.864e-01 6.694e-02 8.161e-03 6.003e-02 4.003e-02 6.595e-05 2.307e-04
 2.620e-04 6.487e-02 9.407e-01 1.206e-01 1.093e-03 2.142e-05 5.434e-02
 4.674e-02 1.320e-04 2.378e-01 2.376e-04 2.809e-01 3.499e-01 7.862e-04
 4.811e-05 1.729e-02 2.237e-01 3.350e-05 3.446e-05 6.076e-05 1.055e-01
 1.147e-04 2.303e-04 1.885e-05 2.780e-05 1.206e-04 8.094e-01 5.742e-05
 1.801e-04 3.149e-05 1.731e-01 2.440e-01 9.474e-05 1.715e-05 3.442e-05
 1.680e-01 8.324e-05 9.592e-05 6.040e-05 1.157e-01 6.997e-04 2.320e-04
 7.903e-04 4.261e-05 2.370e-04 9.132e-05 6.928e-05 2.662e-05 7.396e-06
 1.260e+00 2.536e-01 4.458e-04 1.110e-05 6.831e-05 2.247e+00 3.504e-03
 3.670e-03 2.456e+00 6.534e-02 1.519e-02 1.877e-01 3.558e+00 9.005e-03
 2.243e-01 1.183e-02 1.192e-02 2.388e-01 4.327e-03 5.522e-01 3.248e-01
 8.296e-03 4.059e-02 2.125e-01 1.580e-01 4.829e-05 7.566e-05 2.068e-04
 1.349e-01 7.088e-05 1.108e-03 9.087e-05 4.632e-05 8.334e-04 3.175e+00
 1.398e-01 8.102e-05 3.461e-05 3.319e-01 5.800e-01 6.076e-04 5.260e-05
 2.562e-04 3.987e-01 2.495e-03 7.829e-03 1.874e-02 4.694e-02 2.410e-02
 1.039e-01 3.781e-03 1.438e-02 9.344e-02 2.093e-02 7.627e-02 2.019e-02
 2.697e-01 1.663e-01 1.878e-01 8.340e-03 1.340e-02 2.620e+00 2.527e-01
 1.720e-05 4.935e-04 1.201e-04 6.264e-03 6.274e-04 1.428e-04 1.290e-04
 3.501e-05 5.289e-04 1.111e-04 2.040e-04 5.749e-05 3.570e-05 3.393e-05
 2.400e-01 4.363e-04 1.073e-04 7.430e-05 4.838e-03 1.977e-05 2.509e-04
 5.234e-05 1.456e-04 3.662e-05 2.068e-04 4.849e-04 5.596e-05 7.996e-05
 7.903e-05 7.042e-05 2.063e-05 5.423e-05 2.014e-05 8.736e-03 1.910e-05
 4.276e-05 8.065e-05 9.007e-05 5.478e-03 6.194e-03 2.353e-01 6.814e-02
 2.398e-01 2.251e-01 3.628e-02 1.268e-03 2.723e-01 6.083e-02 5.081e-02
 7.491e-03 4.098e-01 1.842e-03 1.778e+00 1.434e+00 5.431e-04 3.261e-01
 4.274e-01 4.085e-03 4.261e+00 1.488e-02 5.619e-02 2.544e-01 1.215e-02
 1.408e-02 4.908e-03 5.845e-01 2.225e-03 4.449e-03 3.902e-03 1.331e+00
 3.357e-03 1.765e-01 1.498e-01 2.980e+00 1.412e+00 9.416e-01 1.880e-03
 1.493e-03 4.811e-04 6.165e-02 1.181e-02 1.171e+00 2.403e-04 7.305e-05
 8.109e-02 6.157e-02 1.552e-02 1.459e-04 9.452e-01 1.513e-05 3.675e-01
 1.360e+00 3.119e-04 1.795e-04 4.819e-01 8.200e-05 7.530e-05 2.403e-04
 2.074e+00 4.180e-04 1.178e-03 2.396e-04 1.042e-04 4.643e-04 2.877e-05
 5.577e-05 1.204e-04 3.622e-05 2.286e-05 2.152e-01 3.225e-01 3.313e-05
 2.277e-04 1.541e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9985294117647059, 0.9884866910253907)
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
total norm for a microbatch 61.34044779254333 clip 9.735638618462225
total norm for a microbatch 69.620201352908 clip 24.671505942188162
total norm for a microbatch 53.0645325242143 clip 26.799095292333973
total norm for a microbatch 61.49288368191707 clip 54.42411819421261
total norm for a microbatch 35.78299764725255 clip 48.82281073418443
cuda
Objective function 137.17 = squared loss an data 133.81 + 0.5*rho*h**2 2.592865 + alpha*h 0.000000 + L2reg 0.49 + L1reg 0.28 ; SHD = 126 ; DAG False
Proportion of microbatches that were clipped  0.9304583068109484
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.2772199599793126
iteration 1 in outer loop, alpha = 2.2772199599793126, rho = 1.0, h = 2.2772199599793126
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 142.36 = squared loss an data 133.81 + 0.5*rho*h**2 2.592865 + alpha*h 5.185731 + L2reg 0.49 + L1reg 0.28 ; SHD = 126 ; DAG False
total norm for a microbatch 68.68463685851289 clip 5.796128526196119
total norm for a microbatch 90.22638548064972 clip 7.00071832024005
total norm for a microbatch 71.71833739789116 clip 38.02246464080932
total norm for a microbatch 135.69448049409195 clip 58.23645933809036
total norm for a microbatch 113.54547711274316 clip 68.87358411556467
cuda
Objective function 65.81 = squared loss an data 56.33 + 0.5*rho*h**2 2.854446 + alpha*h 5.441027 + L2reg 0.91 + L1reg 0.27 ; SHD = 93 ; DAG False
Proportion of microbatches that were clipped  0.9472915990917937
iteration 1 in inner loop, alpha 2.2772199599793126 rho 1.0 h 2.3893286660317443
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 91.50 = squared loss an data 56.33 + 0.5*rho*h**2 28.544457 + alpha*h 5.441027 + L2reg 0.91 + L1reg 0.27 ; SHD = 93 ; DAG False
total norm for a microbatch 148.94347657462103 clip 3.856639643488005
total norm for a microbatch 94.56514005624554 clip 20.829888840481726
total norm for a microbatch 84.39695611409992 clip 39.41742534075676
total norm for a microbatch 83.80404138441665 clip 86.81494944431877
total norm for a microbatch 158.21186582092818 clip 111.3873044571474
cuda
Objective function 47.51 = squared loss an data 36.88 + 0.5*rho*h**2 6.611178 + alpha*h 2.618541 + L2reg 1.16 + L1reg 0.24 ; SHD = 70 ; DAG False
Proportion of microbatches that were clipped  0.9630909376472436
iteration 2 in inner loop, alpha 2.2772199599793126 rho 10.0 h 1.149884998474704
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 107.01 = squared loss an data 36.88 + 0.5*rho*h**2 66.111775 + alpha*h 2.618541 + L2reg 1.16 + L1reg 0.24 ; SHD = 70 ; DAG False
total norm for a microbatch 167.85067449528768 clip 1.7191142432582052
total norm for a microbatch 205.2864434222659 clip 23.116110703380883
total norm for a microbatch 189.07310567851837 clip 32.907788644336755
total norm for a microbatch 163.35249498401524 clip 137.23228658592726
cuda
Objective function 49.51 = squared loss an data 37.55 + 0.5*rho*h**2 9.427691 + alpha*h 0.988832 + L2reg 1.33 + L1reg 0.21 ; SHD = 61 ; DAG False
Proportion of microbatches that were clipped  0.9751601312294954
iteration 3 in inner loop, alpha 2.2772199599793126 rho 100.0 h 0.43422784794635305
iteration 2 in outer loop, alpha = 45.70000475461462, rho = 100.0, h = 0.43422784794635305
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 68.36 = squared loss an data 37.55 + 0.5*rho*h**2 9.427691 + alpha*h 19.844215 + L2reg 1.33 + L1reg 0.21 ; SHD = 61 ; DAG False
total norm for a microbatch 238.41622730378705 clip 1.7234391714497566
total norm for a microbatch 274.528780070632 clip 1.7234391714497566
total norm for a microbatch 140.32393695923034 clip 2.4961687087424425
total norm for a microbatch 135.00224447107527 clip 6.133355975213887
total norm for a microbatch 229.88098004686114 clip 7.901057120076323
total norm for a microbatch 165.48407764283792 clip 13.659422633934094
total norm for a microbatch 227.55691185023912 clip 16.47420604951095
total norm for a microbatch 289.8754348621618 clip 91.14236758354487
total norm for a microbatch 166.11685687316424 clip 155.70330210341638
total norm for a microbatch 172.44056120044294 clip 166.6234265081439
cuda
Objective function 55.65 = squared loss an data 37.46 + 0.5*rho*h**2 3.837220 + alpha*h 12.660172 + L2reg 1.49 + L1reg 0.20 ; SHD = 52 ; DAG False
Proportion of microbatches that were clipped  0.9766606822262118
iteration 1 in inner loop, alpha 45.70000475461462 rho 100.0 h 0.27702779463141525
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 90.18 = squared loss an data 37.46 + 0.5*rho*h**2 38.372199 + alpha*h 12.660172 + L2reg 1.49 + L1reg 0.20 ; SHD = 52 ; DAG False
total norm for a microbatch 260.31417454306006 clip 2.08949831268089
total norm for a microbatch 180.6803235689597 clip 6.143904674117205
total norm for a microbatch 243.10863375134352 clip 13.769505993346549
cuda
Objective function 51.39 = squared loss an data 36.62 + 0.5*rho*h**2 7.402757 + alpha*h 5.560681 + L2reg 1.62 + L1reg 0.19 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.9776821297624742
iteration 2 in inner loop, alpha 45.70000475461462 rho 1000.0 h 0.12167791084080193
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 118.01 = squared loss an data 36.62 + 0.5*rho*h**2 74.027570 + alpha*h 5.560681 + L2reg 1.62 + L1reg 0.19 ; SHD = 51 ; DAG True
total norm for a microbatch 260.4230712719746 clip 5.06717997469573
total norm for a microbatch 245.68150950733505 clip 47.46975445010847
total norm for a microbatch 274.5042337231282 clip 52.02686497924699
total norm for a microbatch 243.28289420090655 clip 108.16866203959832
total norm for a microbatch 262.44479356597657 clip 157.12828110264869
cuda
Objective function 51.95 = squared loss an data 36.95 + 0.5*rho*h**2 10.979817 + alpha*h 2.141553 + L2reg 1.70 + L1reg 0.18 ; SHD = 62 ; DAG True
Proportion of microbatches that were clipped  0.9854674632649766
iteration 3 in inner loop, alpha 45.70000475461462 rho 10000.0 h 0.04686110833115009
iteration 3 in outer loop, alpha = 514.3110880661155, rho = 10000.0, h = 0.04686110833115009
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 73.91 = squared loss an data 36.95 + 0.5*rho*h**2 10.979817 + alpha*h 24.101188 + L2reg 1.70 + L1reg 0.18 ; SHD = 62 ; DAG True
total norm for a microbatch 296.2809800715572 clip 1.0
total norm for a microbatch 317.71099708801654 clip 1.401524849263198
total norm for a microbatch 190.75292605002286 clip 24.48492849976962
total norm for a microbatch 182.00974470824954 clip 31.635876672410046
total norm for a microbatch 350.7073618533865 clip 60.189203515601456
cuda
Objective function 55.25 = squared loss an data 34.59 + 0.5*rho*h**2 4.035237 + alpha*h 14.610847 + L2reg 1.84 + L1reg 0.18 ; SHD = 65 ; DAG True
Proportion of microbatches that were clipped  0.9907658016239452
iteration 1 in inner loop, alpha 514.3110880661155 rho 10000.0 h 0.02840857807199626
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 91.57 = squared loss an data 34.59 + 0.5*rho*h**2 40.352365 + alpha*h 14.610847 + L2reg 1.84 + L1reg 0.18 ; SHD = 65 ; DAG True
total norm for a microbatch 648.6591543268003 clip 1.0
total norm for a microbatch 663.9662118816822 clip 1.7444983489094892
total norm for a microbatch 217.33681713403237 clip 10.755457944377467
total norm for a microbatch 266.2044431238786 clip 20.31047702811882
total norm for a microbatch 338.2939479933872 clip 54.65658902491213
cuda
Objective function 56.39 = squared loss an data 34.43 + 0.5*rho*h**2 11.924523 + alpha*h 7.942576 + L2reg 1.91 + L1reg 0.19 ; SHD = 70 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 514.3110880661155 rho 100000.0 h 0.015443136596992701
iteration 4 in outer loop, alpha = 15957.447685058816, rho = 1000000.0, h = 0.015443136596992701
Threshold 0.3
[[0.008 0.081 0.023 0.218 0.222 0.062 0.296 0.042 0.453 0.256 0.294 0.132
  0.029 0.212 0.221 0.249 0.103 0.051 0.537 0.041]
 [0.058 0.008 0.028 0.107 0.051 0.057 0.114 0.021 1.026 0.186 0.074 0.034
  0.02  0.081 0.404 0.178 0.009 0.007 0.167 0.037]
 [0.274 0.285 0.007 0.366 0.271 0.215 0.35  0.025 0.97  0.295 0.145 0.546
  0.021 0.167 0.306 0.541 0.143 0.108 2.222 0.623]
 [0.028 0.076 0.014 0.007 0.008 0.031 0.051 0.011 0.068 0.037 0.032 0.026
  0.011 0.025 0.095 0.041 0.033 0.012 0.156 0.006]
 [0.038 0.078 0.036 1.049 0.009 0.025 0.086 0.008 0.682 0.105 0.19  0.092
  0.027 0.06  0.169 0.066 0.03  0.011 0.378 0.027]
 [0.132 0.145 0.036 0.181 0.268 0.009 0.136 0.029 0.601 0.185 0.265 0.057
  0.03  0.119 0.214 0.158 0.192 0.032 0.958 0.035]
 [0.024 0.051 0.022 0.118 0.078 0.025 0.014 0.003 0.099 0.035 0.062 0.046
  0.017 0.045 0.061 0.27  0.052 0.012 0.171 0.029]
 [0.224 0.299 0.189 0.364 0.593 0.307 3.07  0.009 0.327 0.422 0.317 0.59
  0.064 0.347 0.452 0.713 0.317 0.079 0.74  0.153]
 [0.021 0.008 0.007 0.095 0.012 0.015 0.052 0.01  0.01  0.028 0.017 0.019
  0.008 0.02  0.054 0.099 0.007 0.006 0.047 0.016]
 [0.026 0.048 0.036 0.236 0.081 0.042 0.196 0.022 0.259 0.009 0.061 0.074
  0.009 0.045 0.1   0.18  0.035 0.021 0.303 0.056]
 [0.048 0.086 0.07  0.342 0.052 0.031 0.138 0.023 0.325 0.118 0.01  0.109
  0.073 0.096 0.077 0.134 0.073 0.018 0.253 0.094]
 [0.07  0.238 0.017 0.23  0.137 0.081 0.169 0.013 0.29  0.138 0.08  0.008
  0.025 0.066 0.117 0.184 0.051 0.014 0.394 0.036]
 [0.232 0.312 0.45  0.409 0.232 0.365 0.492 0.097 0.488 1.096 0.127 0.41
  0.009 0.237 0.535 0.388 0.189 0.065 0.365 0.348]
 [0.036 0.083 0.022 0.386 0.124 0.085 0.151 0.017 0.387 0.169 0.069 0.148
  0.033 0.01  0.446 0.125 0.062 0.013 0.407 0.096]
 [0.04  0.026 0.02  0.107 0.062 0.036 0.13  0.015 0.129 0.078 0.095 0.056
  0.011 0.018 0.009 0.177 0.008 0.007 0.075 0.029]
 [0.027 0.051 0.013 0.14  0.101 0.041 0.044 0.006 0.085 0.07  0.054 0.044
  0.022 0.061 0.058 0.006 0.025 0.012 0.092 0.014]
 [0.092 0.697 0.028 0.247 0.25  0.055 0.204 0.03  0.448 0.236 0.086 0.238
  0.041 0.173 1.036 0.167 0.008 0.006 0.282 0.043]
 [0.171 1.525 0.115 0.504 0.485 0.184 0.406 0.085 0.614 0.277 0.384 0.317
  0.17  0.529 0.523 0.445 1.462 0.007 0.523 0.112]
 [0.016 0.069 0.004 0.056 0.027 0.008 0.035 0.008 0.19  0.021 0.03  0.017
  0.009 0.028 0.066 0.067 0.023 0.009 0.011 0.018]
 [0.147 0.238 0.014 1.412 0.256 0.159 0.267 0.027 0.425 0.262 0.086 0.286
  0.019 0.103 0.304 0.498 0.146 0.065 0.481 0.009]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.453 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.537 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.026 0.    0.    0.
  0.    0.    0.404 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.366 0.    0.    0.35  0.    0.97  0.    0.    0.546
  0.    0.    0.306 0.541 0.    0.    2.222 0.623]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.049 0.    0.    0.    0.    0.682 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.378 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.601 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.958 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.364 0.593 0.307 3.07  0.    0.327 0.422 0.317 0.59
  0.    0.347 0.452 0.713 0.317 0.    0.74  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.303 0.   ]
 [0.    0.    0.    0.342 0.    0.    0.    0.    0.325 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.394 0.   ]
 [0.    0.312 0.45  0.409 0.    0.365 0.492 0.    0.488 1.096 0.    0.41
  0.    0.    0.535 0.388 0.    0.    0.365 0.348]
 [0.    0.    0.    0.386 0.    0.    0.    0.    0.387 0.    0.    0.
  0.    0.    0.446 0.    0.    0.    0.407 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.697 0.    0.    0.    0.    0.    0.    0.448 0.    0.    0.
  0.    0.    1.036 0.    0.    0.    0.    0.   ]
 [0.    1.525 0.    0.504 0.485 0.    0.406 0.    0.614 0.    0.384 0.317
  0.    0.529 0.523 0.445 1.462 0.    0.523 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.412 0.    0.    0.    0.    0.425 0.    0.    0.
  0.    0.    0.304 0.498 0.    0.    0.481 0.   ]]
{'fdr': 0.7428571428571429, 'tpr': 0.45, 'fpr': 0.3466666666666667, 'f1': 0.32727272727272727, 'shd': 70, 'npred': 70, 'ntrue': 40}
[0.081 0.023 0.218 0.222 0.062 0.296 0.042 0.453 0.256 0.294 0.132 0.029
 0.212 0.221 0.249 0.103 0.051 0.537 0.041 0.058 0.028 0.107 0.051 0.057
 0.114 0.021 1.026 0.186 0.074 0.034 0.02  0.081 0.404 0.178 0.009 0.007
 0.167 0.037 0.274 0.285 0.366 0.271 0.215 0.35  0.025 0.97  0.295 0.145
 0.546 0.021 0.167 0.306 0.541 0.143 0.108 2.222 0.623 0.028 0.076 0.014
 0.008 0.031 0.051 0.011 0.068 0.037 0.032 0.026 0.011 0.025 0.095 0.041
 0.033 0.012 0.156 0.006 0.038 0.078 0.036 1.049 0.025 0.086 0.008 0.682
 0.105 0.19  0.092 0.027 0.06  0.169 0.066 0.03  0.011 0.378 0.027 0.132
 0.145 0.036 0.181 0.268 0.136 0.029 0.601 0.185 0.265 0.057 0.03  0.119
 0.214 0.158 0.192 0.032 0.958 0.035 0.024 0.051 0.022 0.118 0.078 0.025
 0.003 0.099 0.035 0.062 0.046 0.017 0.045 0.061 0.27  0.052 0.012 0.171
 0.029 0.224 0.299 0.189 0.364 0.593 0.307 3.07  0.327 0.422 0.317 0.59
 0.064 0.347 0.452 0.713 0.317 0.079 0.74  0.153 0.021 0.008 0.007 0.095
 0.012 0.015 0.052 0.01  0.028 0.017 0.019 0.008 0.02  0.054 0.099 0.007
 0.006 0.047 0.016 0.026 0.048 0.036 0.236 0.081 0.042 0.196 0.022 0.259
 0.061 0.074 0.009 0.045 0.1   0.18  0.035 0.021 0.303 0.056 0.048 0.086
 0.07  0.342 0.052 0.031 0.138 0.023 0.325 0.118 0.109 0.073 0.096 0.077
 0.134 0.073 0.018 0.253 0.094 0.07  0.238 0.017 0.23  0.137 0.081 0.169
 0.013 0.29  0.138 0.08  0.025 0.066 0.117 0.184 0.051 0.014 0.394 0.036
 0.232 0.312 0.45  0.409 0.232 0.365 0.492 0.097 0.488 1.096 0.127 0.41
 0.237 0.535 0.388 0.189 0.065 0.365 0.348 0.036 0.083 0.022 0.386 0.124
 0.085 0.151 0.017 0.387 0.169 0.069 0.148 0.033 0.446 0.125 0.062 0.013
 0.407 0.096 0.04  0.026 0.02  0.107 0.062 0.036 0.13  0.015 0.129 0.078
 0.095 0.056 0.011 0.018 0.177 0.008 0.007 0.075 0.029 0.027 0.051 0.013
 0.14  0.101 0.041 0.044 0.006 0.085 0.07  0.054 0.044 0.022 0.061 0.058
 0.025 0.012 0.092 0.014 0.092 0.697 0.028 0.247 0.25  0.055 0.204 0.03
 0.448 0.236 0.086 0.238 0.041 0.173 1.036 0.167 0.006 0.282 0.043 0.171
 1.525 0.115 0.504 0.485 0.184 0.406 0.085 0.614 0.277 0.384 0.317 0.17
 0.529 0.523 0.445 1.462 0.523 0.112 0.016 0.069 0.004 0.056 0.027 0.008
 0.035 0.008 0.19  0.021 0.03  0.017 0.009 0.028 0.066 0.067 0.023 0.009
 0.018 0.147 0.238 0.014 1.412 0.256 0.159 0.267 0.027 0.425 0.262 0.086
 0.286 0.019 0.103 0.304 0.498 0.146 0.065 0.481]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7420588235294117, 0.45023518664337703)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
