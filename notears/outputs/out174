samples  5000  graph  20 60 ER mim  minibatch size  100  noise  0.8  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.09213537717594988
iteration 3 in outer loop, alpha = 15.699588471753614, rho = 100.0, h = 0.09213537717594988
cuda
iteration 1 in inner loop,alpha 15.699588471753614 rho 100.0 h 0.05367211647120129
iteration 2 in inner loop,alpha 15.699588471753614 rho 1000.0 h 0.01978300963797608
iteration 4 in outer loop, alpha = 35.48259810972969, rho = 1000.0, h = 0.01978300963797608
cuda
iteration 1 in inner loop,alpha 35.48259810972969 rho 1000.0 h 0.01090959889101839
iteration 2 in inner loop,alpha 35.48259810972969 rho 10000.0 h 0.0033893587627424893
iteration 5 in outer loop, alpha = 69.37618573715459, rho = 10000.0, h = 0.0033893587627424893
cuda
iteration 1 in inner loop,alpha 69.37618573715459 rho 10000.0 h 0.0014690778404897742
iteration 2 in inner loop,alpha 69.37618573715459 rho 100000.0 h 0.0005756136893566577
iteration 6 in outer loop, alpha = 126.93755467282035, rho = 100000.0, h = 0.0005756136893566577
cuda
iteration 1 in inner loop,alpha 126.93755467282035 rho 100000.0 h 0.00026984638721572196
iteration 7 in outer loop, alpha = 396.7839418885423, rho = 1000000.0, h = 0.00026984638721572196
Threshold 0.3
[[0.    0.025 0.009 0.053 0.154 0.198 0.058 0.001 0.044 0.082 0.496 0.667
  1.11  0.002 0.088 0.002 0.078 2.578 0.526 0.018]
 [0.    0.001 0.    0.039 0.079 0.476 0.19  0.    0.007 0.029 0.041 0.
  0.    0.    0.073 0.146 0.017 0.    0.122 0.801]
 [0.002 0.29  0.002 0.126 0.539 0.153 0.1   0.001 1.506 0.886 0.119 0.004
  0.006 0.    0.032 0.214 0.058 0.046 0.135 0.115]
 [0.    0.006 0.    0.002 0.002 0.004 0.002 0.    0.    0.001 0.002 0.
  0.    0.    0.05  0.002 0.    0.    0.002 0.024]
 [0.    0.003 0.    0.175 0.002 0.005 0.004 0.    0.001 0.009 0.008 0.
  0.001 0.    0.039 0.104 0.003 0.    0.001 0.003]
 [0.    0.002 0.006 0.099 0.031 0.005 0.013 0.    0.04  0.229 0.002 0.
  0.003 0.    1.204 0.151 0.008 0.    0.435 0.051]
 [0.    0.003 0.001 0.227 0.314 0.192 0.003 0.    0.002 0.004 0.003 0.001
  0.002 0.    0.096 0.012 0.001 0.    0.004 0.013]
 [0.004 0.099 0.001 1.268 0.044 0.064 0.198 0.    0.393 0.072 0.082 2.216
  1.376 0.026 0.109 0.013 0.03  2.253 0.045 0.04 ]
 [0.    0.015 0.    1.448 1.387 0.005 0.093 0.    0.003 0.508 1.271 0.
  0.001 0.    0.426 0.153 0.002 0.    0.005 0.016]
 [0.    0.001 0.001 0.052 0.013 0.011 0.09  0.    0.003 0.003 1.081 0.001
  0.001 0.    0.085 0.006 0.038 0.    0.002 0.667]
 [0.    0.006 0.    0.034 0.01  0.008 0.315 0.    0.    0.001 0.002 0.
  0.    0.    0.016 0.004 0.347 0.    0.002 0.74 ]
 [0.    2.095 0.002 1.584 1.304 0.037 0.623 0.    1.057 0.053 1.129 0.001
  0.001 0.003 0.003 0.928 0.414 0.    0.072 0.121]
 [0.    0.034 0.002 0.185 0.077 0.152 0.488 0.    0.002 0.637 0.055 0.577
  0.003 0.004 1.603 0.154 0.258 0.    0.013 0.062]
 [0.002 1.038 4.622 0.098 0.031 0.194 0.107 0.002 0.862 0.187 0.047 0.022
  0.053 0.    0.055 0.691 0.002 0.1   0.728 1.199]
 [0.    0.001 0.    0.014 0.02  0.001 0.002 0.    0.001 0.005 0.002 0.
  0.    0.    0.002 0.008 0.001 0.    0.002 0.002]
 [0.    0.002 0.002 0.085 0.006 0.008 0.003 0.    0.004 0.329 0.033 0.
  0.    0.    0.041 0.003 0.001 0.    0.001 0.089]
 [0.    0.011 0.001 1.012 0.114 0.021 0.968 0.    0.001 0.001 0.002 0.001
  0.002 0.    0.555 0.001 0.002 0.    0.004 0.021]
 [0.    0.054 0.004 0.283 0.051 0.062 0.067 0.    0.103 0.048 0.109 0.863
  2.171 0.002 0.005 0.074 0.051 0.002 0.021 1.255]
 [0.    0.003 0.01  0.099 0.771 0.008 0.01  0.001 0.452 0.368 0.084 0.001
  0.003 0.    0.7   1.085 0.002 0.005 0.004 0.064]
 [0.    0.    0.    0.013 0.009 0.007 0.069 0.    0.001 0.001 0.    0.
  0.    0.    0.056 0.002 0.002 0.    0.001 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.496 0.667
  1.11  0.    0.    0.    0.    2.578 0.526 0.   ]
 [0.    0.    0.    0.    0.    0.476 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.801]
 [0.    0.    0.    0.    0.539 0.    0.    0.    1.506 0.886 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.204 0.    0.    0.    0.435 0.   ]
 [0.    0.    0.    0.    0.314 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.268 0.    0.    0.    0.    0.393 0.    0.    2.216
  1.376 0.    0.    0.    0.    2.253 0.    0.   ]
 [0.    0.    0.    1.448 1.387 0.    0.    0.    0.    0.508 1.271 0.
  0.    0.    0.426 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.081 0.
  0.    0.    0.    0.    0.    0.    0.    0.667]
 [0.    0.    0.    0.    0.    0.    0.315 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.347 0.    0.    0.74 ]
 [0.    2.095 0.    1.584 1.304 0.    0.623 0.    1.057 0.    1.129 0.
  0.    0.    0.    0.928 0.414 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.488 0.    0.    0.637 0.    0.577
  0.    0.    1.603 0.    0.    0.    0.    0.   ]
 [0.    1.038 4.622 0.    0.    0.    0.    0.    0.862 0.    0.    0.
  0.    0.    0.    0.691 0.    0.    0.728 1.199]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.329 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.012 0.    0.    0.968 0.    0.    0.    0.    0.
  0.    0.    0.555 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.863
  2.171 0.    0.    0.    0.    0.    0.    1.255]
 [0.    0.    0.    0.    0.771 0.    0.    0.    0.452 0.368 0.    0.
  0.    0.    0.7   1.085 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.15517241379310345, 'tpr': 0.8166666666666667, 'fpr': 0.06923076923076923, 'f1': 0.8305084745762712, 'shd': 14, 'npred': 58, 'ntrue': 60}
[2.547e-02 9.343e-03 5.263e-02 1.536e-01 1.978e-01 5.768e-02 1.161e-03
 4.420e-02 8.159e-02 4.956e-01 6.671e-01 1.110e+00 2.101e-03 8.771e-02
 2.438e-03 7.765e-02 2.578e+00 5.257e-01 1.772e-02 8.707e-06 3.021e-04
 3.870e-02 7.881e-02 4.756e-01 1.898e-01 1.189e-05 6.895e-03 2.868e-02
 4.113e-02 9.192e-05 1.350e-04 1.710e-05 7.294e-02 1.456e-01 1.741e-02
 1.443e-04 1.217e-01 8.015e-01 1.861e-03 2.897e-01 1.265e-01 5.392e-01
 1.527e-01 9.957e-02 1.412e-03 1.506e+00 8.860e-01 1.187e-01 3.756e-03
 6.387e-03 3.645e-05 3.181e-02 2.142e-01 5.813e-02 4.582e-02 1.349e-01
 1.154e-01 2.072e-05 5.545e-03 4.665e-05 2.468e-03 3.803e-03 2.164e-03
 3.501e-06 3.892e-04 1.183e-03 2.337e-03 1.061e-04 2.153e-04 9.748e-06
 5.028e-02 1.558e-03 3.483e-04 3.050e-05 2.007e-03 2.371e-02 1.576e-04
 2.723e-03 1.948e-04 1.746e-01 4.749e-03 3.585e-03 3.301e-06 5.141e-04
 8.640e-03 7.916e-03 1.146e-04 7.567e-04 1.227e-05 3.886e-02 1.040e-01
 2.979e-03 1.036e-04 7.020e-04 3.334e-03 8.988e-05 1.835e-03 5.656e-03
 9.945e-02 3.058e-02 1.305e-02 1.212e-04 4.001e-02 2.294e-01 2.110e-03
 3.658e-04 2.954e-03 7.163e-05 1.204e+00 1.512e-01 8.102e-03 4.601e-04
 4.352e-01 5.117e-02 3.479e-05 2.790e-03 1.005e-03 2.272e-01 3.144e-01
 1.922e-01 1.478e-04 1.551e-03 4.108e-03 2.876e-03 6.541e-04 1.783e-03
 6.850e-05 9.620e-02 1.232e-02 9.994e-04 1.038e-04 3.676e-03 1.304e-02
 3.528e-03 9.889e-02 1.062e-03 1.268e+00 4.409e-02 6.391e-02 1.976e-01
 3.926e-01 7.196e-02 8.160e-02 2.216e+00 1.376e+00 2.582e-02 1.085e-01
 1.283e-02 2.986e-02 2.253e+00 4.495e-02 4.014e-02 3.335e-05 1.532e-02
 2.424e-04 1.448e+00 1.387e+00 4.874e-03 9.252e-02 1.096e-04 5.085e-01
 1.271e+00 2.016e-04 5.790e-04 2.780e-05 4.258e-01 1.531e-01 2.343e-03
 1.444e-04 5.005e-03 1.628e-02 2.529e-05 1.473e-03 5.082e-04 5.227e-02
 1.340e-02 1.118e-02 8.979e-02 2.282e-05 2.500e-03 1.081e+00 5.594e-04
 1.484e-03 4.256e-06 8.530e-02 5.745e-03 3.804e-02 1.454e-04 2.499e-03
 6.666e-01 6.811e-05 6.332e-03 1.506e-04 3.381e-02 1.027e-02 8.274e-03
 3.152e-01 4.584e-06 3.816e-04 7.935e-04 1.285e-04 1.319e-04 3.986e-06
 1.557e-02 4.103e-03 3.466e-01 1.092e-04 1.990e-03 7.397e-01 7.040e-06
 2.095e+00 2.488e-03 1.584e+00 1.304e+00 3.685e-02 6.227e-01 6.955e-06
 1.057e+00 5.251e-02 1.129e+00 1.160e-03 2.802e-03 3.203e-03 9.281e-01
 4.143e-01 1.630e-04 7.162e-02 1.206e-01 1.413e-05 3.423e-02 2.435e-03
 1.851e-01 7.733e-02 1.521e-01 4.876e-01 4.656e-06 2.443e-03 6.371e-01
 5.509e-02 5.773e-01 3.583e-03 1.603e+00 1.543e-01 2.578e-01 7.223e-05
 1.310e-02 6.203e-02 1.599e-03 1.038e+00 4.622e+00 9.759e-02 3.133e-02
 1.937e-01 1.072e-01 1.599e-03 8.624e-01 1.868e-01 4.734e-02 2.219e-02
 5.255e-02 5.545e-02 6.906e-01 2.265e-03 1.001e-01 7.283e-01 1.199e+00
 3.034e-06 1.076e-03 2.601e-04 1.438e-02 2.022e-02 8.444e-04 1.931e-03
 3.549e-06 1.489e-03 4.921e-03 1.607e-03 5.000e-04 2.470e-04 3.061e-05
 8.051e-03 7.663e-04 3.197e-05 1.520e-03 2.056e-03 5.678e-05 2.173e-03
 1.959e-03 8.487e-02 6.224e-03 7.707e-03 3.495e-03 8.719e-05 3.999e-03
 3.295e-01 3.305e-02 4.154e-04 4.018e-04 4.181e-05 4.065e-02 1.365e-03
 1.597e-04 1.307e-03 8.879e-02 4.883e-05 1.074e-02 1.143e-03 1.012e+00
 1.142e-01 2.087e-02 9.679e-01 1.785e-04 5.815e-04 1.248e-03 1.914e-03
 7.525e-04 2.130e-03 4.799e-04 5.554e-01 1.476e-03 1.357e-04 3.980e-03
 2.065e-02 3.292e-06 5.366e-02 3.569e-03 2.832e-01 5.084e-02 6.202e-02
 6.733e-02 1.332e-05 1.026e-01 4.821e-02 1.090e-01 8.628e-01 2.171e+00
 2.271e-03 4.555e-03 7.432e-02 5.103e-02 2.111e-02 1.255e+00 1.501e-04
 2.528e-03 9.930e-03 9.915e-02 7.710e-01 8.427e-03 1.041e-02 6.321e-04
 4.522e-01 3.675e-01 8.361e-02 1.304e-03 3.141e-03 1.055e-04 7.001e-01
 1.085e+00 2.008e-03 4.722e-03 6.416e-02 8.493e-06 3.522e-04 5.478e-05
 1.330e-02 9.118e-03 6.642e-03 6.862e-02 3.092e-06 5.512e-04 5.291e-04
 2.415e-04 3.285e-05 4.529e-04 9.515e-06 5.618e-02 2.028e-03 1.913e-03
 3.457e-04 1.020e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9263020833333334, 0.8581408782157629)
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
total norm for a microbatch 18.59692441573644 clip 9.658064220834715
total norm for a microbatch 12.989379072739416 clip 12.105367776663085
total norm for a microbatch 19.26820849314158 clip 11.897973948555109
total norm for a microbatch 13.698096718253275 clip 12.438635624015237
total norm for a microbatch 14.304733078811209 clip 11.801157698763465
cuda
Objective function 19.31 = squared loss an data 17.89 + 0.5*rho*h**2 0.946970 + alpha*h 0.000000 + L2reg 0.25 + L1reg 0.23 ; SHD = 53 ; DAG False
Proportion of microbatches that were clipped  0.8787396562698918
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.3762047185645514
iteration 1 in outer loop, alpha = 1.3762047185645514, rho = 1.0, h = 1.3762047185645514
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 21.21 = squared loss an data 17.89 + 0.5*rho*h**2 0.946970 + alpha*h 1.893939 + L2reg 0.25 + L1reg 0.23 ; SHD = 53 ; DAG False
total norm for a microbatch 15.088324115630085 clip 5.796128526196119
total norm for a microbatch 14.623075695967152 clip 7.00071832024005
total norm for a microbatch 14.183640345315455 clip 14.38487287752681
total norm for a microbatch 23.769638871552317 clip 14.88734625797916
total norm for a microbatch 12.847318704334539 clip 14.943444518972472
cuda
Objective function 18.63 = squared loss an data 16.10 + 0.5*rho*h**2 0.506272 + alpha*h 1.384809 + L2reg 0.43 + L1reg 0.22 ; SHD = 44 ; DAG False
Proportion of microbatches that were clipped  0.882906260136231
iteration 1 in inner loop, alpha 1.3762047185645514 rho 1.0 h 1.0062521891842522
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 23.19 = squared loss an data 16.10 + 0.5*rho*h**2 5.062717 + alpha*h 1.384809 + L2reg 0.43 + L1reg 0.22 ; SHD = 44 ; DAG False
total norm for a microbatch 23.06112004206415 clip 3.856639643488005
total norm for a microbatch 14.285256869832969 clip 16.385370904842215
total norm for a microbatch 32.156877740969875 clip 17.430263657453807
total norm for a microbatch 16.903749968694555 clip 18.316070345718487
total norm for a microbatch 22.441589580371648 clip 18.63447495671674
cuda
Objective function 18.95 = squared loss an data 16.79 + 0.5*rho*h**2 0.855742 + alpha*h 0.569337 + L2reg 0.54 + L1reg 0.19 ; SHD = 45 ; DAG True
Proportion of microbatches that were clipped  0.8910004711795194
iteration 2 in inner loop, alpha 1.3762047185645514 rho 10.0 h 0.41370075801925665
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 26.65 = squared loss an data 16.79 + 0.5*rho*h**2 8.557416 + alpha*h 0.569337 + L2reg 0.54 + L1reg 0.19 ; SHD = 45 ; DAG True
total norm for a microbatch 27.01947150617782 clip 1.7191142432582052
total norm for a microbatch 20.35006702379269 clip 18.92587072441303
total norm for a microbatch 30.0123639468286 clip 20.281504587244072
total norm for a microbatch 26.25482958876783 clip 19.9588970746895
cuda
Objective function 19.74 = squared loss an data 17.95 + 0.5*rho*h**2 0.828163 + alpha*h 0.177115 + L2reg 0.62 + L1reg 0.16 ; SHD = 41 ; DAG True
Proportion of microbatches that were clipped  0.898140915481956
iteration 3 in inner loop, alpha 1.3762047185645514 rho 100.0 h 0.12869832780659962
iteration 2 in outer loop, alpha = 14.246037499224514, rho = 100.0, h = 0.12869832780659962
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 21.40 = squared loss an data 17.95 + 0.5*rho*h**2 0.828163 + alpha*h 1.833441 + L2reg 0.62 + L1reg 0.16 ; SHD = 41 ; DAG True
total norm for a microbatch 33.17396651533774 clip 1.7234391714497566
total norm for a microbatch 31.16237610570723 clip 1.7234391714497566
total norm for a microbatch 18.803608859296396 clip 2.4961687087424425
total norm for a microbatch 26.007222354670656 clip 6.133355975213887
total norm for a microbatch 37.38078751607812 clip 7.901057120076323
total norm for a microbatch 46.67195856416694 clip 13.550583191110325
total norm for a microbatch 30.168567718450994 clip 16.147994913512754
total norm for a microbatch 19.729534182117252 clip 21.33657556887105
total norm for a microbatch 29.396510739568182 clip 21.241403805544707
total norm for a microbatch 29.114642529502696 clip 21.66582935100011
cuda
Objective function 20.69 = squared loss an data 18.19 + 0.5*rho*h**2 0.387039 + alpha*h 1.253391 + L2reg 0.70 + L1reg 0.16 ; SHD = 49 ; DAG True
Proportion of microbatches that were clipped  0.8952178880365594
iteration 1 in inner loop, alpha 14.246037499224514 rho 100.0 h 0.0879817377035117
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 24.17 = squared loss an data 18.19 + 0.5*rho*h**2 3.870393 + alpha*h 1.253391 + L2reg 0.70 + L1reg 0.16 ; SHD = 49 ; DAG True
total norm for a microbatch 22.9172910658803 clip 2.08949831268089
total norm for a microbatch 20.939803384293914 clip 6.143904674117205
total norm for a microbatch 29.093857675418416 clip 13.714537978693137
cuda
Objective function 20.78 = squared loss an data 18.70 + 0.5*rho*h**2 0.673896 + alpha*h 0.523005 + L2reg 0.73 + L1reg 0.15 ; SHD = 53 ; DAG True
Proportion of microbatches that were clipped  0.8978160369838992
iteration 2 in inner loop, alpha 14.246037499224514 rho 1000.0 h 0.036712289611589455
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 26.85 = squared loss an data 18.70 + 0.5*rho*h**2 6.738961 + alpha*h 0.523005 + L2reg 0.73 + L1reg 0.15 ; SHD = 53 ; DAG True
total norm for a microbatch 31.865116734575352 clip 5.06717997469573
total norm for a microbatch 30.75888572179134 clip 25.43415001558929
total norm for a microbatch 52.40549612373746 clip 26.46333577640968
total norm for a microbatch 25.876731171437378 clip 25.834006218333514
total norm for a microbatch 24.478858521449304 clip 25.66331521984161
cuda
Objective function 21.01 = squared loss an data 19.10 + 0.5*rho*h**2 0.849641 + alpha*h 0.185706 + L2reg 0.74 + L1reg 0.14 ; SHD = 50 ; DAG True
Proportion of microbatches that were clipped  0.9026320038753431
iteration 3 in inner loop, alpha 14.246037499224514 rho 10000.0 h 0.013035653250895507
iteration 3 in outer loop, alpha = 144.60257000817958, rho = 10000.0, h = 0.013035653250895507
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 22.71 = squared loss an data 19.10 + 0.5*rho*h**2 0.849641 + alpha*h 1.884989 + L2reg 0.74 + L1reg 0.14 ; SHD = 50 ; DAG True
total norm for a microbatch 50.796343793835746 clip 1.0
total norm for a microbatch 54.55186968022832 clip 1.401524849263198
total norm for a microbatch 23.578705647271047 clip 24.19286524206927
total norm for a microbatch 34.39472722712113 clip 27.06636503252453
total norm for a microbatch 34.968577720560326 clip 28.03610076151044
cuda
Objective function 21.50 = squared loss an data 19.07 + 0.5*rho*h**2 0.324747 + alpha*h 1.165369 + L2reg 0.80 + L1reg 0.14 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.9094093297245661
iteration 1 in inner loop, alpha 144.60257000817958 rho 10000.0 h 0.008059114799205958
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 24.43 = squared loss an data 19.07 + 0.5*rho*h**2 3.247467 + alpha*h 1.165369 + L2reg 0.80 + L1reg 0.14 ; SHD = 46 ; DAG True
total norm for a microbatch 77.80694485094953 clip 1.0
total norm for a microbatch 117.10804443554098 clip 1.7444983489094892
total norm for a microbatch 36.61052816449043 clip 10.755457944377467
total norm for a microbatch 42.55571967639064 clip 20.31047702811882
total norm for a microbatch 43.81035622915574 clip 42.48154622040986
cuda
Objective function 21.73 = squared loss an data 19.33 + 0.5*rho*h**2 0.846236 + alpha*h 0.594890 + L2reg 0.81 + L1reg 0.15 ; SHD = 58 ; DAG True
Proportion of microbatches that were clipped  0.9165177846353744
iteration 2 in inner loop, alpha 144.60257000817958 rho 100000.0 h 0.004113966462465868
iteration 4 in outer loop, alpha = 4258.569032474048, rho = 1000000.0, h = 0.004113966462465868
Threshold 0.3
[[0.004 0.106 0.406 0.349 0.37  0.256 0.378 0.042 0.308 0.457 0.398 0.148
  0.711 0.089 0.379 0.47  0.388 0.605 0.611 0.382]
 [0.03  0.003 0.174 0.139 0.237 0.598 0.083 0.008 0.072 0.311 0.225 0.248
  0.067 0.025 0.196 0.265 0.205 0.059 0.11  0.438]
 [0.01  0.018 0.004 0.07  0.205 0.016 0.052 0.01  0.016 0.253 0.079 0.048
  0.018 0.002 0.051 0.168 0.096 0.081 0.046 0.064]
 [0.01  0.015 0.061 0.004 0.019 0.047 0.033 0.005 0.007 0.025 0.027 0.006
  0.044 0.004 0.099 0.073 0.023 0.045 0.024 0.086]
 [0.011 0.013 0.024 0.114 0.004 0.024 0.018 0.004 0.004 0.057 0.087 0.008
  0.037 0.007 0.072 0.064 0.036 0.029 0.009 0.045]
 [0.012 0.007 0.189 0.08  0.146 0.003 0.011 0.01  0.036 0.224 0.227 0.019
  0.021 0.014 0.495 0.111 0.159 0.03  0.017 0.198]
 [0.008 0.039 0.073 0.088 0.254 0.406 0.004 0.017 0.048 0.313 0.282 0.028
  0.018 0.023 0.235 0.227 0.637 0.064 0.083 0.279]
 [0.131 0.472 0.295 0.518 0.598 0.317 0.229 0.003 0.161 0.398 0.385 0.866
  0.563 0.081 0.479 0.235 0.488 1.277 0.357 0.53 ]
 [0.014 0.05  0.371 0.646 0.711 0.092 0.077 0.019 0.005 0.285 0.414 0.04
  0.098 0.004 0.432 0.362 0.335 0.109 0.406 0.313]
 [0.01  0.011 0.026 0.157 0.083 0.017 0.012 0.01  0.012 0.004 0.265 0.029
  0.012 0.009 0.093 0.252 0.047 0.033 0.015 0.146]
 [0.006 0.02  0.073 0.19  0.071 0.023 0.013 0.011 0.009 0.018 0.004 0.015
  0.037 0.008 0.058 0.141 0.087 0.05  0.027 0.375]
 [0.028 0.022 0.103 0.704 0.496 0.263 0.19  0.003 0.094 0.182 0.355 0.004
  0.224 0.022 0.176 0.384 0.113 0.314 0.118 0.213]
 [0.004 0.067 0.187 0.115 0.114 0.177 0.27  0.007 0.057 0.4   0.132 0.017
  0.004 0.018 0.669 0.156 0.164 0.015 0.069 0.2  ]
 [0.052 0.148 1.441 0.736 0.453 0.265 0.238 0.056 0.71  0.322 0.302 0.173
  0.24  0.003 0.604 0.528 0.363 0.442 0.644 0.731]
 [0.007 0.02  0.074 0.05  0.075 0.01  0.014 0.005 0.008 0.065 0.093 0.016
  0.005 0.005 0.004 0.074 0.038 0.034 0.014 0.193]
 [0.008 0.013 0.035 0.061 0.086 0.041 0.017 0.008 0.012 0.02  0.035 0.009
  0.017 0.005 0.057 0.002 0.071 0.024 0.006 0.064]
 [0.009 0.02  0.043 0.257 0.097 0.036 0.007 0.008 0.015 0.098 0.04  0.033
  0.02  0.005 0.189 0.067 0.004 0.04  0.019 0.143]
 [0.005 0.032 0.081 0.118 0.144 0.101 0.073 0.004 0.051 0.133 0.112 0.011
  0.214 0.009 0.175 0.145 0.112 0.003 0.056 0.447]
 [0.007 0.055 0.06  0.168 0.515 0.366 0.047 0.011 0.009 0.343 0.184 0.048
  0.062 0.006 0.296 0.574 0.192 0.072 0.005 0.218]
 [0.008 0.011 0.023 0.054 0.11  0.025 0.014 0.005 0.019 0.04  0.012 0.016
  0.013 0.004 0.037 0.073 0.025 0.007 0.018 0.005]]
[[0.    0.    0.406 0.349 0.37  0.    0.378 0.    0.308 0.457 0.398 0.
  0.711 0.    0.379 0.47  0.388 0.605 0.611 0.382]
 [0.    0.    0.    0.    0.    0.598 0.    0.    0.    0.311 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.438]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.495 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.406 0.    0.    0.    0.313 0.    0.
  0.    0.    0.    0.    0.637 0.    0.    0.   ]
 [0.    0.472 0.    0.518 0.598 0.317 0.    0.    0.    0.398 0.385 0.866
  0.563 0.    0.479 0.    0.488 1.277 0.357 0.53 ]
 [0.    0.    0.371 0.646 0.711 0.    0.    0.    0.    0.    0.414 0.
  0.    0.    0.432 0.362 0.335 0.    0.406 0.313]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.375]
 [0.    0.    0.    0.704 0.496 0.    0.    0.    0.    0.    0.355 0.
  0.    0.    0.    0.384 0.    0.314 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.4   0.    0.
  0.    0.    0.669 0.    0.    0.    0.    0.   ]
 [0.    0.    1.441 0.736 0.453 0.    0.    0.    0.71  0.322 0.302 0.
  0.    0.    0.604 0.528 0.363 0.442 0.644 0.731]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.447]
 [0.    0.    0.    0.    0.515 0.366 0.    0.    0.    0.343 0.    0.
  0.    0.    0.    0.574 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5, 'tpr': 0.5666666666666667, 'fpr': 0.26153846153846155, 'f1': 0.53125, 'shd': 58, 'npred': 68, 'ntrue': 60}
[0.106 0.406 0.349 0.37  0.256 0.378 0.042 0.308 0.457 0.398 0.148 0.711
 0.089 0.379 0.47  0.388 0.605 0.611 0.382 0.03  0.174 0.139 0.237 0.598
 0.083 0.008 0.072 0.311 0.225 0.248 0.067 0.025 0.196 0.265 0.205 0.059
 0.11  0.438 0.01  0.018 0.07  0.205 0.016 0.052 0.01  0.016 0.253 0.079
 0.048 0.018 0.002 0.051 0.168 0.096 0.081 0.046 0.064 0.01  0.015 0.061
 0.019 0.047 0.033 0.005 0.007 0.025 0.027 0.006 0.044 0.004 0.099 0.073
 0.023 0.045 0.024 0.086 0.011 0.013 0.024 0.114 0.024 0.018 0.004 0.004
 0.057 0.087 0.008 0.037 0.007 0.072 0.064 0.036 0.029 0.009 0.045 0.012
 0.007 0.189 0.08  0.146 0.011 0.01  0.036 0.224 0.227 0.019 0.021 0.014
 0.495 0.111 0.159 0.03  0.017 0.198 0.008 0.039 0.073 0.088 0.254 0.406
 0.017 0.048 0.313 0.282 0.028 0.018 0.023 0.235 0.227 0.637 0.064 0.083
 0.279 0.131 0.472 0.295 0.518 0.598 0.317 0.229 0.161 0.398 0.385 0.866
 0.563 0.081 0.479 0.235 0.488 1.277 0.357 0.53  0.014 0.05  0.371 0.646
 0.711 0.092 0.077 0.019 0.285 0.414 0.04  0.098 0.004 0.432 0.362 0.335
 0.109 0.406 0.313 0.01  0.011 0.026 0.157 0.083 0.017 0.012 0.01  0.012
 0.265 0.029 0.012 0.009 0.093 0.252 0.047 0.033 0.015 0.146 0.006 0.02
 0.073 0.19  0.071 0.023 0.013 0.011 0.009 0.018 0.015 0.037 0.008 0.058
 0.141 0.087 0.05  0.027 0.375 0.028 0.022 0.103 0.704 0.496 0.263 0.19
 0.003 0.094 0.182 0.355 0.224 0.022 0.176 0.384 0.113 0.314 0.118 0.213
 0.004 0.067 0.187 0.115 0.114 0.177 0.27  0.007 0.057 0.4   0.132 0.017
 0.018 0.669 0.156 0.164 0.015 0.069 0.2   0.052 0.148 1.441 0.736 0.453
 0.265 0.238 0.056 0.71  0.322 0.302 0.173 0.24  0.604 0.528 0.363 0.442
 0.644 0.731 0.007 0.02  0.074 0.05  0.075 0.01  0.014 0.005 0.008 0.065
 0.093 0.016 0.005 0.005 0.074 0.038 0.034 0.014 0.193 0.008 0.013 0.035
 0.061 0.086 0.041 0.017 0.008 0.012 0.02  0.035 0.009 0.017 0.005 0.057
 0.071 0.024 0.006 0.064 0.009 0.02  0.043 0.257 0.097 0.036 0.007 0.008
 0.015 0.098 0.04  0.033 0.02  0.005 0.189 0.067 0.04  0.019 0.143 0.005
 0.032 0.081 0.118 0.144 0.101 0.073 0.004 0.051 0.133 0.112 0.011 0.214
 0.009 0.175 0.145 0.112 0.056 0.447 0.007 0.055 0.06  0.168 0.515 0.366
 0.047 0.011 0.009 0.343 0.184 0.048 0.062 0.006 0.296 0.574 0.192 0.072
 0.218 0.008 0.011 0.023 0.054 0.11  0.025 0.014 0.005 0.019 0.04  0.012
 0.016 0.013 0.004 0.037 0.073 0.025 0.007 0.018]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.84078125, 0.6174346069597691)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
