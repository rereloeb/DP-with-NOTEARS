samples  5000  graph  13 26 ER mlp  minibatch size  175  noise  0.5  minibatches per NN training  20 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3261408551088962
iteration 1 in outer loop, alpha = 1.3261408551088962, rho = 1.0, h = 1.3261408551088962
cuda
iteration 1 in inner loop,alpha 1.3261408551088962 rho 1.0 h 0.8858818726451396
iteration 2 in inner loop,alpha 1.3261408551088962 rho 10.0 h 0.3965979721331152
iteration 3 in inner loop,alpha 1.3261408551088962 rho 100.0 h 0.14379358626344718
iteration 2 in outer loop, alpha = 15.705499481453614, rho = 100.0, h = 0.14379358626344718
cuda
iteration 1 in inner loop,alpha 15.705499481453614 rho 100.0 h 0.09252493068144574
iteration 2 in inner loop,alpha 15.705499481453614 rho 1000.0 h 0.036433332819168385
iteration 3 in inner loop,alpha 15.705499481453614 rho 10000.0 h 0.01049287196394566
iteration 3 in outer loop, alpha = 120.63421912091022, rho = 10000.0, h = 0.01049287196394566
cuda
iteration 1 in inner loop,alpha 120.63421912091022 rho 10000.0 h 0.004137894839422174
iteration 2 in inner loop,alpha 120.63421912091022 rho 100000.0 h 0.0014638289958170958
iteration 4 in outer loop, alpha = 267.01711870261977, rho = 100000.0, h = 0.0014638289958170958
cuda
iteration 1 in inner loop,alpha 267.01711870261977 rho 100000.0 h 0.0007413895029326767
iteration 5 in outer loop, alpha = 1008.4066216352965, rho = 1000000.0, h = 0.0007413895029326767
Threshold 0.3
[[0.001 0.    0.18  2.515 0.143 0.002 0.018 0.    0.    0.426 0.    1.771
  0.   ]
 [0.664 0.001 0.334 0.059 0.18  0.002 0.045 0.    0.    2.109 0.009 0.214
  0.   ]
 [0.    0.    0.002 0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.654 0.001 0.123 0.    0.092 0.    0.    1.585 0.    0.301
  0.   ]
 [0.    0.    0.276 0.    0.002 0.    0.004 0.    0.    0.001 0.    0.37
  0.   ]
 [0.092 0.036 0.324 0.076 0.402 0.001 0.014 0.    0.116 2.257 0.041 0.132
  0.016]
 [0.01  0.011 1.073 0.024 0.086 0.041 0.026 0.001 0.002 1.01  0.002 0.395
  0.004]
 [0.028 0.013 0.027 0.024 1.175 3.739 0.083 0.001 0.621 0.456 0.015 0.159
  0.024]
 [0.02  1.52  2.197 0.032 0.095 0.004 0.37  0.001 0.    1.539 0.017 0.141
  0.037]
 [0.    0.    1.102 0.    0.723 0.    0.001 0.    0.    0.004 0.    2.27
  0.   ]
 [1.716 0.013 2.528 0.078 0.17  0.002 0.015 0.001 0.012 0.971 0.    0.077
  0.013]
 [0.    0.    0.942 0.    0.005 0.    0.001 0.    0.    0.    0.    0.001
  0.   ]
 [0.021 0.989 2.29  0.009 0.379 0.01  0.094 0.001 0.003 2.247 0.014 0.482
  0.   ]]
[[0.    0.    0.    2.515 0.    0.    0.    0.    0.    0.426 0.    1.771
  0.   ]
 [0.664 0.    0.334 0.    0.    0.    0.    0.    0.    2.109 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.654 0.    0.    0.    0.    0.    0.    1.585 0.    0.301
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.37
  0.   ]
 [0.    0.    0.324 0.    0.402 0.    0.    0.    0.    2.257 0.    0.
  0.   ]
 [0.    0.    1.073 0.    0.    0.    0.    0.    0.    1.01  0.    0.395
  0.   ]
 [0.    0.    0.    0.    1.175 3.739 0.    0.    0.621 0.456 0.    0.
  0.   ]
 [0.    1.52  2.197 0.    0.    0.    0.37  0.    0.    1.539 0.    0.
  0.   ]
 [0.    0.    1.102 0.    0.723 0.    0.    0.    0.    0.    0.    2.27
  0.   ]
 [1.716 0.    2.528 0.    0.    0.    0.    0.    0.    0.971 0.    0.
  0.   ]
 [0.    0.    0.942 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.989 2.29  0.    0.379 0.    0.    0.    0.    2.247 0.    0.482
  0.   ]]
{'fdr': 0.5, 'tpr': 0.6923076923076923, 'fpr': 0.34615384615384615, 'f1': 0.5806451612903226, 'shd': 20, 'npred': 36, 'ntrue': 26}
[2.795e-04 1.804e-01 2.515e+00 1.429e-01 1.981e-03 1.833e-02 1.288e-04
 9.631e-05 4.255e-01 3.536e-05 1.771e+00 7.893e-05 6.636e-01 3.336e-01
 5.902e-02 1.804e-01 1.692e-03 4.477e-02 3.605e-04 1.260e-04 2.109e+00
 9.429e-03 2.138e-01 2.017e-04 4.010e-05 1.748e-05 1.101e-04 2.535e-04
 2.897e-05 9.634e-04 5.853e-06 2.201e-05 2.013e-04 1.759e-06 6.859e-05
 2.173e-05 1.379e-04 6.178e-05 2.654e+00 1.226e-01 3.767e-04 9.214e-02
 1.745e-04 4.978e-05 1.585e+00 1.227e-05 3.007e-01 4.194e-04 9.596e-05
 1.058e-04 2.764e-01 4.709e-04 3.219e-05 3.975e-03 2.137e-05 3.200e-05
 8.422e-04 2.106e-05 3.705e-01 8.374e-06 9.160e-02 3.616e-02 3.238e-01
 7.634e-02 4.022e-01 1.425e-02 1.168e-04 1.163e-01 2.257e+00 4.120e-02
 1.322e-01 1.629e-02 9.510e-03 1.099e-02 1.073e+00 2.450e-02 8.617e-02
 4.061e-02 9.995e-04 2.262e-03 1.010e+00 1.530e-03 3.948e-01 3.659e-03
 2.848e-02 1.294e-02 2.657e-02 2.395e-02 1.175e+00 3.739e+00 8.275e-02
 6.212e-01 4.558e-01 1.519e-02 1.587e-01 2.383e-02 2.039e-02 1.520e+00
 2.197e+00 3.233e-02 9.538e-02 3.927e-03 3.701e-01 7.745e-04 1.539e+00
 1.656e-02 1.413e-01 3.698e-02 4.693e-05 8.792e-05 1.102e+00 1.756e-04
 7.232e-01 2.178e-04 1.004e-03 4.622e-05 1.286e-05 1.939e-05 2.270e+00
 4.266e-05 1.716e+00 1.266e-02 2.528e+00 7.779e-02 1.699e-01 2.406e-03
 1.459e-02 1.131e-03 1.212e-02 9.709e-01 7.711e-02 1.321e-02 3.210e-06
 7.131e-06 9.416e-01 4.028e-05 4.742e-03 7.917e-06 9.788e-04 5.090e-05
 2.152e-06 6.772e-05 1.520e-06 1.260e-06 2.078e-02 9.894e-01 2.290e+00
 9.323e-03 3.788e-01 1.004e-02 9.427e-02 1.318e-03 2.940e-03 2.247e+00
 1.432e-02 4.821e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.8366863905325445, 0.7221549515485748)
cuda
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 480.43 = squared loss an data 285.64 + 0.5*rho*h**2 194.315677 + alpha*h 0.000000 + L2reg 0.24 + L1reg 0.24 ; SHD = 91 ; DAG False
total norm for a microbatch 64.04486262146156 clip 1.4466516907725984
total norm for a microbatch 78.17622397287933 clip 1.4466516907725984
cuda
Objective function 199.59 = squared loss an data 198.63 + 0.5*rho*h**2 0.766778 + alpha*h 0.000000 + L2reg 0.09 + L1reg 0.10 ; SHD = 38 ; DAG False
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.2383681641903976
iteration 1 in outer loop, alpha = 1.2383681641903976, rho = 1.0, h = 1.2383681641903976
cuda
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 201.13 = squared loss an data 198.63 + 0.5*rho*h**2 0.766778 + alpha*h 1.533556 + L2reg 0.09 + L1reg 0.10 ; SHD = 38 ; DAG False
total norm for a microbatch 56.724519396306164 clip 2.9480246639496883
total norm for a microbatch 63.45702425210925 clip 4.28430593887873
cuda
Objective function 139.64 = squared loss an data 137.97 + 0.5*rho*h**2 0.374357 + alpha*h 1.071538 + L2reg 0.13 + L1reg 0.10 ; SHD = 32 ; DAG False
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 1.2383681641903976 rho 1.0 h 0.865282232407063
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 143.01 = squared loss an data 137.97 + 0.5*rho*h**2 3.743567 + alpha*h 1.071538 + L2reg 0.13 + L1reg 0.10 ; SHD = 32 ; DAG False
total norm for a microbatch 65.25802982957116 clip 2.3168205722192132
cuda
Objective function 89.95 = squared loss an data 88.45 + 0.5*rho*h**2 0.755743 + alpha*h 0.481451 + L2reg 0.17 + L1reg 0.09 ; SHD = 27 ; DAG False
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 1.2383681641903976 rho 10.0 h 0.3887783171196997
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 96.75 = squared loss an data 88.45 + 0.5*rho*h**2 7.557429 + alpha*h 0.481451 + L2reg 0.17 + L1reg 0.09 ; SHD = 27 ; DAG False
total norm for a microbatch 43.573522839110936 clip 1.1997699920449214
cuda
Objective function 58.25 = squared loss an data 56.99 + 0.5*rho*h**2 0.780114 + alpha*h 0.154683 + L2reg 0.24 + L1reg 0.08 ; SHD = 29 ; DAG False
Proportion of microbatches that were clipped  1.0
iteration 3 in inner loop, alpha 1.2383681641903976 rho 100.0 h 0.12490910035658231
iteration 2 in outer loop, alpha = 13.729278199848629, rho = 100.0, h = 0.12490910035658231
cuda
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 59.81 = squared loss an data 56.99 + 0.5*rho*h**2 0.780114 + alpha*h 1.714912 + L2reg 0.24 + L1reg 0.08 ; SHD = 29 ; DAG False
total norm for a microbatch 57.03451275869004 clip 1.1974728217748518
total norm for a microbatch 32.30478139151767 clip 2.500583771258989
total norm for a microbatch 41.08097172453533 clip 2.9703867206372627
total norm for a microbatch 44.323392318933294 clip 4.170614086725301
total norm for a microbatch 51.13288763959124 clip 4.551280485161192
cuda
Objective function 39.25 = squared loss an data 37.63 + 0.5*rho*h**2 0.249391 + alpha*h 0.969623 + L2reg 0.33 + L1reg 0.08 ; SHD = 28 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 13.729278199848629 rho 100.0 h 0.07062450319509672
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 41.49 = squared loss an data 37.63 + 0.5*rho*h**2 2.493910 + alpha*h 0.969623 + L2reg 0.33 + L1reg 0.08 ; SHD = 28 ; DAG True
total norm for a microbatch 66.16026175331668 clip 3.2604780929578636
total norm for a microbatch 130.8798457281426 clip 3.889146999897546
total norm for a microbatch 83.47232394758481 clip 3.889146999897546
cuda
Objective function 28.29 = squared loss an data 27.15 + 0.5*rho*h**2 0.325113 + alpha*h 0.350090 + L2reg 0.39 + L1reg 0.07 ; SHD = 28 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 13.729278199848629 rho 1000.0 h 0.025499536709537196
iteration 3 in outer loop, alpha = 39.228814909385825, rho = 1000.0, h = 0.025499536709537196
cuda
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 28.94 = squared loss an data 27.15 + 0.5*rho*h**2 0.325113 + alpha*h 1.000317 + L2reg 0.39 + L1reg 0.07 ; SHD = 28 ; DAG True
total norm for a microbatch 82.38911569916 clip 1.3074458847381951
total norm for a microbatch 82.11800206506207 clip 2.060121334074481
total norm for a microbatch 53.84298730700087 clip 2.4324278705651743
total norm for a microbatch 61.893722856567955 clip 4.095564784469689
total norm for a microbatch 125.04413508590045 clip 4.934924989132526
cuda
Objective function 22.65 = squared loss an data 21.30 + 0.5*rho*h**2 0.150025 + alpha*h 0.679519 + L2reg 0.45 + L1reg 0.08 ; SHD = 31 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 39.228814909385825 rho 1000.0 h 0.017321934520666105
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 24.00 = squared loss an data 21.30 + 0.5*rho*h**2 1.500247 + alpha*h 0.679519 + L2reg 0.45 + L1reg 0.08 ; SHD = 31 ; DAG True
total norm for a microbatch 65.90748020479765 clip 1.087964762425925
cuda
Objective function 21.51 = squared loss an data 20.42 + 0.5*rho*h**2 0.246555 + alpha*h 0.275472 + L2reg 0.50 + L1reg 0.08 ; SHD = 30 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 39.228814909385825 rho 10000.0 h 0.00702218455700887
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 23.73 = squared loss an data 20.42 + 0.5*rho*h**2 2.465554 + alpha*h 0.275472 + L2reg 0.50 + L1reg 0.08 ; SHD = 30 ; DAG True
total norm for a microbatch 446.10787890862355 clip 1.0999014789577637
total norm for a microbatch 51.61065975443792 clip 2.2786193605575686
cuda
Objective function 20.35 = squared loss an data 19.26 + 0.5*rho*h**2 0.388093 + alpha*h 0.109292 + L2reg 0.52 + L1reg 0.07 ; SHD = 28 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 3 in inner loop, alpha 39.228814909385825 rho 100000.0 h 0.002786011805513411
iteration 4 in outer loop, alpha = 317.82999546072693, rho = 100000.0, h = 0.002786011805513411
cuda
noise_multiplier  0.5  noise_multiplier_b  8.75  noise_multiplier_delta  0.5002042066656508
cuda
Objective function 21.13 = squared loss an data 19.26 + 0.5*rho*h**2 0.388093 + alpha*h 0.885478 + L2reg 0.52 + L1reg 0.07 ; SHD = 28 ; DAG True
total norm for a microbatch 93.26913363577124 clip 1.5739612441777064
total norm for a microbatch 153.72427265547313 clip 1.5739612441777064
total norm for a microbatch 72.80709592186946 clip 2.240854594515653
total norm for a microbatch 108.15331575521928 clip 4.267569848799865
cuda
Objective function 19.82 = squared loss an data 18.51 + 0.5*rho*h**2 0.143445 + alpha*h 0.538335 + L2reg 0.56 + L1reg 0.07 ; SHD = 26 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 317.82999546072693 rho 100000.0 h 0.0016937835620662867
iteration 5 in outer loop, alpha = 2011.6135575270137, rho = 1000000.0, h = 0.0016937835620662867
Threshold 0.3
[[0.003 0.1   0.44  1.17  0.755 0.501 0.167 0.155 0.116 0.22  0.016 0.883
  0.143]
 [0.034 0.004 0.237 0.119 0.442 0.17  0.359 0.178 0.469 0.24  0.064 0.655
  0.007]
 [0.003 0.01  0.004 0.004 0.023 0.135 0.247 0.013 0.005 0.202 0.005 0.02
  0.002]
 [0.004 0.027 1.203 0.003 0.052 0.113 0.242 0.051 0.028 0.177 0.014 0.036
  0.015]
 [0.004 0.007 0.143 0.095 0.004 0.135 0.02  0.011 0.007 0.012 0.012 0.004
  0.01 ]
 [0.006 0.007 0.012 0.019 0.02  0.003 0.054 0.003 0.002 0.017 0.017 0.01
  0.006]
 [0.012 0.008 0.01  0.012 0.091 0.083 0.003 0.016 0.004 0.114 0.012 0.118
  0.011]
 [0.023 0.016 0.254 0.083 0.163 0.973 0.178 0.004 0.017 1.261 0.02  0.268
  0.039]
 [0.034 0.008 0.714 0.102 0.324 0.699 0.568 0.137 0.003 0.454 0.045 0.2
  0.012]
 [0.01  0.016 0.018 0.016 0.247 0.651 0.043 0.004 0.009 0.019 0.015 0.398
  0.004]
 [0.219 0.048 0.519 0.249 0.263 0.1   0.287 0.159 0.075 0.213 0.005 0.071
  0.067]
 [0.005 0.004 0.088 0.106 0.766 0.367 0.033 0.009 0.013 0.007 0.024 0.004
  0.008]
 [0.026 0.541 0.812 0.211 0.222 0.381 0.243 0.074 0.151 0.984 0.039 0.211
  0.004]]
[[0.    0.    0.44  1.17  0.755 0.501 0.    0.    0.    0.    0.    0.883
  0.   ]
 [0.    0.    0.    0.    0.442 0.    0.359 0.    0.469 0.    0.    0.655
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    1.203 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.973 0.    0.    0.    1.261 0.    0.
  0.   ]
 [0.    0.    0.714 0.    0.324 0.699 0.568 0.    0.    0.454 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.651 0.    0.    0.    0.    0.    0.398
  0.   ]
 [0.    0.    0.519 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.766 0.367 0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.541 0.812 0.    0.    0.381 0.    0.    0.    0.984 0.    0.
  0.   ]]
{'fdr': 0.5384615384615384, 'tpr': 0.46153846153846156, 'fpr': 0.2692307692307692, 'f1': 0.46153846153846156, 'shd': 26, 'npred': 26, 'ntrue': 26}
[0.1   0.44  1.17  0.755 0.501 0.167 0.155 0.116 0.22  0.016 0.883 0.143
 0.034 0.237 0.119 0.442 0.17  0.359 0.178 0.469 0.24  0.064 0.655 0.007
 0.003 0.01  0.004 0.023 0.135 0.247 0.013 0.005 0.202 0.005 0.02  0.002
 0.004 0.027 1.203 0.052 0.113 0.242 0.051 0.028 0.177 0.014 0.036 0.015
 0.004 0.007 0.143 0.095 0.135 0.02  0.011 0.007 0.012 0.012 0.004 0.01
 0.006 0.007 0.012 0.019 0.02  0.054 0.003 0.002 0.017 0.017 0.01  0.006
 0.012 0.008 0.01  0.012 0.091 0.083 0.016 0.004 0.114 0.012 0.118 0.011
 0.023 0.016 0.254 0.083 0.163 0.973 0.178 0.017 1.261 0.02  0.268 0.039
 0.034 0.008 0.714 0.102 0.324 0.699 0.568 0.137 0.454 0.045 0.2   0.012
 0.01  0.016 0.018 0.016 0.247 0.651 0.043 0.004 0.009 0.015 0.398 0.004
 0.219 0.048 0.519 0.249 0.263 0.1   0.287 0.159 0.075 0.213 0.071 0.067
 0.005 0.004 0.088 0.106 0.766 0.367 0.033 0.009 0.013 0.007 0.024 0.008
 0.026 0.541 0.812 0.211 0.222 0.381 0.243 0.074 0.151 0.984 0.039 0.211]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.8150887573964497, 0.525610163968743)
Iterations 200
Achieves (22.758190739507555, 1e-05)-DP
