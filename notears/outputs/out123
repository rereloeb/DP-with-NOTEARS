samples  5000  graph  30 60 ER mim  minibatch size  100  noise  0.6  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.144444827137683
iteration 1 in outer loop, alpha = 2.144444827137683, rho = 1.0, h = 2.144444827137683
cuda
iteration 1 in inner loop,alpha 2.144444827137683 rho 1.0 h 1.3256466874605835
iteration 2 in inner loop,alpha 2.144444827137683 rho 10.0 h 0.5131010328032026
iteration 2 in outer loop, alpha = 7.275455155169709, rho = 10.0, h = 0.5131010328032026
cuda
iteration 1 in inner loop,alpha 7.275455155169709 rho 10.0 h 0.28442514981965417
iteration 2 in inner loop,alpha 7.275455155169709 rho 100.0 h 0.09687583592673477
iteration 3 in outer loop, alpha = 16.963038747843186, rho = 100.0, h = 0.09687583592673477
cuda
iteration 1 in inner loop,alpha 16.963038747843186 rho 100.0 h 0.05565872169768937
iteration 2 in inner loop,alpha 16.963038747843186 rho 1000.0 h 0.017246591836673986
iteration 4 in outer loop, alpha = 34.20963058451717, rho = 1000.0, h = 0.017246591836673986
cuda
iteration 1 in inner loop,alpha 34.20963058451717 rho 1000.0 h 0.0075624153202191735
iteration 2 in inner loop,alpha 34.20963058451717 rho 10000.0 h 0.002425937204748152
iteration 5 in outer loop, alpha = 58.46900263199869, rho = 10000.0, h = 0.002425937204748152
cuda
iteration 1 in inner loop,alpha 58.46900263199869 rho 10000.0 h 0.0010962531235136908
iteration 2 in inner loop,alpha 58.46900263199869 rho 100000.0 h 0.0004198889922406579
iteration 6 in outer loop, alpha = 100.45790185606448, rho = 100000.0, h = 0.0004198889922406579
cuda
iteration 1 in inner loop,alpha 100.45790185606448 rho 100000.0 h 0.00023771111627368668
iteration 7 in outer loop, alpha = 338.1690181297512, rho = 1000000.0, h = 0.00023771111627368668
Threshold 0.3
[[0.    0.    0.101 0.046 0.057 0.    0.113 0.022 0.056 0.348 0.001 0.052
  0.    0.    0.    0.026 0.    0.049 0.055 0.878 0.044 0.003 0.15  0.021
  0.    0.034 0.012 0.018 0.022 0.022]
 [0.001 0.    0.017 0.003 0.011 0.68  1.879 0.085 0.113 0.001 0.    0.026
  0.    0.    0.    0.057 0.    0.072 0.027 0.052 0.003 0.    0.103 0.055
  0.    0.013 0.001 0.081 0.085 0.029]
 [0.    0.    0.004 0.009 0.106 0.    0.068 0.049 0.001 0.    0.    0.
  0.    0.    0.    0.005 0.    0.015 0.058 0.122 0.001 0.    0.011 0.02
  0.    0.    0.018 0.013 0.023 0.008]
 [0.    0.    0.006 0.003 0.02  0.003 0.02  0.    0.    0.    0.    0.001
  0.    0.    0.    0.002 0.    0.    0.001 0.054 0.    0.    0.001 0.001
  0.    0.    0.    0.    0.059 0.   ]
 [0.    0.    0.003 0.006 0.003 0.002 0.012 0.013 0.002 0.    0.    0.
  0.    0.    0.    0.001 0.    0.001 0.001 0.164 0.    0.    0.008 0.001
  0.    0.    0.001 0.009 0.012 0.001]
 [0.    0.    0.05  0.041 0.015 0.003 0.049 0.042 0.086 0.    0.    0.06
  0.    0.    0.    0.003 0.    0.006 0.004 0.83  0.001 0.    0.029 0.009
  0.    0.001 0.009 0.003 0.003 0.001]
 [0.    0.    0.011 0.028 0.026 0.    0.003 0.019 0.006 0.    0.    0.001
  0.    0.    0.    0.017 0.    0.01  0.007 0.091 0.    0.    0.094 0.044
  0.    0.    0.004 0.009 0.021 0.002]
 [0.    0.    0.009 1.484 0.009 0.014 0.027 0.003 0.001 0.    0.    0.001
  0.    0.    0.    0.003 0.    0.    0.001 0.01  0.001 0.    0.006 0.001
  0.    0.    0.001 0.    0.043 0.002]
 [0.    0.    0.052 0.002 0.042 0.005 0.041 0.058 0.003 0.001 0.    0.059
  0.    0.    0.    0.085 0.    0.047 0.755 1.089 0.    0.001 1.651 0.067
  0.    0.003 0.051 0.001 0.055 0.008]
 [0.    0.    0.126 0.142 0.03  1.132 0.055 0.634 0.091 0.    0.    2.437
  0.    0.    0.    0.243 0.    0.175 0.024 0.097 0.008 0.003 0.06  0.005
  0.001 0.012 0.563 0.021 0.01  0.024]
 [0.005 0.001 0.03  0.187 0.033 0.016 0.09  0.08  0.9   0.017 0.    0.037
  0.    0.    0.022 0.028 0.    0.058 0.064 0.103 0.049 0.011 0.031 0.068
  0.004 0.038 0.088 4.125 0.042 0.013]
 [0.    0.    4.283 0.009 0.03  0.012 0.019 0.057 0.013 0.    0.    0.002
  0.    0.    0.    0.119 0.    0.087 0.033 0.151 0.002 0.    0.003 0.061
  0.    0.    0.399 0.024 0.03  0.013]
 [0.005 0.    0.001 0.052 0.048 0.007 1.878 0.013 1.18  0.017 0.002 0.051
  0.    0.    0.    0.116 0.002 0.096 0.001 0.    0.022 0.02  0.085 0.31
  0.    0.037 0.437 0.039 0.02  3.056]
 [0.    0.    0.082 0.001 1.226 0.043 0.023 0.125 0.064 0.    0.    0.07
  0.    0.    0.    0.105 0.003 0.911 0.03  1.339 0.019 0.    0.043 0.037
  0.    0.005 0.13  0.067 0.009 0.   ]
 [0.    0.    0.033 0.057 0.06  0.059 0.038 0.101 0.049 0.002 0.    1.707
  0.005 0.003 0.    0.012 0.    0.181 0.023 0.003 0.012 0.01  0.109 0.746
  0.005 3.165 0.    0.012 0.121 0.056]
 [0.    0.    0.047 0.003 0.071 0.03  0.019 0.054 0.015 0.001 0.    0.005
  0.    0.    0.    0.004 0.    0.084 0.01  0.026 0.004 0.002 0.453 0.013
  0.    0.007 0.726 0.005 0.008 0.074]
 [0.005 0.    2.537 0.093 0.088 0.019 0.035 0.013 0.456 0.    0.    0.002
  0.    0.    0.    0.013 0.    0.119 0.002 0.034 2.833 0.399 0.122 0.016
  0.    0.    0.046 0.05  0.074 0.041]
 [0.    0.    0.014 0.06  1.532 0.088 0.015 1.116 0.002 0.    0.    0.
  0.    0.    0.    0.004 0.    0.004 0.003 0.096 0.002 0.    0.018 0.001
  0.    0.001 0.004 0.011 0.207 0.001]
 [0.    0.    0.007 0.026 0.021 0.008 0.005 0.036 0.001 0.    0.    0.
  0.    0.    0.    0.001 0.    0.134 0.002 0.009 0.    0.    0.018 0.001
  0.    0.    0.01  0.006 0.035 0.001]
 [0.    0.    0.004 0.003 0.001 0.001 0.007 0.    0.001 0.    0.    0.001
  0.    0.    0.    0.023 0.    0.001 0.016 0.003 0.    0.    0.072 0.009
  0.    0.    0.006 0.003 0.009 0.001]
 [0.    0.    0.037 0.    0.004 0.839 0.015 0.032 1.733 0.    0.    0.062
  0.    0.    0.    0.014 0.    0.075 0.024 0.081 0.002 0.001 0.004 0.031
  0.    0.019 0.009 0.012 2.129 0.037]
 [0.    0.    0.045 1.338 1.121 0.025 0.052 0.121 0.239 0.    0.    0.043
  0.    0.    0.    0.053 0.    0.694 0.069 0.066 0.332 0.001 0.099 0.198
  0.    0.028 0.228 0.049 1.58  2.032]
 [0.    0.    0.001 0.122 0.056 0.002 0.002 0.1   0.    0.    0.    0.002
  0.    0.    0.    0.005 0.    0.008 0.026 0.001 0.    0.    0.004 0.
  0.    0.    0.153 0.    0.006 0.   ]
 [0.    0.    0.005 0.617 0.001 0.023 0.011 0.087 0.011 0.001 0.    0.005
  0.    0.    0.    0.023 0.    0.127 1.182 0.025 0.005 0.    2.415 0.003
  0.    0.    0.358 0.004 0.177 0.   ]
 [0.    0.    0.058 0.076 0.016 0.099 1.495 0.027 0.003 0.021 0.    0.016
  0.002 0.    0.    0.08  0.    0.057 0.064 0.059 0.031 0.068 1.695 0.038
  0.    0.038 0.074 0.06  0.009 0.053]
 [0.    0.003 0.047 0.063 0.179 0.043 0.008 2.522 0.041 0.    0.    0.307
  0.    0.    0.    0.142 0.    1.158 0.07  0.035 0.003 0.    0.167 1.561
  0.    0.002 0.202 0.029 2.217 0.07 ]
 [0.    0.    0.041 0.01  0.178 0.025 0.033 0.066 0.003 0.    0.    0.002
  0.    0.    0.    0.003 0.    0.602 0.056 0.03  0.001 0.001 0.014 0.003
  0.    0.    0.004 0.01  0.044 0.002]
 [0.    0.    0.037 0.088 0.    0.035 0.03  1.36  0.163 0.001 0.    0.007
  0.    0.    0.    0.02  0.    0.034 0.039 0.054 0.043 0.005 0.011 0.02
  0.    0.014 0.    0.003 0.024 0.013]
 [0.    0.    0.036 0.003 0.052 0.111 0.016 0.006 0.014 0.    0.    0.002
  0.    0.    0.    0.021 0.    0.002 0.009 0.059 0.    0.    0.045 0.001
  0.    0.    0.    0.    0.002 0.002]
 [0.    0.    0.03  0.081 0.061 0.619 0.202 0.047 0.036 0.001 0.    0.018
  0.    0.    0.    0.002 0.    0.192 1.025 0.037 0.007 0.    0.057 0.379
  0.    0.    0.039 0.016 0.069 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.348 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.878 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.68  1.879 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.83  0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.484 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.755 1.089 0.    0.    1.651 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.132 0.    0.634 0.    0.    0.    2.437
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.563 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.9   0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    4.125 0.    0.   ]
 [0.    0.    4.283 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.399 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.878 0.    1.18  0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.31
  0.    0.    0.437 0.    0.    3.056]
 [0.    0.    0.    0.    1.226 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.911 0.    1.339 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.707
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.746
  0.    3.165 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.453 0.
  0.    0.    0.726 0.    0.    0.   ]
 [0.    0.    2.537 0.    0.    0.    0.    0.    0.456 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    2.833 0.399 0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.532 0.    0.    1.116 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.839 0.    0.    1.733 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    2.129 0.   ]
 [0.    0.    0.    1.338 1.121 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.694 0.    0.    0.332 0.    0.    0.
  0.    0.    0.    0.    1.58  2.032]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.617 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.182 0.    0.    0.    2.415 0.
  0.    0.    0.358 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.495 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.695 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    2.522 0.    0.    0.    0.307
  0.    0.    0.    0.    0.    1.158 0.    0.    0.    0.    0.    1.561
  0.    0.    0.    0.    2.217 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.602 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.36  0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.619 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.025 0.    0.    0.    0.    0.379
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.11475409836065574, 'tpr': 0.9, 'fpr': 0.018666666666666668, 'f1': 0.8925619834710743, 'shd': 9, 'npred': 61, 'ntrue': 60}
[2.582e-04 1.006e-01 4.590e-02 5.744e-02 2.020e-04 1.131e-01 2.204e-02
 5.643e-02 3.477e-01 1.062e-03 5.173e-02 1.160e-04 2.478e-04 1.773e-04
 2.614e-02 1.904e-04 4.876e-02 5.546e-02 8.784e-01 4.443e-02 2.760e-03
 1.502e-01 2.142e-02 3.480e-05 3.380e-02 1.200e-02 1.750e-02 2.231e-02
 2.214e-02 6.856e-04 1.714e-02 3.259e-03 1.122e-02 6.800e-01 1.879e+00
 8.458e-02 1.130e-01 5.068e-04 1.797e-04 2.631e-02 1.542e-04 3.205e-04
 2.366e-04 5.659e-02 2.254e-04 7.183e-02 2.728e-02 5.190e-02 3.462e-03
 1.988e-04 1.030e-01 5.457e-02 1.573e-04 1.263e-02 1.328e-03 8.092e-02
 8.493e-02 2.922e-02 1.570e-05 1.209e-04 8.844e-03 1.059e-01 3.719e-04
 6.822e-02 4.912e-02 1.101e-03 7.920e-06 9.823e-06 5.962e-05 3.100e-05
 1.223e-05 6.267e-07 5.322e-03 2.631e-05 1.458e-02 5.821e-02 1.219e-01
 7.484e-04 4.393e-04 1.123e-02 2.014e-02 1.334e-04 3.644e-05 1.777e-02
 1.302e-02 2.286e-02 8.103e-03 8.548e-05 1.488e-05 6.032e-03 1.991e-02
 2.785e-03 2.017e-02 4.605e-04 3.455e-04 8.422e-05 1.286e-06 8.803e-04
 5.686e-05 1.400e-05 6.565e-06 1.944e-03 1.815e-05 7.371e-05 9.928e-04
 5.391e-02 3.873e-04 1.128e-04 1.246e-03 1.143e-03 3.425e-05 2.668e-05
 1.149e-04 5.855e-05 5.928e-02 4.474e-04 1.186e-04 2.984e-04 2.624e-03
 6.045e-03 2.432e-03 1.220e-02 1.252e-02 1.509e-03 2.072e-05 6.558e-05
 3.047e-04 2.976e-05 2.719e-05 5.807e-06 7.735e-04 1.603e-05 6.935e-04
 1.035e-03 1.640e-01 4.360e-04 8.897e-05 7.876e-03 1.320e-03 1.922e-04
 1.605e-04 1.248e-03 9.197e-03 1.232e-02 1.142e-03 3.990e-05 2.721e-05
 5.047e-02 4.093e-02 1.482e-02 4.852e-02 4.232e-02 8.622e-02 1.342e-04
 1.902e-04 5.976e-02 1.403e-05 1.447e-04 1.039e-04 2.983e-03 6.820e-06
 6.396e-03 3.798e-03 8.302e-01 6.324e-04 1.394e-04 2.853e-02 9.061e-03
 1.708e-04 5.626e-04 9.004e-03 3.393e-03 3.212e-03 1.159e-03 5.672e-05
 8.891e-06 1.062e-02 2.783e-02 2.584e-02 4.093e-04 1.931e-02 5.648e-03
 1.103e-04 8.612e-05 5.440e-04 8.038e-06 1.892e-04 2.672e-05 1.709e-02
 7.848e-05 9.525e-03 7.343e-03 9.075e-02 2.899e-04 1.022e-04 9.421e-02
 4.424e-02 3.125e-06 7.925e-05 4.047e-03 9.230e-03 2.106e-02 2.069e-03
 3.546e-05 1.347e-04 9.378e-03 1.484e+00 8.581e-03 1.440e-02 2.748e-02
 8.434e-04 1.059e-04 1.936e-06 1.339e-03 1.330e-04 2.088e-05 1.496e-06
 3.442e-03 3.856e-05 3.667e-04 8.760e-04 9.562e-03 9.462e-04 2.117e-04
 5.723e-03 5.472e-04 1.052e-04 1.099e-04 9.618e-04 3.220e-04 4.260e-02
 1.907e-03 1.612e-04 1.860e-05 5.166e-02 1.509e-03 4.187e-02 5.192e-03
 4.091e-02 5.764e-02 8.391e-04 8.508e-06 5.852e-02 5.733e-06 2.304e-04
 2.152e-04 8.512e-02 2.597e-06 4.721e-02 7.545e-01 1.089e+00 2.406e-04
 5.707e-04 1.651e+00 6.703e-02 2.459e-04 2.636e-03 5.076e-02 1.453e-03
 5.482e-02 8.400e-03 1.032e-04 1.577e-04 1.261e-01 1.419e-01 2.966e-02
 1.132e+00 5.539e-02 6.336e-01 9.109e-02 1.989e-04 2.437e+00 2.115e-04
 2.194e-04 1.884e-04 2.429e-01 2.729e-04 1.753e-01 2.437e-02 9.709e-02
 8.140e-03 2.859e-03 5.978e-02 4.817e-03 5.542e-04 1.232e-02 5.634e-01
 2.073e-02 1.017e-02 2.392e-02 4.553e-03 1.044e-03 3.005e-02 1.868e-01
 3.285e-02 1.557e-02 9.005e-02 7.955e-02 9.002e-01 1.668e-02 3.666e-02
 1.879e-04 2.528e-04 2.179e-02 2.776e-02 9.593e-05 5.814e-02 6.435e-02
 1.035e-01 4.909e-02 1.092e-02 3.051e-02 6.756e-02 4.118e-03 3.786e-02
 8.826e-02 4.125e+00 4.196e-02 1.334e-02 1.565e-05 2.473e-04 4.283e+00
 9.304e-03 2.974e-02 1.167e-02 1.920e-02 5.650e-02 1.275e-02 3.174e-05
 1.170e-04 2.058e-04 1.644e-05 8.681e-06 1.192e-01 2.571e-05 8.745e-02
 3.265e-02 1.505e-01 1.875e-03 3.563e-04 3.479e-03 6.080e-02 1.818e-04
 2.199e-04 3.987e-01 2.420e-02 2.961e-02 1.273e-02 4.655e-03 1.729e-04
 1.491e-03 5.199e-02 4.792e-02 7.444e-03 1.878e+00 1.318e-02 1.180e+00
 1.716e-02 2.497e-03 5.111e-02 2.574e-04 2.046e-04 1.157e-01 2.331e-03
 9.599e-02 6.408e-04 1.841e-04 2.240e-02 1.986e-02 8.475e-02 3.103e-01
 1.349e-04 3.730e-02 4.373e-01 3.901e-02 2.044e-02 3.056e+00 2.215e-04
 1.933e-04 8.216e-02 1.238e-03 1.226e+00 4.264e-02 2.296e-02 1.254e-01
 6.445e-02 2.810e-04 1.863e-04 7.006e-02 1.863e-04 1.832e-04 1.052e-01
 3.235e-03 9.110e-01 3.040e-02 1.339e+00 1.937e-02 1.145e-04 4.297e-02
 3.681e-02 2.267e-04 5.029e-03 1.300e-01 6.735e-02 8.946e-03 3.664e-04
 1.791e-04 1.812e-04 3.257e-02 5.735e-02 6.013e-02 5.908e-02 3.838e-02
 1.008e-01 4.852e-02 2.159e-03 1.523e-04 1.707e+00 4.671e-03 2.901e-03
 1.201e-02 6.544e-05 1.806e-01 2.312e-02 2.501e-03 1.233e-02 1.028e-02
 1.088e-01 7.461e-01 4.630e-03 3.165e+00 1.763e-04 1.249e-02 1.214e-01
 5.647e-02 2.691e-04 1.574e-04 4.687e-02 3.384e-03 7.079e-02 3.028e-02
 1.893e-02 5.447e-02 1.519e-02 5.843e-04 1.953e-05 5.076e-03 6.551e-06
 1.085e-04 6.825e-05 3.944e-05 8.404e-02 9.636e-03 2.590e-02 3.629e-03
 1.600e-03 4.533e-01 1.324e-02 2.877e-05 6.689e-03 7.260e-01 4.774e-03
 8.143e-03 7.400e-02 4.546e-03 2.284e-04 2.537e+00 9.324e-02 8.814e-02
 1.924e-02 3.451e-02 1.337e-02 4.561e-01 9.555e-05 2.238e-04 2.119e-03
 2.245e-04 1.637e-04 2.528e-04 1.314e-02 1.190e-01 1.718e-03 3.378e-02
 2.833e+00 3.986e-01 1.219e-01 1.601e-02 2.095e-04 2.572e-04 4.597e-02
 5.018e-02 7.393e-02 4.124e-02 1.613e-05 1.928e-04 1.380e-02 6.013e-02
 1.532e+00 8.767e-02 1.464e-02 1.116e+00 1.556e-03 1.791e-05 2.302e-05
 3.710e-04 1.259e-05 1.255e-05 1.267e-05 4.354e-03 4.473e-05 2.713e-03
 9.628e-02 2.217e-03 3.831e-04 1.799e-02 8.200e-04 1.744e-04 5.681e-04
 3.634e-03 1.078e-02 2.071e-01 1.351e-03 1.400e-04 2.757e-04 6.764e-03
 2.585e-02 2.053e-02 7.534e-03 5.036e-03 3.623e-02 1.005e-03 1.164e-04
 1.570e-05 1.278e-04 2.208e-05 1.958e-04 1.727e-05 6.697e-04 1.388e-05
 1.336e-01 9.414e-03 1.887e-04 6.283e-05 1.849e-02 6.760e-04 1.325e-04
 1.728e-04 1.040e-02 6.207e-03 3.473e-02 5.557e-04 3.485e-05 1.363e-05
 4.310e-03 3.243e-03 6.733e-04 9.379e-04 7.120e-03 6.198e-05 7.024e-04
 4.824e-05 1.683e-05 5.103e-04 6.377e-06 1.446e-05 1.413e-05 2.252e-02
 1.055e-05 8.977e-04 1.602e-02 1.460e-04 1.364e-04 7.235e-02 9.135e-03
 1.587e-04 4.075e-04 5.934e-03 2.767e-03 8.679e-03 9.478e-04 2.524e-04
 3.472e-04 3.744e-02 2.158e-04 4.187e-03 8.393e-01 1.483e-02 3.191e-02
 1.733e+00 2.207e-04 1.695e-04 6.179e-02 1.728e-04 2.064e-04 1.515e-04
 1.395e-02 5.966e-06 7.527e-02 2.403e-02 8.088e-02 1.489e-03 3.862e-03
 3.060e-02 2.047e-04 1.858e-02 9.086e-03 1.201e-02 2.129e+00 3.709e-02
 2.365e-04 2.934e-04 4.486e-02 1.338e+00 1.121e+00 2.474e-02 5.164e-02
 1.208e-01 2.388e-01 1.776e-04 2.082e-04 4.294e-02 1.351e-04 1.449e-04
 3.222e-04 5.350e-02 1.956e-04 6.945e-01 6.944e-02 6.646e-02 3.316e-01
 9.945e-02 1.979e-01 1.760e-04 2.755e-02 2.282e-01 4.914e-02 1.580e+00
 2.032e+00 2.692e-05 7.507e-06 9.056e-04 1.218e-01 5.625e-02 2.079e-03
 1.689e-03 1.002e-01 2.463e-04 2.750e-04 1.308e-05 2.199e-03 8.723e-06
 1.706e-04 3.642e-06 5.003e-03 2.511e-06 8.098e-03 2.597e-02 5.156e-04
 4.800e-05 4.408e-05 2.356e-04 5.621e-06 6.244e-05 1.528e-01 4.818e-04
 5.781e-03 6.307e-05 2.767e-04 1.397e-05 4.972e-03 6.168e-01 6.331e-04
 2.330e-02 1.149e-02 8.740e-02 1.105e-02 5.127e-04 1.984e-04 4.660e-03
 2.653e-05 9.169e-05 7.074e-06 2.330e-02 2.479e-05 1.266e-01 1.182e+00
 2.484e-02 5.032e-03 3.405e-05 2.415e+00 2.598e-04 3.313e-04 3.582e-01
 3.744e-03 1.765e-01 4.580e-04 1.009e-04 2.168e-04 5.782e-02 7.601e-02
 1.643e-02 9.866e-02 1.495e+00 2.652e-02 3.358e-03 2.068e-02 1.292e-04
 1.637e-02 2.115e-03 2.268e-04 2.528e-05 8.007e-02 1.742e-04 5.727e-02
 6.360e-02 5.921e-02 3.138e-02 6.755e-02 1.695e+00 3.818e-02 3.816e-02
 7.425e-02 6.014e-02 9.349e-03 5.287e-02 2.233e-04 2.986e-03 4.659e-02
 6.254e-02 1.790e-01 4.276e-02 8.017e-03 2.522e+00 4.054e-02 1.829e-04
 2.628e-04 3.069e-01 1.174e-04 1.396e-04 1.146e-05 1.422e-01 1.216e-05
 1.158e+00 7.042e-02 3.509e-02 2.811e-03 4.038e-04 1.667e-01 1.561e+00
 1.890e-04 2.016e-01 2.946e-02 2.217e+00 6.969e-02 3.125e-05 2.127e-04
 4.084e-02 9.564e-03 1.784e-01 2.498e-02 3.277e-02 6.616e-02 2.627e-03
 2.294e-04 1.697e-04 2.485e-03 2.144e-05 3.172e-04 5.015e-06 2.842e-03
 2.477e-05 6.018e-01 5.611e-02 2.961e-02 8.177e-04 1.016e-03 1.362e-02
 3.298e-03 2.908e-05 3.929e-04 9.678e-03 4.351e-02 2.295e-03 1.221e-04
 2.629e-04 3.711e-02 8.782e-02 1.938e-04 3.459e-02 2.970e-02 1.360e+00
 1.631e-01 6.709e-04 1.550e-06 6.761e-03 1.477e-04 1.659e-05 1.440e-04
 2.013e-02 2.658e-05 3.369e-02 3.856e-02 5.446e-02 4.299e-02 4.951e-03
 1.081e-02 1.973e-02 1.559e-04 1.419e-02 4.276e-04 2.404e-02 1.339e-02
 1.581e-04 1.839e-04 3.554e-02 2.821e-03 5.166e-02 1.111e-01 1.603e-02
 5.675e-03 1.378e-02 4.290e-04 9.123e-06 1.648e-03 2.382e-05 1.672e-04
 5.712e-06 2.127e-02 1.459e-06 2.038e-03 9.045e-03 5.892e-02 1.241e-04
 9.567e-05 4.536e-02 1.374e-03 1.105e-04 1.487e-04 4.036e-04 2.869e-04
 1.763e-03 2.248e-04 1.402e-04 2.991e-02 8.088e-02 6.136e-02 6.189e-01
 2.017e-01 4.696e-02 3.643e-02 1.245e-03 2.301e-04 1.776e-02 1.655e-06
 2.546e-04 7.351e-05 1.624e-03 6.188e-06 1.919e-01 1.025e+00 3.731e-02
 6.553e-03 8.620e-05 5.733e-02 3.792e-01 2.001e-05 3.822e-04 3.868e-02
 1.569e-02 6.863e-02]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9598559670781892, 0.9244556423718486)
cuda
9630
cuda
Objective function 291.21 = squared loss an data 35.20 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 416 ; DAG False
||w||^2 0.007906260996271857
exp ma of ||w||^2 4.460475502468029
||w|| 0.08891715805327932
exp ma of ||w|| 0.13060956328177437
||w||^2 0.024828565009322033
exp ma of ||w||^2 0.029235393196202976
||w|| 0.1575708253748835
exp ma of ||w|| 0.16870943363796292
||w||^2 0.030647231890988922
exp ma of ||w||^2 0.030446562060494252
||w|| 0.17506350816486263
exp ma of ||w|| 0.17199328698414257
||w||^2 0.04360270252574583
exp ma of ||w||^2 0.03353561550275812
||w|| 0.20881260145342243
exp ma of ||w|| 0.1808535519789446
||w||^2 0.024165712047669284
exp ma of ||w||^2 0.03779825147680875
||w|| 0.15545324714417927
exp ma of ||w|| 0.1918950419406855
||w||^2 0.04022969021775985
exp ma of ||w||^2 0.052651097723776616
||w|| 0.20057340356527795
exp ma of ||w|| 0.2259512507615134
||w||^2 0.05013892447740099
exp ma of ||w||^2 0.06113328781128338
||w|| 0.2239172268437625
exp ma of ||w|| 0.24376664719897082
cuda
Objective function 30.33 = squared loss an data 28.91 + 0.5*rho*h**2 0.730477 + alpha*h 0.000000 + L2reg 0.25 + L1reg 0.44 ; SHD = 67 ; DAG True
Proportion of microbatches that were clipped  0.7119720546205145
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.208699103446591
iteration 1 in outer loop, alpha = 1.208699103446591, rho = 1.0, h = 1.208699103446591
cuda
9630
cuda
Objective function 31.79 = squared loss an data 28.91 + 0.5*rho*h**2 0.730477 + alpha*h 1.460954 + L2reg 0.25 + L1reg 0.44 ; SHD = 67 ; DAG True
||w||^2 32287.069415526053
exp ma of ||w||^2 827947.0693917287
||w|| 179.68603010675608
exp ma of ||w|| 603.9858437293752
||w||^2 14979.667343089859
exp ma of ||w||^2 494295.5898434987
||w|| 122.39145126637669
exp ma of ||w|| 432.94448241648587
||w||^2 205.52156308770265
exp ma of ||w||^2 23698.87230186418
||w|| 14.336023266153786
exp ma of ||w|| 65.25097246267273
||w||^2 0.03368506474442609
exp ma of ||w||^2 0.025106632651059717
||w|| 0.18353491423820725
exp ma of ||w|| 0.15658868253952288
||w||^2 0.05536560146988798
exp ma of ||w||^2 0.03661016136057629
||w|| 0.2352989618971745
exp ma of ||w|| 0.18841371596868198
cuda
Objective function 30.65 = squared loss an data 28.09 + 0.5*rho*h**2 0.559694 + alpha*h 1.278818 + L2reg 0.29 + L1reg 0.44 ; SHD = 72 ; DAG True
Proportion of microbatches that were clipped  0.7079344475093299
iteration 1 in inner loop, alpha 1.208699103446591 rho 1.0 h 1.0580117740758226
9630
cuda
Objective function 35.69 = squared loss an data 28.09 + 0.5*rho*h**2 5.596945 + alpha*h 1.278818 + L2reg 0.29 + L1reg 0.44 ; SHD = 72 ; DAG True
||w||^2 0.028898053465224588
exp ma of ||w||^2 0.028274749559471818
||w|| 0.1699942748013138
exp ma of ||w|| 0.16604107573094928
||w||^2 0.03030652443203791
exp ma of ||w||^2 0.04385922577580301
||w|| 0.17408769178789726
exp ma of ||w|| 0.20567163081330034
||w||^2 0.04610819393377018
exp ma of ||w||^2 0.05766035854769298
||w|| 0.21472818616513803
exp ma of ||w|| 0.23588645364449753
cuda
Objective function 29.95 = squared loss an data 28.24 + 0.5*rho*h**2 0.662369 + alpha*h 0.439930 + L2reg 0.26 + L1reg 0.35 ; SHD = 57 ; DAG True
Proportion of microbatches that were clipped  0.7155991896524856
iteration 2 in inner loop, alpha 1.208699103446591 rho 10.0 h 0.36396948306615684
9630
cuda
Objective function 35.91 = squared loss an data 28.24 + 0.5*rho*h**2 6.623689 + alpha*h 0.439930 + L2reg 0.26 + L1reg 0.35 ; SHD = 57 ; DAG True
||w||^2 8839525287.523027
exp ma of ||w||^2 3302619421.823523
||w|| 94018.74965943243
exp ma of ||w|| 33952.737100210674
||w||^2 9998481429.706635
exp ma of ||w||^2 4096761340.7642117
||w|| 99992.40686025431
exp ma of ||w|| 42141.916041520235
||w||^2 210756205.92200086
exp ma of ||w||^2 607218413.4868348
||w|| 14517.444882692025
exp ma of ||w|| 21791.100449705013
||w||^2 113890.46037513233
exp ma of ||w||^2 1277130.7208130634
||w|| 337.47660715245485
exp ma of ||w|| 747.1282780291222
||w||^2 563.633553319131
exp ma of ||w||^2 36795.08451025981
||w|| 23.74096782608348
exp ma of ||w|| 81.47963873296389
||w||^2 0.31651609949363907
exp ma of ||w||^2 13.202987270243144
||w|| 0.5625976355208392
exp ma of ||w|| 0.6833253363264216
||w||^2 0.03039152265874829
exp ma of ||w||^2 2.1393654385978618
||w|| 0.17433164560328193
exp ma of ||w|| 0.3598982099988451
||w||^2 0.03371097432565526
exp ma of ||w||^2 0.47189551579612055
||w|| 0.18360548555436806
exp ma of ||w|| 0.22788044636028823
||w||^2 0.030020128721584084
exp ma of ||w||^2 0.035629643207293886
||w|| 0.17326317762751578
exp ma of ||w|| 0.1856769365010074
cuda
Objective function 29.75 = squared loss an data 28.53 + 0.5*rho*h**2 0.564142 + alpha*h 0.128389 + L2reg 0.26 + L1reg 0.27 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.7137709540968197
iteration 3 in inner loop, alpha 1.208699103446591 rho 100.0 h 0.10622074070509058
iteration 2 in outer loop, alpha = 11.830773173955649, rho = 100.0, h = 0.10622074070509058
cuda
9630
cuda
Objective function 30.88 = squared loss an data 28.53 + 0.5*rho*h**2 0.564142 + alpha*h 1.256673 + L2reg 0.26 + L1reg 0.27 ; SHD = 54 ; DAG True
||w||^2 118856643.9025216
exp ma of ||w||^2 372881962.4769814
||w|| 10902.139418596773
exp ma of ||w|| 17140.341456093614
||w||^2 12061.44846492355
exp ma of ||w||^2 315193.3649141543
||w|| 109.82462594939055
exp ma of ||w|| 297.661767086924
||w||^2 0.06611482678742375
exp ma of ||w||^2 23.935123091355052
||w|| 0.25712803578650023
exp ma of ||w|| 0.8526900989116581
||w||^2 0.10839846601860355
exp ma of ||w||^2 17.920522680621094
||w|| 0.32923922308650216
exp ma of ||w|| 0.730624083589588
||w||^2 0.16427890681902993
exp ma of ||w||^2 6.252171925622742
||w|| 0.4053133439932985
exp ma of ||w|| 0.46758694018059943
||w||^2 0.056889539725449426
exp ma of ||w||^2 0.6151481032707506
||w|| 0.23851528195369248
exp ma of ||w|| 0.2498670958085871
||w||^2 0.037022257007504285
exp ma of ||w||^2 0.04754919987424932
||w|| 0.19241168625503047
exp ma of ||w|| 0.19192579597636564
||w||^2 0.019627093887990842
exp ma of ||w||^2 0.032926251731786926
||w|| 0.14009673046859744
exp ma of ||w|| 0.17926406278937618
||w||^2 0.023577353329459323
exp ma of ||w||^2 0.04988764640288877
||w|| 0.15354918863171932
exp ma of ||w|| 0.21955404085781763
cuda
Objective function 30.10 = squared loss an data 28.49 + 0.5*rho*h**2 0.244252 + alpha*h 0.826889 + L2reg 0.28 + L1reg 0.26 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.7034325687327151
iteration 1 in inner loop, alpha 11.830773173955649 rho 100.0 h 0.0698930637612527
9630
cuda
Objective function 32.30 = squared loss an data 28.49 + 0.5*rho*h**2 2.442520 + alpha*h 0.826889 + L2reg 0.28 + L1reg 0.26 ; SHD = 51 ; DAG True
||w||^2 23289.053392557402
exp ma of ||w||^2 583627.9671860896
||w|| 152.6075142073856
exp ma of ||w|| 418.9756107113348
||w||^2 7.942788547818007
exp ma of ||w||^2 1116.242207349906
||w|| 2.818295326579173
exp ma of ||w|| 6.597915351792473
||w||^2 0.058664061141996
exp ma of ||w||^2 0.16345790684127734
||w|| 0.24220664966510727
exp ma of ||w|| 0.2544465187007496
v before min max tensor([[ 5.092e+02, -4.598e+00,  1.291e+00,  ...,  1.125e+00, -4.412e+00,
          2.525e+01],
        [ 4.561e+01, -2.096e+00, -4.005e+00,  ...,  2.277e+01, -7.583e-01,
          1.246e+02],
        [-6.562e+00, -3.517e+00, -5.461e+00,  ..., -2.656e+00,  1.279e+01,
          2.600e+00],
        ...,
        [ 3.024e-01, -4.268e+00, -2.958e+00,  ..., -1.462e+00, -1.864e+00,
          1.233e+02],
        [-8.769e-01, -5.060e+00,  7.348e+00,  ...,  7.575e+00, -4.855e+00,
          1.153e+01],
        [-5.109e+00,  1.006e+00, -3.727e+00,  ...,  1.907e+01,  2.142e+01,
          1.155e+02]], device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.291e+00,  ..., 1.125e+00, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         2.600e+00],
        ...,
        [3.024e-01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 7.348e+00,  ..., 7.575e+00, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.006e+00, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01]], device='cuda:0')
v before min max tensor([-1.796e+00, -2.917e+00,  2.151e+01,  3.024e-01, -3.857e+00, -2.459e+00,
         8.329e+00, -8.054e-02,  1.131e+01, -2.500e+00,  2.834e+01, -5.002e+00,
        -3.760e+00,  1.233e-01,  1.660e+00,  3.848e+01, -3.986e+00, -4.171e+00,
        -4.936e-02, -3.916e+00, -1.792e+00, -2.685e-01, -4.723e+00,  1.494e+00,
        -4.571e+00, -3.217e+00,  4.681e+00,  3.236e+00, -9.673e-01, -1.808e+00,
         1.614e+00,  1.031e+00, -4.333e+00, -1.333e+00, -4.282e+00,  1.312e+01,
         2.107e+01,  4.946e+00, -2.380e+00,  4.682e+00,  1.403e+00, -4.737e+00,
         1.500e+00, -4.255e-01, -3.779e-03, -3.688e+00,  2.735e+00, -4.422e+00,
         7.589e+01, -1.247e+00,  1.959e+01, -5.175e+00, -3.605e+00, -2.428e+00,
         1.550e+01,  9.004e-01,  6.273e+00, -1.612e+00, -2.984e+00, -4.272e+00,
        -2.805e+00, -1.669e+00,  3.657e+01,  6.578e+00, -2.257e+00, -2.332e-01,
         7.929e+00,  4.048e+00,  8.958e+00,  6.297e+00, -2.209e+00, -2.128e+00,
        -2.476e+00,  2.923e+00, -6.823e-01,  9.031e+00,  4.975e-01, -2.184e+00,
        -4.789e+00, -2.738e+00,  1.714e+00, -3.560e+00, -1.626e+00, -3.190e+00,
        -1.628e+00, -3.883e+00, -3.910e+00, -4.008e+00,  3.102e+00, -2.300e+00,
        -5.261e+00,  3.045e+01,  3.486e+00, -3.907e+00,  2.358e+00, -5.345e+00,
        -5.613e+00,  3.080e-01,  9.946e+00, -3.683e+00,  2.776e+01,  2.795e+01,
         9.093e+00, -1.339e+00,  4.239e+00,  2.284e+01, -7.818e-01, -3.931e+00,
         1.414e+00,  2.473e+00,  5.088e+00, -2.889e+00, -4.805e+00, -3.466e+00,
         1.448e+01, -4.634e+00,  1.354e+01,  2.616e+00,  8.243e+00,  3.074e-01,
        -1.327e+00, -5.147e+00,  2.975e-01,  2.717e+00, -1.738e+00, -4.375e+00,
         1.106e+00, -4.434e+00, -4.443e-01,  1.982e+00, -2.388e+00, -4.000e+00,
        -3.672e+00,  7.115e+00, -4.276e+00, -2.949e+00,  1.724e+01, -5.601e+00,
         1.835e+01, -9.494e-01, -1.467e+00, -5.083e+00,  6.684e+00,  4.197e+00,
        -2.028e-01, -1.726e+00, -5.062e+00, -1.937e-02,  1.246e+01, -1.596e+00,
        -4.796e+00,  2.349e+00,  2.351e+00,  3.699e+01, -1.688e-02,  4.013e+00,
        -1.915e+00, -3.727e+00, -3.242e+00,  1.130e+00,  3.353e+00,  4.877e-01,
        -3.084e+00, -4.306e+00, -6.292e-01,  6.736e+00, -5.339e-01,  2.243e+00,
        -3.333e+00,  5.722e+00, -3.830e+00,  9.191e+00, -4.244e+00, -1.115e+00,
         1.195e+01,  6.797e+00, -2.338e+00, -3.827e-01,  9.266e-01, -2.755e+00,
        -2.750e+00, -4.326e+00,  5.000e+00,  2.216e+01,  2.141e+01, -4.587e+00,
        -5.015e+00,  7.213e+00, -1.739e+00,  4.589e+00, -4.227e+00, -2.632e+00,
        -1.360e+00, -3.694e+00,  9.385e+00,  4.471e+00,  2.122e+01, -1.495e+00,
         1.105e+00, -5.073e+00, -3.006e+00, -3.687e-01, -3.421e+00, -3.741e+00,
        -2.459e+00,  1.247e+01, -4.706e+00, -4.728e+00, -2.552e+00, -2.156e+00,
         2.236e+00, -4.616e+00, -4.849e+00,  8.022e+00, -1.034e+00,  3.973e+01,
        -3.252e+00,  5.635e+00, -2.768e+00, -1.483e+00,  3.012e-01, -2.773e+00,
        -3.380e+00, -5.881e+00, -3.851e+00, -3.374e+00,  2.793e+01,  1.449e+01,
        -3.538e+00, -3.926e+00,  1.185e+01, -2.380e+00,  1.391e+00, -2.293e+00,
        -4.440e+00,  4.165e+00, -7.062e-01,  2.386e+00, -3.685e+00, -3.774e+00,
         2.204e-01, -4.018e+00,  1.742e+01, -3.485e+00, -4.159e+00, -2.730e+00,
         9.660e+00, -3.067e+00, -1.147e+00, -1.362e+00, -2.139e+00,  1.075e-01,
         4.250e+00,  7.390e+00, -4.418e-01,  4.372e-02,  1.262e+00, -3.013e+00,
        -3.252e+00,  4.844e+00, -3.028e+00,  4.940e+00, -5.378e+00, -3.021e+00,
        -3.448e+00, -7.617e-01, -4.943e+00, -4.096e+00, -3.299e+00, -3.723e+00,
         6.075e+01, -5.468e-01, -5.701e+00,  6.630e+00, -1.320e+00, -1.197e+00,
        -4.361e+00, -7.133e-01,  1.173e+00,  5.656e-01, -1.108e+00,  2.859e+00,
        -1.916e+00, -2.798e+00, -5.147e-01,  8.562e+00,  5.249e+00,  2.402e+01,
        -4.558e+00, -1.788e+00, -5.379e+00, -3.382e+00, -3.278e+00, -5.085e+00,
        -3.441e+00,  8.056e+00,  9.069e+00, -2.205e+00,  1.637e+01, -1.346e+00],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 3.024e-01, 1.000e-12, 1.000e-12,
        8.329e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.233e-01, 1.660e+00, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.494e+00,
        1.000e-12, 1.000e-12, 4.681e+00, 3.236e+00, 1.000e-12, 1.000e-12,
        1.614e+00, 1.031e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 4.946e+00, 1.000e-12, 4.682e+00, 1.403e+00, 1.000e-12,
        1.500e+00, 1.000e-12, 1.000e-12, 1.000e-12, 2.735e+00, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 9.004e-01, 6.273e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 6.578e+00, 1.000e-12, 1.000e-12,
        7.929e+00, 4.048e+00, 8.958e+00, 6.297e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 2.923e+00, 1.000e-12, 9.031e+00, 4.975e-01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.714e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.102e+00, 1.000e-12,
        1.000e-12, 1.000e+01, 3.486e+00, 1.000e-12, 2.358e+00, 1.000e-12,
        1.000e-12, 3.080e-01, 9.946e+00, 1.000e-12, 1.000e+01, 1.000e+01,
        9.093e+00, 1.000e-12, 4.239e+00, 1.000e+01, 1.000e-12, 1.000e-12,
        1.414e+00, 2.473e+00, 5.088e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 2.616e+00, 8.243e+00, 3.074e-01,
        1.000e-12, 1.000e-12, 2.975e-01, 2.717e+00, 1.000e-12, 1.000e-12,
        1.106e+00, 1.000e-12, 1.000e-12, 1.982e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 7.115e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 6.684e+00, 4.197e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 2.349e+00, 2.351e+00, 1.000e+01, 1.000e-12, 4.013e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.130e+00, 3.353e+00, 4.877e-01,
        1.000e-12, 1.000e-12, 1.000e-12, 6.736e+00, 1.000e-12, 2.243e+00,
        1.000e-12, 5.722e+00, 1.000e-12, 9.191e+00, 1.000e-12, 1.000e-12,
        1.000e+01, 6.797e+00, 1.000e-12, 1.000e-12, 9.266e-01, 1.000e-12,
        1.000e-12, 1.000e-12, 5.000e+00, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 7.213e+00, 1.000e-12, 4.589e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 9.385e+00, 4.471e+00, 1.000e+01, 1.000e-12,
        1.105e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        2.236e+00, 1.000e-12, 1.000e-12, 8.022e+00, 1.000e-12, 1.000e+01,
        1.000e-12, 5.635e+00, 1.000e-12, 1.000e-12, 3.012e-01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.391e+00, 1.000e-12,
        1.000e-12, 4.165e+00, 1.000e-12, 2.386e+00, 1.000e-12, 1.000e-12,
        2.204e-01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        9.660e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.075e-01,
        4.250e+00, 7.390e+00, 1.000e-12, 4.372e-02, 1.262e+00, 1.000e-12,
        1.000e-12, 4.844e+00, 1.000e-12, 4.940e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 6.630e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.173e+00, 5.656e-01, 1.000e-12, 2.859e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 8.562e+00, 5.249e+00, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 8.056e+00, 9.069e+00, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 1.361e+01],
         [-2.032e+00],
         [ 2.387e+01],
         [ 4.321e-01],
         [ 2.234e+00],
         [ 1.282e+01],
         [-4.862e+00],
         [ 7.416e+00],
         [-3.167e+00],
         [ 8.851e+00]],

        [[-3.517e+00],
         [-4.300e+00],
         [-1.215e+00],
         [-5.009e+00],
         [-1.964e+00],
         [ 1.762e+00],
         [ 3.433e+00],
         [-1.695e+00],
         [ 8.239e+00],
         [ 9.120e+00]],

        [[ 5.936e-01],
         [-3.021e+00],
         [-4.018e+00],
         [-2.838e+00],
         [-3.395e+00],
         [-6.972e-01],
         [ 3.931e+00],
         [-4.509e+00],
         [ 1.048e+00],
         [ 6.195e-01]],

        [[-5.057e+00],
         [ 9.322e+00],
         [ 4.173e+00],
         [ 1.846e+01],
         [ 2.285e+00],
         [ 3.578e+01],
         [-2.423e+00],
         [-3.843e+00],
         [ 6.049e+00],
         [ 6.657e+00]],

        [[ 1.245e+00],
         [ 2.813e+00],
         [ 6.680e+00],
         [ 9.107e+00],
         [ 8.214e+00],
         [-4.830e+00],
         [-4.692e+00],
         [-5.042e+00],
         [-4.548e+00],
         [-3.360e+00]],

        [[ 1.526e+00],
         [-3.419e+00],
         [ 1.896e-01],
         [-3.643e+00],
         [-2.541e+00],
         [ 1.466e-01],
         [ 2.410e+00],
         [-1.562e+00],
         [-8.279e-01],
         [ 4.209e+01]],

        [[ 2.528e+01],
         [-3.624e+00],
         [-4.112e+00],
         [ 3.962e+00],
         [ 8.258e-01],
         [-2.509e+00],
         [-1.378e+00],
         [-4.105e+00],
         [-4.189e+00],
         [ 5.539e+00]],

        [[-3.091e-01],
         [-4.815e+00],
         [ 2.196e+01],
         [-2.991e+00],
         [-4.728e-01],
         [ 8.314e+00],
         [-1.572e+00],
         [-1.718e+00],
         [-4.016e-01],
         [ 7.089e+00]],

        [[-2.177e+00],
         [-3.892e+00],
         [-3.653e+00],
         [-3.150e+00],
         [-4.115e+00],
         [-4.522e+00],
         [ 1.296e+01],
         [ 1.012e+01],
         [ 2.077e+01],
         [ 9.919e+00]],

        [[-4.456e+00],
         [-1.773e+00],
         [ 5.813e+00],
         [-5.041e+00],
         [-3.023e+00],
         [-2.463e+00],
         [ 6.160e-01],
         [-2.889e+00],
         [-1.771e+00],
         [ 7.162e+00]],

        [[-4.806e+00],
         [-2.077e+00],
         [-3.049e+00],
         [-2.004e+00],
         [-1.407e+00],
         [ 4.099e+00],
         [-2.229e+00],
         [ 1.370e+00],
         [-5.481e-01],
         [ 5.813e+00]],

        [[ 4.710e-04],
         [-4.009e+00],
         [-3.132e+00],
         [ 7.708e+00],
         [-7.782e-01],
         [-2.550e+00],
         [ 3.106e+00],
         [ 7.523e-02],
         [-3.662e+00],
         [-4.475e+00]],

        [[-6.003e-01],
         [-3.679e+00],
         [ 2.085e+00],
         [ 2.207e+00],
         [-2.546e+00],
         [ 2.084e+01],
         [-1.510e+00],
         [ 2.098e+00],
         [-3.589e+00],
         [ 5.231e-01]],

        [[-4.024e+00],
         [ 2.043e+01],
         [-3.018e+00],
         [-3.352e+00],
         [-3.136e+00],
         [-3.926e+00],
         [-4.035e+00],
         [-3.046e+00],
         [-5.283e+00],
         [-2.487e+00]],

        [[-4.930e+00],
         [-4.710e-01],
         [ 1.495e+01],
         [-9.347e-01],
         [-1.117e+00],
         [-1.837e+00],
         [-3.132e-01],
         [-3.533e+00],
         [ 2.747e+00],
         [ 1.764e+00]],

        [[-4.245e+00],
         [-3.443e+00],
         [-5.260e+00],
         [ 3.099e+00],
         [-3.325e+00],
         [-5.016e+00],
         [-3.224e+00],
         [ 8.271e-01],
         [ 5.209e-01],
         [-1.238e+00]],

        [[-3.724e+00],
         [-5.826e+00],
         [ 1.523e+00],
         [ 2.007e-01],
         [-2.902e+00],
         [-2.300e+00],
         [ 1.638e+00],
         [-6.704e-01],
         [-3.475e+00],
         [-1.789e+00]],

        [[-1.053e+00],
         [-5.228e-01],
         [-3.484e+00],
         [-4.182e+00],
         [ 4.574e+01],
         [-1.552e+00],
         [ 6.112e+00],
         [ 2.806e+00],
         [ 1.106e+01],
         [-1.570e+00]],

        [[ 4.474e-01],
         [-5.112e+00],
         [-9.470e-01],
         [ 1.628e+01],
         [-3.128e+00],
         [-3.302e+00],
         [ 3.620e+00],
         [-2.977e+00],
         [ 1.836e+01],
         [ 2.283e+01]],

        [[ 9.685e+00],
         [-3.280e+00],
         [-4.071e+00],
         [-3.386e+00],
         [ 1.631e+01],
         [-4.735e+00],
         [ 2.766e+00],
         [-3.865e+00],
         [ 5.015e+00],
         [-2.560e+00]],

        [[-1.974e+00],
         [-4.626e-01],
         [ 8.411e-01],
         [-3.620e+00],
         [ 5.222e+00],
         [-3.381e+00],
         [ 5.446e+00],
         [-4.153e+00],
         [-5.469e+00],
         [-8.757e-02]],

        [[-3.044e+00],
         [-1.470e+00],
         [ 5.571e+00],
         [ 4.797e+00],
         [-2.022e+00],
         [-3.250e+00],
         [-2.869e+00],
         [-6.662e-01],
         [-4.201e+00],
         [-2.826e+00]],

        [[-2.714e+00],
         [-4.012e+00],
         [ 3.794e+00],
         [-4.001e+00],
         [-2.167e+00],
         [-4.076e+00],
         [-6.935e-01],
         [ 7.369e-01],
         [-7.817e-01],
         [-5.314e+00]],

        [[-4.027e+00],
         [-2.695e+00],
         [-2.972e+00],
         [-3.827e+00],
         [-3.307e+00],
         [-4.008e+00],
         [-2.391e+00],
         [-4.046e+00],
         [-2.298e+00],
         [ 2.498e+01]],

        [[-3.124e+00],
         [-4.442e+00],
         [-2.968e+00],
         [ 1.262e+01],
         [-4.709e+00],
         [ 1.923e-01],
         [-9.616e-01],
         [-4.137e+00],
         [ 7.457e+00],
         [ 7.703e+00]],

        [[-2.851e+00],
         [-1.556e+00],
         [ 6.693e+01],
         [-5.022e+00],
         [-3.610e-01],
         [-1.581e+00],
         [-6.134e+00],
         [-5.109e+00],
         [-1.707e+00],
         [-1.682e+00]],

        [[ 4.217e+00],
         [ 6.484e+00],
         [ 2.341e+01],
         [-3.021e+00],
         [ 1.297e+01],
         [-3.289e+00],
         [ 1.079e+01],
         [-7.423e-01],
         [-3.496e+00],
         [ 1.730e-01]],

        [[-3.406e+00],
         [ 1.052e+01],
         [-1.155e+00],
         [ 1.093e+00],
         [ 3.145e+01],
         [-3.199e+00],
         [-7.218e-02],
         [ 2.626e-01],
         [-3.889e+00],
         [ 1.508e+01]],

        [[-3.376e+00],
         [-1.540e+00],
         [-2.935e+00],
         [ 2.253e+00],
         [-3.880e+00],
         [ 3.689e-01],
         [ 2.262e+00],
         [-2.193e+00],
         [-3.512e+00],
         [ 3.522e+00]],

        [[ 4.521e+00],
         [-2.655e+00],
         [ 5.102e+00],
         [-4.094e+00],
         [-4.725e+00],
         [-3.916e+00],
         [-2.313e+00],
         [-2.076e+00],
         [-1.385e+00],
         [-4.549e+00]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [4.321e-01],
         [2.234e+00],
         [1.000e+01],
         [1.000e-12],
         [7.416e+00],
         [1.000e-12],
         [8.851e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.762e+00],
         [3.433e+00],
         [1.000e-12],
         [8.239e+00],
         [9.120e+00]],

        [[5.936e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.931e+00],
         [1.000e-12],
         [1.048e+00],
         [6.195e-01]],

        [[1.000e-12],
         [9.322e+00],
         [4.173e+00],
         [1.000e+01],
         [2.285e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [6.049e+00],
         [6.657e+00]],

        [[1.245e+00],
         [2.813e+00],
         [6.680e+00],
         [9.107e+00],
         [8.214e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.526e+00],
         [1.000e-12],
         [1.896e-01],
         [1.000e-12],
         [1.000e-12],
         [1.466e-01],
         [2.410e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [3.962e+00],
         [8.258e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.539e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [8.314e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.089e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [9.919e+00]],

        [[1.000e-12],
         [1.000e-12],
         [5.813e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.160e-01],
         [1.000e-12],
         [1.000e-12],
         [7.162e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.099e+00],
         [1.000e-12],
         [1.370e+00],
         [1.000e-12],
         [5.813e+00]],

        [[4.710e-04],
         [1.000e-12],
         [1.000e-12],
         [7.708e+00],
         [1.000e-12],
         [1.000e-12],
         [3.106e+00],
         [7.523e-02],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [2.085e+00],
         [2.207e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [2.098e+00],
         [1.000e-12],
         [5.231e-01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.747e+00],
         [1.764e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.099e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.271e-01],
         [5.209e-01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.523e+00],
         [2.007e-01],
         [1.000e-12],
         [1.000e-12],
         [1.638e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [6.112e+00],
         [2.806e+00],
         [1.000e+01],
         [1.000e-12]],

        [[4.474e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [3.620e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[9.685e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [2.766e+00],
         [1.000e-12],
         [5.015e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [8.411e-01],
         [1.000e-12],
         [5.222e+00],
         [1.000e-12],
         [5.446e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [5.571e+00],
         [4.797e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [3.794e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.369e-01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.923e-01],
         [1.000e-12],
         [1.000e-12],
         [7.457e+00],
         [7.703e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[4.217e+00],
         [6.484e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.730e-01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.093e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [2.626e-01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.253e+00],
         [1.000e-12],
         [3.689e-01],
         [2.262e+00],
         [1.000e-12],
         [1.000e-12],
         [3.522e+00]],

        [[4.521e+00],
         [1.000e-12],
         [5.102e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-3.441],
        [-4.348],
        [-4.160],
        [-5.436],
        [-2.630],
        [-3.144],
        [-1.531],
        [-3.495],
        [10.001],
        [ 2.440],
        [-1.310],
        [-2.802],
        [-0.083],
        [-0.311],
        [-3.408],
        [ 7.429],
        [11.488],
        [ 4.145],
        [-2.698],
        [-1.943],
        [ 0.085],
        [-3.456],
        [-2.846],
        [ 0.202],
        [10.942],
        [ 1.946],
        [38.824],
        [-2.905],
        [ 0.634],
        [-4.863]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [2.440e+00],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [7.429e+00],
        [1.000e+01],
        [4.145e+00],
        [1.000e-12],
        [1.000e-12],
        [8.484e-02],
        [1.000e-12],
        [1.000e-12],
        [2.019e-01],
        [1.000e+01],
        [1.946e+00],
        [1.000e+01],
        [1.000e-12],
        [6.336e-01],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.014, -0.017, -0.002,  ..., -0.010, -0.007, -0.001],
        [-0.010, -0.029, -0.014,  ...,  0.033, -0.039, -0.004],
        [ 0.034,  0.001,  0.012,  ..., -0.003, -0.052,  0.009],
        ...,
        [ 0.042, -0.009, -0.014,  ..., -0.038, -0.007,  0.071],
        [-0.037, -0.042, -0.022,  ..., -0.009,  0.020, -0.036],
        [-0.011, -0.048,  0.004,  ...,  0.002, -0.002, -0.031]],
       device='cuda:0')
s after update for 1 param tensor([[2.462, 1.633, 1.601,  ..., 2.041, 1.628, 2.223],
        [2.231, 1.623, 1.616,  ..., 1.806, 1.695, 2.310],
        [2.366, 1.327, 1.999,  ..., 0.938, 2.203, 1.666],
        ...,
        [2.025, 1.520, 1.713,  ..., 1.669, 1.493, 2.616],
        [1.850, 1.804, 1.810,  ..., 2.134, 1.714, 2.333],
        [1.919, 1.881, 2.055,  ..., 2.083, 1.507, 2.254]], device='cuda:0')
b after update for 1 param tensor([[200.307, 163.124, 161.524,  ..., 182.379, 162.872, 190.343],
        [190.668, 162.628, 162.304,  ..., 171.547, 166.229, 194.014],
        [196.363, 147.040, 180.487,  ..., 123.626, 189.470, 164.798],
        ...,
        [181.651, 157.417, 167.071,  ..., 164.942, 156.011, 206.466],
        [173.654, 171.445, 171.762,  ..., 186.500, 167.160, 195.011],
        [176.837, 175.097, 183.008,  ..., 184.244, 156.732, 191.669]],
       device='cuda:0')
clipping threshold 0.21512474853382574
a after update for 1 param tensor([ 1.634e-02, -6.861e-02, -1.210e-02,  5.819e-02, -2.054e-02,  5.173e-03,
         2.826e-02,  2.353e-02,  1.594e-02,  1.034e-02, -2.691e-02, -2.244e-02,
         4.479e-02,  1.190e-02, -4.131e-03, -4.988e-03, -1.389e-02, -4.885e-02,
         9.219e-02,  4.438e-02,  2.646e-02,  1.460e-03, -1.440e-02,  1.891e-02,
        -2.394e-02, -1.837e-02,  3.620e-02, -4.831e-02, -1.274e-02, -2.093e-02,
        -7.035e-03, -3.258e-02, -1.779e-02, -2.398e-02,  1.867e-02,  5.312e-03,
         5.090e-02,  1.305e-03,  1.085e-02,  2.349e-02, -5.437e-03,  7.048e-03,
        -5.897e-03, -3.338e-02,  2.429e-03,  2.089e-02, -2.428e-02,  5.680e-02,
        -3.693e-03, -7.781e-03,  9.698e-03, -1.668e-02, -5.764e-03, -1.724e-02,
         4.804e-03, -6.811e-03, -1.989e-02, -4.654e-04,  2.297e-02,  1.152e-02,
        -2.041e-02, -3.663e-02, -6.248e-02,  2.713e-02,  9.494e-03, -1.310e-02,
         9.896e-02,  9.241e-02,  6.315e-03, -3.406e-02,  5.727e-03,  1.569e-03,
         4.725e-03, -6.249e-02, -6.716e-02, -5.816e-03,  5.347e-04, -3.258e-03,
        -1.001e-01, -2.169e-02,  1.261e-02,  1.299e-03, -5.873e-03,  1.253e-02,
         1.375e-02,  4.764e-02, -6.412e-02, -5.300e-03,  2.987e-02,  1.540e-02,
         1.218e-02,  6.728e-02,  6.918e-03, -1.119e-02,  1.502e-02, -7.114e-03,
         4.316e-02, -1.796e-02, -6.873e-02,  1.419e-02,  1.668e-02, -6.891e-02,
         5.642e-04,  4.177e-03,  1.602e-02,  4.454e-02,  6.190e-05, -1.185e-02,
        -1.549e-02, -3.937e-03,  2.899e-02,  2.407e-02,  3.560e-03, -2.330e-03,
        -2.205e-02, -1.917e-02, -1.799e-03, -7.218e-03, -9.838e-02,  6.465e-04,
        -3.189e-02, -5.617e-02,  4.436e-03, -6.202e-03, -2.061e-02,  3.098e-02,
        -6.588e-03,  8.248e-02,  6.571e-02,  2.885e-02,  2.193e-02,  1.254e-02,
         1.327e-02, -5.845e-02,  2.125e-02,  6.362e-03, -2.716e-02,  1.609e-02,
         9.194e-03,  5.140e-02,  9.006e-03, -3.468e-03, -1.709e-02,  4.942e-02,
        -8.476e-02, -2.678e-03, -1.817e-02,  4.516e-03,  1.452e-02,  2.192e-02,
         9.094e-03,  3.053e-02, -2.339e-02, -1.308e-02,  2.092e-02,  8.994e-03,
         4.442e-02, -2.203e-02, -3.415e-02, -3.108e-03,  2.243e-03, -1.688e-04,
         4.186e-02,  7.755e-02,  1.951e-02, -1.991e-03,  1.443e-02, -9.868e-03,
        -1.936e-02,  1.481e-02,  1.167e-02,  2.595e-02, -3.419e-02,  9.089e-02,
        -2.519e-02,  3.322e-03, -2.660e-02,  5.436e-02, -2.241e-02,  1.716e-02,
        -4.574e-02,  5.419e-03,  1.251e-02, -1.393e-02,  7.920e-03, -7.609e-02,
        -6.208e-02, -2.575e-02,  3.136e-03, -1.511e-03,  1.005e-02, -2.434e-02,
        -1.846e-02,  1.062e-02,  1.678e-02, -3.865e-02, -3.969e-02, -1.190e-03,
        -5.449e-04,  4.867e-03, -2.194e-03,  1.257e-03, -1.272e-03, -2.082e-02,
        -1.673e-02, -1.785e-02, -3.109e-02,  5.644e-03,  5.300e-02,  1.089e-02,
        -9.486e-03, -4.458e-02,  1.286e-02, -9.536e-03,  1.043e-03,  1.332e-03,
         3.216e-02, -9.786e-03, -5.102e-03,  8.445e-03,  3.608e-02,  2.463e-02,
        -1.127e-02, -2.558e-02, -3.467e-02,  3.101e-02,  3.673e-03, -1.032e-01,
         6.195e-02,  6.383e-03, -1.993e-02,  3.120e-03,  3.400e-03, -1.938e-02,
         7.595e-03, -5.553e-04, -2.180e-02,  7.392e-02, -3.304e-02, -3.493e-02,
         1.275e-02, -1.356e-02,  3.056e-02,  3.872e-03,  5.477e-03, -1.561e-02,
        -3.162e-02, -6.848e-02, -4.710e-02,  1.029e-03,  6.092e-03, -5.047e-02,
        -7.952e-03,  2.218e-02, -4.036e-02,  2.475e-02, -2.411e-02, -3.739e-02,
        -1.333e-02, -3.108e-02, -1.633e-02, -5.866e-02,  7.140e-04,  2.281e-03,
        -5.652e-02,  2.156e-03,  1.211e-03,  1.364e-03,  1.622e-02, -3.455e-02,
         1.643e-03, -1.026e-02, -7.239e-05,  3.747e-02,  4.846e-03,  2.116e-03,
         3.045e-02, -6.527e-03,  5.755e-03,  3.947e-02,  1.651e-02, -9.882e-02,
         9.353e-03, -3.398e-02, -3.157e-02, -2.679e-02, -1.693e-02,  1.027e-02,
         3.828e-03, -2.109e-02,  3.768e-02,  1.260e-02, -3.431e-02, -9.464e-02,
        -1.510e-02, -4.651e-02,  2.939e-03, -3.813e-02, -7.408e-02, -6.725e-03],
       device='cuda:0')
s after update for 1 param tensor([1.641, 2.028, 2.182, 1.529, 1.468, 1.435, 1.708, 1.780, 2.104, 1.808,
        1.759, 1.802, 2.064, 1.289, 1.718, 1.783, 1.598, 1.683, 1.836, 2.038,
        1.747, 0.113, 1.694, 1.754, 1.685, 1.136, 1.984, 1.995, 1.668, 1.407,
        2.046, 1.539, 1.639, 1.869, 1.528, 1.994, 1.774, 1.013, 1.147, 1.966,
        1.389, 1.823, 1.707, 1.971, 0.286, 1.400, 1.848, 1.613, 2.086, 0.848,
        2.290, 1.828, 1.746, 1.086, 2.088, 1.865, 1.819, 1.727, 1.910, 1.531,
        1.559, 1.759, 2.172, 1.838, 1.481, 1.405, 1.891, 1.809, 2.124, 1.249,
        0.809, 0.827, 1.287, 1.741, 1.381, 2.254, 1.965, 1.529, 1.700, 1.291,
        1.622, 1.275, 0.667, 1.148, 1.586, 1.455, 1.536, 1.472, 1.716, 1.784,
        1.858, 2.058, 2.172, 1.459, 1.441, 1.908, 2.016, 1.698, 2.323, 1.700,
        1.842, 1.936, 1.699, 0.483, 1.458, 1.859, 1.002, 1.394, 1.184, 0.570,
        1.170, 1.394, 1.982, 1.224, 2.156, 1.686, 1.788, 1.429, 1.984, 0.178,
        2.028, 1.906, 0.572, 1.174, 0.748, 1.844, 1.507, 1.872, 1.723, 1.825,
        1.057, 1.419, 1.402, 1.674, 1.529, 1.042, 1.826, 1.980, 1.555, 1.860,
        0.695, 1.796, 2.035, 1.647, 1.770, 1.740, 1.789, 0.113, 1.588, 1.299,
        1.728, 1.732, 1.439, 2.162, 1.569, 1.763, 1.288, 1.409, 1.567, 1.071,
        1.065, 1.540, 1.740, 1.534, 1.762, 1.647, 1.301, 1.005, 1.678, 1.953,
        1.359, 1.890, 1.734, 1.273, 1.412, 1.059, 1.446, 1.396, 1.585, 1.088,
        1.218, 1.816, 1.707, 1.706, 1.469, 1.628, 1.991, 1.613, 0.918, 1.825,
        1.980, 1.705, 1.649, 1.533, 2.038, 1.479, 1.564, 1.756, 0.952, 1.830,
        1.526, 1.533, 1.514, 1.434, 1.125, 1.853, 1.679, 1.670, 2.057, 1.677,
        0.893, 1.670, 1.974, 1.346, 0.638, 1.847, 1.162, 1.927, 1.244, 1.317,
        1.548, 2.046, 1.236, 2.169, 1.712, 1.321, 2.135, 2.213, 2.081, 1.390,
        2.094, 1.219, 1.445, 1.388, 1.573, 1.762, 1.659, 1.652, 1.324, 1.821,
        1.331, 1.440, 1.990, 1.245, 1.512, 1.044, 1.859, 1.711, 1.843, 0.508,
        1.135, 1.982, 2.053, 1.555, 1.793, 1.615, 1.286, 1.905, 1.329, 1.667,
        1.175, 2.177, 2.108, 1.755, 1.418, 0.704, 1.811, 1.448, 1.812, 1.430,
        1.648, 1.702, 2.056, 1.943, 1.372, 0.440, 1.540, 0.870, 0.811, 1.802,
        1.415, 2.312, 1.366, 1.681, 1.493, 1.715, 1.374, 1.401, 1.911, 1.214,
        1.902, 1.455, 1.204, 1.801, 1.219, 2.050, 1.893, 1.752, 1.959, 1.670],
       device='cuda:0')
b after update for 1 param tensor([163.516, 181.780, 188.575, 157.854, 154.667, 152.940, 166.842, 170.314,
        185.166, 171.677, 169.339, 171.378, 183.424, 144.918, 167.332, 170.444,
        161.359, 165.609, 172.983, 182.234, 168.732,  42.957, 166.141, 169.054,
        165.739, 136.080, 179.813, 180.298, 164.866, 151.414, 182.603, 158.355,
        163.429, 174.546, 157.795, 180.275, 170.030, 128.503, 136.704, 179.018,
        150.435, 172.353, 166.776, 179.217,  68.298, 151.026, 173.567, 162.144,
        184.397, 117.548, 193.204, 172.584, 168.701, 133.050, 184.471, 174.365,
        172.191, 167.769, 176.419, 157.979, 159.395, 169.293, 188.142, 173.066,
        155.351, 151.302, 175.537, 171.710, 186.068, 142.658, 114.791, 116.099,
        144.814, 168.461, 150.037, 191.654, 178.946, 157.850, 166.465, 145.032,
        162.596, 144.135, 104.265, 136.789, 160.779, 154.001, 158.202, 154.882,
        167.229, 170.517, 174.006, 183.146, 188.163, 154.203, 153.268, 176.354,
        181.279, 166.340, 194.574, 166.448, 173.280, 177.608, 166.421,  88.698,
        154.134, 174.065, 127.765, 150.719, 138.927,  96.363, 138.073, 150.745,
        179.719, 141.245, 187.457, 165.743, 170.704, 152.618, 179.811,  53.803,
        181.794, 176.271,  96.553, 138.309, 110.407, 173.348, 156.736, 174.660,
        167.596, 172.468, 131.233, 152.094, 151.176, 165.153, 157.872, 130.296,
        172.495, 179.631, 159.205, 174.091, 106.429, 171.096, 182.107, 163.849,
        169.824, 168.420, 170.750,  42.873, 160.871, 145.518, 167.840, 168.034,
        153.166, 187.706, 159.897, 169.506, 144.888, 151.511, 159.796, 132.104,
        131.770, 158.444, 168.405, 158.120, 169.438, 163.836, 145.611, 127.953,
        165.378, 178.423, 148.799, 175.504, 168.106, 144.059, 151.706, 131.358,
        153.512, 150.843, 160.717, 133.182, 140.885, 172.054, 166.816, 166.746,
        154.718, 162.906, 180.115, 162.133, 122.331, 172.446, 179.638, 166.701,
        163.960, 158.084, 182.230, 155.275, 159.677, 169.188, 124.540, 172.693,
        157.720, 158.040, 157.079, 152.873, 135.401, 173.763, 165.408, 164.956,
        183.076, 165.336, 120.625, 164.995, 179.371, 148.131, 101.934, 173.486,
        137.594, 177.213, 142.368, 146.518, 158.819, 182.608, 141.915, 188.009,
        167.017, 146.746, 186.553, 189.928, 184.170, 150.510, 184.735, 140.950,
        153.481, 150.379, 160.122, 169.472, 164.423, 164.081, 146.914, 172.267,
        147.274, 153.215, 180.110, 142.459, 156.995, 130.454, 174.086, 166.978,
        173.331,  91.021, 136.017, 179.714, 182.912, 159.219, 170.937, 162.242,
        144.747, 176.210, 147.161, 164.840, 138.365, 188.347, 185.349, 169.120,
        151.996, 107.123, 171.780, 153.613, 171.845, 152.642, 163.909, 166.568,
        183.054, 177.956, 149.516,  84.669, 158.420, 119.062, 114.958, 171.351,
        151.853, 194.124, 149.223, 165.516, 155.980, 167.161, 149.636, 151.096,
        176.491, 140.664, 176.042, 153.983, 140.073, 171.311, 140.936, 182.787,
        175.667, 168.991, 178.670, 164.953], device='cuda:0')
clipping threshold 0.21512474853382574
a after update for 1 param tensor([[[-0.010],
         [ 0.018],
         [-0.088],
         [ 0.008],
         [ 0.002],
         [-0.003],
         [-0.013],
         [ 0.042],
         [-0.022],
         [-0.064]],

        [[-0.038],
         [ 0.072],
         [ 0.027],
         [ 0.018],
         [ 0.066],
         [-0.028],
         [-0.001],
         [-0.009],
         [-0.005],
         [-0.017]],

        [[ 0.042],
         [-0.001],
         [-0.019],
         [-0.005],
         [ 0.029],
         [-0.032],
         [-0.021],
         [-0.051],
         [ 0.008],
         [-0.014]],

        [[ 0.004],
         [ 0.036],
         [ 0.017],
         [ 0.044],
         [-0.016],
         [-0.030],
         [-0.001],
         [-0.018],
         [ 0.009],
         [ 0.035]],

        [[-0.018],
         [-0.003],
         [ 0.024],
         [ 0.017],
         [-0.006],
         [ 0.038],
         [ 0.001],
         [-0.021],
         [-0.012],
         [ 0.018]],

        [[-0.006],
         [-0.021],
         [-0.008],
         [-0.028],
         [ 0.010],
         [ 0.026],
         [ 0.054],
         [-0.026],
         [-0.055],
         [ 0.015]],

        [[-0.011],
         [ 0.031],
         [ 0.076],
         [ 0.004],
         [-0.022],
         [-0.076],
         [ 0.025],
         [ 0.000],
         [-0.002],
         [ 0.011]],

        [[ 0.052],
         [-0.061],
         [ 0.000],
         [-0.025],
         [ 0.041],
         [-0.004],
         [ 0.018],
         [-0.052],
         [-0.036],
         [-0.034]],

        [[-0.039],
         [-0.008],
         [ 0.055],
         [ 0.008],
         [-0.062],
         [ 0.016],
         [ 0.006],
         [ 0.002],
         [ 0.003],
         [ 0.020]],

        [[ 0.020],
         [-0.007],
         [ 0.002],
         [-0.026],
         [-0.006],
         [ 0.020],
         [ 0.047],
         [-0.047],
         [-0.011],
         [ 0.007]],

        [[ 0.055],
         [-0.068],
         [ 0.031],
         [ 0.022],
         [-0.027],
         [ 0.031],
         [-0.036],
         [-0.022],
         [-0.024],
         [ 0.003]],

        [[-0.011],
         [-0.031],
         [-0.017],
         [-0.017],
         [-0.026],
         [ 0.003],
         [ 0.041],
         [-0.011],
         [ 0.039],
         [-0.036]],

        [[-0.036],
         [ 0.030],
         [ 0.073],
         [-0.003],
         [ 0.043],
         [ 0.035],
         [ 0.005],
         [ 0.014],
         [ 0.029],
         [-0.046]],

        [[-0.003],
         [-0.017],
         [-0.009],
         [ 0.014],
         [-0.009],
         [-0.012],
         [ 0.028],
         [ 0.006],
         [-0.024],
         [-0.016]],

        [[ 0.005],
         [ 0.010],
         [-0.033],
         [-0.000],
         [-0.002],
         [-0.006],
         [ 0.021],
         [ 0.019],
         [-0.027],
         [-0.016]],

        [[-0.029],
         [-0.004],
         [ 0.013],
         [ 0.013],
         [ 0.044],
         [-0.053],
         [-0.031],
         [ 0.011],
         [-0.051],
         [-0.009]],

        [[-0.008],
         [-0.005],
         [-0.007],
         [-0.007],
         [ 0.008],
         [ 0.046],
         [-0.040],
         [ 0.012],
         [ 0.063],
         [ 0.012]],

        [[-0.004],
         [ 0.002],
         [ 0.005],
         [-0.001],
         [ 0.005],
         [ 0.014],
         [ 0.004],
         [ 0.002],
         [-0.068],
         [-0.002]],

        [[ 0.067],
         [ 0.057],
         [ 0.017],
         [ 0.057],
         [-0.002],
         [ 0.010],
         [-0.005],
         [-0.016],
         [ 0.010],
         [-0.001]],

        [[ 0.078],
         [ 0.010],
         [ 0.045],
         [-0.009],
         [ 0.024],
         [-0.079],
         [ 0.062],
         [-0.034],
         [-0.014],
         [-0.018]],

        [[ 0.031],
         [ 0.049],
         [ 0.025],
         [ 0.008],
         [-0.003],
         [-0.030],
         [-0.010],
         [-0.000],
         [ 0.024],
         [ 0.047]],

        [[ 0.015],
         [-0.016],
         [ 0.004],
         [ 0.065],
         [-0.002],
         [ 0.006],
         [ 0.000],
         [-0.002],
         [ 0.044],
         [-0.100]],

        [[-0.023],
         [ 0.029],
         [ 0.004],
         [ 0.050],
         [ 0.007],
         [ 0.055],
         [-0.008],
         [ 0.001],
         [ 0.005],
         [-0.021]],

        [[ 0.024],
         [-0.017],
         [-0.008],
         [-0.004],
         [ 0.029],
         [ 0.041],
         [ 0.004],
         [-0.033],
         [-0.036],
         [-0.001]],

        [[-0.041],
         [-0.020],
         [-0.017],
         [ 0.002],
         [ 0.002],
         [-0.021],
         [-0.099],
         [-0.050],
         [ 0.021],
         [ 0.019]],

        [[-0.014],
         [ 0.015],
         [-0.081],
         [ 0.000],
         [ 0.010],
         [-0.024],
         [ 0.013],
         [-0.013],
         [-0.011],
         [-0.008]],

        [[ 0.015],
         [ 0.040],
         [-0.024],
         [-0.027],
         [ 0.004],
         [-0.033],
         [-0.005],
         [-0.006],
         [ 0.034],
         [-0.001]],

        [[-0.002],
         [-0.046],
         [ 0.025],
         [ 0.020],
         [-0.011],
         [ 0.020],
         [ 0.021],
         [-0.006],
         [ 0.026],
         [-0.032]],

        [[ 0.032],
         [ 0.007],
         [-0.025],
         [-0.022],
         [-0.012],
         [-0.017],
         [ 0.023],
         [ 0.049],
         [ 0.061],
         [-0.004]],

        [[ 0.011],
         [-0.039],
         [ 0.029],
         [ 0.016],
         [-0.022],
         [-0.030],
         [ 0.019],
         [ 0.008],
         [-0.045],
         [ 0.090]]], device='cuda:0')
s after update for 1 param tensor([[[1.937],
         [1.440],
         [2.095],
         [1.788],
         [1.145],
         [1.547],
         [1.809],
         [2.440],
         [1.654],
         [2.063]],

        [[1.652],
         [1.612],
         [1.376],
         [2.028],
         [1.338],
         [1.726],
         [1.587],
         [1.161],
         [1.754],
         [1.508]],

        [[1.923],
         [1.451],
         [1.511],
         [1.983],
         [1.259],
         [2.064],
         [1.370],
         [1.899],
         [1.355],
         [1.253]],

        [[1.887],
         [1.925],
         [1.721],
         [1.400],
         [0.925],
         [1.854],
         [0.979],
         [1.427],
         [1.604],
         [2.067]],

        [[2.044],
         [1.840],
         [1.456],
         [1.702],
         [1.808],
         [2.158],
         [1.666],
         [1.985],
         [2.227],
         [1.802]],

        [[1.566],
         [1.599],
         [1.297],
         [1.303],
         [1.575],
         [1.301],
         [2.093],
         [1.393],
         [1.996],
         [2.028]],

        [[2.009],
         [1.317],
         [2.012],
         [1.482],
         [1.249],
         [1.719],
         [1.349],
         [1.480],
         [1.548],
         [1.397]],

        [[1.134],
         [1.700],
         [1.931],
         [1.553],
         [1.620],
         [1.952],
         [1.304],
         [1.851],
         [1.579],
         [1.908]],

        [[0.769],
         [1.629],
         [1.554],
         [1.170],
         [1.542],
         [1.602],
         [1.695],
         [1.738],
         [2.190],
         [1.665]],

        [[2.090],
         [0.886],
         [1.951],
         [1.812],
         [1.233],
         [1.648],
         [1.054],
         [1.501],
         [0.968],
         [0.923]],

        [[1.700],
         [1.368],
         [1.777],
         [1.831],
         [1.804],
         [1.122],
         [1.889],
         [1.986],
         [1.371],
         [1.474]],

        [[1.413],
         [1.732],
         [1.225],
         [1.719],
         [1.540],
         [1.007],
         [1.969],
         [1.476],
         [1.431],
         [1.582]],

        [[2.021],
         [1.693],
         [1.746],
         [0.955],
         [1.152],
         [1.742],
         [0.801],
         [1.390],
         [1.394],
         [2.169]],

        [[1.529],
         [1.904],
         [1.173],
         [1.807],
         [1.488],
         [1.481],
         [1.899],
         [1.080],
         [1.868],
         [1.712]],

        [[1.762],
         [1.327],
         [1.781],
         [1.727],
         [0.413],
         [1.433],
         [0.900],
         [1.463],
         [1.813],
         [1.695]],

        [[1.563],
         [1.883],
         [2.022],
         [1.377],
         [1.728],
         [1.961],
         [1.260],
         [1.695],
         [1.711],
         [1.501]],

        [[1.337],
         [2.107],
         [1.094],
         [0.759],
         [1.669],
         [1.898],
         [2.045],
         [1.082],
         [2.212],
         [1.477]],

        [[1.699],
         [1.159],
         [2.042],
         [1.491],
         [1.971],
         [1.449],
         [1.813],
         [1.763],
         [1.800],
         [1.890]],

        [[1.431],
         [1.814],
         [1.031],
         [1.964],
         [1.196],
         [1.814],
         [1.575],
         [1.374],
         [1.937],
         [1.679]],

        [[1.992],
         [1.506],
         [1.444],
         [1.607],
         [2.196],
         [1.909],
         [1.595],
         [1.449],
         [1.466],
         [1.272]],

        [[1.460],
         [1.465],
         [1.711],
         [1.578],
         [1.679],
         [1.398],
         [1.425],
         [2.212],
         [1.980],
         [1.454]],

        [[1.318],
         [1.498],
         [1.489],
         [2.001],
         [0.850],
         [1.445],
         [1.692],
         [1.312],
         [2.045],
         [1.754]],

        [[1.255],
         [1.417],
         [1.753],
         [1.568],
         [1.365],
         [1.443],
         [1.924],
         [1.216],
         [0.938],
         [1.891]],

        [[1.670],
         [1.503],
         [1.331],
         [1.607],
         [1.489],
         [1.965],
         [2.043],
         [1.725],
         [1.445],
         [1.816]],

        [[1.111],
         [1.834],
         [1.346],
         [1.897],
         [1.771],
         [1.547],
         [1.980],
         [1.462],
         [1.623],
         [1.653]],

        [[1.659],
         [1.560],
         [1.726],
         [2.009],
         [1.259],
         [1.278],
         [2.197],
         [2.134],
         [2.071],
         [1.082]],

        [[1.952],
         [1.438],
         [1.962],
         [1.258],
         [1.782],
         [1.627],
         [1.701],
         [1.413],
         [1.376],
         [0.181]],

        [[1.717],
         [1.590],
         [2.041],
         [1.673],
         [1.603],
         [1.318],
         [1.650],
         [1.036],
         [1.732],
         [1.632]],

        [[1.692],
         [1.707],
         [1.222],
         [2.059],
         [1.530],
         [1.487],
         [1.685],
         [1.147],
         [1.282],
         [1.484]],

        [[1.476],
         [1.506],
         [1.845],
         [1.598],
         [1.790],
         [1.499],
         [1.595],
         [1.191],
         [1.415],
         [1.631]]], device='cuda:0')
b after update for 1 param tensor([[[177.680],
         [153.196],
         [184.767],
         [170.706],
         [136.613],
         [158.768],
         [171.724],
         [199.409],
         [164.196],
         [183.369]],

        [[164.090],
         [162.076],
         [149.771],
         [181.795],
         [147.683],
         [167.701],
         [160.843],
         [137.565],
         [169.064],
         [156.794]],

        [[177.050],
         [153.757],
         [156.944],
         [179.774],
         [143.271],
         [183.405],
         [149.430],
         [175.933],
         [148.620],
         [142.924]],

        [[175.380],
         [177.115],
         [167.456],
         [151.071],
         [122.796],
         [173.809],
         [126.344],
         [152.489],
         [161.688],
         [183.545]],

        [[182.520],
         [173.186],
         [154.044],
         [166.542],
         [171.638],
         [187.534],
         [164.768],
         [179.863],
         [190.498],
         [171.353]],

        [[159.737],
         [161.428],
         [145.418],
         [145.736],
         [160.219],
         [145.603],
         [184.703],
         [150.658],
         [180.344],
         [181.800]],

        [[180.935],
         [146.518],
         [181.078],
         [155.438],
         [142.675],
         [167.390],
         [148.276],
         [155.304],
         [158.841],
         [150.894]],

        [[135.924],
         [166.473],
         [177.386],
         [159.099],
         [162.512],
         [178.356],
         [145.775],
         [173.707],
         [160.439],
         [176.343]],

        [[111.939],
         [162.947],
         [159.153],
         [138.082],
         [158.513],
         [161.591],
         [166.211],
         [168.289],
         [188.941],
         [164.753]],

        [[184.549],
         [120.148],
         [178.295],
         [171.840],
         [141.772],
         [163.868],
         [131.089],
         [156.406],
         [125.609],
         [122.682]],

        [[166.432],
         [149.296],
         [170.165],
         [172.753],
         [171.467],
         [135.218],
         [175.479],
         [179.910],
         [149.470],
         [154.995]],

        [[151.727],
         [167.996],
         [141.273],
         [167.372],
         [158.415],
         [128.133],
         [179.132],
         [155.084],
         [152.700],
         [160.549]],

        [[181.472],
         [166.131],
         [168.680],
         [124.733],
         [137.000],
         [168.498],
         [114.276],
         [150.535],
         [150.754],
         [188.024]],

        [[157.878],
         [176.173],
         [138.283],
         [171.618],
         [155.719],
         [155.362],
         [175.909],
         [132.669],
         [174.497],
         [167.055]],

        [[169.480],
         [147.081],
         [170.394],
         [167.778],
         [ 81.996],
         [152.819],
         [121.088],
         [154.420],
         [171.893],
         [166.209]],

        [[159.622],
         [175.200],
         [181.547],
         [149.831],
         [167.831],
         [178.777],
         [143.284],
         [166.200],
         [167.001],
         [156.420]],

        [[147.624],
         [185.322],
         [133.546],
         [111.240],
         [164.918],
         [175.897],
         [182.552],
         [132.792],
         [189.882],
         [155.128]],

        [[166.404],
         [137.435],
         [182.446],
         [155.867],
         [179.240],
         [153.683],
         [171.886],
         [169.494],
         [171.283],
         [175.509]],

        [[152.707],
         [171.934],
         [129.623],
         [178.901],
         [139.624],
         [171.940],
         [160.215],
         [149.624],
         [177.663],
         [165.431]],

        [[180.164],
         [156.663],
         [153.427],
         [161.846],
         [189.187],
         [176.406],
         [161.211],
         [153.671],
         [154.577],
         [144.000]],

        [[154.243],
         [154.535],
         [166.995],
         [160.357],
         [165.397],
         [150.929],
         [152.393],
         [189.891],
         [179.632],
         [153.943]],

        [[146.550],
         [156.239],
         [155.763],
         [180.572],
         [117.674],
         [153.445],
         [166.074],
         [146.228],
         [182.541],
         [169.073]],

        [[143.037],
         [151.956],
         [169.020],
         [159.880],
         [149.158],
         [153.350],
         [177.077],
         [140.793],
         [123.665],
         [175.552]],

        [[164.969],
         [156.532],
         [147.297],
         [161.835],
         [155.781],
         [178.966],
         [182.469],
         [167.685],
         [153.467],
         [172.051]],

        [[134.536],
         [172.896],
         [148.088],
         [175.854],
         [169.904],
         [158.808],
         [179.625],
         [154.349],
         [162.630],
         [164.127]],

        [[164.456],
         [159.444],
         [167.712],
         [180.964],
         [143.254],
         [144.328],
         [189.225],
         [186.485],
         [183.711],
         [132.776]],

        [[178.343],
         [153.093],
         [178.817],
         [143.177],
         [170.414],
         [162.832],
         [166.524],
         [151.736],
         [149.778],
         [ 54.239]],

        [[167.283],
         [160.975],
         [182.374],
         [165.147],
         [161.635],
         [146.550],
         [163.974],
         [129.931],
         [168.006],
         [163.103]],

        [[166.084],
         [166.813],
         [141.114],
         [183.208],
         [157.904],
         [155.665],
         [165.695],
         [136.754],
         [144.568],
         [155.523]],

        [[155.085],
         [156.677],
         [173.421],
         [161.359],
         [170.801],
         [156.294],
         [161.243],
         [139.299],
         [151.883],
         [163.021]]], device='cuda:0')
clipping threshold 0.21512474853382574
a after update for 1 param tensor([[ 0.007],
        [ 0.000],
        [ 0.020],
        [ 0.025],
        [-0.005],
        [ 0.037],
        [ 0.039],
        [ 0.022],
        [-0.048],
        [ 0.005],
        [-0.007],
        [ 0.007],
        [-0.014],
        [ 0.003],
        [ 0.008],
        [ 0.010],
        [-0.030],
        [ 0.007],
        [ 0.019],
        [ 0.017],
        [-0.066],
        [ 0.025],
        [ 0.043],
        [-0.025],
        [ 0.040],
        [ 0.053],
        [-0.078],
        [ 0.016],
        [ 0.021],
        [ 0.043]], device='cuda:0')
s after update for 1 param tensor([[1.223],
        [1.554],
        [1.483],
        [1.920],
        [1.758],
        [1.685],
        [1.956],
        [1.235],
        [1.916],
        [1.515],
        [0.470],
        [2.109],
        [1.497],
        [1.134],
        [1.938],
        [1.915],
        [1.510],
        [1.779],
        [0.985],
        [1.073],
        [1.969],
        [1.371],
        [1.932],
        [1.758],
        [2.003],
        [1.848],
        [2.253],
        [1.110],
        [1.450],
        [2.175]], device='cuda:0')
b after update for 1 param tensor([[141.195],
        [159.156],
        [155.453],
        [176.877],
        [169.250],
        [165.739],
        [178.544],
        [141.895],
        [176.689],
        [157.137],
        [ 87.503],
        [185.404],
        [156.199],
        [135.976],
        [177.701],
        [176.681],
        [156.872],
        [170.272],
        [126.705],
        [132.229],
        [179.134],
        [149.487],
        [177.453],
        [169.245],
        [180.666],
        [173.563],
        [191.611],
        [134.522],
        [153.743],
        [188.257]], device='cuda:0')
clipping threshold 0.21512474853382574
||w||^2 0.04656751569727792
exp ma of ||w||^2 0.06965071714370868
||w|| 0.21579507801911962
exp ma of ||w|| 0.234088711525561
||w||^2 0.06591808268108293
exp ma of ||w||^2 0.06998843031341573
||w|| 0.25674517070644765
exp ma of ||w|| 0.2581324162294962
cuda
Objective function 29.68 = squared loss an data 28.52 + 0.5*rho*h**2 0.343271 + alpha*h 0.309989 + L2reg 0.29 + L1reg 0.22 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.7104924364338591
iteration 2 in inner loop, alpha 11.830773173955649 rho 1000.0 h 0.026201944163418034
iteration 3 in outer loop, alpha = 38.03271733737368, rho = 1000.0, h = 0.026201944163418034
cuda
9630
cuda
Objective function 30.37 = squared loss an data 28.52 + 0.5*rho*h**2 0.343271 + alpha*h 0.996531 + L2reg 0.29 + L1reg 0.22 ; SHD = 54 ; DAG True
||w||^2 3.4563029456379746
exp ma of ||w||^2 546.4539891000028
||w|| 1.85911348379758
exp ma of ||w|| 4.3597097526356405
||w||^2 0.02343760547193543
exp ma of ||w||^2 0.036278105763900385
||w|| 0.15309345339345973
exp ma of ||w|| 0.18827571708192278
||w||^2 0.02751200885124524
exp ma of ||w||^2 0.0415626284556004
||w|| 0.16586744361460823
exp ma of ||w|| 0.1999730215941862
||w||^2 0.11033502714744395
exp ma of ||w||^2 0.05736815705978428
||w|| 0.332167167473614
exp ma of ||w|| 0.23417651908747264
||w||^2 0.04121588665272272
exp ma of ||w||^2 0.06147785820468831
||w|| 0.20301696149022308
exp ma of ||w|| 0.24241725902752032
cuda
Objective function 29.89 = squared loss an data 28.49 + 0.5*rho*h**2 0.173604 + alpha*h 0.708684 + L2reg 0.31 + L1reg 0.21 ; SHD = 53 ; DAG True
Proportion of microbatches that were clipped  0.7086766844401358
iteration 1 in inner loop, alpha 38.03271733737368 rho 1000.0 h 0.01863353586713501
9630
cuda
Objective function 31.45 = squared loss an data 28.49 + 0.5*rho*h**2 1.736043 + alpha*h 0.708684 + L2reg 0.31 + L1reg 0.21 ; SHD = 53 ; DAG True
||w||^2 0.291855639240156
exp ma of ||w||^2 66.99511173780489
||w|| 0.5402366511448071
exp ma of ||w|| 0.9674100924613112
||w||^2 0.2546723884474748
exp ma of ||w||^2 16.317325596658023
||w|| 0.5046507588892291
exp ma of ||w|| 0.6563982001162088
||w||^2 0.17362279623912688
exp ma of ||w||^2 1.6586504058077485
||w|| 0.4166806885843486
exp ma of ||w|| 0.4476051661060071
cuda
Objective function 29.50 = squared loss an data 28.42 + 0.5*rho*h**2 0.291602 + alpha*h 0.290447 + L2reg 0.32 + L1reg 0.18 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.714444797963729
iteration 2 in inner loop, alpha 38.03271733737368 rho 10000.0 h 0.0076367747050589685
9630
cuda
Objective function 32.12 = squared loss an data 28.42 + 0.5*rho*h**2 2.916016 + alpha*h 0.290447 + L2reg 0.32 + L1reg 0.18 ; SHD = 51 ; DAG True
||w||^2 2360445311971.719
exp ma of ||w||^2 1190781579060.8767
||w|| 1536374.0794388973
exp ma of ||w|| 818041.4183132751
||w||^2 11.280670680407834
exp ma of ||w||^2 196070.93404327292
||w|| 3.358670969357944
exp ma of ||w|| 7.976797959155423
||w||^2 0.11095382846836588
exp ma of ||w||^2 0.08938940976144041
||w|| 0.3330973258198959
exp ma of ||w|| 0.2689372176567296
cuda
Objective function 29.48 = squared loss an data 28.55 + 0.5*rho*h**2 0.352722 + alpha*h 0.101016 + L2reg 0.32 + L1reg 0.15 ; SHD = 52 ; DAG True
Proportion of microbatches that were clipped  0.7059870550161812
iteration 3 in inner loop, alpha 38.03271733737368 rho 100000.0 h 0.0026560201630232427
iteration 4 in outer loop, alpha = 303.63473363969797, rho = 100000.0, h = 0.0026560201630232427
cuda
9630
cuda
Objective function 30.18 = squared loss an data 28.55 + 0.5*rho*h**2 0.352722 + alpha*h 0.806460 + L2reg 0.32 + L1reg 0.15 ; SHD = 52 ; DAG True
||w||^2 387.63870360078346
exp ma of ||w||^2 7636807.755405778
||w|| 19.688542444802344
exp ma of ||w|| 59.65720133224661
||w||^2 0.20224805455906986
exp ma of ||w||^2 14.081232876935692
||w|| 0.44971997349358395
exp ma of ||w|| 0.5497790444081372
||w||^2 0.041093612162193545
exp ma of ||w||^2 0.04899567697386344
||w|| 0.20271559427482028
exp ma of ||w|| 0.21924711200958677
||w||^2 0.05943854109919202
exp ma of ||w||^2 0.04571521190460088
||w|| 0.24380020734033844
exp ma of ||w|| 0.21138844217012764
||w||^2 0.042249934190858755
exp ma of ||w||^2 0.04594277019337119
||w|| 0.20554788782874603
exp ma of ||w|| 0.2119293697603303
||w||^2 0.06313973958650697
exp ma of ||w||^2 0.04832417019789118
||w|| 0.2512762216894129
exp ma of ||w|| 0.21617616165931297
cuda
Objective function 29.88 = squared loss an data 28.68 + 0.5*rho*h**2 0.174535 + alpha*h 0.567293 + L2reg 0.33 + L1reg 0.13 ; SHD = 50 ; DAG True
Proportion of microbatches that were clipped  0.7051015512553974
iteration 1 in inner loop, alpha 303.63473363969797 rho 100000.0 h 0.001868340950515801
iteration 5 in outer loop, alpha = 2171.975684155499, rho = 1000000.0, h = 0.001868340950515801
Threshold 0.3
[[0.002 0.074 0.033 0.045 0.075 0.02  0.024 0.037 0.061 0.031 0.046 0.077
  0.026 0.03  0.044 0.043 0.028 0.078 0.113 0.025 0.053 0.025 0.152 0.027
  0.051 0.025 0.017 0.046 0.04  0.022]
 [0.032 0.002 0.03  0.041 0.07  0.012 0.027 0.025 0.071 0.102 0.014 0.122
  0.044 0.055 0.081 0.037 0.056 0.055 0.048 0.08  0.101 0.061 0.138 0.058
  0.06  0.011 0.038 0.047 0.078 0.03 ]
 [0.073 0.05  0.003 0.024 0.079 0.036 0.103 0.024 0.058 0.022 0.037 0.561
  0.038 0.042 0.055 0.076 0.077 0.064 0.024 0.013 0.09  0.023 0.083 0.077
  0.074 0.018 0.043 0.096 0.093 0.036]
 [0.038 0.041 0.042 0.003 0.043 0.024 0.031 0.004 0.032 0.02  0.034 0.087
  0.022 0.015 0.036 0.037 0.021 0.026 0.048 0.016 0.121 0.02  0.119 0.012
  0.013 0.007 0.03  0.069 0.04  0.013]
 [0.028 0.025 0.018 0.054 0.001 0.022 0.036 0.057 0.021 0.018 0.008 0.071
  0.066 0.044 0.029 0.044 0.009 0.007 0.038 0.024 0.061 0.012 0.03  0.039
  0.018 0.008 0.015 0.024 0.019 0.056]
 [0.102 0.123 0.04  0.087 0.072 0.002 0.054 0.085 0.061 0.017 0.05  0.194
  0.082 0.024 0.076 0.046 0.046 0.127 0.032 0.031 0.212 0.083 0.067 0.042
  0.033 0.024 0.059 0.108 0.061 0.016]
 [0.09  0.077 0.014 0.081 0.051 0.03  0.001 0.06  0.068 0.021 0.018 0.043
  0.035 0.037 0.082 0.114 0.061 0.03  0.13  0.046 0.094 0.06  0.033 0.047
  0.103 0.023 0.043 0.051 0.052 0.086]
 [0.047 0.104 0.069 0.365 0.032 0.023 0.042 0.002 0.056 0.03  0.033 0.065
  0.035 0.048 0.04  0.046 0.039 0.065 0.062 0.054 0.088 0.031 0.144 0.024
  0.03  0.004 0.032 0.101 0.056 0.035]
 [0.029 0.031 0.032 0.07  0.047 0.033 0.04  0.039 0.002 0.033 0.025 0.143
  0.016 0.015 0.024 0.072 0.032 0.021 0.129 0.006 0.378 0.047 0.224 0.023
  0.016 0.014 0.027 0.036 0.044 0.108]
 [0.06  0.02  0.092 0.076 0.085 0.093 0.075 0.063 0.066 0.002 0.041 0.126
  0.038 0.04  0.073 0.099 0.024 0.074 0.068 0.098 0.092 0.014 0.048 0.052
  0.028 0.017 0.026 0.065 0.033 0.062]
 [0.066 0.116 0.08  0.069 0.18  0.029 0.086 0.055 0.079 0.05  0.001 0.046
  0.099 0.06  0.085 0.115 0.095 0.059 0.115 0.061 0.053 0.051 0.052 0.064
  0.048 0.007 0.049 0.738 0.037 0.062]
 [0.024 0.021 0.003 0.027 0.029 0.011 0.038 0.025 0.015 0.01  0.04  0.002
  0.017 0.029 0.055 0.03  0.031 0.027 0.026 0.022 0.039 0.04  0.048 0.016
  0.015 0.011 0.008 0.042 0.045 0.015]
 [0.046 0.042 0.05  0.066 0.032 0.025 0.045 0.059 0.121 0.049 0.024 0.097
  0.001 0.016 0.042 0.025 0.047 0.077 0.075 0.039 0.045 0.041 0.11  0.09
  0.042 0.008 0.121 0.056 0.018 0.092]
 [0.067 0.034 0.054 0.095 0.04  0.075 0.074 0.036 0.062 0.059 0.026 0.083
  0.105 0.002 0.019 0.052 0.018 0.061 0.071 0.092 0.135 0.03  0.073 0.021
  0.021 0.027 0.054 0.054 0.036 0.076]
 [0.049 0.02  0.038 0.065 0.082 0.032 0.02  0.05  0.062 0.02  0.022 0.033
  0.042 0.049 0.001 0.107 0.044 0.032 0.089 0.047 0.031 0.016 0.059 0.075
  0.046 0.003 0.024 0.139 0.018 0.025]
 [0.043 0.067 0.028 0.04  0.037 0.048 0.015 0.055 0.027 0.023 0.012 0.061
  0.035 0.037 0.012 0.001 0.032 0.016 0.059 0.02  0.096 0.033 0.19  0.025
  0.046 0.007 0.013 0.048 0.058 0.04 ]
 [0.064 0.04  0.016 0.129 0.085 0.037 0.044 0.061 0.068 0.067 0.018 0.047
  0.032 0.105 0.03  0.045 0.002 0.102 0.042 0.044 0.188 0.059 0.072 0.022
  0.06  0.02  0.056 0.057 0.051 0.038]
 [0.023 0.044 0.027 0.071 0.333 0.014 0.07  0.026 0.07  0.024 0.023 0.061
  0.02  0.043 0.052 0.112 0.023 0.001 0.072 0.056 0.061 0.015 0.028 0.046
  0.034 0.006 0.01  0.053 0.022 0.029]
 [0.018 0.046 0.042 0.031 0.052 0.067 0.015 0.042 0.017 0.029 0.012 0.049
  0.034 0.023 0.018 0.02  0.043 0.028 0.002 0.043 0.059 0.022 0.035 0.006
  0.041 0.01  0.017 0.079 0.063 0.005]
 [0.096 0.03  0.097 0.131 0.109 0.089 0.041 0.022 0.315 0.027 0.027 0.1
  0.045 0.024 0.034 0.078 0.026 0.039 0.033 0.002 0.239 0.031 0.1   0.02
  0.098 0.011 0.044 0.076 0.082 0.05 ]
 [0.038 0.026 0.018 0.017 0.034 0.008 0.011 0.013 0.008 0.021 0.021 0.046
  0.046 0.008 0.045 0.019 0.014 0.024 0.025 0.004 0.002 0.032 0.061 0.014
  0.009 0.007 0.031 0.021 0.009 0.025]
 [0.085 0.027 0.049 0.117 0.177 0.026 0.032 0.07  0.055 0.132 0.04  0.04
  0.048 0.022 0.115 0.086 0.025 0.135 0.087 0.042 0.066 0.003 0.055 0.037
  0.082 0.015 0.087 0.056 0.027 0.145]
 [0.014 0.014 0.03  0.014 0.079 0.036 0.038 0.017 0.013 0.029 0.035 0.046
  0.018 0.018 0.032 0.009 0.017 0.063 0.022 0.017 0.025 0.03  0.001 0.003
  0.027 0.006 0.019 0.035 0.017 0.028]
 [0.083 0.033 0.024 0.189 0.061 0.061 0.046 0.062 0.063 0.039 0.031 0.098
  0.017 0.062 0.046 0.058 0.071 0.042 0.382 0.102 0.14  0.069 0.621 0.002
  0.079 0.003 0.051 0.043 0.103 0.124]
 [0.028 0.026 0.032 0.095 0.068 0.058 0.029 0.039 0.097 0.055 0.025 0.115
  0.036 0.099 0.047 0.042 0.036 0.069 0.038 0.028 0.192 0.028 0.068 0.03
  0.002 0.021 0.035 0.039 0.069 0.018]
 [0.073 0.1   0.11  0.076 0.231 0.101 0.072 0.502 0.156 0.081 0.169 0.225
  0.199 0.088 0.643 0.193 0.091 0.465 0.163 0.113 0.228 0.137 0.194 0.655
  0.104 0.002 0.092 0.146 0.62  0.091]
 [0.162 0.046 0.067 0.036 0.109 0.043 0.046 0.048 0.061 0.102 0.046 0.164
  0.021 0.044 0.071 0.207 0.04  0.222 0.072 0.038 0.082 0.025 0.147 0.067
  0.067 0.019 0.001 0.054 0.042 0.047]
 [0.033 0.045 0.019 0.034 0.083 0.012 0.044 0.034 0.042 0.024 0.002 0.036
  0.029 0.037 0.018 0.039 0.034 0.04  0.028 0.016 0.098 0.042 0.054 0.038
  0.036 0.009 0.041 0.002 0.013 0.051]
 [0.043 0.017 0.018 0.066 0.071 0.055 0.038 0.054 0.064 0.046 0.05  0.048
  0.082 0.07  0.066 0.042 0.036 0.091 0.029 0.047 0.234 0.06  0.112 0.021
  0.029 0.002 0.049 0.079 0.002 0.03 ]
 [0.094 0.042 0.042 0.165 0.045 0.159 0.019 0.049 0.015 0.026 0.036 0.127
  0.018 0.024 0.069 0.049 0.06  0.053 0.322 0.037 0.063 0.021 0.072 0.012
  0.102 0.021 0.049 0.038 0.067 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.561
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.365 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.378 0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.738 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.333 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.315 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.382 0.    0.    0.    0.621 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.502 0.    0.    0.    0.
  0.    0.    0.643 0.    0.    0.465 0.    0.    0.    0.    0.    0.655
  0.    0.    0.    0.    0.62  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.322 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.2857142857142857, 'tpr': 0.16666666666666666, 'fpr': 0.010666666666666666, 'f1': 0.2702702702702703, 'shd': 50, 'npred': 14, 'ntrue': 60}
[0.074 0.033 0.045 0.075 0.02  0.024 0.037 0.061 0.031 0.046 0.077 0.026
 0.03  0.044 0.043 0.028 0.078 0.113 0.025 0.053 0.025 0.152 0.027 0.051
 0.025 0.017 0.046 0.04  0.022 0.032 0.03  0.041 0.07  0.012 0.027 0.025
 0.071 0.102 0.014 0.122 0.044 0.055 0.081 0.037 0.056 0.055 0.048 0.08
 0.101 0.061 0.138 0.058 0.06  0.011 0.038 0.047 0.078 0.03  0.073 0.05
 0.024 0.079 0.036 0.103 0.024 0.058 0.022 0.037 0.561 0.038 0.042 0.055
 0.076 0.077 0.064 0.024 0.013 0.09  0.023 0.083 0.077 0.074 0.018 0.043
 0.096 0.093 0.036 0.038 0.041 0.042 0.043 0.024 0.031 0.004 0.032 0.02
 0.034 0.087 0.022 0.015 0.036 0.037 0.021 0.026 0.048 0.016 0.121 0.02
 0.119 0.012 0.013 0.007 0.03  0.069 0.04  0.013 0.028 0.025 0.018 0.054
 0.022 0.036 0.057 0.021 0.018 0.008 0.071 0.066 0.044 0.029 0.044 0.009
 0.007 0.038 0.024 0.061 0.012 0.03  0.039 0.018 0.008 0.015 0.024 0.019
 0.056 0.102 0.123 0.04  0.087 0.072 0.054 0.085 0.061 0.017 0.05  0.194
 0.082 0.024 0.076 0.046 0.046 0.127 0.032 0.031 0.212 0.083 0.067 0.042
 0.033 0.024 0.059 0.108 0.061 0.016 0.09  0.077 0.014 0.081 0.051 0.03
 0.06  0.068 0.021 0.018 0.043 0.035 0.037 0.082 0.114 0.061 0.03  0.13
 0.046 0.094 0.06  0.033 0.047 0.103 0.023 0.043 0.051 0.052 0.086 0.047
 0.104 0.069 0.365 0.032 0.023 0.042 0.056 0.03  0.033 0.065 0.035 0.048
 0.04  0.046 0.039 0.065 0.062 0.054 0.088 0.031 0.144 0.024 0.03  0.004
 0.032 0.101 0.056 0.035 0.029 0.031 0.032 0.07  0.047 0.033 0.04  0.039
 0.033 0.025 0.143 0.016 0.015 0.024 0.072 0.032 0.021 0.129 0.006 0.378
 0.047 0.224 0.023 0.016 0.014 0.027 0.036 0.044 0.108 0.06  0.02  0.092
 0.076 0.085 0.093 0.075 0.063 0.066 0.041 0.126 0.038 0.04  0.073 0.099
 0.024 0.074 0.068 0.098 0.092 0.014 0.048 0.052 0.028 0.017 0.026 0.065
 0.033 0.062 0.066 0.116 0.08  0.069 0.18  0.029 0.086 0.055 0.079 0.05
 0.046 0.099 0.06  0.085 0.115 0.095 0.059 0.115 0.061 0.053 0.051 0.052
 0.064 0.048 0.007 0.049 0.738 0.037 0.062 0.024 0.021 0.003 0.027 0.029
 0.011 0.038 0.025 0.015 0.01  0.04  0.017 0.029 0.055 0.03  0.031 0.027
 0.026 0.022 0.039 0.04  0.048 0.016 0.015 0.011 0.008 0.042 0.045 0.015
 0.046 0.042 0.05  0.066 0.032 0.025 0.045 0.059 0.121 0.049 0.024 0.097
 0.016 0.042 0.025 0.047 0.077 0.075 0.039 0.045 0.041 0.11  0.09  0.042
 0.008 0.121 0.056 0.018 0.092 0.067 0.034 0.054 0.095 0.04  0.075 0.074
 0.036 0.062 0.059 0.026 0.083 0.105 0.019 0.052 0.018 0.061 0.071 0.092
 0.135 0.03  0.073 0.021 0.021 0.027 0.054 0.054 0.036 0.076 0.049 0.02
 0.038 0.065 0.082 0.032 0.02  0.05  0.062 0.02  0.022 0.033 0.042 0.049
 0.107 0.044 0.032 0.089 0.047 0.031 0.016 0.059 0.075 0.046 0.003 0.024
 0.139 0.018 0.025 0.043 0.067 0.028 0.04  0.037 0.048 0.015 0.055 0.027
 0.023 0.012 0.061 0.035 0.037 0.012 0.032 0.016 0.059 0.02  0.096 0.033
 0.19  0.025 0.046 0.007 0.013 0.048 0.058 0.04  0.064 0.04  0.016 0.129
 0.085 0.037 0.044 0.061 0.068 0.067 0.018 0.047 0.032 0.105 0.03  0.045
 0.102 0.042 0.044 0.188 0.059 0.072 0.022 0.06  0.02  0.056 0.057 0.051
 0.038 0.023 0.044 0.027 0.071 0.333 0.014 0.07  0.026 0.07  0.024 0.023
 0.061 0.02  0.043 0.052 0.112 0.023 0.072 0.056 0.061 0.015 0.028 0.046
 0.034 0.006 0.01  0.053 0.022 0.029 0.018 0.046 0.042 0.031 0.052 0.067
 0.015 0.042 0.017 0.029 0.012 0.049 0.034 0.023 0.018 0.02  0.043 0.028
 0.043 0.059 0.022 0.035 0.006 0.041 0.01  0.017 0.079 0.063 0.005 0.096
 0.03  0.097 0.131 0.109 0.089 0.041 0.022 0.315 0.027 0.027 0.1   0.045
 0.024 0.034 0.078 0.026 0.039 0.033 0.239 0.031 0.1   0.02  0.098 0.011
 0.044 0.076 0.082 0.05  0.038 0.026 0.018 0.017 0.034 0.008 0.011 0.013
 0.008 0.021 0.021 0.046 0.046 0.008 0.045 0.019 0.014 0.024 0.025 0.004
 0.032 0.061 0.014 0.009 0.007 0.031 0.021 0.009 0.025 0.085 0.027 0.049
 0.117 0.177 0.026 0.032 0.07  0.055 0.132 0.04  0.04  0.048 0.022 0.115
 0.086 0.025 0.135 0.087 0.042 0.066 0.055 0.037 0.082 0.015 0.087 0.056
 0.027 0.145 0.014 0.014 0.03  0.014 0.079 0.036 0.038 0.017 0.013 0.029
 0.035 0.046 0.018 0.018 0.032 0.009 0.017 0.063 0.022 0.017 0.025 0.03
 0.003 0.027 0.006 0.019 0.035 0.017 0.028 0.083 0.033 0.024 0.189 0.061
 0.061 0.046 0.062 0.063 0.039 0.031 0.098 0.017 0.062 0.046 0.058 0.071
 0.042 0.382 0.102 0.14  0.069 0.621 0.079 0.003 0.051 0.043 0.103 0.124
 0.028 0.026 0.032 0.095 0.068 0.058 0.029 0.039 0.097 0.055 0.025 0.115
 0.036 0.099 0.047 0.042 0.036 0.069 0.038 0.028 0.192 0.028 0.068 0.03
 0.021 0.035 0.039 0.069 0.018 0.073 0.1   0.11  0.076 0.231 0.101 0.072
 0.502 0.156 0.081 0.169 0.225 0.199 0.088 0.643 0.193 0.091 0.465 0.163
 0.113 0.228 0.137 0.194 0.655 0.104 0.092 0.146 0.62  0.091 0.162 0.046
 0.067 0.036 0.109 0.043 0.046 0.048 0.061 0.102 0.046 0.164 0.021 0.044
 0.071 0.207 0.04  0.222 0.072 0.038 0.082 0.025 0.147 0.067 0.067 0.019
 0.054 0.042 0.047 0.033 0.045 0.019 0.034 0.083 0.012 0.044 0.034 0.042
 0.024 0.002 0.036 0.029 0.037 0.018 0.039 0.034 0.04  0.028 0.016 0.098
 0.042 0.054 0.038 0.036 0.009 0.041 0.013 0.051 0.043 0.017 0.018 0.066
 0.071 0.055 0.038 0.054 0.064 0.046 0.05  0.048 0.082 0.07  0.066 0.042
 0.036 0.091 0.029 0.047 0.234 0.06  0.112 0.021 0.029 0.002 0.049 0.079
 0.03  0.094 0.042 0.042 0.165 0.045 0.159 0.019 0.049 0.015 0.026 0.036
 0.127 0.018 0.024 0.069 0.049 0.06  0.053 0.322 0.037 0.063 0.021 0.072
 0.012 0.102 0.021 0.049 0.038 0.067]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.6288271604938271, 0.2833922240437649)
Iterations 630
Achieves (14.15454886112812, 1e-05)-DP
