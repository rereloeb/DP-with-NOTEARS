samples  5000  graph  20 60 ER mim  minibatch size  50  noise  1.0  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.09203800849621757
iteration 3 in outer loop, alpha = 15.689851603780383, rho = 100.0, h = 0.09203800849621757
cuda
iteration 1 in inner loop,alpha 15.689851603780383 rho 100.0 h 0.0536024646479234
iteration 2 in inner loop,alpha 15.689851603780383 rho 1000.0 h 0.0203269958158252
iteration 4 in outer loop, alpha = 36.01684741960558, rho = 1000.0, h = 0.0203269958158252
cuda
iteration 1 in inner loop,alpha 36.01684741960558 rho 1000.0 h 0.010940366722987704
iteration 2 in inner loop,alpha 36.01684741960558 rho 10000.0 h 0.0033601520578088184
iteration 5 in outer loop, alpha = 69.61836799769377, rho = 10000.0, h = 0.0033601520578088184
cuda
iteration 1 in inner loop,alpha 69.61836799769377 rho 10000.0 h 0.001493162115650648
iteration 2 in inner loop,alpha 69.61836799769377 rho 100000.0 h 0.0005719714195642212
iteration 6 in outer loop, alpha = 126.81550995411588, rho = 100000.0, h = 0.0005719714195642212
cuda
iteration 1 in inner loop,alpha 126.81550995411588 rho 100000.0 h 0.0003383813052906248
iteration 7 in outer loop, alpha = 465.1968152447407, rho = 1000000.0, h = 0.0003383813052906248
Threshold 0.3
[[0.    0.024 0.009 0.044 0.151 0.189 0.079 0.003 0.044 0.064 0.473 0.672
  1.098 0.002 0.088 0.002 0.127 2.574 0.524 0.014]
 [0.    0.001 0.    0.036 0.082 0.473 0.144 0.    0.006 0.059 0.11  0.
  0.    0.    0.074 0.141 0.007 0.    0.095 0.803]
 [0.002 0.289 0.002 0.125 0.538 0.115 0.091 0.001 1.489 0.84  0.08  0.006
  0.004 0.    0.032 0.215 0.121 0.026 0.137 0.112]
 [0.    0.006 0.    0.002 0.001 0.005 0.    0.    0.    0.009 0.126 0.
  0.    0.    0.05  0.002 0.001 0.    0.002 0.029]
 [0.    0.004 0.    0.172 0.002 0.007 0.004 0.    0.    0.061 0.136 0.
  0.    0.    0.041 0.119 0.003 0.    0.001 0.009]
 [0.    0.002 0.006 0.097 0.032 0.005 0.009 0.    0.033 0.042 0.049 0.
  0.003 0.    1.197 0.091 0.005 0.001 0.33  0.043]
 [0.    0.004 0.004 0.223 0.315 0.254 0.003 0.    0.011 0.166 0.43  0.001
  0.003 0.    0.094 0.06  0.001 0.    0.007 0.145]
 [0.002 0.099 0.005 1.218 0.045 0.062 0.181 0.    0.384 0.085 0.059 2.202
  1.369 0.022 0.109 0.016 0.03  2.227 0.047 0.033]
 [0.    0.021 0.    1.45  1.388 0.011 0.107 0.    0.003 0.421 1.19  0.
  0.001 0.    0.421 0.142 0.023 0.    0.005 0.016]
 [0.    0.002 0.001 0.04  0.005 0.135 0.012 0.    0.004 0.004 1.113 0.001
  0.002 0.    0.092 0.007 0.006 0.    0.005 0.665]
 [0.    0.002 0.    0.002 0.001 0.014 0.003 0.    0.    0.001 0.002 0.
  0.    0.    0.014 0.004 0.002 0.    0.002 0.749]
 [0.    2.092 0.004 1.587 1.303 0.059 0.601 0.    1.052 0.082 0.996 0.001
  0.001 0.004 0.004 0.904 0.467 0.    0.07  0.138]
 [0.    0.034 0.005 0.182 0.077 0.131 0.362 0.    0.003 0.626 0.057 0.575
  0.003 0.    1.596 0.16  0.225 0.    0.006 0.043]
 [0.001 1.038 4.617 0.098 0.037 0.181 0.079 0.002 0.853 0.187 0.037 0.022
  0.052 0.    0.056 0.682 0.004 0.081 0.721 1.198]
 [0.    0.001 0.    0.013 0.018 0.001 0.001 0.    0.002 0.006 0.009 0.
  0.    0.    0.002 0.01  0.001 0.    0.001 0.003]
 [0.    0.001 0.002 0.075 0.004 0.012 0.008 0.    0.004 0.309 0.041 0.
  0.    0.    0.038 0.003 0.007 0.    0.001 0.097]
 [0.    0.015 0.005 1.003 0.108 0.004 0.964 0.    0.027 0.012 0.116 0.001
  0.003 0.    0.555 0.009 0.002 0.    0.011 0.053]
 [0.    0.049 0.004 0.286 0.048 0.053 0.072 0.    0.102 0.043 0.118 0.919
  2.138 0.001 0.003 0.07  0.036 0.002 0.024 1.251]
 [0.    0.002 0.01  0.095 0.774 0.014 0.025 0.    0.442 0.244 0.136 0.
  0.004 0.    0.697 1.05  0.01  0.006 0.004 0.065]
 [0.    0.    0.    0.003 0.002 0.019 0.001 0.    0.001 0.001 0.001 0.
  0.    0.    0.053 0.001 0.001 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.473 0.672
  1.098 0.    0.    0.    0.    2.574 0.524 0.   ]
 [0.    0.    0.    0.    0.    0.473 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.803]
 [0.    0.    0.    0.    0.538 0.    0.    0.    1.489 0.84  0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.197 0.    0.    0.    0.33  0.   ]
 [0.    0.    0.    0.    0.315 0.    0.    0.    0.    0.    0.43  0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.218 0.    0.    0.    0.    0.384 0.    0.    2.202
  1.369 0.    0.    0.    0.    2.227 0.    0.   ]
 [0.    0.    0.    1.45  1.388 0.    0.    0.    0.    0.421 1.19  0.
  0.    0.    0.421 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.113 0.
  0.    0.    0.    0.    0.    0.    0.    0.665]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.749]
 [0.    2.092 0.    1.587 1.303 0.    0.601 0.    1.052 0.    0.996 0.
  0.    0.    0.    0.904 0.467 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.362 0.    0.    0.626 0.    0.575
  0.    0.    1.596 0.    0.    0.    0.    0.   ]
 [0.    1.038 4.617 0.    0.    0.    0.    0.    0.853 0.    0.    0.
  0.    0.    0.    0.682 0.    0.    0.721 1.198]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.309 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.003 0.    0.    0.964 0.    0.    0.    0.    0.
  0.    0.    0.555 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.919
  2.138 0.    0.    0.    0.    0.    0.    1.251]
 [0.    0.    0.    0.    0.774 0.    0.    0.    0.442 0.    0.    0.
  0.    0.    0.697 1.05  0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.16071428571428573, 'tpr': 0.7833333333333333, 'fpr': 0.06923076923076923, 'f1': 0.8103448275862069, 'shd': 15, 'npred': 56, 'ntrue': 60}
[2.408e-02 9.472e-03 4.355e-02 1.507e-01 1.888e-01 7.898e-02 3.129e-03
 4.354e-02 6.402e-02 4.735e-01 6.720e-01 1.098e+00 1.854e-03 8.845e-02
 1.991e-03 1.273e-01 2.574e+00 5.244e-01 1.390e-02 6.178e-06 3.851e-04
 3.586e-02 8.214e-02 4.731e-01 1.442e-01 5.568e-06 5.500e-03 5.933e-02
 1.105e-01 9.015e-05 1.316e-04 3.975e-05 7.361e-02 1.415e-01 6.519e-03
 9.795e-05 9.450e-02 8.034e-01 1.999e-03 2.891e-01 1.249e-01 5.378e-01
 1.148e-01 9.107e-02 1.383e-03 1.489e+00 8.400e-01 8.049e-02 5.513e-03
 3.650e-03 7.842e-06 3.238e-02 2.152e-01 1.208e-01 2.609e-02 1.371e-01
 1.123e-01 1.080e-05 6.470e-03 4.619e-05 1.270e-03 4.878e-03 3.770e-04
 2.379e-06 3.614e-04 8.750e-03 1.257e-01 1.079e-04 2.372e-04 3.262e-06
 5.047e-02 1.721e-03 5.143e-04 3.421e-05 1.745e-03 2.878e-02 7.258e-05
 3.530e-03 1.755e-04 1.716e-01 7.418e-03 3.704e-03 2.699e-06 4.861e-04
 6.082e-02 1.361e-01 2.730e-04 4.241e-04 1.735e-05 4.102e-02 1.190e-01
 3.049e-03 8.232e-05 6.419e-04 8.725e-03 2.212e-05 1.684e-03 5.598e-03
 9.720e-02 3.208e-02 9.096e-03 9.237e-05 3.254e-02 4.184e-02 4.864e-02
 3.395e-04 2.825e-03 6.537e-05 1.197e+00 9.054e-02 5.215e-03 5.867e-04
 3.299e-01 4.326e-02 1.841e-05 3.627e-03 3.769e-03 2.233e-01 3.149e-01
 2.537e-01 2.237e-05 1.140e-02 1.661e-01 4.300e-01 7.719e-04 3.218e-03
 4.521e-04 9.382e-02 5.955e-02 1.009e-03 7.371e-05 7.103e-03 1.452e-01
 1.612e-03 9.901e-02 4.650e-03 1.218e+00 4.498e-02 6.153e-02 1.812e-01
 3.836e-01 8.501e-02 5.881e-02 2.202e+00 1.369e+00 2.228e-02 1.093e-01
 1.626e-02 2.969e-02 2.227e+00 4.681e-02 3.253e-02 2.728e-05 2.136e-02
 2.175e-04 1.450e+00 1.388e+00 1.126e-02 1.068e-01 8.502e-05 4.214e-01
 1.190e+00 2.436e-04 5.511e-04 1.341e-05 4.210e-01 1.420e-01 2.338e-02
 1.274e-04 5.205e-03 1.565e-02 2.706e-05 2.437e-03 5.968e-04 3.994e-02
 5.340e-03 1.351e-01 1.205e-02 2.970e-05 3.757e-03 1.113e+00 6.007e-04
 1.660e-03 5.006e-06 9.178e-02 6.557e-03 5.763e-03 1.209e-04 4.761e-03
 6.646e-01 7.400e-05 2.200e-03 1.728e-04 1.948e-03 6.124e-04 1.377e-02
 2.946e-03 6.908e-06 4.348e-04 7.425e-04 1.493e-04 9.349e-05 3.351e-06
 1.400e-02 3.772e-03 2.295e-03 9.714e-05 2.426e-03 7.487e-01 6.231e-06
 2.092e+00 3.539e-03 1.587e+00 1.303e+00 5.874e-02 6.006e-01 6.779e-06
 1.052e+00 8.172e-02 9.959e-01 8.248e-04 4.052e-03 4.477e-03 9.036e-01
 4.672e-01 1.518e-04 6.980e-02 1.382e-01 1.369e-05 3.445e-02 4.801e-03
 1.817e-01 7.712e-02 1.307e-01 3.615e-01 3.586e-06 2.650e-03 6.260e-01
 5.666e-02 5.751e-01 4.982e-04 1.596e+00 1.603e-01 2.249e-01 7.173e-05
 5.767e-03 4.337e-02 1.300e-03 1.038e+00 4.617e+00 9.751e-02 3.678e-02
 1.810e-01 7.852e-02 2.234e-03 8.532e-01 1.874e-01 3.673e-02 2.198e-02
 5.216e-02 5.592e-02 6.822e-01 3.553e-03 8.133e-02 7.211e-01 1.198e+00
 1.771e-06 8.016e-04 4.132e-04 1.300e-02 1.791e-02 8.201e-04 1.352e-03
 4.319e-06 1.615e-03 6.254e-03 8.548e-03 2.326e-04 4.261e-04 1.101e-05
 9.936e-03 7.260e-04 3.121e-05 1.327e-03 2.588e-03 1.987e-05 6.853e-04
 1.900e-03 7.534e-02 4.401e-03 1.204e-02 8.146e-03 6.473e-05 3.866e-03
 3.094e-01 4.115e-02 4.252e-04 2.501e-04 3.008e-05 3.848e-02 7.304e-03
 8.103e-05 1.419e-03 9.696e-02 4.608e-05 1.541e-02 4.710e-03 1.003e+00
 1.083e-01 3.749e-03 9.645e-01 5.219e-05 2.686e-02 1.170e-02 1.158e-01
 9.135e-04 2.659e-03 2.043e-04 5.550e-01 9.162e-03 2.753e-04 1.060e-02
 5.271e-02 6.093e-06 4.928e-02 4.039e-03 2.863e-01 4.788e-02 5.318e-02
 7.213e-02 1.399e-05 1.016e-01 4.300e-02 1.183e-01 9.194e-01 2.138e+00
 1.447e-03 3.384e-03 7.021e-02 3.555e-02 2.419e-02 1.251e+00 1.689e-04
 1.759e-03 9.976e-03 9.521e-02 7.742e-01 1.354e-02 2.518e-02 2.499e-04
 4.420e-01 2.436e-01 1.363e-01 4.365e-04 4.463e-03 1.081e-04 6.971e-01
 1.050e+00 9.842e-03 5.538e-03 6.488e-02 5.105e-06 3.295e-04 9.058e-05
 3.401e-03 2.044e-03 1.894e-02 1.035e-03 3.292e-06 6.205e-04 5.507e-04
 8.239e-04 1.814e-05 4.026e-04 1.307e-05 5.341e-02 7.359e-04 1.131e-03
 3.871e-04 1.701e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9108333333333334, 0.8414162182496675)
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
total norm for a microbatch 16.51717786821892 clip 12.788061952071454
total norm for a microbatch 16.41109316814348 clip 13.314947614936917
total norm for a microbatch 11.19674042846567 clip 12.716508164734666
total norm for a microbatch 15.296710042052304 clip 13.423080127185797
total norm for a microbatch 19.40588172879373 clip 13.046070152079468
total norm for a microbatch 15.95258891493014 clip 14.220903771088617
total norm for a microbatch 20.845113637606612 clip 14.26042888832911
total norm for a microbatch 21.782090753263283 clip 15.30055858889795
total norm for a microbatch 17.992731596575656 clip 14.260607718520742
total norm for a microbatch 26.401680416829272 clip 14.989057716511224
total norm for a microbatch 12.102262224085624 clip 14.74381763136421
total norm for a microbatch 21.879442614634545 clip 15.028509581371491
cuda
Objective function 18.65 = squared loss an data 16.21 + 0.5*rho*h**2 1.758582 + alpha*h 0.000000 + L2reg 0.43 + L1reg 0.25 ; SHD = 70 ; DAG False
Proportion of microbatches that were clipped  0.7977709578420288
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.875410281712334
iteration 1 in outer loop, alpha = 1.875410281712334, rho = 1.0, h = 1.875410281712334
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 22.17 = squared loss an data 16.21 + 0.5*rho*h**2 1.758582 + alpha*h 3.517164 + L2reg 0.43 + L1reg 0.25 ; SHD = 70 ; DAG False
total norm for a microbatch 11.253408481883392 clip 1.2048967501798744
total norm for a microbatch 25.50001297497378 clip 5.734361006279214
total norm for a microbatch 17.478234784565522 clip 12.067532816689715
total norm for a microbatch 23.223329189070093 clip 17.64563486909791
total norm for a microbatch 18.762404673646824 clip 17.207135644643188
total norm for a microbatch 16.86414362567282 clip 19.036091058892477
total norm for a microbatch 20.195805626502793 clip 17.902815158198997
total norm for a microbatch 13.643867983888352 clip 17.483008680484073
total norm for a microbatch 14.669388919182708 clip 18.643355632003885
total norm for a microbatch 33.30620884021039 clip 17.65080022064235
total norm for a microbatch 17.0573400360234 clip 18.46643241529289
total norm for a microbatch 21.9346679602764 clip 19.46375057852817
cuda
Objective function 19.27 = squared loss an data 15.19 + 0.5*rho*h**2 0.786939 + alpha*h 2.352783 + L2reg 0.71 + L1reg 0.23 ; SHD = 69 ; DAG False
Proportion of microbatches that were clipped  0.7998860213302939
iteration 1 in inner loop, alpha 1.875410281712334 rho 1.0 h 1.2545430237988633
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 26.35 = squared loss an data 15.19 + 0.5*rho*h**2 7.869391 + alpha*h 2.352783 + L2reg 0.71 + L1reg 0.23 ; SHD = 69 ; DAG False
total norm for a microbatch 30.274081028279696 clip 6.535292006935008
total norm for a microbatch 24.831970205827137 clip 21.185705918033
total norm for a microbatch 35.9256346748046 clip 21.47190287138078
total norm for a microbatch 23.51540887665325 clip 21.141200885997495
total norm for a microbatch 15.07560897865413 clip 21.688375849075346
total norm for a microbatch 23.88872100473433 clip 21.270085574059042
total norm for a microbatch 31.665496780794726 clip 21.802926496805576
total norm for a microbatch 31.76145710235294 clip 22.658847032411746
total norm for a microbatch 34.412739014477715 clip 22.23396278652695
total norm for a microbatch 27.48338306269011 clip 22.357450588780836
total norm for a microbatch 26.71435311488344 clip 22.12107484045916
cuda
Objective function 20.01 = squared loss an data 16.62 + 0.5*rho*h**2 1.354017 + alpha*h 0.975941 + L2reg 0.85 + L1reg 0.21 ; SHD = 69 ; DAG True
Proportion of microbatches that were clipped  0.8042587384491764
iteration 2 in inner loop, alpha 1.875410281712334 rho 10.0 h 0.5203877871125151
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 32.19 = squared loss an data 16.62 + 0.5*rho*h**2 13.540172 + alpha*h 0.975941 + L2reg 0.85 + L1reg 0.21 ; SHD = 69 ; DAG True
total norm for a microbatch 38.61144375556866 clip 2.948653418152897
total norm for a microbatch 26.565979781183618 clip 24.265553660095026
total norm for a microbatch 36.33529481847431 clip 24.2661896343411
total norm for a microbatch 26.66435142068812 clip 22.943248041697295
total norm for a microbatch 18.510613529613053 clip 23.00335383753592
total norm for a microbatch 40.74593922083508 clip 23.40910795757419
total norm for a microbatch 23.846641234509892 clip 24.999440822712057
total norm for a microbatch 38.5190201284757 clip 25.401279087017745
cuda
Objective function 21.22 = squared loss an data 18.04 + 0.5*rho*h**2 1.731099 + alpha*h 0.348957 + L2reg 0.91 + L1reg 0.19 ; SHD = 65 ; DAG True
Proportion of microbatches that were clipped  0.8063626034734621
iteration 3 in inner loop, alpha 1.875410281712334 rho 100.0 h 0.18606981686209778
iteration 2 in outer loop, alpha = 20.482391967922112, rho = 100.0, h = 0.18606981686209778
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 24.68 = squared loss an data 18.04 + 0.5*rho*h**2 1.731099 + alpha*h 3.811155 + L2reg 0.91 + L1reg 0.19 ; SHD = 65 ; DAG True
total norm for a microbatch 19.44367045284037 clip 1.8892737511837898
total norm for a microbatch 41.399188419908654 clip 4.232688379914981
total norm for a microbatch 27.425324833315944 clip 12.870221525249661
total norm for a microbatch 39.71619996772513 clip 22.033744453236487
total norm for a microbatch 22.60960432748921 clip 27.415467159934177
total norm for a microbatch 39.4478096177131 clip 25.801210995405704
total norm for a microbatch 27.351909492529508 clip 25.95417661211202
total norm for a microbatch 27.97459253801484 clip 26.436201006661673
total norm for a microbatch 31.87478549408537 clip 27.184830646653616
total norm for a microbatch 18.998352434628874 clip 27.324405925612
total norm for a microbatch 24.913967089353992 clip 25.122354166385524
total norm for a microbatch 36.759483222747576 clip 26.666602663435224
total norm for a microbatch 44.79287579267925 clip 26.760718084493774
total norm for a microbatch 29.8428950321456 clip 27.300155074583213
total norm for a microbatch 24.062779530567692 clip 25.905421958053225
cuda
Objective function 22.69 = squared loss an data 18.37 + 0.5*rho*h**2 0.709284 + alpha*h 2.439528 + L2reg 1.00 + L1reg 0.18 ; SHD = 70 ; DAG True
Proportion of microbatches that were clipped  0.8068457679236123
iteration 1 in inner loop, alpha 20.482391967922112 rho 100.0 h 0.11910368951452455
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 29.07 = squared loss an data 18.37 + 0.5*rho*h**2 7.092844 + alpha*h 2.439528 + L2reg 1.00 + L1reg 0.18 ; SHD = 70 ; DAG True
total norm for a microbatch 24.104729930504742 clip 13.748719700092343
total norm for a microbatch 43.145596122570055 clip 30.19165107447787
total norm for a microbatch 43.07146494282327 clip 29.558008889590106
total norm for a microbatch 36.55624510548467 clip 30.06305959047817
total norm for a microbatch 39.18284911721803 clip 29.914550614565083
total norm for a microbatch 43.96869730587481 clip 29.739193339365986
cuda
Objective function 23.03 = squared loss an data 19.16 + 0.5*rho*h**2 1.505563 + alpha*h 1.123945 + L2reg 1.07 + L1reg 0.16 ; SHD = 60 ; DAG True
Proportion of microbatches that were clipped  0.8101487314085739
iteration 2 in inner loop, alpha 20.482391967922112 rho 1000.0 h 0.054873736657878425
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 36.58 = squared loss an data 19.16 + 0.5*rho*h**2 15.055635 + alpha*h 1.123945 + L2reg 1.07 + L1reg 0.16 ; SHD = 60 ; DAG True
total norm for a microbatch 190.21898020684688 clip 1.1023523130427815
total norm for a microbatch 59.30024818186788 clip 1.724667458972577
total norm for a microbatch 44.637932008774804 clip 5.069901930891112
total norm for a microbatch 48.13107614292892 clip 24.459772588059636
total norm for a microbatch 63.982436959777836 clip 52.9529297067612
total norm for a microbatch 50.23634140785649 clip 46.84002268630288
total norm for a microbatch 53.564400390357186 clip 43.92724508672034
total norm for a microbatch 42.3328837994469 clip 33.26932010653155
total norm for a microbatch 39.830383711668645 clip 36.22376712235528
total norm for a microbatch 51.56007712660941 clip 33.30565990783872
total norm for a microbatch 45.69864263223586 clip 34.33038033945954
total norm for a microbatch 40.44022300888088 clip 36.43302662127946
cuda
Objective function 22.95 = squared loss an data 19.51 + 0.5*rho*h**2 1.771856 + alpha*h 0.385576 + L2reg 1.12 + L1reg 0.16 ; SHD = 63 ; DAG True
Proportion of microbatches that were clipped  0.8108302097455937
iteration 3 in inner loop, alpha 20.482391967922112 rho 10000.0 h 0.018824751526704375
iteration 3 in outer loop, alpha = 208.72990723496588, rho = 10000.0, h = 0.018824751526704375
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 26.49 = squared loss an data 19.51 + 0.5*rho*h**2 1.771856 + alpha*h 3.929289 + L2reg 1.12 + L1reg 0.16 ; SHD = 63 ; DAG True
total norm for a microbatch 59.20337833772288 clip 25.04474568717045
total norm for a microbatch 75.12230374281359 clip 61.003513761105026
total norm for a microbatch 86.40902455807905 clip 77.80550857592807
total norm for a microbatch 72.3666626257576 clip 62.88169197922709
total norm for a microbatch 40.270235441800196 clip 44.97308721942052
total norm for a microbatch 38.97188138033857 clip 42.23947619325668
total norm for a microbatch 48.440049599882016 clip 42.61809357222994
total norm for a microbatch 39.47950635613979 clip 36.20439549997287
total norm for a microbatch 43.46493146385348 clip 36.20439549997287
total norm for a microbatch 39.430538516385674 clip 34.74027306091072
total norm for a microbatch 54.95914612391321 clip 42.13356771686748
total norm for a microbatch 52.102874133146145 clip 40.86797252807271
cuda
Objective function 24.12 = squared loss an data 19.63 + 0.5*rho*h**2 0.711343 + alpha*h 2.489655 + L2reg 1.13 + L1reg 0.16 ; SHD = 68 ; DAG True
Proportion of microbatches that were clipped  0.811624203821656
iteration 1 in inner loop, alpha 208.72990723496588 rho 10000.0 h 0.011927640396613981
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 30.52 = squared loss an data 19.63 + 0.5*rho*h**2 7.113430 + alpha*h 2.489655 + L2reg 1.13 + L1reg 0.16 ; SHD = 68 ; DAG True
total norm for a microbatch 158.43130938495074 clip 7.1683676437658574
total norm for a microbatch 238.0596212883331 clip 73.62502321720677
total norm for a microbatch 281.42802582995074 clip 169.08077693103417
total norm for a microbatch 352.97978063774275 clip 305.4371617527371
total norm for a microbatch 68.0247775216449 clip 59.23984836848374
total norm for a microbatch 69.25593102865147 clip 46.718745276865015
total norm for a microbatch 28.877143795548992 clip 28.081482751887457
total norm for a microbatch 45.547769978947024 clip 28.48098864045918
total norm for a microbatch 24.845851693442047 clip 28.789473070267086
total norm for a microbatch 34.52642567949096 clip 28.074883162356524
total norm for a microbatch 33.17568518709533 clip 29.511287458435497
cuda
Objective function 21.92 = squared loss an data 20.13 + 0.5*rho*h**2 0.147712 + alpha*h 0.358763 + L2reg 1.13 + L1reg 0.15 ; SHD = 74 ; DAG True
Proportion of microbatches that were clipped  0.8083447222445534
iteration 2 in inner loop, alpha 208.72990723496588 rho 100000.0 h 0.0017187889454675087
iteration 4 in outer loop, alpha = 380.60880178171675, rho = 100000.0, h = 0.0017187889454675087
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 22.21 = squared loss an data 20.13 + 0.5*rho*h**2 0.147712 + alpha*h 0.654186 + L2reg 1.13 + L1reg 0.15 ; SHD = 74 ; DAG True
total norm for a microbatch 159.05260830644548 clip 14.910992334882119
total norm for a microbatch 159.51183840137648 clip 16.170435013909188
total norm for a microbatch 163.4245249760251 clip 23.28079197829579
total norm for a microbatch 173.4853275163417 clip 28.18538658198441
total norm for a microbatch 609.9056019037677 clip 400.02087584357054
total norm for a microbatch 639.3032301303818 clip 499.5067926501179
total norm for a microbatch 331.7746686465692 clip 336.4999975820744
total norm for a microbatch 68.98834339263594 clip 71.60426161200891
total norm for a microbatch 51.74913976782928 clip 43.432935034870546
total norm for a microbatch 61.621815838510045 clip 31.78810398269346
total norm for a microbatch 41.74348595659364 clip 28.242120515612847
cuda
Objective function 22.22 = squared loss an data 20.44 + 0.5*rho*h**2 0.059011 + alpha*h 0.413485 + L2reg 1.15 + L1reg 0.16 ; SHD = 81 ; DAG True
Proportion of microbatches that were clipped  0.8077323301048587
iteration 1 in inner loop, alpha 380.60880178171675 rho 100000.0 h 0.0010863778304255334
iteration 5 in outer loop, alpha = 1466.98663220725, rho = 1000000.0, h = 0.0010863778304255334
Threshold 0.3
[[0.002 0.041 0.131 0.336 0.446 0.53  0.306 0.008 0.02  0.138 0.099 0.131
  0.159 0.006 0.275 0.258 0.023 0.549 0.368 0.318]
 [0.042 0.002 0.233 0.696 0.575 0.702 0.337 0.008 0.108 0.097 0.221 0.19
  0.376 0.006 0.538 0.237 0.031 0.568 0.289 0.583]
 [0.012 0.006 0.002 0.324 0.159 0.214 0.013 0.003 0.005 0.025 0.016 0.008
  0.014 0.001 0.01  0.047 0.005 0.041 0.005 0.011]
 [0.007 0.001 0.008 0.002 0.026 0.018 0.054 0.001 0.004 0.005 0.006 0.002
  0.009 0.002 0.007 0.01  0.005 0.011 0.01  0.014]
 [0.004 0.004 0.019 0.105 0.003 0.026 0.012 0.002 0.006 0.004 0.007 0.002
  0.016 0.003 0.013 0.025 0.006 0.056 0.017 0.05 ]
 [0.004 0.001 0.011 0.134 0.112 0.002 0.034 0.003 0.006 0.017 0.004 0.006
  0.008 0.002 0.016 0.006 0.006 0.015 0.005 0.026]
 [0.006 0.004 0.139 0.044 0.192 0.087 0.003 0.002 0.004 0.005 0.007 0.004
  0.008 0.003 0.009 0.022 0.002 0.02  0.003 0.071]
 [0.289 0.257 0.567 1.189 0.822 0.653 0.504 0.002 0.134 0.486 0.27  0.656
  0.444 0.031 0.574 0.486 0.145 1.233 0.31  1.001]
 [0.07  0.014 0.326 0.687 0.464 0.41  0.486 0.019 0.002 0.097 0.12  0.056
  0.242 0.002 0.072 0.38  0.068 0.515 0.156 0.444]
 [0.011 0.016 0.2   0.314 0.42  0.205 0.272 0.002 0.025 0.002 0.137 0.025
  0.154 0.002 0.098 0.219 0.027 0.292 0.139 0.168]
 [0.019 0.01  0.144 0.315 0.363 0.316 0.272 0.008 0.037 0.03  0.003 0.057
  0.311 0.005 0.086 0.089 0.007 0.281 0.026 0.417]
 [0.019 0.011 0.207 0.525 0.81  0.271 0.417 0.003 0.074 0.073 0.059 0.002
  0.237 0.004 0.2   0.301 0.02  0.527 0.095 0.531]
 [0.013 0.006 0.193 0.189 0.123 0.187 0.316 0.003 0.005 0.021 0.008 0.006
  0.002 0.002 0.013 0.03  0.011 0.226 0.04  0.12 ]
 [0.273 0.329 1.903 0.646 0.489 0.725 0.666 0.062 0.608 0.635 0.34  0.381
  0.46  0.002 0.27  0.457 0.187 0.48  0.627 0.621]
 [0.01  0.003 0.18  0.308 0.176 0.192 0.225 0.002 0.034 0.038 0.024 0.011
  0.251 0.008 0.002 0.071 0.008 0.069 0.123 0.295]
 [0.009 0.008 0.044 0.166 0.089 0.232 0.105 0.004 0.006 0.013 0.038 0.008
  0.061 0.003 0.038 0.003 0.008 0.092 0.037 0.12 ]
 [0.062 0.079 0.285 0.413 0.439 0.324 0.761 0.016 0.028 0.121 0.28  0.105
  0.199 0.012 0.234 0.341 0.002 0.566 0.06  0.192]
 [0.003 0.002 0.066 0.195 0.05  0.144 0.109 0.001 0.003 0.008 0.01  0.004
  0.019 0.002 0.034 0.025 0.003 0.002 0.011 0.09 ]
 [0.005 0.007 0.235 0.228 0.183 0.453 0.429 0.004 0.029 0.036 0.101 0.029
  0.065 0.003 0.021 0.149 0.022 0.193 0.002 0.174]
 [0.004 0.003 0.156 0.102 0.048 0.075 0.035 0.002 0.004 0.007 0.005 0.003
  0.023 0.004 0.006 0.019 0.01  0.033 0.012 0.002]]
[[0.    0.    0.    0.336 0.446 0.53  0.306 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.549 0.368 0.318]
 [0.    0.    0.    0.696 0.575 0.702 0.337 0.    0.    0.    0.    0.
  0.376 0.    0.538 0.    0.    0.568 0.    0.583]
 [0.    0.    0.    0.324 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.567 1.189 0.822 0.653 0.504 0.    0.    0.486 0.    0.656
  0.444 0.    0.574 0.486 0.    1.233 0.31  1.001]
 [0.    0.    0.326 0.687 0.464 0.41  0.486 0.    0.    0.    0.    0.
  0.    0.    0.    0.38  0.    0.515 0.    0.444]
 [0.    0.    0.    0.314 0.42  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.315 0.363 0.316 0.    0.    0.    0.    0.    0.
  0.311 0.    0.    0.    0.    0.    0.    0.417]
 [0.    0.    0.    0.525 0.81  0.    0.417 0.    0.    0.    0.    0.
  0.    0.    0.    0.301 0.    0.527 0.    0.531]
 [0.    0.    0.    0.    0.    0.    0.316 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.329 1.903 0.646 0.489 0.725 0.666 0.    0.608 0.635 0.34  0.381
  0.46  0.    0.    0.457 0.    0.48  0.627 0.621]
 [0.    0.    0.    0.308 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.413 0.439 0.324 0.761 0.    0.    0.    0.    0.
  0.    0.    0.    0.341 0.    0.566 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.453 0.429 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.6533333333333333, 'tpr': 0.43333333333333335, 'fpr': 0.3769230769230769, 'f1': 0.3851851851851852, 'shd': 81, 'npred': 75, 'ntrue': 60}
[4.121e-02 1.309e-01 3.359e-01 4.463e-01 5.299e-01 3.064e-01 7.682e-03
 1.966e-02 1.377e-01 9.926e-02 1.312e-01 1.589e-01 6.150e-03 2.746e-01
 2.576e-01 2.348e-02 5.493e-01 3.678e-01 3.184e-01 4.232e-02 2.325e-01
 6.961e-01 5.746e-01 7.020e-01 3.375e-01 7.604e-03 1.085e-01 9.720e-02
 2.212e-01 1.904e-01 3.757e-01 5.676e-03 5.376e-01 2.373e-01 3.104e-02
 5.678e-01 2.889e-01 5.832e-01 1.199e-02 6.323e-03 3.237e-01 1.586e-01
 2.145e-01 1.321e-02 2.718e-03 5.393e-03 2.546e-02 1.641e-02 7.981e-03
 1.435e-02 9.352e-04 9.701e-03 4.713e-02 5.404e-03 4.116e-02 5.409e-03
 1.120e-02 7.015e-03 1.129e-03 8.062e-03 2.556e-02 1.843e-02 5.449e-02
 1.314e-03 4.463e-03 5.424e-03 5.638e-03 2.104e-03 8.947e-03 2.265e-03
 6.506e-03 9.937e-03 4.950e-03 1.090e-02 1.049e-02 1.420e-02 4.200e-03
 3.578e-03 1.907e-02 1.053e-01 2.574e-02 1.183e-02 1.811e-03 5.662e-03
 4.046e-03 7.494e-03 1.816e-03 1.561e-02 2.931e-03 1.332e-02 2.460e-02
 5.811e-03 5.572e-02 1.677e-02 4.963e-02 3.949e-03 1.045e-03 1.137e-02
 1.340e-01 1.118e-01 3.384e-02 2.569e-03 6.044e-03 1.701e-02 4.458e-03
 5.636e-03 7.719e-03 1.518e-03 1.565e-02 5.971e-03 5.516e-03 1.453e-02
 4.865e-03 2.643e-02 5.717e-03 4.168e-03 1.385e-01 4.380e-02 1.920e-01
 8.675e-02 2.219e-03 4.357e-03 5.284e-03 7.124e-03 3.968e-03 8.425e-03
 2.605e-03 8.740e-03 2.213e-02 1.617e-03 2.004e-02 3.113e-03 7.052e-02
 2.893e-01 2.574e-01 5.673e-01 1.189e+00 8.220e-01 6.531e-01 5.044e-01
 1.339e-01 4.861e-01 2.703e-01 6.557e-01 4.444e-01 3.091e-02 5.743e-01
 4.859e-01 1.450e-01 1.233e+00 3.099e-01 1.001e+00 6.952e-02 1.428e-02
 3.261e-01 6.865e-01 4.638e-01 4.097e-01 4.858e-01 1.903e-02 9.689e-02
 1.202e-01 5.580e-02 2.421e-01 2.068e-03 7.202e-02 3.801e-01 6.809e-02
 5.152e-01 1.561e-01 4.436e-01 1.080e-02 1.619e-02 1.997e-01 3.135e-01
 4.203e-01 2.049e-01 2.723e-01 1.972e-03 2.543e-02 1.374e-01 2.511e-02
 1.538e-01 2.317e-03 9.780e-02 2.189e-01 2.652e-02 2.922e-01 1.390e-01
 1.681e-01 1.927e-02 1.005e-02 1.435e-01 3.147e-01 3.631e-01 3.158e-01
 2.721e-01 8.099e-03 3.725e-02 3.015e-02 5.718e-02 3.114e-01 5.273e-03
 8.574e-02 8.896e-02 7.398e-03 2.813e-01 2.601e-02 4.169e-01 1.861e-02
 1.107e-02 2.073e-01 5.254e-01 8.098e-01 2.712e-01 4.171e-01 3.220e-03
 7.405e-02 7.348e-02 5.877e-02 2.372e-01 4.154e-03 1.997e-01 3.013e-01
 1.971e-02 5.270e-01 9.464e-02 5.305e-01 1.280e-02 6.121e-03 1.928e-01
 1.886e-01 1.228e-01 1.869e-01 3.157e-01 2.709e-03 5.355e-03 2.109e-02
 7.517e-03 6.491e-03 2.027e-03 1.338e-02 2.974e-02 1.090e-02 2.263e-01
 3.998e-02 1.195e-01 2.734e-01 3.290e-01 1.903e+00 6.462e-01 4.889e-01
 7.250e-01 6.659e-01 6.173e-02 6.080e-01 6.354e-01 3.403e-01 3.805e-01
 4.597e-01 2.702e-01 4.568e-01 1.868e-01 4.802e-01 6.267e-01 6.212e-01
 9.724e-03 3.247e-03 1.795e-01 3.084e-01 1.761e-01 1.924e-01 2.246e-01
 2.184e-03 3.364e-02 3.767e-02 2.434e-02 1.133e-02 2.515e-01 7.811e-03
 7.123e-02 8.232e-03 6.862e-02 1.231e-01 2.947e-01 8.584e-03 8.304e-03
 4.450e-02 1.662e-01 8.912e-02 2.321e-01 1.047e-01 3.611e-03 5.580e-03
 1.335e-02 3.781e-02 7.684e-03 6.129e-02 2.542e-03 3.830e-02 7.853e-03
 9.242e-02 3.677e-02 1.202e-01 6.187e-02 7.950e-02 2.853e-01 4.128e-01
 4.392e-01 3.242e-01 7.610e-01 1.588e-02 2.827e-02 1.208e-01 2.803e-01
 1.053e-01 1.987e-01 1.158e-02 2.336e-01 3.407e-01 5.663e-01 5.984e-02
 1.917e-01 3.102e-03 2.068e-03 6.552e-02 1.952e-01 5.000e-02 1.441e-01
 1.092e-01 1.431e-03 3.034e-03 8.029e-03 9.515e-03 3.976e-03 1.918e-02
 2.083e-03 3.397e-02 2.500e-02 3.338e-03 1.117e-02 9.020e-02 4.653e-03
 7.384e-03 2.352e-01 2.279e-01 1.830e-01 4.533e-01 4.287e-01 4.145e-03
 2.940e-02 3.583e-02 1.013e-01 2.869e-02 6.454e-02 3.096e-03 2.147e-02
 1.491e-01 2.231e-02 1.933e-01 1.739e-01 4.331e-03 3.043e-03 1.557e-01
 1.016e-01 4.812e-02 7.511e-02 3.492e-02 1.865e-03 4.267e-03 6.802e-03
 4.790e-03 2.756e-03 2.290e-02 4.086e-03 6.019e-03 1.852e-02 9.789e-03
 3.302e-02 1.235e-02]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7328125, 0.3660863792354562)
Iterations 2500
Achieves (3.6993101023104464, 1e-05)-DP
