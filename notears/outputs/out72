samples  5000  graph  15 30 ER mlp  minibatch size  50  noise  0.5  minibatches per NN training  125 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.070178732328948
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02596805228377619
iteration 3 in outer loop, alpha = 40.66567187478174, rho = 1000.0, h = 0.02596805228377619
cuda
iteration 1 in inner loop,alpha 40.66567187478174 rho 1000.0 h 0.01645279113621001
iteration 2 in inner loop,alpha 40.66567187478174 rho 10000.0 h 0.0064598020093367126
iteration 4 in outer loop, alpha = 105.26369196814886, rho = 10000.0, h = 0.0064598020093367126
cuda
iteration 1 in inner loop,alpha 105.26369196814886 rho 10000.0 h 0.0018225936000000331
iteration 2 in inner loop,alpha 105.26369196814886 rho 100000.0 h 0.0005338302150796892
iteration 5 in outer loop, alpha = 158.64671347611778, rho = 100000.0, h = 0.0005338302150796892
cuda
iteration 1 in inner loop,alpha 158.64671347611778 rho 100000.0 h 0.00023044312914244358
iteration 6 in outer loop, alpha = 389.08984261856136, rho = 1000000.0, h = 0.00023044312914244358
Threshold 0.3
[[0.002 0.    0.002 0.005 0.209 0.    0.001 0.    0.    2.762 0.22  0.
  0.001 0.    0.008]
 [2.527 0.001 0.002 0.262 0.191 0.009 0.011 0.002 0.001 2.221 0.195 1.352
  0.    0.001 0.097]
 [0.01  0.064 0.001 0.147 0.147 0.02  0.141 0.002 0.064 0.131 0.126 0.163
  0.002 0.001 0.007]
 [0.156 0.    0.001 0.001 0.189 0.    0.001 0.    0.    0.248 0.351 0.002
  0.001 0.    1.022]
 [0.    0.    0.    0.    0.005 0.    0.    0.    0.    0.    0.001 0.
  0.    0.    0.001]
 [2.646 0.032 0.001 0.964 0.353 0.001 1.428 0.    0.055 1.758 1.235 1.48
  0.014 0.002 1.924]
 [0.42  0.043 0.005 0.338 0.42  0.    0.001 0.    0.053 0.18  1.718 0.1
  0.003 0.002 0.166]
 [2.691 0.004 0.003 2.095 0.177 0.633 0.048 0.    0.006 0.357 0.165 0.06
  0.02  0.023 0.117]
 [0.102 0.385 0.002 0.643 0.456 0.002 0.002 0.001 0.004 2.316 2.281 1.036
  0.    0.001 0.367]
 [0.    0.    0.    0.001 2.852 0.    0.    0.    0.    0.003 0.262 0.
  0.    0.    0.003]
 [0.    0.    0.    0.001 3.137 0.    0.    0.    0.001 0.002 0.011 0.
  0.    0.    0.005]
 [0.363 0.001 0.002 0.18  0.135 0.    0.003 0.    0.002 0.408 0.468 0.002
  0.    0.001 0.031]
 [0.04  0.078 0.003 0.102 3.071 0.014 0.005 0.001 1.06  0.112 1.924 0.034
  0.001 0.018 0.017]
 [0.127 1.412 0.003 1.135 1.243 0.003 0.005 0.001 3.433 0.367 0.486 0.155
  0.01  0.002 0.562]
 [0.075 0.001 0.001 0.    0.109 0.    0.    0.    0.    0.115 0.096 0.003
  0.001 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.762 0.    0.
  0.    0.    0.   ]
 [2.527 0.    0.    0.    0.    0.    0.    0.    0.    2.221 0.    1.352
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.351 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.964 0.353 0.    1.428 0.    0.    1.758 1.235 1.48
  0.    0.    1.924]
 [0.42  0.    0.    0.338 0.42  0.    0.    0.    0.    0.    1.718 0.
  0.    0.    0.   ]
 [2.691 0.    0.    2.095 0.    0.633 0.    0.    0.    0.357 0.    0.
  0.    0.    0.   ]
 [0.    0.385 0.    0.643 0.456 0.    0.    0.    0.    2.316 2.281 1.036
  0.    0.    0.367]
 [0.    0.    0.    0.    2.852 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.137 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.363 0.    0.    0.    0.    0.    0.    0.    0.    0.408 0.468 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.071 0.    0.    0.    1.06  0.    1.924 0.
  0.    0.    0.   ]
 [0.    1.412 0.    1.135 1.243 0.    0.    0.    3.433 0.367 0.486 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.3409090909090909, 'tpr': 0.9666666666666667, 'fpr': 0.2, 'f1': 0.7837837837837838, 'shd': 16, 'npred': 44, 'ntrue': 30}
[3.656e-05 2.466e-03 5.264e-03 2.094e-01 6.956e-05 9.728e-04 2.527e-05
 2.749e-04 2.762e+00 2.199e-01 3.467e-04 1.409e-03 1.196e-04 8.024e-03
 2.527e+00 1.891e-03 2.624e-01 1.914e-01 8.996e-03 1.135e-02 1.788e-03
 1.192e-03 2.221e+00 1.946e-01 1.352e+00 4.001e-04 9.451e-04 9.745e-02
 9.801e-03 6.386e-02 1.469e-01 1.470e-01 1.974e-02 1.408e-01 1.818e-03
 6.420e-02 1.314e-01 1.259e-01 1.629e-01 1.520e-03 1.402e-03 6.966e-03
 1.563e-01 2.515e-04 1.101e-03 1.893e-01 1.834e-05 5.335e-04 1.301e-04
 1.287e-04 2.481e-01 3.506e-01 2.284e-03 7.956e-04 1.335e-04 1.022e+00
 6.080e-05 4.644e-06 8.821e-05 2.093e-04 9.586e-06 1.832e-04 3.304e-06
 1.648e-04 3.512e-04 1.137e-03 1.628e-04 2.023e-05 6.301e-05 1.152e-03
 2.646e+00 3.159e-02 8.481e-04 9.644e-01 3.528e-01 1.428e+00 1.116e-04
 5.466e-02 1.758e+00 1.235e+00 1.480e+00 1.391e-02 1.927e-03 1.924e+00
 4.202e-01 4.265e-02 4.817e-03 3.382e-01 4.199e-01 2.221e-04 1.511e-04
 5.302e-02 1.795e-01 1.718e+00 1.002e-01 2.639e-03 1.817e-03 1.659e-01
 2.691e+00 3.595e-03 2.995e-03 2.095e+00 1.767e-01 6.330e-01 4.785e-02
 5.811e-03 3.568e-01 1.651e-01 6.001e-02 1.971e-02 2.319e-02 1.170e-01
 1.024e-01 3.845e-01 2.092e-03 6.435e-01 4.563e-01 2.194e-03 1.630e-03
 1.205e-03 2.316e+00 2.281e+00 1.036e+00 8.175e-05 5.371e-04 3.670e-01
 2.179e-04 6.015e-06 3.494e-04 1.084e-03 2.852e+00 6.861e-06 1.839e-04
 7.855e-06 4.540e-04 2.616e-01 8.602e-05 7.517e-05 1.152e-04 2.814e-03
 4.750e-04 8.905e-05 2.014e-04 1.151e-03 3.137e+00 1.333e-04 4.933e-04
 9.917e-05 6.433e-04 2.117e-03 2.549e-04 1.146e-04 1.828e-04 5.052e-03
 3.629e-01 7.029e-04 2.303e-03 1.797e-01 1.355e-01 2.526e-04 3.465e-03
 1.965e-04 1.710e-03 4.079e-01 4.679e-01 7.688e-05 6.045e-04 3.068e-02
 4.028e-02 7.803e-02 3.003e-03 1.017e-01 3.071e+00 1.390e-02 4.710e-03
 1.383e-03 1.060e+00 1.118e-01 1.924e+00 3.407e-02 1.779e-02 1.741e-02
 1.273e-01 1.412e+00 3.168e-03 1.135e+00 1.243e+00 2.759e-03 5.147e-03
 1.163e-03 3.433e+00 3.669e-01 4.863e-01 1.546e-01 9.780e-03 5.621e-01
 7.514e-02 1.258e-03 1.334e-03 7.654e-05 1.088e-01 1.538e-05 3.662e-04
 4.020e-05 2.548e-04 1.149e-01 9.554e-02 2.816e-03 1.060e-03 9.398e-05]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9787037037037037, 0.9713991372612062)
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 88.41755086630708 clip 1.3306975146315496
total norm for a microbatch 39.874437558357755 clip 9.323439307706348
total norm for a microbatch 36.52994846291962 clip 31.81529229448199
total norm for a microbatch 37.18192627856116 clip 28.12770525756048
total norm for a microbatch 29.765497620858977 clip 29.656443525576062
total norm for a microbatch 35.9956348587184 clip 28.42088340912776
total norm for a microbatch 27.6700361820578 clip 27.660055903415927
total norm for a microbatch 27.267483309324245 clip 26.094335721926686
total norm for a microbatch 36.52994790807809 clip 25.343574008093846
total norm for a microbatch 23.87478866150635 clip 24.738296567223784
total norm for a microbatch 30.047148942603414 clip 24.134713215451963
total norm for a microbatch 22.264923546638226 clip 25.02683405446407
cuda
Objective function 19.23 = squared loss an data 17.10 + 0.5*rho*h**2 1.452845 + alpha*h 0.000000 + L2reg 0.53 + L1reg 0.15 ; SHD = 50 ; DAG False
Proportion of microbatches that were clipped  0.8384778703250041
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.7046082236390419
iteration 1 in outer loop, alpha = 1.7046082236390419, rho = 1.0, h = 1.7046082236390419
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 22.14 = squared loss an data 17.10 + 0.5*rho*h**2 1.452845 + alpha*h 2.905689 + L2reg 0.53 + L1reg 0.15 ; SHD = 50 ; DAG False
total norm for a microbatch 38.42796441705036 clip 3.3810318531239973
total norm for a microbatch 28.649862580205827 clip 14.171084051731384
total norm for a microbatch 62.506190712079174 clip 33.54038367822099
total norm for a microbatch 52.9819600039283 clip 35.31661755487662
total norm for a microbatch 76.30674117663196 clip 38.572807558214045
cuda
Objective function 14.37 = squared loss an data 10.74 + 0.5*rho*h**2 0.648608 + alpha*h 1.941470 + L2reg 0.89 + L1reg 0.15 ; SHD = 52 ; DAG False
Proportion of microbatches that were clipped  0.8498646712306958
iteration 1 in inner loop, alpha 1.7046082236390419 rho 1.0 h 1.1389540622639345
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 20.21 = squared loss an data 10.74 + 0.5*rho*h**2 6.486082 + alpha*h 1.941470 + L2reg 0.89 + L1reg 0.15 ; SHD = 52 ; DAG False
total norm for a microbatch 80.12046218891997 clip 48.131152888736224
total norm for a microbatch 124.49407181587557 clip 46.4503990769421
total norm for a microbatch 53.43701884877721 clip 46.86189608369047
total norm for a microbatch 57.045568507234854 clip 44.88794947248508
total norm for a microbatch 36.131707828821156 clip 49.6648139201046
cuda
Objective function 15.35 = squared loss an data 11.57 + 0.5*rho*h**2 1.616323 + alpha*h 0.969178 + L2reg 1.05 + L1reg 0.14 ; SHD = 40 ; DAG False
Proportion of microbatches that were clipped  0.8492141453831041
iteration 2 in inner loop, alpha 1.7046082236390419 rho 10.0 h 0.5685635424595397
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 29.90 = squared loss an data 11.57 + 0.5*rho*h**2 16.163225 + alpha*h 0.969178 + L2reg 1.05 + L1reg 0.14 ; SHD = 40 ; DAG False
total norm for a microbatch 77.50172435560664 clip 10.13117438112632
total norm for a microbatch 94.59663190161037 clip 55.25535428038913
total norm for a microbatch 110.7888265066067 clip 58.52636660705546
total norm for a microbatch 49.38443568183431 clip 63.20118031101015
total norm for a microbatch 89.7050320245457 clip 60.56956094847953
total norm for a microbatch 85.18360094592829 clip 61.98155677927514
total norm for a microbatch 61.63549703572137 clip 61.97580258990434
total norm for a microbatch 111.96830078793133 clip 54.90224720523901
cuda
Objective function 16.93 = squared loss an data 13.61 + 0.5*rho*h**2 1.713158 + alpha*h 0.315528 + L2reg 1.17 + L1reg 0.13 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.8572583252505658
iteration 3 in inner loop, alpha 1.7046082236390419 rho 100.0 h 0.1851030833475349
iteration 2 in outer loop, alpha = 20.21491655839253, rho = 100.0, h = 0.1851030833475349
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 20.36 = squared loss an data 13.61 + 0.5*rho*h**2 1.713158 + alpha*h 3.741843 + L2reg 1.17 + L1reg 0.13 ; SHD = 39 ; DAG True
total norm for a microbatch 110.3467728392024 clip 14.57597066834908
total norm for a microbatch 71.77872986663381 clip 61.452155935549904
total norm for a microbatch 64.83722720016549 clip 59.63944994329463
total norm for a microbatch 102.1507975502109 clip 62.562352039717055
total norm for a microbatch 37.97718054065375 clip 64.23961962859872
cuda
Objective function 17.81 = squared loss an data 13.64 + 0.5*rho*h**2 0.580030 + alpha*h 2.177269 + L2reg 1.28 + L1reg 0.13 ; SHD = 42 ; DAG True
Proportion of microbatches that were clipped  0.8573268921095009
iteration 1 in inner loop, alpha 20.21491655839253 rho 100.0 h 0.10770606856926968
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 23.03 = squared loss an data 13.64 + 0.5*rho*h**2 5.800299 + alpha*h 2.177269 + L2reg 1.28 + L1reg 0.13 ; SHD = 42 ; DAG True
total norm for a microbatch 129.22040073399626 clip 1.5623032525467524
total norm for a microbatch 70.2732321582322 clip 66.53599764940328
total norm for a microbatch 77.90329319357413 clip 69.24881385417275
total norm for a microbatch 79.41686077594149 clip 67.25830823677994
total norm for a microbatch 67.29749344799137 clip 64.04857561918368
cuda
Objective function 17.59 = squared loss an data 14.25 + 0.5*rho*h**2 0.948552 + alpha*h 0.880476 + L2reg 1.38 + L1reg 0.13 ; SHD = 45 ; DAG True
Proportion of microbatches that were clipped  0.8595306975249116
iteration 2 in inner loop, alpha 20.21491655839253 rho 1000.0 h 0.043555766959118
iteration 3 in outer loop, alpha = 63.770683517510534, rho = 1000.0, h = 0.043555766959118
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 19.49 = squared loss an data 14.25 + 0.5*rho*h**2 0.948552 + alpha*h 2.777581 + L2reg 1.38 + L1reg 0.13 ; SHD = 45 ; DAG True
cuda
Objective function 18.45 = squared loss an data 14.21 + 0.5*rho*h**2 0.538234 + alpha*h 2.092288 + L2reg 1.47 + L1reg 0.14 ; SHD = 49 ; DAG True
Proportion of microbatches that were clipped  0.855860267711394
iteration 1 in inner loop, alpha 63.770683517510534 rho 1000.0 h 0.03280955922041073
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 23.30 = squared loss an data 14.21 + 0.5*rho*h**2 5.382336 + alpha*h 2.092288 + L2reg 1.47 + L1reg 0.14 ; SHD = 49 ; DAG True
total norm for a microbatch 130.5158005158065 clip 1.8482889001712077
total norm for a microbatch 65.93095430905247 clip 72.72683514611924
total norm for a microbatch 71.16024020162564 clip 93.72889321740978
total norm for a microbatch 97.74327620784507 clip 85.91771257895057
cuda
Objective function 17.25 = squared loss an data 13.96 + 0.5*rho*h**2 0.810583 + alpha*h 0.811960 + L2reg 1.52 + L1reg 0.15 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.8595614317961948
iteration 2 in inner loop, alpha 63.770683517510534 rho 10000.0 h 0.012732504207134454
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 24.55 = squared loss an data 13.96 + 0.5*rho*h**2 8.105833 + alpha*h 0.811960 + L2reg 1.52 + L1reg 0.15 ; SHD = 43 ; DAG True
total norm for a microbatch 124.92735402955311 clip 6.496763106911315
total norm for a microbatch 243.9681550050403 clip 88.03016092934921
total norm for a microbatch 164.48503978752453 clip 88.03016092934921
total norm for a microbatch 193.347633289348 clip 192.18628176972092
cuda
Objective function 16.81 = squared loss an data 14.12 + 0.5*rho*h**2 0.697517 + alpha*h 0.238184 + L2reg 1.61 + L1reg 0.15 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.8588330632090762
iteration 3 in inner loop, alpha 63.770683517510534 rho 100000.0 h 0.0037350143270753478
iteration 4 in outer loop, alpha = 437.2721162250453, rho = 100000.0, h = 0.0037350143270753478
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 18.20 = squared loss an data 14.12 + 0.5*rho*h**2 0.697517 + alpha*h 1.633218 + L2reg 1.61 + L1reg 0.15 ; SHD = 46 ; DAG True
total norm for a microbatch 149.27658463250614 clip 12.755702450919646
total norm for a microbatch 186.376196147181 clip 31.94075631365787
total norm for a microbatch 237.86568593573693 clip 65.56129004502188
cuda
Objective function 17.53 = squared loss an data 14.41 + 0.5*rho*h**2 0.289056 + alpha*h 1.051375 + L2reg 1.64 + L1reg 0.15 ; SHD = 47 ; DAG True
Proportion of microbatches that were clipped  0.8594533398026848
iteration 1 in inner loop, alpha 437.2721162250453 rho 100000.0 h 0.002404394496792861
iteration 5 in outer loop, alpha = 2841.6666130179065, rho = 1000000.0, h = 0.002404394496792861
Threshold 0.3
[[0.004 0.145 0.453 0.296 0.49  0.018 0.013 0.002 0.028 1.06  0.108 0.301
  0.031 0.009 0.357]
 [0.035 0.004 0.242 0.287 0.79  0.077 0.044 0.014 0.025 0.592 0.433 0.894
  0.018 0.003 0.216]
 [0.008 0.018 0.002 0.152 0.577 0.01  0.004 0.006 0.014 0.154 0.057 0.056
  0.012 0.006 0.074]
 [0.012 0.017 0.039 0.004 0.216 0.016 0.003 0.004 0.011 0.033 0.026 0.117
  0.002 0.004 0.033]
 [0.002 0.002 0.008 0.019 0.007 0.004 0.001 0.001 0.003 0.002 0.002 0.019
  0.001 0.001 0.006]
 [0.304 0.08  0.537 0.27  0.447 0.002 0.01  0.005 0.022 0.546 0.084 0.643
  0.021 0.008 0.214]
 [0.278 0.096 0.931 1.101 0.684 0.466 0.002 0.039 0.102 0.443 0.783 0.461
  0.093 0.01  0.526]
 [1.333 0.469 0.449 1.233 0.673 0.649 0.167 0.004 0.31  1.003 0.562 0.786
  0.141 0.084 0.587]
 [0.158 0.237 0.184 0.383 0.452 0.206 0.046 0.014 0.004 0.874 0.655 0.902
  0.013 0.001 0.219]
 [0.003 0.008 0.035 0.138 1.824 0.007 0.006 0.002 0.007 0.005 0.014 0.087
  0.004 0.002 0.12 ]
 [0.059 0.013 0.111 0.221 2.367 0.064 0.004 0.009 0.005 0.316 0.007 0.237
  0.003 0.002 0.196]
 [0.02  0.004 0.067 0.048 0.22  0.007 0.011 0.004 0.005 0.04  0.024 0.004
  0.004 0.001 0.087]
 [0.142 0.291 0.359 1.31  1.442 0.187 0.077 0.037 0.387 0.847 1.099 0.755
  0.003 0.021 0.412]
 [0.575 1.22  0.63  0.78  0.881 0.504 0.392 0.066 2.519 1.118 1.042 1.449
  0.301 0.005 0.678]
 [0.017 0.02  0.042 0.077 0.331 0.016 0.007 0.007 0.02  0.028 0.037 0.059
  0.007 0.006 0.003]]
[[0.    0.    0.453 0.    0.49  0.    0.    0.    0.    1.06  0.    0.301
  0.    0.    0.357]
 [0.    0.    0.    0.    0.79  0.    0.    0.    0.    0.592 0.433 0.894
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.577 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.304 0.    0.537 0.    0.447 0.    0.    0.    0.    0.546 0.    0.643
  0.    0.    0.   ]
 [0.    0.    0.931 1.101 0.684 0.466 0.    0.    0.    0.443 0.783 0.461
  0.    0.    0.526]
 [1.333 0.469 0.449 1.233 0.673 0.649 0.    0.    0.31  1.003 0.562 0.786
  0.    0.    0.587]
 [0.    0.    0.    0.383 0.452 0.    0.    0.    0.    0.874 0.655 0.902
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.824 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.367 0.    0.    0.    0.    0.316 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.359 1.31  1.442 0.    0.    0.    0.387 0.847 1.099 0.755
  0.    0.    0.412]
 [0.575 1.22  0.63  0.78  0.881 0.504 0.392 0.    2.519 1.118 1.042 1.449
  0.301 0.    0.678]
 [0.    0.    0.    0.    0.331 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.640625, 'tpr': 0.7666666666666667, 'fpr': 0.5466666666666666, 'f1': 0.4893617021276596, 'shd': 47, 'npred': 64, 'ntrue': 30}
[1.454e-01 4.530e-01 2.955e-01 4.902e-01 1.771e-02 1.345e-02 1.611e-03
 2.784e-02 1.060e+00 1.075e-01 3.010e-01 3.059e-02 8.647e-03 3.565e-01
 3.511e-02 2.417e-01 2.870e-01 7.904e-01 7.712e-02 4.427e-02 1.355e-02
 2.540e-02 5.922e-01 4.331e-01 8.936e-01 1.763e-02 3.067e-03 2.164e-01
 7.527e-03 1.821e-02 1.520e-01 5.769e-01 9.598e-03 3.684e-03 5.887e-03
 1.388e-02 1.537e-01 5.705e-02 5.644e-02 1.179e-02 5.611e-03 7.448e-02
 1.163e-02 1.708e-02 3.922e-02 2.160e-01 1.590e-02 3.142e-03 3.746e-03
 1.058e-02 3.323e-02 2.590e-02 1.167e-01 2.141e-03 3.667e-03 3.347e-02
 2.309e-03 1.992e-03 8.424e-03 1.867e-02 4.000e-03 1.174e-03 1.032e-03
 3.288e-03 1.992e-03 1.849e-03 1.913e-02 1.477e-03 1.207e-03 6.377e-03
 3.038e-01 7.998e-02 5.369e-01 2.697e-01 4.465e-01 9.699e-03 4.857e-03
 2.164e-02 5.462e-01 8.394e-02 6.428e-01 2.132e-02 8.365e-03 2.145e-01
 2.782e-01 9.563e-02 9.306e-01 1.101e+00 6.836e-01 4.660e-01 3.932e-02
 1.016e-01 4.426e-01 7.825e-01 4.609e-01 9.291e-02 9.811e-03 5.259e-01
 1.333e+00 4.688e-01 4.486e-01 1.233e+00 6.728e-01 6.494e-01 1.669e-01
 3.105e-01 1.003e+00 5.620e-01 7.855e-01 1.406e-01 8.373e-02 5.873e-01
 1.580e-01 2.370e-01 1.843e-01 3.829e-01 4.521e-01 2.056e-01 4.584e-02
 1.433e-02 8.739e-01 6.551e-01 9.016e-01 1.302e-02 1.426e-03 2.192e-01
 3.058e-03 8.438e-03 3.542e-02 1.378e-01 1.824e+00 7.346e-03 5.534e-03
 2.072e-03 7.424e-03 1.427e-02 8.682e-02 3.811e-03 1.658e-03 1.196e-01
 5.867e-02 1.304e-02 1.109e-01 2.209e-01 2.367e+00 6.383e-02 4.156e-03
 9.049e-03 5.342e-03 3.165e-01 2.369e-01 3.386e-03 2.304e-03 1.963e-01
 2.035e-02 4.410e-03 6.657e-02 4.832e-02 2.197e-01 6.883e-03 1.077e-02
 4.150e-03 5.147e-03 4.008e-02 2.440e-02 3.662e-03 1.351e-03 8.680e-02
 1.422e-01 2.913e-01 3.589e-01 1.310e+00 1.442e+00 1.871e-01 7.731e-02
 3.689e-02 3.875e-01 8.465e-01 1.099e+00 7.552e-01 2.127e-02 4.121e-01
 5.752e-01 1.220e+00 6.302e-01 7.803e-01 8.809e-01 5.038e-01 3.919e-01
 6.595e-02 2.519e+00 1.118e+00 1.042e+00 1.449e+00 3.013e-01 6.784e-01
 1.741e-02 1.972e-02 4.151e-02 7.654e-02 3.314e-01 1.606e-02 6.794e-03
 6.877e-03 1.981e-02 2.805e-02 3.671e-02 5.883e-02 7.027e-03 5.839e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8511111111111112, 0.5731048162999486)
Iterations 1250
Achieves (18.188967874884245, 1e-05)-DP
