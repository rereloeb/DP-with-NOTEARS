samples  5000  graph  30 60 ER mim  minibatch size  100  noise  1.0  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.144444827137683
iteration 1 in outer loop, alpha = 2.144444827137683, rho = 1.0, h = 2.144444827137683
cuda
iteration 1 in inner loop,alpha 2.144444827137683 rho 1.0 h 1.3256466874605835
iteration 2 in inner loop,alpha 2.144444827137683 rho 10.0 h 0.5131010328032026
iteration 2 in outer loop, alpha = 7.275455155169709, rho = 10.0, h = 0.5131010328032026
cuda
iteration 1 in inner loop,alpha 7.275455155169709 rho 10.0 h 0.2844361290576565
iteration 2 in inner loop,alpha 7.275455155169709 rho 100.0 h 0.09728722039011117
iteration 3 in outer loop, alpha = 17.004177194180826, rho = 100.0, h = 0.09728722039011117
cuda
iteration 1 in inner loop,alpha 17.004177194180826 rho 100.0 h 0.055084839421063236
iteration 2 in inner loop,alpha 17.004177194180826 rho 1000.0 h 0.015168849391027805
iteration 4 in outer loop, alpha = 32.17302658520863, rho = 1000.0, h = 0.015168849391027805
cuda
iteration 1 in inner loop,alpha 32.17302658520863 rho 1000.0 h 0.007489183028791757
iteration 2 in inner loop,alpha 32.17302658520863 rho 10000.0 h 0.002386466019451916
iteration 5 in outer loop, alpha = 56.03768677972779, rho = 10000.0, h = 0.002386466019451916
cuda
iteration 1 in inner loop,alpha 56.03768677972779 rho 10000.0 h 0.0009751502768189368
iteration 2 in inner loop,alpha 56.03768677972779 rho 100000.0 h 0.00038861670830669937
iteration 6 in outer loop, alpha = 94.89935761039773, rho = 100000.0, h = 0.00038861670830669937
cuda
iteration 1 in inner loop,alpha 94.89935761039773 rho 100000.0 h 0.00022474219736068335
iteration 7 in outer loop, alpha = 319.64155497108106, rho = 1000000.0, h = 0.00022474219736068335
Threshold 0.3
[[0.    0.001 0.102 0.045 0.057 0.002 0.115 0.024 0.058 0.348 0.    0.074
  0.    0.002 0.003 0.121 0.    0.046 0.052 0.877 0.039 0.002 0.151 0.019
  0.    0.035 0.035 0.017 0.022 0.026]
 [0.    0.    0.022 0.004 0.013 0.68  1.878 0.092 0.115 0.003 0.    0.034
  0.002 0.001 0.003 0.04  0.    0.071 0.03  0.052 0.008 0.003 0.103 0.051
  0.002 0.013 0.035 0.082 0.085 0.037]
 [0.    0.001 0.004 0.008 0.106 0.    0.069 0.048 0.001 0.    0.    0.
  0.    0.    0.    0.049 0.    0.    0.056 0.122 0.001 0.001 0.011 0.012
  0.    0.    0.008 0.014 0.022 0.006]
 [0.    0.001 0.007 0.003 0.022 0.003 0.023 0.    0.001 0.    0.    0.001
  0.    0.    0.    0.022 0.    0.    0.005 0.06  0.001 0.    0.005 0.001
  0.001 0.    0.002 0.    0.06  0.   ]
 [0.    0.001 0.003 0.005 0.003 0.001 0.014 0.01  0.002 0.    0.001 0.
  0.    0.    0.    0.016 0.    0.001 0.009 0.176 0.001 0.    0.011 0.003
  0.    0.    0.001 0.011 0.014 0.001]
 [0.    0.    0.05  0.041 0.017 0.003 0.047 0.048 0.086 0.    0.001 0.088
  0.    0.001 0.    0.002 0.    0.005 0.004 0.83  0.001 0.    0.031 0.011
  0.    0.001 0.065 0.004 0.003 0.001]
 [0.    0.    0.011 0.026 0.024 0.001 0.003 0.017 0.006 0.    0.001 0.001
  0.    0.001 0.    0.036 0.    0.003 0.009 0.09  0.    0.    0.104 0.042
  0.    0.    0.01  0.009 0.021 0.002]
 [0.001 0.001 0.009 1.475 0.009 0.013 0.03  0.003 0.003 0.    0.    0.002
  0.    0.    0.    0.021 0.    0.    0.006 0.007 0.002 0.    0.094 0.003
  0.    0.    0.008 0.    0.045 0.002]
 [0.    0.001 0.053 0.002 0.043 0.005 0.04  0.036 0.003 0.001 0.    0.059
  0.    0.    0.001 0.042 0.    0.067 0.747 1.088 0.    0.001 1.559 0.058
  0.    0.002 0.052 0.002 0.055 0.004]
 [0.    0.    0.123 0.139 0.029 1.135 0.054 0.649 0.084 0.    0.002 2.531
  0.003 0.002 0.    0.121 0.002 0.001 0.028 0.1   0.008 0.002 0.017 0.006
  0.    0.013 0.921 0.022 0.011 0.028]
 [0.003 0.    0.032 0.185 0.033 0.014 0.09  0.078 0.906 0.017 0.    0.036
  0.001 0.    0.031 0.078 0.002 0.08  0.065 0.101 0.055 0.014 0.024 0.065
  0.001 0.037 0.072 4.124 0.042 0.01 ]
 [0.    0.    4.324 0.009 0.029 0.007 0.019 0.055 0.013 0.    0.    0.002
  0.    0.    0.    0.074 0.001 0.015 0.038 0.149 0.002 0.001 0.025 0.057
  0.    0.    0.594 0.024 0.027 0.007]
 [0.003 0.001 0.003 0.052 0.047 0.006 1.878 0.014 1.164 0.02  0.001 0.051
  0.    0.    0.002 0.051 0.    0.089 0.003 0.    0.022 0.023 0.108 0.287
  0.001 0.038 0.617 0.038 0.021 2.844]
 [0.001 0.001 0.081 0.001 1.229 0.046 0.024 0.135 0.062 0.003 0.001 0.064
  0.001 0.    0.001 0.031 0.001 0.95  0.021 1.339 0.019 0.003 0.027 0.028
  0.002 0.005 0.508 0.068 0.011 0.003]
 [0.001 0.001 0.031 0.058 0.06  0.059 0.038 0.111 0.047 0.002 0.    1.8
  0.    0.    0.    0.011 0.    0.204 0.025 0.002 0.027 0.012 0.104 0.719
  0.    3.171 0.032 0.013 0.121 0.049]
 [0.    0.001 0.011 0.004 0.054 0.002 0.004 0.01  0.    0.    0.    0.
  0.    0.    0.    0.003 0.    0.    0.003 0.008 0.    0.    0.001 0.
  0.    0.    0.    0.001 0.002 0.   ]
 [0.    0.001 2.567 0.092 0.088 0.02  0.033 0.019 0.446 0.001 0.002 0.006
  0.    0.001 0.    0.035 0.    0.105 0.005 0.034 2.901 0.403 0.104 0.019
  0.002 0.002 0.085 0.051 0.074 0.053]
 [0.    0.    0.016 0.059 1.536 0.102 0.018 1.128 0.002 0.002 0.001 0.009
  0.    0.    0.    0.063 0.    0.004 0.049 0.109 0.003 0.    0.019 0.038
  0.001 0.001 1.001 0.012 0.214 0.001]
 [0.001 0.001 0.008 0.021 0.017 0.006 0.006 0.027 0.001 0.    0.    0.001
  0.    0.001 0.    0.022 0.    0.018 0.003 0.002 0.    0.    0.016 0.001
  0.    0.    0.036 0.007 0.031 0.001]
 [0.    0.    0.004 0.003 0.001 0.001 0.008 0.    0.001 0.    0.    0.001
  0.    0.    0.    0.016 0.    0.001 0.06  0.003 0.    0.    0.048 0.011
  0.    0.    0.026 0.003 0.009 0.001]
 [0.    0.    0.033 0.002 0.004 0.837 0.015 0.033 1.7   0.    0.001 0.064
  0.001 0.002 0.    0.014 0.    0.084 0.023 0.082 0.002 0.002 0.006 0.03
  0.    0.017 0.019 0.013 2.131 0.026]
 [0.    0.    0.04  1.338 1.124 0.025 0.05  0.118 0.232 0.001 0.001 0.032
  0.003 0.    0.002 0.054 0.    0.752 0.076 0.066 0.32  0.001 0.087 0.173
  0.001 0.028 0.088 0.048 1.579 1.626]
 [0.001 0.    0.003 0.106 0.051 0.002 0.001 0.008 0.    0.    0.    0.003
  0.    0.    0.    1.06  0.    0.002 0.028 0.002 0.    0.    0.004 0.
  0.    0.    0.002 0.001 0.006 0.   ]
 [0.001 0.001 0.009 0.615 0.003 0.022 0.013 0.099 0.014 0.001 0.001 0.006
  0.    0.001 0.    0.118 0.    0.014 1.169 0.024 0.007 0.    2.457 0.004
  0.    0.    0.664 0.005 0.177 0.001]
 [0.002 0.    0.06  0.076 0.017 0.1   1.493 0.028 0.007 0.034 0.003 0.017
  0.002 0.    0.002 0.11  0.001 0.062 0.067 0.06  0.04  0.07  1.677 0.039
  0.    0.034 0.029 0.062 0.01  0.032]
 [0.001 0.002 0.048 0.062 0.175 0.038 0.009 2.535 0.044 0.001 0.001 0.291
  0.001 0.001 0.    0.036 0.    1.29  0.083 0.037 0.007 0.001 0.117 1.542
  0.    0.002 0.093 0.03  2.216 0.058]
 [0.    0.    0.059 0.01  0.174 0.004 0.028 0.063 0.011 0.    0.001 0.001
  0.    0.    0.    2.22  0.    0.001 0.029 0.025 0.002 0.    0.096 0.002
  0.001 0.    0.004 0.009 0.05  0.001]
 [0.004 0.    0.035 0.09  0.002 0.034 0.029 1.368 0.147 0.001 0.    0.007
  0.001 0.001 0.    0.021 0.    0.03  0.039 0.054 0.041 0.006 0.006 0.018
  0.001 0.014 0.014 0.003 0.024 0.008]
 [0.001 0.001 0.036 0.003 0.049 0.111 0.017 0.006 0.015 0.    0.    0.002
  0.001 0.    0.    0.048 0.    0.002 0.016 0.058 0.    0.    0.046 0.002
  0.    0.    0.006 0.    0.003 0.003]
 [0.002 0.    0.029 0.078 0.062 0.615 0.202 0.045 0.04  0.002 0.001 0.023
  0.    0.    0.001 0.073 0.    0.223 1.014 0.039 0.015 0.    0.067 0.327
  0.    0.001 0.024 0.017 0.069 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.348 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.877 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.68  1.878 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.83  0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.475 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.747 1.088 0.    0.    1.559 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.135 0.    0.649 0.    0.    0.    2.531
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.921 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.906 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    4.124 0.    0.   ]
 [0.    0.    4.324 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.594 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.878 0.    1.164 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.617 0.    0.    2.844]
 [0.    0.    0.    0.    1.229 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.95  0.    1.339 0.    0.    0.    0.
  0.    0.    0.508 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.8
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.719
  0.    3.171 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    2.567 0.    0.    0.    0.    0.    0.446 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    2.901 0.403 0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.536 0.    0.    1.128 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.001 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.837 0.    0.    1.7   0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    2.131 0.   ]
 [0.    0.    0.    1.338 1.124 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.752 0.    0.    0.32  0.    0.    0.
  0.    0.    0.    0.    1.579 1.626]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.06  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.615 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.169 0.    0.    0.    2.457 0.
  0.    0.    0.664 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.493 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.677 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    2.535 0.    0.    0.    0.
  0.    0.    0.    0.    0.    1.29  0.    0.    0.    0.    0.    1.542
  0.    0.    0.    0.    2.216 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    2.22  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.368 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.615 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.014 0.    0.    0.    0.    0.327
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.05, 'tpr': 0.95, 'fpr': 0.008, 'f1': 0.9500000000000001, 'shd': 5, 'npred': 60, 'ntrue': 60}
[6.668e-04 1.023e-01 4.492e-02 5.721e-02 2.080e-03 1.148e-01 2.444e-02
 5.756e-02 3.479e-01 4.376e-04 7.389e-02 1.379e-04 2.029e-03 3.216e-03
 1.215e-01 2.550e-04 4.602e-02 5.204e-02 8.767e-01 3.889e-02 2.428e-03
 1.506e-01 1.856e-02 4.566e-04 3.457e-02 3.506e-02 1.746e-02 2.182e-02
 2.602e-02 1.704e-04 2.219e-02 4.135e-03 1.254e-02 6.798e-01 1.878e+00
 9.233e-02 1.153e-01 3.493e-03 3.406e-04 3.378e-02 1.848e-03 5.373e-04
 2.503e-03 3.979e-02 4.674e-04 7.143e-02 2.976e-02 5.207e-02 8.251e-03
 2.529e-03 1.033e-01 5.149e-02 2.084e-03 1.267e-02 3.538e-02 8.154e-02
 8.508e-02 3.701e-02 7.412e-06 1.090e-03 7.512e-03 1.058e-01 3.531e-04
 6.873e-02 4.848e-02 1.458e-03 1.289e-05 2.956e-04 6.069e-05 2.622e-04
 3.523e-04 3.524e-06 4.904e-02 1.120e-05 3.102e-04 5.558e-02 1.222e-01
 7.034e-04 9.481e-04 1.070e-02 1.201e-02 1.560e-04 4.892e-05 8.318e-03
 1.360e-02 2.209e-02 5.890e-03 1.548e-04 7.006e-04 6.514e-03 2.210e-02
 2.524e-03 2.288e-02 4.910e-04 1.223e-03 9.763e-05 2.970e-06 1.390e-03
 2.206e-04 4.953e-05 3.345e-07 2.150e-02 1.803e-04 6.989e-05 5.084e-03
 6.022e-02 9.253e-04 1.196e-04 4.785e-03 1.419e-03 8.645e-04 1.666e-05
 1.550e-03 6.167e-05 5.974e-02 2.908e-04 1.952e-04 5.371e-04 2.657e-03
 5.309e-03 7.791e-04 1.427e-02 1.040e-02 2.308e-03 8.628e-05 6.893e-04
 4.540e-04 2.917e-05 7.104e-06 3.624e-05 1.566e-02 1.101e-04 7.005e-04
 9.059e-03 1.764e-01 1.034e-03 9.055e-05 1.071e-02 3.349e-03 2.506e-04
 1.365e-04 9.908e-04 1.074e-02 1.447e-02 1.053e-03 1.959e-04 2.900e-05
 4.990e-02 4.134e-02 1.729e-02 4.700e-02 4.791e-02 8.611e-02 1.434e-04
 8.747e-04 8.778e-02 4.318e-06 5.104e-04 3.313e-04 1.821e-03 8.700e-06
 4.552e-03 3.663e-03 8.298e-01 6.626e-04 2.247e-04 3.078e-02 1.052e-02
 4.248e-04 8.523e-04 6.528e-02 3.769e-03 3.412e-03 1.204e-03 2.749e-04
 6.501e-06 1.065e-02 2.620e-02 2.441e-02 1.078e-03 1.691e-02 6.453e-03
 1.649e-04 5.361e-04 6.042e-04 6.441e-06 5.179e-04 3.496e-04 3.638e-02
 3.612e-04 3.231e-03 9.025e-03 8.962e-02 4.347e-04 2.979e-04 1.039e-01
 4.186e-02 1.724e-06 3.684e-04 1.024e-02 9.176e-03 2.106e-02 2.194e-03
 6.857e-04 7.739e-04 9.222e-03 1.475e+00 9.476e-03 1.308e-02 3.049e-02
 3.167e-03 1.250e-04 3.763e-06 2.303e-03 1.701e-05 7.875e-05 6.396e-06
 2.067e-02 3.779e-04 3.801e-04 6.037e-03 7.067e-03 1.813e-03 2.078e-04
 9.428e-02 3.373e-03 4.298e-04 9.483e-05 7.652e-03 3.320e-04 4.474e-02
 1.745e-03 3.914e-04 6.725e-04 5.255e-02 2.244e-03 4.290e-02 5.317e-03
 4.035e-02 3.596e-02 1.048e-03 3.784e-06 5.853e-02 6.225e-06 4.049e-04
 1.069e-03 4.205e-02 4.102e-06 6.690e-02 7.468e-01 1.088e+00 2.567e-04
 6.113e-04 1.559e+00 5.783e-02 2.901e-04 2.247e-03 5.207e-02 1.909e-03
 5.478e-02 4.265e-03 3.000e-04 4.913e-04 1.228e-01 1.391e-01 2.926e-02
 1.135e+00 5.354e-02 6.490e-01 8.429e-02 2.404e-03 2.531e+00 3.322e-03
 2.015e-03 4.744e-04 1.208e-01 2.179e-03 1.132e-03 2.840e-02 1.003e-01
 7.666e-03 2.377e-03 1.656e-02 5.810e-03 4.121e-04 1.311e-02 9.213e-01
 2.184e-02 1.123e-02 2.806e-02 2.693e-03 2.930e-04 3.200e-02 1.852e-01
 3.346e-02 1.426e-02 9.012e-02 7.787e-02 9.056e-01 1.737e-02 3.557e-02
 1.395e-03 4.035e-04 3.070e-02 7.772e-02 2.023e-03 8.016e-02 6.508e-02
 1.015e-01 5.504e-02 1.426e-02 2.352e-02 6.476e-02 9.160e-04 3.722e-02
 7.211e-02 4.124e+00 4.194e-02 9.553e-03 4.831e-05 2.497e-05 4.324e+00
 9.311e-03 2.917e-02 6.622e-03 1.939e-02 5.472e-02 1.293e-02 3.921e-05
 4.570e-04 3.776e-04 3.497e-04 8.978e-06 7.361e-02 8.285e-04 1.523e-02
 3.830e-02 1.492e-01 1.827e-03 5.500e-04 2.526e-02 5.746e-02 7.946e-05
 2.793e-04 5.941e-01 2.357e-02 2.697e-02 6.994e-03 2.543e-03 5.468e-04
 2.882e-03 5.238e-02 4.715e-02 6.365e-03 1.878e+00 1.353e-02 1.164e+00
 1.971e-02 5.617e-04 5.057e-02 2.982e-04 2.078e-03 5.142e-02 3.825e-04
 8.873e-02 3.129e-03 1.417e-04 2.204e-02 2.317e-02 1.076e-01 2.874e-01
 1.149e-03 3.778e-02 6.174e-01 3.820e-02 2.087e-02 2.844e+00 5.425e-04
 5.507e-04 8.063e-02 1.391e-03 1.229e+00 4.592e-02 2.359e-02 1.353e-01
 6.183e-02 3.325e-03 5.413e-04 6.448e-02 1.280e-03 1.148e-03 3.150e-02
 6.082e-04 9.498e-01 2.145e-02 1.339e+00 1.949e-02 2.551e-03 2.703e-02
 2.830e-02 2.024e-03 5.257e-03 5.084e-01 6.757e-02 1.101e-02 2.627e-03
 7.159e-04 6.819e-04 3.131e-02 5.771e-02 5.965e-02 5.861e-02 3.808e-02
 1.108e-01 4.728e-02 2.149e-03 2.648e-04 1.800e+00 3.995e-04 4.916e-04
 1.129e-02 4.145e-04 2.042e-01 2.462e-02 2.419e-03 2.666e-02 1.164e-02
 1.043e-01 7.189e-01 4.841e-04 3.171e+00 3.202e-02 1.301e-02 1.213e-01
 4.895e-02 1.491e-04 1.390e-03 1.147e-02 3.630e-03 5.352e-02 1.741e-03
 3.953e-03 9.738e-03 1.154e-04 1.861e-05 1.879e-05 9.769e-05 8.755e-06
 3.736e-06 2.844e-06 5.125e-06 1.601e-04 2.824e-03 8.478e-03 4.369e-05
 7.088e-05 8.974e-04 1.191e-05 5.108e-06 4.504e-05 2.971e-04 5.848e-04
 2.274e-03 3.878e-04 4.980e-04 7.269e-04 2.567e+00 9.246e-02 8.833e-02
 1.973e-02 3.321e-02 1.850e-02 4.456e-01 5.539e-04 2.363e-03 6.454e-03
 4.313e-04 5.974e-04 4.732e-04 3.498e-02 1.055e-01 4.664e-03 3.394e-02
 2.901e+00 4.027e-01 1.040e-01 1.949e-02 2.198e-03 2.402e-03 8.487e-02
 5.110e-02 7.421e-02 5.307e-02 3.228e-04 4.698e-04 1.585e-02 5.896e-02
 1.536e+00 1.023e-01 1.841e-02 1.128e+00 2.296e-03 2.141e-03 5.378e-04
 8.799e-03 1.627e-04 6.640e-06 4.142e-06 6.314e-02 1.326e-04 4.929e-02
 1.091e-01 2.801e-03 4.156e-04 1.869e-02 3.822e-02 7.109e-04 5.091e-04
 1.001e+00 1.249e-02 2.143e-01 1.441e-03 5.790e-04 5.546e-04 7.552e-03
 2.114e-02 1.678e-02 5.721e-03 5.534e-03 2.733e-02 1.128e-03 2.742e-04
 7.260e-05 1.142e-03 2.437e-05 6.221e-04 3.754e-05 2.215e-02 6.742e-06
 1.826e-02 1.862e-03 2.126e-04 1.068e-04 1.650e-02 7.920e-04 1.875e-04
 1.907e-04 3.583e-02 7.405e-03 3.092e-02 5.674e-04 1.142e-05 1.186e-04
 4.411e-03 2.932e-03 8.662e-04 9.938e-04 7.704e-03 4.983e-04 7.463e-04
 4.678e-05 9.653e-05 5.193e-04 4.079e-06 1.316e-05 3.638e-05 1.593e-02
 3.690e-05 7.056e-04 6.005e-02 1.567e-04 1.854e-04 4.800e-02 1.117e-02
 4.018e-04 3.733e-04 2.581e-02 3.321e-03 9.313e-03 9.903e-04 2.775e-04
 4.275e-04 3.269e-02 1.565e-03 4.343e-03 8.369e-01 1.480e-02 3.324e-02
 1.700e+00 2.771e-04 5.172e-04 6.402e-02 7.743e-04 2.326e-03 4.194e-04
 1.352e-02 3.325e-06 8.408e-02 2.346e-02 8.230e-02 1.693e-03 5.733e-03
 2.966e-02 4.243e-04 1.729e-02 1.943e-02 1.332e-02 2.131e+00 2.619e-02
 3.728e-04 2.802e-04 3.964e-02 1.338e+00 1.124e+00 2.536e-02 5.043e-02
 1.179e-01 2.321e-01 1.196e-03 5.042e-04 3.202e-02 2.504e-03 2.458e-04
 2.311e-03 5.366e-02 5.832e-05 7.522e-01 7.550e-02 6.611e-02 3.199e-01
 8.736e-02 1.729e-01 6.051e-04 2.771e-02 8.842e-02 4.812e-02 1.579e+00
 1.626e+00 5.367e-04 3.200e-04 2.626e-03 1.058e-01 5.094e-02 2.166e-03
 1.283e-03 8.426e-03 2.925e-04 4.333e-04 4.860e-06 2.895e-03 6.531e-06
 3.847e-04 1.946e-06 1.060e+00 3.179e-06 1.875e-03 2.769e-02 1.633e-03
 5.961e-05 7.769e-05 2.591e-04 9.018e-06 7.204e-05 2.213e-03 5.635e-04
 5.993e-03 1.275e-04 5.228e-04 6.500e-04 8.642e-03 6.154e-01 3.020e-03
 2.229e-02 1.278e-02 9.908e-02 1.396e-02 6.940e-04 5.267e-04 6.483e-03
 3.763e-05 6.336e-04 8.682e-06 1.180e-01 6.503e-05 1.427e-02 1.169e+00
 2.429e-02 6.721e-03 7.505e-05 2.457e+00 3.830e-04 3.522e-04 6.643e-01
 5.391e-03 1.772e-01 6.392e-04 1.974e-03 3.049e-04 6.045e-02 7.579e-02
 1.671e-02 1.004e-01 1.493e+00 2.768e-02 6.532e-03 3.445e-02 2.572e-03
 1.700e-02 1.794e-03 4.281e-04 2.454e-03 1.103e-01 6.287e-04 6.239e-02
 6.650e-02 5.981e-02 3.951e-02 6.972e-02 1.677e+00 3.857e-02 3.421e-02
 2.866e-02 6.220e-02 9.805e-03 3.163e-02 9.483e-04 1.882e-03 4.767e-02
 6.221e-02 1.753e-01 3.837e-02 9.255e-03 2.535e+00 4.431e-02 5.548e-04
 7.667e-04 2.909e-01 5.309e-04 5.702e-04 1.376e-05 3.646e-02 4.032e-04
 1.290e+00 8.292e-02 3.652e-02 7.335e-03 5.722e-04 1.165e-01 1.542e+00
 3.181e-04 9.315e-02 3.009e-02 2.216e+00 5.764e-02 6.649e-05 1.839e-04
 5.874e-02 1.005e-02 1.742e-01 3.793e-03 2.789e-02 6.320e-02 1.105e-02
 1.013e-04 6.585e-04 1.199e-03 4.079e-05 4.568e-06 2.106e-05 2.220e+00
 3.502e-04 1.184e-03 2.906e-02 2.483e-02 1.984e-03 2.523e-04 9.622e-02
 1.663e-03 6.882e-04 8.421e-05 9.100e-03 4.965e-02 7.212e-04 3.707e-03
 3.590e-04 3.533e-02 8.979e-02 2.080e-03 3.427e-02 2.900e-02 1.368e+00
 1.466e-01 9.198e-04 1.098e-06 6.997e-03 5.964e-04 6.119e-04 4.646e-04
 2.053e-02 3.781e-04 2.980e-02 3.891e-02 5.366e-02 4.113e-02 5.818e-03
 5.525e-03 1.800e-02 5.318e-04 1.383e-02 1.447e-02 2.426e-02 8.325e-03
 6.419e-04 7.533e-04 3.558e-02 2.685e-03 4.945e-02 1.105e-01 1.654e-02
 5.781e-03 1.488e-02 4.593e-04 3.666e-04 2.304e-03 7.023e-04 2.483e-04
 1.669e-06 4.824e-02 1.753e-06 1.856e-03 1.645e-02 5.779e-02 1.175e-04
 1.021e-04 4.577e-02 1.653e-03 2.044e-04 1.566e-04 5.920e-03 4.308e-04
 2.607e-03 2.225e-03 3.798e-04 2.867e-02 7.810e-02 6.230e-02 6.148e-01
 2.017e-01 4.519e-02 3.962e-02 2.249e-03 5.056e-04 2.301e-02 4.063e-06
 3.333e-04 7.817e-04 7.308e-02 2.479e-04 2.227e-01 1.014e+00 3.926e-02
 1.458e-02 1.431e-04 6.708e-02 3.267e-01 4.574e-04 1.281e-03 2.438e-02
 1.711e-02 6.913e-02]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.98619341563786, 0.9825829893223818)
cuda
9630
cuda
Objective function 291.21 = squared loss an data 35.20 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 416 ; DAG False
||w||^2 0.009279350702977257
exp ma of ||w||^2 4.759611952343796
||w|| 0.09632938649746119
exp ma of ||w|| 0.12502374030377106
||w||^2 0.017382921235413556
exp ma of ||w||^2 0.020847145745166683
||w|| 0.13184430679939713
exp ma of ||w|| 0.14293738054546037
||w||^2 0.020742161153388554
exp ma of ||w||^2 0.020701905835437517
||w|| 0.1440213913048633
exp ma of ||w|| 0.14223224498369746
||w||^2 0.026228293423645358
exp ma of ||w||^2 0.021245038107334253
||w|| 0.16195151565714153
exp ma of ||w|| 0.1442410219790408
||w||^2 0.014609456563079723
exp ma of ||w||^2 0.022465481542911087
||w|| 0.12086958493798067
exp ma of ||w|| 0.14830783814860699
||w||^2 0.020162017144212226
exp ma of ||w||^2 0.025260889123418253
||w|| 0.14199301794177144
exp ma of ||w|| 0.15687687183315221
||w||^2 0.023327622824158657
exp ma of ||w||^2 0.027582582581564565
||w|| 0.15273382999243704
exp ma of ||w|| 0.1640471661937961
cuda
Objective function 31.54 = squared loss an data 29.81 + 0.5*rho*h**2 1.001321 + alpha*h 0.000000 + L2reg 0.26 + L1reg 0.47 ; SHD = 81 ; DAG True
Proportion of microbatches that were clipped  0.6960939980946332
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.4151476805125753
iteration 1 in outer loop, alpha = 1.4151476805125753, rho = 1.0, h = 1.4151476805125753
cuda
9630
cuda
Objective function 33.54 = squared loss an data 29.81 + 0.5*rho*h**2 1.001321 + alpha*h 2.002643 + L2reg 0.26 + L1reg 0.47 ; SHD = 81 ; DAG True
||w||^2 723.9502564738933
exp ma of ||w||^2 130933.63748177448
||w|| 26.906323726475406
exp ma of ||w|| 139.5108649209622
||w||^2 438.78292053474667
exp ma of ||w||^2 75810.46046410654
||w|| 20.947145880399713
exp ma of ||w|| 94.21696515150953
||w||^2 7.502146374224643
exp ma of ||w||^2 3197.4413234449485
||w|| 2.739004632019567
exp ma of ||w|| 11.889964630092583
||w||^2 0.026867836499812375
exp ma of ||w||^2 0.02146445113225249
||w|| 0.163914113180691
exp ma of ||w|| 0.14544566276954715
||w||^2 0.030653552768793467
exp ma of ||w||^2 0.022480709731427503
||w|| 0.1750815603334442
exp ma of ||w|| 0.14825697908717397
cuda
Objective function 32.81 = squared loss an data 29.45 + 0.5*rho*h**2 0.814037 + alpha*h 1.805672 + L2reg 0.28 + L1reg 0.46 ; SHD = 92 ; DAG False
Proportion of microbatches that were clipped  0.692682135323706
iteration 1 in inner loop, alpha 1.4151476805125753 rho 1.0 h 1.275960283917744
9630
cuda
Objective function 40.13 = squared loss an data 29.45 + 0.5*rho*h**2 8.140373 + alpha*h 1.805672 + L2reg 0.28 + L1reg 0.46 ; SHD = 92 ; DAG False
||w||^2 0.021129072299958973
exp ma of ||w||^2 0.021162224938170975
||w|| 0.14535842700015356
exp ma of ||w|| 0.1443010352497945
||w||^2 0.018005830040075884
exp ma of ||w||^2 0.02396241601932403
||w|| 0.13418580416748965
exp ma of ||w|| 0.15267789584825395
||w||^2 0.020391235172496087
exp ma of ||w||^2 0.02717887098622587
||w|| 0.1427978822409355
exp ma of ||w|| 0.16248193002145814
cuda
Objective function 31.53 = squared loss an data 29.57 + 0.5*rho*h**2 0.805362 + alpha*h 0.567953 + L2reg 0.22 + L1reg 0.37 ; SHD = 61 ; DAG True
Proportion of microbatches that were clipped  0.7015739442106903
iteration 2 in inner loop, alpha 1.4151476805125753 rho 10.0 h 0.4013382400852237
9630
cuda
Objective function 38.77 = squared loss an data 29.57 + 0.5*rho*h**2 8.053619 + alpha*h 0.567953 + L2reg 0.22 + L1reg 0.37 ; SHD = 61 ; DAG True
||w||^2 8501236339.562512
exp ma of ||w||^2 3145534731.618228
||w|| 92202.14932181631
exp ma of ||w|| 33144.93715815417
||w||^2 9677544491.322542
exp ma of ||w||^2 3903860147.162722
||w|| 98374.51139051488
exp ma of ||w|| 41151.86870910467
||w||^2 48853039.656476684
exp ma of ||w||^2 337975587.2267901
||w|| 6989.494950028698
exp ma of ||w|| 14342.846326068699
||w||^2 3995.1535373364372
exp ma of ||w||^2 207262.81411241222
||w|| 63.207226939143894
exp ma of ||w|| 193.47005187841367
||w||^2 13.615009183532868
exp ma of ||w||^2 5032.377626372805
||w|| 3.6898521899302237
exp ma of ||w|| 13.685279643437514
||w||^2 0.055212949045313146
exp ma of ||w||^2 1.7326968247360917
||w|| 0.23497435827194665
exp ma of ||w|| 0.2667199876527545
||w||^2 0.03143663072997148
exp ma of ||w||^2 0.30708393783750587
||w|| 0.17730378092407245
exp ma of ||w|| 0.2180283025917887
||w||^2 0.033098050085637036
exp ma of ||w||^2 0.09177865760975258
||w|| 0.18192869505835807
exp ma of ||w|| 0.19462849037738095
||w||^2 0.019617843509252324
exp ma of ||w||^2 0.021783746384571503
||w|| 0.14006371232140152
exp ma of ||w|| 0.14584170637350433
cuda
Objective function 31.01 = squared loss an data 29.75 + 0.5*rho*h**2 0.627687 + alpha*h 0.158558 + L2reg 0.19 + L1reg 0.28 ; SHD = 62 ; DAG True
Proportion of microbatches that were clipped  0.6995143349522168
iteration 3 in inner loop, alpha 1.4151476805125753 rho 100.0 h 0.11204346580841928
iteration 2 in outer loop, alpha = 12.619494261354504, rho = 100.0, h = 0.11204346580841928
cuda
9630
cuda
Objective function 32.26 = squared loss an data 29.75 + 0.5*rho*h**2 0.627687 + alpha*h 1.413932 + L2reg 0.19 + L1reg 0.28 ; SHD = 62 ; DAG True
||w||^2 22737722.118866146
exp ma of ||w||^2 132833695.34862463
||w|| 4768.408761721896
exp ma of ||w|| 9176.063172776754
||w||^2 332.8461620746408
exp ma of ||w||^2 40120.76931578061
||w|| 18.244071970770143
exp ma of ||w|| 52.414516483078
||w||^2 0.044795969308854565
exp ma of ||w||^2 2.7864334467642733
||w|| 0.21165058305814932
exp ma of ||w|| 0.30126576122320514
||w||^2 0.04952308857314964
exp ma of ||w||^2 2.095879097235076
||w|| 0.2225378362731822
exp ma of ||w|| 0.2842634012452543
||w||^2 0.05434793015274373
exp ma of ||w||^2 0.7560849557171115
||w|| 0.23312642525621957
exp ma of ||w|| 0.2463280932426861
||w||^2 0.04760063138739516
exp ma of ||w||^2 0.10817528891369099
||w|| 0.21817568926760644
exp ma of ||w|| 0.21185461160852237
||w||^2 0.04024954257768048
exp ma of ||w||^2 0.040843212149685275
||w|| 0.2006228864752984
exp ma of ||w|| 0.1986273597023146
||w||^2 0.015808388961547598
exp ma of ||w||^2 0.024169404715691765
||w|| 0.12573141596891207
exp ma of ||w|| 0.15419177416224378
||w||^2 0.014026333334099193
exp ma of ||w||^2 0.026605857178609332
||w|| 0.11843282203046246
exp ma of ||w|| 0.16094514979228838
cuda
Objective function 31.49 = squared loss an data 29.67 + 0.5*rho*h**2 0.327369 + alpha*h 1.021118 + L2reg 0.20 + L1reg 0.27 ; SHD = 59 ; DAG True
Proportion of microbatches that were clipped  0.6894420042297056
iteration 1 in inner loop, alpha 12.619494261354504 rho 100.0 h 0.08091591552129529
9630
cuda
Objective function 34.44 = squared loss an data 29.67 + 0.5*rho*h**2 3.273693 + alpha*h 1.021118 + L2reg 0.20 + L1reg 0.27 ; SHD = 59 ; DAG True
||w||^2 649.4402561362964
exp ma of ||w||^2 98588.28282595667
||w|| 25.484117723325177
exp ma of ||w|| 91.04174683763236
||w||^2 0.6523328589278639
exp ma of ||w||^2 170.21746414935484
||w|| 0.8076712567176474
exp ma of ||w|| 1.586352368436335
||w||^2 0.08407622706263677
exp ma of ||w||^2 0.1064408571183205
||w|| 0.28995900927999596
exp ma of ||w|| 0.3020617061425646
v before min max tensor([[ 133.463,  -17.334,    3.203,  ...,   -6.429,   -1.733,   23.061],
        [ 553.174,    5.553,  -19.516,  ...,   55.847,    3.491,  328.897],
        [ 182.083,  -14.197,  -11.525,  ...,  -13.872,   48.679,   18.297],
        ...,
        [  20.162,  -14.939,  -12.578,  ...,   13.775,   36.889, 1487.628],
        [  -4.259,  -17.194,   49.667,  ...,   27.009,  -20.906,  -17.385],
        [ -21.801,   -3.924,  -10.833,  ...,  128.355,   76.141,   -3.666]],
       device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 3.203e+00,  ..., 1.000e-12, 1.000e-12,
         1.000e+01],
        [1.000e+01, 5.553e+00, 1.000e-12,  ..., 1.000e+01, 3.491e+00,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        ...,
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e-12]], device='cuda:0')
v before min max tensor([-8.001e+00, -1.058e+01,  1.026e+02, -7.918e+00, -1.785e+01, -8.457e+00,
         3.157e+01,  1.610e+00,  4.340e+01, -1.022e+01,  1.407e+02, -1.963e+01,
        -1.774e+01,  7.354e-01,  4.730e+00,  1.571e+02, -1.865e+01, -1.687e+01,
         1.346e+01, -1.789e+01, -4.235e+00, -1.009e+01, -1.957e+01,  2.717e+00,
        -2.040e+01, -1.617e+01,  1.192e+01,  3.806e+00, -4.447e+00, -9.343e+00,
         1.132e+01,  8.875e+00, -1.430e+01, -2.883e+00, -1.758e+01,  5.240e+01,
         9.152e+01,  2.102e+01, -1.538e+01,  2.009e+01,  7.360e-01, -1.864e+01,
         5.311e+00,  3.175e-01,  3.500e+00, -1.517e+01,  1.329e+00, -1.555e+01,
         3.588e+02, -6.917e+00,  7.293e+01, -2.145e+01, -1.203e+01, -1.123e+01,
         6.641e+01,  2.697e+00,  3.695e+01, -4.614e+00, -8.615e+00, -1.694e+01,
        -1.326e+01, -3.690e+00,  1.600e+02,  2.520e+01, -8.503e+00,  3.742e-01,
         1.471e+01,  1.209e+01,  3.443e+01,  3.211e+01, -1.105e+01, -1.258e+01,
        -6.318e+00,  2.297e+01, -2.017e+00,  5.299e+01,  1.068e+01, -1.032e+01,
        -2.133e+01, -1.169e+01,  8.935e+00, -1.571e+01, -1.036e+01, -1.455e+01,
        -4.634e+00, -1.730e+01, -1.609e+01, -1.737e+01,  3.682e+01, -5.265e+00,
        -2.020e+01,  1.106e+02,  1.594e+01, -1.603e+01,  8.204e+00, -1.999e+01,
        -2.354e+01,  7.153e+00,  3.974e+01, -1.674e+01,  1.285e+02,  1.141e+02,
         4.269e+01, -1.156e+01,  1.951e+01,  1.233e+02, -2.326e+00, -1.623e+01,
         5.640e+00,  2.680e+01,  2.670e+01, -1.052e+01, -2.045e+01, -1.642e+01,
         6.778e+01, -2.044e+01,  5.963e+01,  1.792e+01,  3.881e+01,  2.798e+01,
        -8.513e+00, -1.879e+01,  4.743e+00,  1.779e+01, -8.265e+00, -1.846e+01,
         5.836e+00, -1.281e+01,  7.728e+00,  9.125e+00, -9.777e+00, -1.688e+01,
        -1.710e+01,  3.741e+01, -1.846e+01, -1.387e+01,  8.606e+01, -2.559e+01,
         8.959e+01, -5.135e-03, -1.160e+01, -1.866e+01,  2.163e+01,  3.424e+01,
        -1.196e+01, -9.966e+00, -2.180e+01, -1.391e+00,  5.261e+01, -6.331e+00,
        -1.753e+01,  1.553e+01,  9.431e+00,  1.666e+02,  1.824e+00,  1.696e+01,
        -6.239e+00, -1.726e+01, -2.012e+01,  9.627e+00,  1.262e+01,  3.052e+00,
        -1.444e+01, -1.743e+01, -7.066e+00,  2.887e+01, -3.271e+00,  1.109e+01,
        -1.442e+01,  2.257e+01, -1.539e+01,  5.079e+01, -1.650e+01, -4.143e+00,
         7.852e+01,  5.517e+01, -9.964e+00, -8.672e+00,  9.644e+00, -1.384e+01,
        -1.285e+01, -1.608e+01,  3.193e+01,  9.605e+01,  1.035e+02, -1.792e+01,
        -2.353e+01,  3.829e+01, -6.016e+00,  1.619e+01, -1.390e+01, -1.162e+01,
        -5.602e+00, -1.413e+01,  3.807e+01,  2.144e+01,  7.038e+01, -1.693e+00,
         6.696e+00, -1.665e+01, -1.168e+01, -1.465e+00, -1.719e+01, -1.479e+01,
        -1.003e+01,  5.222e+01, -2.005e+01, -2.013e+01, -1.049e+01, -1.071e+01,
         1.568e+01, -2.290e+01, -2.024e+01,  4.678e+01, -1.348e+00,  1.774e+02,
        -1.441e+01,  3.261e+01, -1.108e+01, -4.753e+00,  8.472e+00, -1.234e+01,
        -1.499e+01, -2.415e+01, -1.483e+01, -1.505e+01,  1.020e+02,  7.519e+01,
        -1.479e+01, -1.860e+01,  6.805e+01, -1.510e+01,  6.267e+00, -7.971e+00,
        -2.061e+01,  1.931e+01, -9.842e-01,  6.194e+00, -1.608e+01, -1.470e+01,
        -1.266e+00, -1.958e+01,  9.247e+01, -1.804e+01, -2.044e+01, -1.409e+01,
         3.975e+01, -1.340e+01, -8.201e-01, -1.065e+01, -7.291e+00,  1.329e-01,
         2.417e+01,  2.373e+01, -2.209e+00,  2.391e+00,  1.878e+01, -1.244e+01,
        -1.507e+01,  3.204e+01, -1.472e+01,  6.222e+00, -2.017e+01, -1.254e+01,
        -1.484e+01, -5.488e+00, -2.268e+01, -2.010e+01, -1.275e+01, -1.982e+01,
         2.960e+02,  2.474e+00, -2.221e+01,  3.398e+01, -6.455e+00, -4.432e+00,
        -1.947e+01, -1.480e+00,  8.486e+00,  1.942e+00, -6.110e+00,  8.067e+00,
        -8.864e+00, -8.492e+00, -7.719e-01,  3.341e+01,  3.945e+01,  1.037e+02,
        -2.089e+01, -6.191e+00, -2.338e+01, -1.664e+01, -1.379e+01, -2.213e+01,
        -1.436e+01,  3.679e+01,  3.802e+01, -7.753e+00,  6.422e+01, -4.413e+00],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.610e+00, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 7.354e-01, 4.730e+00, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.717e+00,
        1.000e-12, 1.000e-12, 1.000e+01, 3.806e+00, 1.000e-12, 1.000e-12,
        1.000e+01, 8.875e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 7.360e-01, 1.000e-12,
        5.311e+00, 3.175e-01, 3.500e+00, 1.000e-12, 1.329e+00, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 2.697e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 3.742e-01,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 8.935e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 8.204e+00, 1.000e-12,
        1.000e-12, 7.153e+00, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        5.640e+00, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 4.743e+00, 1.000e+01, 1.000e-12, 1.000e-12,
        5.836e+00, 1.000e-12, 7.728e+00, 9.125e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 9.431e+00, 1.000e+01, 1.824e+00, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 9.627e+00, 1.000e+01, 3.052e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 9.644e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        6.696e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 8.472e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 6.267e+00, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 6.194e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.329e-01,
        1.000e+01, 1.000e+01, 1.000e-12, 2.391e+00, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 6.222e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 2.474e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 8.486e+00, 1.942e+00, 1.000e-12, 8.067e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 6.690e+01],
         [-1.129e+01],
         [ 7.523e+01],
         [-7.986e+00],
         [ 2.836e+00],
         [ 5.179e+01],
         [-2.181e+01],
         [ 2.540e+01],
         [-1.536e+01],
         [ 2.451e+01]],

        [[-1.596e+01],
         [-2.037e+01],
         [-8.794e+00],
         [-1.649e+01],
         [-1.770e+00],
         [ 2.898e+00],
         [ 1.990e+01],
         [-1.012e+01],
         [ 3.892e+01],
         [ 5.914e+01]],

        [[-7.949e+00],
         [-1.427e+01],
         [-1.428e+01],
         [-1.346e+01],
         [-1.793e+01],
         [-3.855e+00],
         [ 1.731e+01],
         [-1.235e+01],
         [ 1.118e+01],
         [-4.958e+00]],

        [[-1.934e+01],
         [ 2.384e+01],
         [ 2.972e+01],
         [ 7.241e+01],
         [ 7.216e+00],
         [ 1.472e+02],
         [-1.136e+01],
         [-1.801e+01],
         [ 4.824e+01],
         [ 3.807e+01]],

        [[-2.627e-01],
         [ 1.730e+01],
         [ 2.047e+01],
         [ 4.734e+01],
         [ 2.865e+01],
         [-2.599e+01],
         [-1.710e+01],
         [-2.172e+01],
         [-1.555e+01],
         [-1.348e+01]],

        [[ 2.071e+01],
         [-1.666e+01],
         [-1.771e+00],
         [-1.431e+01],
         [-1.454e+01],
         [-1.760e+00],
         [ 2.123e+01],
         [-1.656e+01],
         [-1.383e+01],
         [ 1.709e+02]],

        [[ 9.352e+01],
         [-1.497e+01],
         [-2.096e+01],
         [ 3.596e+01],
         [ 6.436e+00],
         [-1.279e+01],
         [-1.835e+00],
         [-1.567e+01],
         [-1.619e+01],
         [ 2.705e+01]],

        [[-8.674e+00],
         [-1.834e+01],
         [ 6.519e+01],
         [-1.102e+01],
         [ 9.272e+00],
         [ 5.459e+01],
         [-3.467e+00],
         [-9.857e+00],
         [-7.768e+00],
         [ 3.639e+01]],

        [[-6.698e+00],
         [-2.060e+01],
         [-5.960e+00],
         [-1.572e+01],
         [-1.652e+01],
         [-1.645e+01],
         [ 3.666e+01],
         [ 2.117e+01],
         [ 1.202e+02],
         [ 2.428e+01]],

        [[-2.375e+01],
         [-5.449e+00],
         [ 2.424e+01],
         [-1.827e+01],
         [-1.111e+01],
         [-5.246e+00],
         [ 2.309e+00],
         [-1.204e+01],
         [-8.158e+00],
         [ 7.313e+01]],

        [[-1.927e+01],
         [-1.462e+01],
         [-1.270e+01],
         [-1.154e+01],
         [-4.575e-02],
         [ 1.552e+01],
         [ 1.179e-01],
         [ 5.970e+00],
         [-7.430e+00],
         [ 3.355e+01]],

        [[-3.869e+00],
         [-1.779e+01],
         [-1.219e+01],
         [ 2.831e+01],
         [ 1.126e-01],
         [-1.118e+01],
         [ 2.708e+01],
         [ 2.209e+00],
         [-1.519e+01],
         [-1.860e+01]],

        [[ 9.311e+00],
         [-9.494e+00],
         [ 1.439e+01],
         [ 7.022e+00],
         [-1.226e+01],
         [ 9.524e+01],
         [-7.820e-01],
         [ 1.262e+01],
         [-1.239e+01],
         [ 1.724e+01]],

        [[-2.012e+01],
         [ 9.962e+01],
         [-1.342e+01],
         [-1.488e+01],
         [-1.051e+01],
         [-1.453e+01],
         [-1.969e+01],
         [-1.227e+01],
         [-2.195e+01],
         [-9.705e+00]],

        [[-2.227e+01],
         [-1.009e+01],
         [ 6.249e+01],
         [-7.328e+00],
         [-5.155e+00],
         [-1.178e+01],
         [ 1.727e+00],
         [-1.517e+01],
         [ 2.780e+01],
         [ 1.235e+01]],

        [[-2.020e+01],
         [-2.056e+01],
         [-2.694e+01],
         [ 3.895e+01],
         [-1.173e+01],
         [-1.475e+01],
         [-1.210e+01],
         [ 7.199e+00],
         [-8.976e+00],
         [-8.832e+00]],

        [[-1.622e+01],
         [-2.110e+01],
         [-2.228e+00],
         [ 2.954e+00],
         [-2.000e+01],
         [-1.120e+01],
         [ 9.799e+00],
         [-9.580e+00],
         [-1.865e+00],
         [-9.621e+00]],

        [[-1.668e+01],
         [-1.363e+01],
         [-9.271e+00],
         [-1.794e+01],
         [ 1.621e+02],
         [-1.253e+01],
         [-1.047e+00],
         [-6.316e+00],
         [ 3.640e+01],
         [-7.012e+00]],

        [[ 2.175e+01],
         [-1.935e+01],
         [ 7.991e+00],
         [ 3.767e+01],
         [-1.506e+01],
         [-1.299e+01],
         [ 6.791e+00],
         [-1.318e+01],
         [ 7.344e+01],
         [ 7.452e+01]],

        [[ 4.758e+01],
         [-1.632e+01],
         [-1.970e+01],
         [-1.475e+01],
         [ 6.527e+01],
         [-2.469e+01],
         [ 1.215e+01],
         [-1.311e+01],
         [ 2.849e+01],
         [-9.937e+00]],

        [[-1.042e+01],
         [-1.092e+01],
         [ 1.015e+01],
         [-7.415e+00],
         [ 3.021e+01],
         [-1.405e+01],
         [ 3.747e+01],
         [-2.215e+01],
         [-2.015e+01],
         [ 6.432e+00]],

        [[-1.057e+01],
         [-5.192e+00],
         [ 2.709e+01],
         [ 2.452e+01],
         [-1.559e+01],
         [-1.206e+01],
         [-1.755e+01],
         [-4.364e+00],
         [-1.787e+01],
         [-2.003e+01]],

        [[-1.033e+01],
         [-1.443e+01],
         [ 2.450e+01],
         [-1.865e+01],
         [-1.187e+01],
         [-1.660e+01],
         [ 3.256e+00],
         [-4.663e-01],
         [ 8.703e-01],
         [-2.278e+01]],

        [[-8.549e+00],
         [-1.972e+01],
         [-1.604e+01],
         [-1.163e+00],
         [-1.905e+01],
         [-1.766e+01],
         [-1.684e+01],
         [-1.800e+01],
         [-1.709e+01],
         [ 8.678e+01]],

        [[-1.229e+01],
         [-9.256e+00],
         [-1.757e+01],
         [ 4.433e+01],
         [-1.906e+01],
         [-4.104e+00],
         [-6.470e+00],
         [-1.534e+01],
         [ 3.011e+01],
         [ 2.274e+01]],

        [[-1.808e+01],
         [-1.016e+01],
         [ 3.005e+02],
         [-2.223e+01],
         [-1.076e+01],
         [ 1.701e+00],
         [-2.609e+01],
         [-1.743e+01],
         [ 7.162e-01],
         [-9.567e+00]],

        [[ 3.473e+01],
         [ 9.995e+00],
         [ 6.836e+01],
         [-1.602e+01],
         [ 5.315e+01],
         [-1.811e+01],
         [ 2.736e+01],
         [-1.207e+01],
         [-1.870e+01],
         [-4.429e+00]],

        [[-7.765e+00],
         [ 3.206e+01],
         [-8.712e+00],
         [ 3.130e+00],
         [ 1.260e+02],
         [-1.625e+01],
         [-3.833e+00],
         [-4.386e+00],
         [-1.640e+01],
         [ 4.434e+01]],

        [[-1.374e+01],
         [-6.523e+00],
         [-1.477e+01],
         [ 1.243e+01],
         [-1.546e+01],
         [ 1.769e+00],
         [ 4.773e+00],
         [-9.238e+00],
         [-1.561e+01],
         [ 8.489e+00]],

        [[ 1.494e+01],
         [-1.201e+01],
         [ 6.015e+00],
         [-1.995e+01],
         [-1.532e+01],
         [-1.419e+01],
         [-1.369e+01],
         [-2.920e+00],
         [-2.982e+00],
         [-2.058e+01]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [2.836e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.898e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [7.216e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [6.436e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [9.272e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.309e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.179e-01],
         [5.970e+00],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.126e-01],
         [1.000e-12],
         [1.000e+01],
         [2.209e+00],
         [1.000e-12],
         [1.000e-12]],

        [[9.311e+00],
         [1.000e-12],
         [1.000e+01],
         [7.022e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.727e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.199e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.954e+00],
         [1.000e-12],
         [1.000e-12],
         [9.799e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [7.991e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [6.791e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [6.432e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.256e+00],
         [1.000e-12],
         [8.703e-01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.701e+00],
         [1.000e-12],
         [1.000e-12],
         [7.162e-01],
         [1.000e-12]],

        [[1.000e+01],
         [9.995e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [3.130e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.769e+00],
         [4.773e+00],
         [1.000e-12],
         [1.000e-12],
         [8.489e+00]],

        [[1.000e+01],
         [1.000e-12],
         [6.015e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-13.108],
        [-18.635],
        [-16.451],
        [-20.316],
        [-13.607],
        [-16.858],
        [  5.561],
        [ -9.840],
        [ 17.199],
        [  5.616],
        [-12.422],
        [ -9.581],
        [-10.403],
        [ -1.744],
        [ -6.528],
        [ 13.077],
        [ 34.936],
        [-13.269],
        [ -3.471],
        [ -6.445],
        [ -4.387],
        [-13.047],
        [-17.713],
        [-18.898],
        [ 36.131],
        [ -4.949],
        [113.319],
        [-10.437],
        [  2.247],
        [-22.866]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [5.561e+00],
        [1.000e-12],
        [1.000e+01],
        [5.616e+00],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [2.247e+00],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.125,  0.036, -0.009,  ..., -0.057, -0.052,  0.049],
        [ 0.049,  0.002,  0.032,  ...,  0.010, -0.041, -0.052],
        [ 0.025,  0.034,  0.010,  ..., -0.015, -0.043,  0.014],
        ...,
        [ 0.029,  0.056, -0.048,  ..., -0.040, -0.033,  0.096],
        [-0.057, -0.046,  0.064,  ..., -0.014,  0.072, -0.049],
        [-0.019, -0.054,  0.022,  ..., -0.011, -0.013, -0.025]],
       device='cuda:0')
s after update for 1 param tensor([[2.742, 1.670, 1.774,  ..., 2.044, 1.932, 2.118],
        [2.382, 1.965, 1.752,  ..., 1.756, 1.796, 2.473],
        [2.586, 1.394, 2.066,  ..., 1.199, 2.138, 1.956],
        ...,
        [2.160, 1.356, 1.539,  ..., 1.886, 1.909, 2.735],
        [2.380, 1.712, 2.203,  ..., 2.096, 1.814, 2.346],
        [2.029, 2.164, 2.118,  ..., 2.044, 1.542, 1.816]], device='cuda:0')
b after update for 1 param tensor([[218.991, 170.918, 176.151,  ..., 189.071, 183.847, 192.477],
        [204.123, 185.385, 175.070,  ..., 175.249, 177.245, 207.970],
        [212.662, 156.134, 190.088,  ..., 144.799, 193.387, 184.961],
        ...,
        [194.350, 154.015, 164.061,  ..., 181.635, 182.716, 218.701],
        [204.046, 173.035, 196.292,  ..., 191.461, 178.100, 202.562],
        [188.369, 194.556, 192.467,  ..., 189.086, 164.248, 178.199]],
       device='cuda:0')
clipping threshold 0.2514434430194545
a after update for 1 param tensor([ 1.939e-02, -1.323e-01, -1.240e-02, -4.239e-03,  5.988e-03,  6.211e-03,
         2.841e-02,  2.077e-02, -2.426e-02,  1.651e-02, -5.565e-02, -5.946e-02,
         5.111e-02,  2.824e-02,  2.478e-02,  6.277e-03, -7.179e-02, -1.208e-01,
         6.351e-02,  4.295e-02,  2.415e-02,  1.435e-02, -2.680e-02,  1.537e-02,
        -8.693e-02, -1.484e-02,  5.352e-03, -1.462e-02,  7.170e-03, -7.823e-02,
         2.959e-02, -2.829e-02,  3.858e-02, -4.160e-02,  6.878e-02, -2.460e-02,
         9.468e-02,  1.107e-02,  7.105e-03,  7.466e-02, -7.821e-02,  4.424e-02,
        -3.016e-02, -5.976e-02, -3.091e-02,  3.523e-02,  6.213e-02, -3.320e-02,
        -2.041e-02, -2.473e-02,  3.600e-02, -2.726e-02, -1.139e-01, -2.486e-02,
         3.612e-03, -2.918e-02, -8.590e-02, -7.386e-03, -3.326e-02,  4.006e-02,
        -1.535e-02, -1.102e-01, -1.110e-01,  8.030e-02,  9.941e-03, -1.998e-02,
         3.129e-02,  1.299e-01,  1.718e-03, -7.128e-02,  9.915e-03,  1.459e-02,
         1.104e-02, -1.334e-01, -1.405e-01,  3.826e-02,  3.862e-02, -2.543e-02,
        -1.595e-01, -8.722e-02,  9.495e-03,  3.314e-02,  1.905e-03,  6.042e-02,
         1.378e-02,  1.560e-02, -1.046e-01,  4.566e-02, -2.680e-02,  2.528e-02,
        -2.650e-02,  4.736e-02,  1.009e-02, -1.154e-02, -4.895e-03, -4.959e-02,
         1.254e-01, -9.518e-02, -1.234e-01,  2.161e-02,  1.300e-02, -7.235e-02,
        -1.841e-04,  4.572e-02,  3.390e-02,  1.393e-01,  1.908e-02,  6.610e-03,
        -5.380e-02,  1.076e-02,  5.275e-02,  1.340e-01,  6.954e-02, -2.041e-02,
        -2.162e-02, -9.148e-02,  9.500e-03,  1.361e-02, -2.108e-01,  1.221e-02,
        -5.287e-03, -6.497e-02,  1.266e-02, -5.123e-02, -6.756e-02,  5.360e-02,
        -1.394e-02,  6.064e-02,  4.932e-02,  5.322e-02,  1.448e-02,  2.792e-02,
         3.325e-02, -9.082e-02, -4.650e-02,  3.702e-02, -1.845e-02,  8.192e-02,
         3.664e-02,  6.125e-02,  4.187e-03, -1.052e-02, -5.183e-03,  1.365e-02,
        -2.459e-02, -5.287e-02, -2.922e-02, -2.382e-03,  3.363e-02,  6.575e-02,
         8.319e-03,  3.457e-02, -4.539e-02, -2.550e-02,  3.020e-02, -4.677e-03,
         9.069e-02,  1.221e-02,  4.246e-02, -1.837e-02,  2.949e-02, -9.838e-03,
         5.408e-02,  1.717e-01, -3.306e-02, -4.547e-03,  1.465e-02, -1.210e-02,
        -6.343e-02,  2.247e-03, -9.890e-03,  6.646e-02, -1.019e-01,  1.982e-01,
        -4.106e-02,  1.593e-04,  1.909e-02,  3.993e-02, -9.129e-02,  3.932e-02,
        -5.882e-02, -1.228e-02, -3.089e-02, -1.779e-03,  4.880e-02, -1.272e-01,
        -5.709e-02, -3.844e-02,  1.069e-02, -1.182e-02,  8.882e-02, -7.078e-02,
        -3.062e-02,  6.564e-03,  3.070e-02, -8.526e-02, -1.051e-01,  4.989e-02,
         4.105e-03,  8.091e-03, -2.450e-02, -2.482e-03, -2.351e-02, -1.360e-02,
        -2.632e-02, -1.220e-02, -4.247e-02,  4.531e-02,  1.156e-01,  4.941e-02,
        -2.224e-04, -1.611e-01,  2.394e-02, -4.102e-02,  1.892e-03, -3.791e-02,
         6.133e-02,  5.153e-02, -7.091e-02, -1.350e-02,  8.622e-03,  7.787e-03,
        -2.619e-02, -4.947e-02, -1.027e-01,  5.812e-02, -1.837e-02, -1.583e-01,
         1.412e-01, -1.328e-02, -1.441e-01, -3.300e-03, -3.340e-02, -8.436e-03,
        -1.496e-02, -2.074e-02, -3.188e-02,  1.240e-01, -7.016e-02, -8.618e-02,
        -2.602e-02, -4.339e-02,  5.554e-03,  2.698e-02,  4.985e-02, -2.416e-02,
        -9.586e-02, -1.482e-01, -1.216e-01, -1.394e-02, -1.843e-02, -9.712e-02,
        -2.880e-02,  3.386e-03, -9.115e-02,  9.292e-03,  5.329e-02, -5.269e-02,
         3.744e-02,  1.042e-02, -7.435e-02, -3.015e-02, -4.094e-02, -1.583e-03,
        -9.173e-02,  1.785e-02,  8.393e-03,  8.362e-02,  9.398e-03, -1.149e-01,
        -4.342e-02, -5.382e-02,  6.750e-02,  5.192e-02,  2.606e-02,  6.086e-03,
         6.970e-02,  1.833e-02,  1.441e-02,  9.441e-02,  6.452e-02, -1.557e-01,
         1.416e-02, -2.015e-02, -8.598e-02, -4.204e-02,  3.801e-02,  2.236e-02,
        -3.959e-02, -2.977e-02,  1.066e-02,  4.170e-02, -3.658e-02, -1.466e-01,
        -1.034e-01, -8.959e-02, -1.008e-02, -5.529e-02, -1.633e-01, -2.540e-02],
       device='cuda:0')
s after update for 1 param tensor([1.717, 2.143, 2.261, 1.815, 1.587, 1.267, 1.771, 1.891, 2.178, 1.890,
        1.958, 1.749, 2.140, 1.040, 1.920, 1.761, 1.724, 1.756, 2.096, 2.030,
        1.821, 0.916, 1.719, 1.556, 1.902, 1.398, 2.033, 1.762, 1.262, 1.511,
        2.251, 1.682, 1.504, 1.939, 1.555, 2.057, 1.802, 1.261, 1.630, 2.204,
        1.439, 1.871, 1.815, 1.982, 0.955, 1.433, 1.899, 1.829, 2.259, 1.146,
        2.190, 1.854, 2.045, 1.271, 2.062, 1.974, 1.927, 1.738, 2.062, 1.512,
        1.654, 1.795, 2.155, 1.950, 1.529, 1.480, 2.185, 2.202, 2.035, 1.506,
        1.090, 1.180, 1.177, 2.139, 1.531, 2.399, 2.322, 1.712, 1.843, 1.516,
        1.993, 1.366, 0.993, 1.286, 1.338, 1.661, 1.590, 1.540, 2.193, 1.629,
        1.761, 2.066, 2.371, 1.482, 1.801, 1.789, 2.040, 1.704, 2.264, 1.862,
        1.912, 2.101, 1.883, 0.999, 1.724, 2.061, 1.157, 1.414, 1.573, 1.352,
        1.423, 1.821, 1.882, 1.423, 2.213, 1.769, 1.844, 1.924, 2.083, 1.316,
        1.952, 1.791, 1.360, 1.599, 0.902, 1.962, 1.513, 1.890, 1.973, 1.947,
        1.215, 1.468, 1.593, 1.832, 1.609, 1.201, 1.901, 2.220, 1.643, 1.543,
        1.269, 1.615, 1.905, 2.013, 1.852, 1.842, 1.890, 0.519, 1.547, 1.137,
        1.522, 1.901, 1.665, 2.216, 1.665, 2.055, 1.166, 1.492, 1.858, 1.779,
        1.394, 1.756, 1.809, 1.554, 1.660, 1.785, 1.417, 1.407, 1.616, 2.072,
        1.361, 2.038, 1.834, 1.707, 1.546, 1.827, 1.530, 1.539, 2.108, 1.216,
        1.268, 1.873, 1.926, 1.783, 1.661, 1.608, 2.087, 1.854, 0.991, 1.924,
        2.046, 1.671, 1.687, 1.404, 1.958, 1.723, 1.418, 1.934, 1.448, 1.476,
        1.622, 1.548, 1.762, 1.353, 1.179, 1.960, 1.773, 1.745, 2.183, 1.716,
        1.430, 1.980, 2.042, 1.750, 0.193, 2.000, 1.257, 1.930, 1.515, 1.705,
        1.815, 1.887, 1.311, 2.197, 1.826, 1.472, 1.994, 2.278, 2.172, 1.617,
        2.027, 1.528, 1.691, 1.258, 1.799, 1.962, 1.467, 1.800, 1.402, 1.784,
        1.347, 1.707, 2.075, 1.595, 1.787, 1.302, 1.933, 1.823, 1.731, 1.011,
        1.260, 1.986, 2.366, 1.741, 1.920, 1.841, 1.774, 1.952, 1.364, 1.839,
        1.471, 2.123, 2.258, 1.488, 1.473, 0.873, 1.980, 1.768, 1.957, 1.946,
        1.727, 1.844, 1.921, 2.123, 1.610, 0.408, 1.684, 1.400, 1.425, 1.882,
        1.596, 2.277, 1.550, 1.807, 1.560, 1.762, 1.865, 1.406, 2.023, 0.982,
        2.085, 1.715, 1.308, 1.915, 1.307, 2.184, 1.997, 1.815, 1.892, 1.663],
       device='cuda:0')
b after update for 1 param tensor([173.310, 193.584, 198.841, 178.181, 166.586, 148.854, 175.980, 181.861,
        195.191, 181.808, 185.038, 174.923, 193.455, 134.897, 183.234, 175.511,
        173.631, 175.265, 191.467, 188.409, 178.443, 126.568, 173.384, 164.985,
        182.369, 156.343, 188.567, 175.570, 148.551, 162.571, 198.411, 171.533,
        162.170, 184.174, 164.890, 189.667, 177.544, 148.495, 168.873, 196.327,
        158.630, 180.888, 178.192, 186.204, 129.270, 158.338, 182.267, 178.839,
        198.792, 141.550, 195.725, 180.069, 189.118, 149.102, 189.904, 185.788,
        183.583, 174.332, 189.923, 162.595, 170.086, 177.184, 194.147, 184.691,
        163.514, 160.903, 195.512, 196.271, 188.681, 162.289, 138.070, 143.671,
        143.449, 193.426, 163.640, 204.849, 201.531, 173.043, 179.563, 162.844,
        186.699, 154.573, 131.801, 149.959, 152.971, 170.431, 166.738, 164.143,
        195.843, 168.783, 175.499, 190.077, 203.649, 161.014, 177.505, 176.883,
        188.875, 172.646, 198.984, 180.448, 182.860, 191.704, 181.502, 132.190,
        173.641, 189.870, 142.253, 157.241, 165.870, 153.754, 157.782, 178.462,
        181.416, 157.783, 196.729, 175.913, 179.605, 183.436, 190.875, 151.707,
        184.764, 176.987, 154.243, 167.256, 125.638, 185.249, 162.675, 181.804,
        185.745, 184.547, 145.762, 160.251, 166.929, 179.002, 167.766, 144.909,
        182.349, 197.035, 169.530, 164.264, 148.986, 168.075, 182.521, 187.661,
        179.992, 179.505, 181.794,  95.284, 164.466, 141.000, 163.159, 182.343,
        170.640, 196.870, 170.645, 189.575, 142.790, 161.553, 180.260, 176.387,
        156.131, 175.240, 177.875, 164.860, 170.384, 176.698, 157.405, 156.871,
        168.115, 190.386, 154.311, 188.821, 179.113, 172.800, 164.457, 178.766,
        163.572, 164.056, 192.014, 145.839, 148.920, 180.997, 183.540, 176.571,
        170.436, 167.712, 191.069, 180.073, 131.671, 183.446, 189.179, 170.943,
        171.769, 156.690, 185.045, 173.581, 157.464, 183.898, 159.163, 160.647,
        168.439, 164.539, 175.532, 153.831, 143.581, 185.146, 176.122, 174.704,
        195.412, 173.240, 158.166, 186.089, 188.997, 174.973,  58.159, 187.050,
        148.303, 183.748, 162.777, 172.707, 178.193, 181.660, 151.403, 196.030,
        178.725, 160.464, 186.769, 199.603, 194.926, 168.181, 188.291, 163.474,
        171.965, 148.311, 177.397, 185.250, 160.175, 177.438, 156.611, 176.638,
        153.483, 172.787, 190.503, 167.045, 176.791, 150.904, 183.884, 178.574,
        173.984, 132.949, 148.478, 186.388, 203.411, 174.521, 183.236, 179.463,
        176.162, 184.750, 154.436, 179.348, 160.382, 192.686, 198.726, 161.323,
        160.522, 123.569, 186.111, 175.857, 185.001, 184.507, 173.815, 179.573,
        183.280, 192.701, 167.804,  84.500, 171.599, 156.501, 157.850, 181.423,
        167.102, 199.542, 164.676, 177.755, 165.189, 175.560, 180.620, 156.803,
        188.125, 131.086, 190.974, 173.190, 151.226, 182.990, 151.216, 195.449,
        186.875, 178.189, 181.907, 170.538], device='cuda:0')
clipping threshold 0.2514434430194545
a after update for 1 param tensor([[[-2.267e-02],
         [ 1.852e-04],
         [-8.359e-02],
         [ 6.530e-02],
         [ 4.749e-02],
         [ 7.328e-03],
         [ 6.618e-03],
         [ 8.700e-02],
         [-8.749e-02],
         [-7.678e-02]],

        [[-2.247e-02],
         [ 8.988e-02],
         [ 1.457e-02],
         [ 8.123e-02],
         [ 8.383e-02],
         [ 1.696e-02],
         [-1.837e-02],
         [-4.458e-02],
         [ 1.522e-02],
         [-8.950e-02]],

        [[ 4.033e-03],
         [ 5.026e-02],
         [-3.964e-02],
         [ 1.270e-02],
         [ 1.096e-01],
         [-3.426e-02],
         [ 2.120e-02],
         [-1.722e-01],
         [ 4.910e-02],
         [-6.255e-02]],

        [[-5.518e-02],
         [ 2.374e-02],
         [ 5.152e-02],
         [ 9.543e-02],
         [-1.738e-02],
         [-5.395e-02],
         [-8.186e-02],
         [-3.159e-02],
         [-1.457e-02],
         [ 1.321e-01]],

        [[ 2.665e-02],
         [ 6.490e-03],
         [ 1.514e-02],
         [ 4.616e-02],
         [-7.997e-02],
         [-4.371e-02],
         [-7.260e-03],
         [-9.262e-02],
         [-9.809e-03],
         [ 3.697e-02]],

        [[-2.049e-02],
         [-1.094e-01],
         [-6.750e-02],
         [ 1.285e-02],
         [ 2.278e-02],
         [ 2.341e-02],
         [ 8.153e-02],
         [-2.089e-02],
         [-6.002e-02],
         [ 1.520e-02]],

        [[-9.115e-02],
         [ 4.020e-02],
         [ 9.143e-02],
         [-6.530e-02],
         [-4.554e-02],
         [-1.540e-01],
         [ 2.937e-02],
         [-3.228e-02],
         [-1.051e-02],
         [ 5.761e-02]],

        [[ 1.277e-01],
         [-1.054e-01],
         [-1.359e-03],
         [-4.324e-02],
         [ 9.191e-02],
         [ 6.866e-04],
         [ 3.140e-02],
         [-5.980e-02],
         [-1.456e-02],
         [-2.689e-02]],

        [[-9.761e-02],
         [-1.573e-02],
         [ 9.917e-02],
         [ 6.205e-03],
         [-7.259e-02],
         [ 7.378e-02],
         [ 7.680e-03],
         [ 8.574e-03],
         [ 3.877e-02],
         [ 2.541e-02]],

        [[-4.836e-02],
         [-1.402e-02],
         [-5.234e-04],
         [-2.187e-02],
         [-6.381e-02],
         [-1.655e-02],
         [ 1.099e-01],
         [-6.953e-02],
         [-1.019e-02],
         [ 2.571e-02]],

        [[ 5.825e-02],
         [-3.870e-02],
         [ 1.899e-02],
         [ 2.039e-02],
         [-5.912e-02],
         [ 8.720e-02],
         [-4.698e-02],
         [-1.057e-01],
         [-2.050e-02],
         [-8.456e-02]],

        [[-2.874e-03],
         [-1.987e-02],
         [-4.827e-04],
         [-3.306e-02],
         [-4.777e-02],
         [-6.779e-02],
         [ 1.201e-01],
         [-1.337e-02],
         [ 7.195e-02],
         [-6.445e-03]],

        [[-4.675e-02],
         [ 9.494e-02],
         [ 1.102e-01],
         [-1.843e-02],
         [ 1.045e-01],
         [ 4.732e-02],
         [ 4.768e-02],
         [-1.507e-02],
         [ 1.077e-02],
         [-4.323e-02]],

        [[-1.188e-01],
         [ 2.437e-02],
         [ 1.352e-02],
         [ 4.292e-02],
         [-6.214e-02],
         [-6.931e-02],
         [ 8.320e-02],
         [-1.658e-03],
         [-2.724e-02],
         [-1.453e-02]],

        [[-2.857e-02],
         [-1.884e-02],
         [-4.629e-02],
         [-4.647e-02],
         [-2.179e-02],
         [-2.482e-02],
         [ 4.183e-02],
         [ 2.524e-02],
         [-1.070e-01],
         [ 3.393e-02]],

        [[-5.120e-02],
         [ 1.540e-02],
         [-5.127e-02],
         [ 4.387e-02],
         [ 2.002e-02],
         [-7.809e-02],
         [-1.081e-01],
         [-3.319e-02],
         [-5.374e-02],
         [-7.862e-02]],

        [[-9.491e-03],
         [-5.539e-02],
         [-5.035e-02],
         [ 2.387e-02],
         [-9.952e-02],
         [ 1.343e-01],
         [-2.830e-02],
         [ 1.174e-03],
         [ 6.782e-02],
         [ 6.554e-02]],

        [[-6.345e-03],
         [-2.318e-02],
         [ 7.445e-02],
         [ 5.485e-02],
         [ 3.209e-02],
         [ 5.213e-02],
         [-2.341e-02],
         [-1.140e-02],
         [-8.160e-02],
         [ 5.953e-02]],

        [[ 7.382e-02],
         [ 8.371e-02],
         [ 8.334e-03],
         [ 9.249e-02],
         [ 5.938e-03],
         [ 4.183e-02],
         [-1.380e-02],
         [ 6.227e-03],
         [ 3.220e-02],
         [ 1.299e-02]],

        [[ 9.108e-02],
         [ 2.207e-02],
         [ 1.332e-01],
         [ 1.317e-02],
         [ 5.784e-03],
         [-1.069e-01],
         [ 1.129e-01],
         [-4.697e-03],
         [-5.682e-02],
         [-5.823e-02]],

        [[ 4.599e-02],
         [-7.183e-04],
         [ 6.908e-02],
         [-7.948e-03],
         [ 5.172e-02],
         [-5.481e-02],
         [-7.163e-02],
         [-2.214e-02],
         [-2.976e-02],
         [ 3.959e-02]],

        [[ 4.113e-02],
         [-4.565e-02],
         [-2.883e-02],
         [ 6.756e-02],
         [-2.208e-02],
         [ 1.395e-03],
         [ 5.318e-02],
         [ 3.249e-02],
         [ 4.527e-02],
         [-5.794e-02]],

        [[ 2.667e-02],
         [ 4.103e-02],
         [ 3.860e-02],
         [-3.525e-03],
         [ 6.808e-02],
         [ 8.919e-02],
         [-1.344e-02],
         [ 4.194e-03],
         [ 3.987e-02],
         [-1.704e-01]],

        [[ 4.528e-02],
         [-3.376e-02],
         [-4.879e-03],
         [ 4.614e-02],
         [ 9.247e-03],
         [ 6.563e-02],
         [ 3.099e-03],
         [-1.083e-01],
         [-1.953e-02],
         [-7.665e-03]],

        [[ 3.643e-03],
         [ 5.306e-02],
         [-2.040e-02],
         [ 2.888e-02],
         [-1.057e-02],
         [-1.497e-02],
         [-2.311e-01],
         [ 1.656e-02],
         [ 2.861e-02],
         [ 3.880e-03]],

        [[-5.212e-02],
         [ 3.219e-02],
         [-1.507e-01],
         [-1.612e-03],
         [-1.930e-02],
         [-8.172e-02],
         [-2.255e-02],
         [ 9.436e-03],
         [-1.069e-02],
         [-5.880e-03]],

        [[ 1.993e-02],
         [ 8.193e-02],
         [-4.842e-02],
         [ 2.149e-02],
         [ 9.811e-02],
         [-6.473e-02],
         [-5.620e-02],
         [-2.799e-02],
         [ 1.681e-02],
         [ 2.301e-02]],

        [[-5.655e-02],
         [-7.499e-02],
         [-3.407e-03],
         [ 6.073e-03],
         [ 9.377e-03],
         [ 7.469e-02],
         [ 2.780e-02],
         [-2.718e-02],
         [ 5.272e-02],
         [-1.254e-02]],

        [[ 8.018e-02],
         [ 3.012e-02],
         [ 4.649e-02],
         [-5.532e-02],
         [-1.491e-02],
         [-3.673e-02],
         [ 2.857e-02],
         [ 6.675e-02],
         [ 7.132e-02],
         [-1.756e-02]],

        [[-8.255e-02],
         [-2.163e-02],
         [-9.441e-03],
         [ 2.739e-02],
         [ 3.279e-02],
         [-7.394e-02],
         [ 3.532e-02],
         [ 5.148e-03],
         [-5.173e-02],
         [ 1.941e-01]]], device='cuda:0')
s after update for 1 param tensor([[[1.897],
         [1.703],
         [2.153],
         [1.841],
         [1.291],
         [1.683],
         [1.885],
         [2.507],
         [1.833],
         [2.094]],

        [[1.690],
         [1.762],
         [1.421],
         [1.852],
         [1.342],
         [1.910],
         [1.858],
         [1.471],
         [1.872],
         [1.716]],

        [[1.970],
         [1.795],
         [1.599],
         [2.016],
         [1.604],
         [2.137],
         [1.647],
         [1.882],
         [1.803],
         [1.402]],

        [[1.710],
         [1.994],
         [1.953],
         [1.429],
         [1.378],
         [2.000],
         [0.982],
         [1.626],
         [1.812],
         [2.261]],

        [[1.493],
         [1.898],
         [1.515],
         [1.724],
         [1.916],
         [2.258],
         [1.541],
         [2.048],
         [2.178],
         [2.014]],

        [[2.082],
         [1.739],
         [1.420],
         [1.501],
         [1.564],
         [1.365],
         [2.254],
         [1.654],
         [2.086],
         [2.225]],

        [[2.097],
         [1.464],
         [2.094],
         [1.841],
         [1.503],
         [1.980],
         [1.475],
         [1.485],
         [1.421],
         [1.635]],

        [[1.267],
         [1.683],
         [1.971],
         [1.639],
         [1.941],
         [2.014],
         [1.453],
         [1.964],
         [1.625],
         [2.040]],

        [[0.721],
         [1.801],
         [1.621],
         [1.364],
         [1.616],
         [1.459],
         [1.789],
         [1.955],
         [2.340],
         [1.579]],

        [[2.232],
         [0.616],
         [2.163],
         [1.610],
         [0.985],
         [1.778],
         [0.959],
         [1.583],
         [1.181],
         [1.450]],

        [[1.711],
         [1.409],
         [1.955],
         [1.841],
         [1.774],
         [1.677],
         [1.952],
         [2.277],
         [1.327],
         [1.838]],

        [[1.387],
         [1.633],
         [1.315],
         [1.818],
         [1.567],
         [1.000],
         [2.284],
         [1.304],
         [1.437],
         [1.669]],

        [[2.305],
         [1.748],
         [2.034],
         [1.498],
         [1.154],
         [1.894],
         [1.119],
         [1.786],
         [1.171],
         [2.300]],

        [[1.739],
         [1.938],
         [1.238],
         [1.864],
         [1.629],
         [1.519],
         [2.095],
         [1.061],
         [1.897],
         [1.667]],

        [[1.925],
         [1.792],
         [1.734],
         [1.409],
         [0.448],
         [1.530],
         [1.111],
         [1.482],
         [1.927],
         [2.083]],

        [[1.753],
         [2.049],
         [2.365],
         [1.772],
         [1.566],
         [2.066],
         [1.461],
         [1.764],
         [1.849],
         [1.581]],

        [[1.420],
         [2.045],
         [1.304],
         [0.964],
         [1.799],
         [1.794],
         [2.200],
         [1.419],
         [2.260],
         [1.581]],

        [[1.753],
         [1.244],
         [1.922],
         [1.651],
         [1.915],
         [1.478],
         [1.695],
         [1.540],
         [1.636],
         [1.835]],

        [[1.854],
         [1.887],
         [1.458],
         [1.623],
         [1.321],
         [2.020],
         [1.467],
         [1.472],
         [2.085],
         [1.627]],

        [[1.838],
         [1.730],
         [1.703],
         [1.661],
         [2.247],
         [2.263],
         [1.893],
         [1.368],
         [1.964],
         [1.218]],

        [[1.367],
         [1.661],
         [2.114],
         [1.047],
         [2.041],
         [1.476],
         [1.650],
         [2.331],
         [1.999],
         [1.616]],

        [[1.017],
         [1.117],
         [1.637],
         [2.174],
         [1.622],
         [1.317],
         [1.900],
         [1.424],
         [2.078],
         [1.913]],

        [[1.346],
         [1.272],
         [1.941],
         [1.660],
         [1.573],
         [1.636],
         [2.081],
         [1.029],
         [1.105],
         [2.042]],

        [[1.542],
         [1.717],
         [1.424],
         [1.843],
         [1.651],
         [1.785],
         [1.888],
         [1.811],
         [1.478],
         [2.031]],

        [[1.289],
         [1.670],
         [1.725],
         [2.072],
         [1.694],
         [1.544],
         [1.963],
         [1.550],
         [1.800],
         [1.436]],

        [[1.786],
         [1.673],
         [1.807],
         [1.974],
         [1.499],
         [1.640],
         [2.275],
         [1.912],
         [2.107],
         [1.089]],

        [[2.015],
         [1.716],
         [1.952],
         [1.389],
         [1.976],
         [1.672],
         [1.866],
         [1.453],
         [1.630],
         [0.721]],

        [[1.775],
         [1.599],
         [2.058],
         [1.731],
         [1.680],
         [1.561],
         [1.824],
         [1.274],
         [1.961],
         [1.390]],

        [[1.744],
         [2.031],
         [1.346],
         [2.327],
         [1.541],
         [1.651],
         [1.746],
         [1.346],
         [1.367],
         [1.729]],

        [[1.902],
         [1.506],
         [1.946],
         [1.772],
         [1.594],
         [1.641],
         [1.668],
         [1.326],
         [1.267],
         [1.795]]], device='cuda:0')
b after update for 1 param tensor([[[182.152],
         [172.598],
         [194.052],
         [179.442],
         [150.286],
         [171.560],
         [181.577],
         [209.410],
         [179.071],
         [191.364]],

        [[171.911],
         [175.544],
         [157.658],
         [179.974],
         [153.228],
         [182.772],
         [180.286],
         [160.388],
         [180.945],
         [173.236]],

        [[185.642],
         [177.172],
         [167.213],
         [187.800],
         [167.513],
         [193.346],
         [169.712],
         [181.428],
         [177.577],
         [156.582]],

        [[172.951],
         [186.729],
         [184.840],
         [158.109],
         [155.266],
         [187.055],
         [131.070],
         [168.658],
         [178.027],
         [198.861]],

        [[161.588],
         [182.197],
         [162.771],
         [173.660],
         [183.085],
         [198.728],
         [164.164],
         [189.240],
         [195.195],
         [187.688]],

        [[190.826],
         [174.381],
         [157.592],
         [162.010],
         [165.399],
         [154.516],
         [198.574],
         [170.089],
         [190.995],
         [197.275]],

        [[191.527],
         [160.007],
         [191.387],
         [179.456],
         [162.137],
         [186.075],
         [160.604],
         [161.175],
         [157.629],
         [169.126]],

        [[148.839],
         [171.595],
         [185.652],
         [169.327],
         [184.252],
         [187.695],
         [159.438],
         [185.342],
         [168.601],
         [188.870]],

        [[112.316],
         [177.466],
         [168.401],
         [154.474],
         [168.100],
         [159.744],
         [176.872],
         [184.899],
         [202.313],
         [166.174]],

        [[197.580],
         [103.784],
         [194.495],
         [167.821],
         [131.255],
         [176.351],
         [129.519],
         [166.372],
         [143.740],
         [159.273]],

        [[173.014],
         [157.007],
         [184.897],
         [179.428],
         [176.162],
         [171.264],
         [184.785],
         [199.569],
         [152.360],
         [179.304]],

        [[155.758],
         [169.006],
         [151.658],
         [178.317],
         [165.568],
         [132.249],
         [199.859],
         [151.022],
         [158.533],
         [170.861]],

        [[200.804],
         [174.829],
         [188.635],
         [161.848],
         [142.061],
         [182.003],
         [139.868],
         [176.755],
         [143.118],
         [200.583]],

        [[174.421],
         [184.103],
         [147.151],
         [180.560],
         [168.775],
         [162.994],
         [191.411],
         [136.219],
         [182.137],
         [170.733]],

        [[183.476],
         [177.040],
         [174.127],
         [156.973],
         [ 88.530],
         [163.607],
         [139.397],
         [160.983],
         [183.567],
         [190.855]],

        [[175.078],
         [189.298],
         [203.363],
         [176.031],
         [165.499],
         [190.109],
         [159.848],
         [175.659],
         [179.814],
         [166.314]],

        [[157.603],
         [189.119],
         [151.014],
         [129.840],
         [177.383],
         [177.145],
         [196.165],
         [157.518],
         [198.800],
         [166.271]],

        [[175.116],
         [147.529],
         [183.368],
         [169.951],
         [183.007],
         [160.764],
         [172.186],
         [164.112],
         [169.163],
         [179.167]],

        [[180.094],
         [181.673],
         [159.665],
         [168.475],
         [152.004],
         [187.979],
         [160.166],
         [160.458],
         [190.970],
         [168.679]],

        [[179.304],
         [173.926],
         [172.608],
         [170.459],
         [198.255],
         [198.930],
         [181.968],
         [154.687],
         [185.329],
         [145.967]],

        [[154.638],
         [170.421],
         [192.279],
         [135.342],
         [188.944],
         [160.658],
         [169.881],
         [201.930],
         [186.994],
         [168.100]],

        [[133.364],
         [139.774],
         [169.196],
         [195.013],
         [168.446],
         [151.800],
         [182.303],
         [157.833],
         [190.655],
         [182.909]],

        [[153.432],
         [149.171],
         [184.270],
         [170.387],
         [165.873],
         [169.140],
         [190.799],
         [134.135],
         [139.031],
         [189.008]],

        [[164.210],
         [173.296],
         [157.805],
         [179.552],
         [169.906],
         [176.687],
         [181.701],
         [177.968],
         [160.779],
         [188.470]],

        [[150.125],
         [170.889],
         [173.692],
         [190.362],
         [172.152],
         [164.334],
         [185.292],
         [164.650],
         [177.413],
         [158.508]],

        [[176.723],
         [171.042],
         [177.790],
         [185.788],
         [161.938],
         [169.375],
         [199.476],
         [182.862],
         [191.950],
         [138.025]],

        [[187.740],
         [173.250],
         [184.752],
         [155.873],
         [185.913],
         [171.011],
         [180.674],
         [159.398],
         [168.824],
         [112.320]],

        [[176.209],
         [167.230],
         [189.732],
         [173.987],
         [171.404],
         [165.260],
         [178.606],
         [149.290],
         [185.180],
         [155.934]],

        [[174.631],
         [188.476],
         [153.426],
         [201.749],
         [164.182],
         [169.937],
         [174.727],
         [153.424],
         [154.645],
         [173.906]],

        [[182.378],
         [162.271],
         [184.502],
         [176.041],
         [166.989],
         [169.399],
         [170.828],
         [152.273],
         [148.891],
         [177.208]]], device='cuda:0')
clipping threshold 0.2514434430194545
a after update for 1 param tensor([[-0.032],
        [-0.019],
        [ 0.003],
        [ 0.029],
        [-0.056],
        [ 0.042],
        [ 0.032],
        [ 0.009],
        [-0.088],
        [-0.003],
        [-0.047],
        [-0.020],
        [ 0.014],
        [ 0.018],
        [-0.031],
        [-0.022],
        [-0.050],
        [-0.020],
        [-0.018],
        [ 0.022],
        [ 0.005],
        [ 0.090],
        [ 0.051],
        [ 0.002],
        [ 0.061],
        [ 0.086],
        [-0.065],
        [ 0.022],
        [ 0.032],
        [ 0.007]], device='cuda:0')
s after update for 1 param tensor([[1.225],
        [1.611],
        [1.547],
        [1.919],
        [1.840],
        [1.508],
        [2.118],
        [1.473],
        [2.036],
        [1.609],
        [1.641],
        [2.167],
        [1.485],
        [1.345],
        [1.862],
        [1.925],
        [1.638],
        [1.830],
        [1.350],
        [0.793],
        [1.968],
        [1.508],
        [1.947],
        [1.801],
        [2.113],
        [1.782],
        [2.312],
        [1.138],
        [1.289],
        [2.018]], device='cuda:0')
b after update for 1 param tensor([[146.351],
        [167.850],
        [164.487],
        [183.201],
        [179.373],
        [162.384],
        [192.470],
        [160.492],
        [188.699],
        [167.763],
        [169.438],
        [194.686],
        [161.142],
        [153.359],
        [180.487],
        [183.512],
        [169.275],
        [178.930],
        [153.650],
        [117.803],
        [185.530],
        [162.409],
        [184.521],
        [177.468],
        [192.237],
        [176.531],
        [201.071],
        [141.103],
        [150.140],
        [187.859]], device='cuda:0')
clipping threshold 0.2514434430194545
||w||^2 0.05635922601067381
exp ma of ||w||^2 0.07298159323169304
||w|| 0.23740098148633212
exp ma of ||w|| 0.26509711854873047
||w||^2 0.031152809812729314
exp ma of ||w||^2 0.03183470366713616
||w|| 0.17650158586462988
exp ma of ||w|| 0.17510072690621056
cuda
Objective function 31.16 = squared loss an data 29.90 + 0.5*rho*h**2 0.461164 + alpha*h 0.383252 + L2reg 0.19 + L1reg 0.22 ; SHD = 58 ; DAG True
Proportion of microbatches that were clipped  0.6956871580302543
iteration 2 in inner loop, alpha 12.619494261354504 rho 1000.0 h 0.030369860886828803
9630
cuda
Objective function 35.31 = squared loss an data 29.90 + 0.5*rho*h**2 4.611642 + alpha*h 0.383252 + L2reg 0.19 + L1reg 0.22 ; SHD = 58 ; DAG True
||w||^2 0.4724789105638161
exp ma of ||w||^2 162.46035317299578
||w|| 0.6873710137646306
exp ma of ||w|| 1.01306985754351
||w||^2 0.02987263094572003
exp ma of ||w||^2 0.0371517608115329
||w|| 0.17283700687561107
exp ma of ||w|| 0.1917902662423293
||w||^2 0.019671875139814464
exp ma of ||w||^2 0.02784423948014887
||w|| 0.14025646202515757
exp ma of ||w|| 0.165044071074324
||w||^2 0.05168667805256162
exp ma of ||w||^2 0.028738295824306658
||w|| 0.22734704320171314
exp ma of ||w|| 0.1666432203498605
||w||^2 0.019560282969798656
exp ma of ||w||^2 0.028953700735601273
||w|| 0.13985808153195387
exp ma of ||w|| 0.1672125111442573
cuda
Objective function 31.10 = squared loss an data 30.10 + 0.5*rho*h**2 0.507695 + alpha*h 0.127162 + L2reg 0.19 + L1reg 0.18 ; SHD = 59 ; DAG True
Proportion of microbatches that were clipped  0.6944579091937308
iteration 3 in inner loop, alpha 12.619494261354504 rho 10000.0 h 0.010076656171030152
iteration 3 in outer loop, alpha = 113.38605597165602, rho = 10000.0, h = 0.010076656171030152
cuda
9630
cuda
Objective function 32.11 = squared loss an data 30.10 + 0.5*rho*h**2 0.507695 + alpha*h 1.142552 + L2reg 0.19 + L1reg 0.18 ; SHD = 59 ; DAG True
||w||^2 0.3163517058660496
exp ma of ||w||^2 46.78156865036068
||w|| 0.5624515142357158
exp ma of ||w|| 0.6968667075406841
||w||^2 0.42049876272382175
exp ma of ||w||^2 11.544983328477262
||w|| 0.6484587594626368
exp ma of ||w|| 0.658538684873786
||w||^2 0.2318250226159018
exp ma of ||w||^2 1.3017633686364254
||w|| 0.48148211037992034
exp ma of ||w|| 0.5302471134323147
cuda
Objective function 31.43 = squared loss an data 30.04 + 0.5*rho*h**2 0.241084 + alpha*h 0.787334 + L2reg 0.19 + L1reg 0.16 ; SHD = 59 ; DAG True
Proportion of microbatches that were clipped  0.7002863506204263
iteration 1 in inner loop, alpha 113.38605597165602 rho 10000.0 h 0.006943832303957009
9630
cuda
Objective function 33.60 = squared loss an data 30.04 + 0.5*rho*h**2 2.410840 + alpha*h 0.787334 + L2reg 0.19 + L1reg 0.16 ; SHD = 59 ; DAG True
||w||^2 2748901744751.582
exp ma of ||w||^2 1381485117850.3464
||w|| 1657981.2256933376
exp ma of ||w|| 870172.3788868707
||w||^2 5.299233434120229
exp ma of ||w||^2 228664.98661500862
||w|| 2.3020063931536394
exp ma of ||w|| 4.0643464221160075
||w||^2 0.17584062825779637
exp ma of ||w||^2 0.20504582894254372
||w|| 0.41933355250658916
exp ma of ||w|| 0.42686732346435363
cuda
Objective function 31.64 = squared loss an data 30.42 + 0.5*rho*h**2 0.519910 + alpha*h 0.365627 + L2reg 0.19 + L1reg 0.14 ; SHD = 58 ; DAG True
Proportion of microbatches that were clipped  0.6922330097087379
iteration 2 in inner loop, alpha 113.38605597165602 rho 100000.0 h 0.003224623016016892
iteration 4 in outer loop, alpha = 3338.009071988548, rho = 1000000.0, h = 0.003224623016016892
Threshold 0.3
[[0.002 0.04  0.033 0.106 0.037 0.021 0.044 0.039 0.043 0.02  0.051 0.02
  0.048 0.075 0.081 0.018 0.053 0.029 0.131 0.036 0.105 0.04  0.018 0.017
  0.036 0.017 0.054 0.015 0.09  0.035]
 [0.053 0.003 0.023 0.035 0.091 0.013 0.078 0.074 0.029 0.021 0.013 0.047
  0.051 0.074 0.038 0.067 0.022 0.024 0.067 0.04  0.142 0.069 0.112 0.025
  0.058 0.046 0.057 0.144 0.034 0.028]
 [0.071 0.093 0.002 0.063 0.103 0.022 0.056 0.053 0.028 0.045 0.026 0.27
  0.034 0.056 0.056 0.101 0.12  0.1   0.13  0.04  0.062 0.071 0.126 0.044
  0.107 0.064 0.086 0.047 0.061 0.075]
 [0.03  0.046 0.032 0.003 0.037 0.018 0.025 0.006 0.054 0.025 0.015 0.058
  0.021 0.028 0.029 0.035 0.073 0.025 0.047 0.022 0.045 0.034 0.025 0.016
  0.041 0.03  0.04  0.056 0.008 0.012]
 [0.064 0.03  0.023 0.072 0.003 0.031 0.021 0.021 0.018 0.043 0.017 0.034
  0.086 0.032 0.024 0.019 0.057 0.025 0.081 0.018 0.105 0.028 0.034 0.021
  0.032 0.026 0.042 0.044 0.088 0.03 ]
 [0.148 0.162 0.152 0.192 0.084 0.003 0.097 0.085 0.143 0.033 0.082 0.214
  0.076 0.035 0.104 0.071 0.135 0.067 0.043 0.103 0.228 0.098 0.034 0.054
  0.047 0.049 0.036 0.049 0.055 0.151]
 [0.078 0.036 0.042 0.084 0.134 0.033 0.002 0.061 0.112 0.12  0.022 0.126
  0.049 0.039 0.046 0.153 0.114 0.045 0.14  0.061 0.165 0.037 0.076 0.046
  0.149 0.025 0.057 0.074 0.023 0.115]
 [0.054 0.045 0.025 0.289 0.095 0.026 0.052 0.002 0.107 0.044 0.016 0.056
  0.072 0.132 0.089 0.059 0.077 0.035 0.189 0.083 0.081 0.029 0.027 0.019
  0.037 0.026 0.141 0.037 0.1   0.05 ]
 [0.07  0.081 0.108 0.04  0.132 0.018 0.017 0.023 0.001 0.058 0.03  0.067
  0.032 0.056 0.056 0.108 0.112 0.032 0.155 0.032 0.187 0.073 0.038 0.027
  0.042 0.055 0.096 0.052 0.049 0.071]
 [0.103 0.092 0.069 0.093 0.061 0.061 0.017 0.056 0.046 0.001 0.04  0.047
  0.132 0.068 0.101 0.115 0.109 0.037 0.122 0.037 0.205 0.174 0.044 0.041
  0.028 0.015 0.043 0.06  0.108 0.099]
 [0.06  0.118 0.123 0.175 0.15  0.036 0.119 0.11  0.086 0.068 0.003 0.038
  0.169 0.089 0.049 0.058 0.045 0.087 0.176 0.128 0.09  0.069 0.056 0.035
  0.067 0.067 0.048 0.211 0.034 0.177]
 [0.072 0.06  0.01  0.038 0.05  0.015 0.011 0.044 0.016 0.045 0.047 0.002
  0.051 0.047 0.027 0.147 0.108 0.028 0.061 0.046 0.062 0.019 0.037 0.033
  0.049 0.039 0.019 0.025 0.063 0.018]
 [0.041 0.06  0.1   0.134 0.032 0.034 0.045 0.029 0.061 0.018 0.022 0.06
  0.002 0.026 0.135 0.039 0.021 0.036 0.105 0.102 0.036 0.118 0.034 0.012
  0.033 0.058 0.105 0.041 0.029 0.086]
 [0.03  0.031 0.054 0.11  0.115 0.082 0.069 0.013 0.059 0.043 0.023 0.066
  0.103 0.003 0.087 0.048 0.032 0.033 0.081 0.051 0.101 0.053 0.061 0.034
  0.018 0.017 0.057 0.031 0.086 0.058]
 [0.052 0.051 0.03  0.135 0.082 0.025 0.047 0.024 0.048 0.037 0.039 0.069
  0.02  0.042 0.002 0.085 0.115 0.055 0.05  0.057 0.157 0.028 0.089 0.079
  0.044 0.016 0.033 0.138 0.017 0.025]
 [0.125 0.027 0.033 0.06  0.103 0.035 0.015 0.039 0.025 0.02  0.045 0.019
  0.052 0.041 0.042 0.002 0.086 0.026 0.034 0.06  0.036 0.024 0.045 0.043
  0.038 0.021 0.045 0.056 0.088 0.056]
 [0.036 0.089 0.019 0.04  0.056 0.017 0.008 0.032 0.032 0.026 0.042 0.02
  0.094 0.06  0.021 0.023 0.002 0.02  0.083 0.044 0.1   0.042 0.043 0.037
  0.036 0.013 0.043 0.05  0.056 0.031]
 [0.109 0.113 0.043 0.111 0.108 0.034 0.066 0.078 0.06  0.066 0.031 0.094
  0.057 0.079 0.061 0.077 0.111 0.003 0.042 0.061 0.045 0.065 0.088 0.056
  0.028 0.015 0.046 0.045 0.032 0.119]
 [0.025 0.027 0.028 0.057 0.036 0.033 0.015 0.017 0.014 0.014 0.016 0.039
  0.021 0.017 0.049 0.067 0.027 0.047 0.002 0.023 0.097 0.083 0.059 0.007
  0.033 0.03  0.029 0.071 0.039 0.022]
 [0.061 0.059 0.065 0.111 0.123 0.041 0.061 0.026 0.136 0.05  0.019 0.06
  0.029 0.041 0.053 0.033 0.05  0.043 0.093 0.002 0.27  0.082 0.1   0.043
  0.073 0.056 0.04  0.052 0.034 0.062]
 [0.028 0.021 0.031 0.058 0.021 0.011 0.017 0.037 0.015 0.015 0.019 0.023
  0.065 0.027 0.018 0.05  0.028 0.05  0.024 0.016 0.002 0.015 0.018 0.015
  0.028 0.012 0.045 0.034 0.011 0.03 ]
 [0.095 0.042 0.037 0.093 0.094 0.018 0.066 0.075 0.028 0.023 0.032 0.097
  0.018 0.047 0.078 0.069 0.047 0.048 0.03  0.026 0.117 0.002 0.041 0.028
  0.058 0.042 0.034 0.104 0.054 0.078]
 [0.119 0.013 0.021 0.087 0.074 0.04  0.041 0.06  0.071 0.073 0.035 0.041
  0.087 0.042 0.024 0.059 0.064 0.043 0.051 0.026 0.115 0.092 0.003 0.009
  0.053 0.023 0.03  0.026 0.017 0.049]
 [0.145 0.085 0.048 0.163 0.088 0.043 0.05  0.15  0.098 0.07  0.092 0.084
  0.144 0.106 0.047 0.06  0.068 0.037 0.306 0.041 0.234 0.086 0.325 0.001
  0.11  0.029 0.091 0.071 0.043 0.142]
 [0.098 0.04  0.024 0.059 0.057 0.048 0.015 0.063 0.053 0.115 0.027 0.052
  0.087 0.105 0.067 0.067 0.055 0.066 0.06  0.031 0.104 0.041 0.037 0.023
  0.003 0.05  0.132 0.06  0.058 0.058]
 [0.154 0.041 0.037 0.072 0.094 0.058 0.112 0.133 0.029 0.143 0.043 0.086
  0.055 0.096 0.17  0.165 0.09  0.209 0.073 0.04  0.245 0.073 0.138 0.159
  0.067 0.002 0.078 0.262 0.204 0.066]
 [0.038 0.047 0.027 0.066 0.067 0.058 0.049 0.012 0.025 0.042 0.039 0.136
  0.015 0.037 0.096 0.07  0.059 0.073 0.083 0.059 0.04  0.067 0.102 0.021
  0.018 0.025 0.002 0.119 0.041 0.024]
 [0.129 0.026 0.049 0.065 0.039 0.046 0.025 0.08  0.051 0.042 0.011 0.085
  0.063 0.093 0.018 0.038 0.048 0.053 0.036 0.044 0.079 0.023 0.107 0.048
  0.052 0.012 0.015 0.002 0.03  0.034]
 [0.036 0.09  0.049 0.221 0.057 0.047 0.048 0.032 0.041 0.016 0.082 0.044
  0.072 0.021 0.119 0.021 0.043 0.083 0.053 0.072 0.224 0.055 0.094 0.049
  0.047 0.014 0.071 0.111 0.003 0.086]
 [0.08  0.089 0.038 0.152 0.108 0.024 0.026 0.043 0.035 0.022 0.018 0.167
  0.029 0.049 0.116 0.041 0.073 0.026 0.079 0.056 0.072 0.036 0.062 0.017
  0.049 0.029 0.1   0.075 0.033 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.306 0.    0.    0.    0.325 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.0, 'tpr': 0.03333333333333333, 'fpr': 0.0, 'f1': 0.06451612903225806, 'shd': 58, 'npred': 2, 'ntrue': 60}
[0.04  0.033 0.106 0.037 0.021 0.044 0.039 0.043 0.02  0.051 0.02  0.048
 0.075 0.081 0.018 0.053 0.029 0.131 0.036 0.105 0.04  0.018 0.017 0.036
 0.017 0.054 0.015 0.09  0.035 0.053 0.023 0.035 0.091 0.013 0.078 0.074
 0.029 0.021 0.013 0.047 0.051 0.074 0.038 0.067 0.022 0.024 0.067 0.04
 0.142 0.069 0.112 0.025 0.058 0.046 0.057 0.144 0.034 0.028 0.071 0.093
 0.063 0.103 0.022 0.056 0.053 0.028 0.045 0.026 0.27  0.034 0.056 0.056
 0.101 0.12  0.1   0.13  0.04  0.062 0.071 0.126 0.044 0.107 0.064 0.086
 0.047 0.061 0.075 0.03  0.046 0.032 0.037 0.018 0.025 0.006 0.054 0.025
 0.015 0.058 0.021 0.028 0.029 0.035 0.073 0.025 0.047 0.022 0.045 0.034
 0.025 0.016 0.041 0.03  0.04  0.056 0.008 0.012 0.064 0.03  0.023 0.072
 0.031 0.021 0.021 0.018 0.043 0.017 0.034 0.086 0.032 0.024 0.019 0.057
 0.025 0.081 0.018 0.105 0.028 0.034 0.021 0.032 0.026 0.042 0.044 0.088
 0.03  0.148 0.162 0.152 0.192 0.084 0.097 0.085 0.143 0.033 0.082 0.214
 0.076 0.035 0.104 0.071 0.135 0.067 0.043 0.103 0.228 0.098 0.034 0.054
 0.047 0.049 0.036 0.049 0.055 0.151 0.078 0.036 0.042 0.084 0.134 0.033
 0.061 0.112 0.12  0.022 0.126 0.049 0.039 0.046 0.153 0.114 0.045 0.14
 0.061 0.165 0.037 0.076 0.046 0.149 0.025 0.057 0.074 0.023 0.115 0.054
 0.045 0.025 0.289 0.095 0.026 0.052 0.107 0.044 0.016 0.056 0.072 0.132
 0.089 0.059 0.077 0.035 0.189 0.083 0.081 0.029 0.027 0.019 0.037 0.026
 0.141 0.037 0.1   0.05  0.07  0.081 0.108 0.04  0.132 0.018 0.017 0.023
 0.058 0.03  0.067 0.032 0.056 0.056 0.108 0.112 0.032 0.155 0.032 0.187
 0.073 0.038 0.027 0.042 0.055 0.096 0.052 0.049 0.071 0.103 0.092 0.069
 0.093 0.061 0.061 0.017 0.056 0.046 0.04  0.047 0.132 0.068 0.101 0.115
 0.109 0.037 0.122 0.037 0.205 0.174 0.044 0.041 0.028 0.015 0.043 0.06
 0.108 0.099 0.06  0.118 0.123 0.175 0.15  0.036 0.119 0.11  0.086 0.068
 0.038 0.169 0.089 0.049 0.058 0.045 0.087 0.176 0.128 0.09  0.069 0.056
 0.035 0.067 0.067 0.048 0.211 0.034 0.177 0.072 0.06  0.01  0.038 0.05
 0.015 0.011 0.044 0.016 0.045 0.047 0.051 0.047 0.027 0.147 0.108 0.028
 0.061 0.046 0.062 0.019 0.037 0.033 0.049 0.039 0.019 0.025 0.063 0.018
 0.041 0.06  0.1   0.134 0.032 0.034 0.045 0.029 0.061 0.018 0.022 0.06
 0.026 0.135 0.039 0.021 0.036 0.105 0.102 0.036 0.118 0.034 0.012 0.033
 0.058 0.105 0.041 0.029 0.086 0.03  0.031 0.054 0.11  0.115 0.082 0.069
 0.013 0.059 0.043 0.023 0.066 0.103 0.087 0.048 0.032 0.033 0.081 0.051
 0.101 0.053 0.061 0.034 0.018 0.017 0.057 0.031 0.086 0.058 0.052 0.051
 0.03  0.135 0.082 0.025 0.047 0.024 0.048 0.037 0.039 0.069 0.02  0.042
 0.085 0.115 0.055 0.05  0.057 0.157 0.028 0.089 0.079 0.044 0.016 0.033
 0.138 0.017 0.025 0.125 0.027 0.033 0.06  0.103 0.035 0.015 0.039 0.025
 0.02  0.045 0.019 0.052 0.041 0.042 0.086 0.026 0.034 0.06  0.036 0.024
 0.045 0.043 0.038 0.021 0.045 0.056 0.088 0.056 0.036 0.089 0.019 0.04
 0.056 0.017 0.008 0.032 0.032 0.026 0.042 0.02  0.094 0.06  0.021 0.023
 0.02  0.083 0.044 0.1   0.042 0.043 0.037 0.036 0.013 0.043 0.05  0.056
 0.031 0.109 0.113 0.043 0.111 0.108 0.034 0.066 0.078 0.06  0.066 0.031
 0.094 0.057 0.079 0.061 0.077 0.111 0.042 0.061 0.045 0.065 0.088 0.056
 0.028 0.015 0.046 0.045 0.032 0.119 0.025 0.027 0.028 0.057 0.036 0.033
 0.015 0.017 0.014 0.014 0.016 0.039 0.021 0.017 0.049 0.067 0.027 0.047
 0.023 0.097 0.083 0.059 0.007 0.033 0.03  0.029 0.071 0.039 0.022 0.061
 0.059 0.065 0.111 0.123 0.041 0.061 0.026 0.136 0.05  0.019 0.06  0.029
 0.041 0.053 0.033 0.05  0.043 0.093 0.27  0.082 0.1   0.043 0.073 0.056
 0.04  0.052 0.034 0.062 0.028 0.021 0.031 0.058 0.021 0.011 0.017 0.037
 0.015 0.015 0.019 0.023 0.065 0.027 0.018 0.05  0.028 0.05  0.024 0.016
 0.015 0.018 0.015 0.028 0.012 0.045 0.034 0.011 0.03  0.095 0.042 0.037
 0.093 0.094 0.018 0.066 0.075 0.028 0.023 0.032 0.097 0.018 0.047 0.078
 0.069 0.047 0.048 0.03  0.026 0.117 0.041 0.028 0.058 0.042 0.034 0.104
 0.054 0.078 0.119 0.013 0.021 0.087 0.074 0.04  0.041 0.06  0.071 0.073
 0.035 0.041 0.087 0.042 0.024 0.059 0.064 0.043 0.051 0.026 0.115 0.092
 0.009 0.053 0.023 0.03  0.026 0.017 0.049 0.145 0.085 0.048 0.163 0.088
 0.043 0.05  0.15  0.098 0.07  0.092 0.084 0.144 0.106 0.047 0.06  0.068
 0.037 0.306 0.041 0.234 0.086 0.325 0.11  0.029 0.091 0.071 0.043 0.142
 0.098 0.04  0.024 0.059 0.057 0.048 0.015 0.063 0.053 0.115 0.027 0.052
 0.087 0.105 0.067 0.067 0.055 0.066 0.06  0.031 0.104 0.041 0.037 0.023
 0.05  0.132 0.06  0.058 0.058 0.154 0.041 0.037 0.072 0.094 0.058 0.112
 0.133 0.029 0.143 0.043 0.086 0.055 0.096 0.17  0.165 0.09  0.209 0.073
 0.04  0.245 0.073 0.138 0.159 0.067 0.078 0.262 0.204 0.066 0.038 0.047
 0.027 0.066 0.067 0.058 0.049 0.012 0.025 0.042 0.039 0.136 0.015 0.037
 0.096 0.07  0.059 0.073 0.083 0.059 0.04  0.067 0.102 0.021 0.018 0.025
 0.119 0.041 0.024 0.129 0.026 0.049 0.065 0.039 0.046 0.025 0.08  0.051
 0.042 0.011 0.085 0.063 0.093 0.018 0.038 0.048 0.053 0.036 0.044 0.079
 0.023 0.107 0.048 0.052 0.012 0.015 0.03  0.034 0.036 0.09  0.049 0.221
 0.057 0.047 0.048 0.032 0.041 0.016 0.082 0.044 0.072 0.021 0.119 0.021
 0.043 0.083 0.053 0.072 0.224 0.055 0.094 0.049 0.047 0.014 0.071 0.111
 0.086 0.08  0.089 0.038 0.152 0.108 0.024 0.026 0.043 0.035 0.022 0.018
 0.167 0.029 0.049 0.116 0.041 0.073 0.026 0.079 0.056 0.072 0.036 0.062
 0.017 0.049 0.029 0.1   0.075 0.033]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.5713991769547324, 0.1569536680490481)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
