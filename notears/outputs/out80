samples  5000  graph  15 30 ER mlp  minibatch size  75  noise  0.75  minibatches per NN training  111 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006583550827196305
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0011537970975332712
iteration 4 in outer loop, alpha = 156.00431709949223, rho = 100000.0, h = 0.0011537970975332712
cuda
iteration 1 in inner loop,alpha 156.00431709949223 rho 100000.0 h 0.00033575511004180214
iteration 5 in outer loop, alpha = 491.7594271412944, rho = 1000000.0, h = 0.00033575511004180214
Threshold 0.3
[[0.002 0.    0.    0.005 0.194 0.    0.    0.    0.    2.734 0.179 0.001
  0.001 0.    0.006]
 [2.52  0.001 0.002 0.262 0.215 0.01  0.01  0.001 0.002 2.189 0.199 1.342
  0.001 0.001 0.098]
 [0.027 0.068 0.001 0.147 0.166 0.035 0.137 0.001 0.067 0.125 0.135 0.162
  0.001 0.001 0.006]
 [0.153 0.    0.001 0.001 0.204 0.    0.    0.    0.    0.214 0.335 0.001
  0.    0.    1.022]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    0.001]
 [2.646 0.03  0.001 0.968 0.376 0.001 1.377 0.    0.055 1.746 0.956 1.472
  0.012 0.002 1.923]
 [0.414 0.044 0.    0.336 0.408 0.    0.001 0.    0.059 0.139 1.162 0.096
  0.001 0.    0.163]
 [2.69  0.002 0.    2.092 0.137 0.612 0.046 0.    0.003 0.313 0.139 0.061
  0.014 0.022 0.118]
 [0.103 0.383 0.002 0.638 0.415 0.002 0.001 0.001 0.003 2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.001 0.    0.    0.002 0.402 0.001
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.057 0.    0.001 0.    0.001 0.001 0.015 0.
  0.    0.    0.005]
 [0.37  0.    0.    0.18  0.153 0.    0.004 0.    0.002 0.427 0.58  0.001
  0.    0.001 0.032]
 [0.041 0.078 0.    0.112 3.047 0.017 0.008 0.    1.212 0.107 1.585 0.031
  0.001 0.016 0.021]
 [0.126 1.398 0.    1.127 1.231 0.003 0.006 0.    3.368 0.386 0.434 0.158
  0.011 0.002 0.562]
 [0.089 0.    0.    0.    0.111 0.    0.    0.    0.    0.106 0.081 0.002
  0.004 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.734 0.    0.
  0.    0.    0.   ]
 [2.52  0.    0.    0.    0.    0.    0.    0.    0.    2.189 0.    1.342
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.335 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.968 0.376 0.    1.377 0.    0.    1.746 0.956 1.472
  0.    0.    1.923]
 [0.414 0.    0.    0.336 0.408 0.    0.    0.    0.    0.    1.162 0.
  0.    0.    0.   ]
 [2.69  0.    0.    2.092 0.    0.612 0.    0.    0.    0.313 0.    0.
  0.    0.    0.   ]
 [0.    0.383 0.    0.638 0.415 0.    0.    0.    0.    2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.    0.    0.    0.    0.402 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.057 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.427 0.58  0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.047 0.    0.    0.    1.212 0.    1.585 0.
  0.    0.    0.   ]
 [0.    1.398 0.    1.127 1.231 0.    0.    0.    3.368 0.386 0.434 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[7.166e-05 3.031e-04 4.737e-03 1.942e-01 6.655e-05 2.207e-04 4.993e-05
 2.937e-04 2.734e+00 1.793e-01 5.105e-04 1.050e-03 1.242e-04 5.740e-03
 2.520e+00 1.954e-03 2.620e-01 2.147e-01 9.724e-03 9.801e-03 9.328e-04
 1.620e-03 2.189e+00 1.994e-01 1.342e+00 5.488e-04 9.886e-04 9.781e-02
 2.679e-02 6.849e-02 1.472e-01 1.660e-01 3.508e-02 1.368e-01 5.716e-04
 6.687e-02 1.254e-01 1.352e-01 1.619e-01 8.477e-04 1.272e-03 6.092e-03
 1.532e-01 1.509e-04 9.882e-04 2.036e-01 4.044e-05 3.897e-04 2.489e-04
 6.858e-05 2.135e-01 3.353e-01 1.155e-03 4.106e-04 1.293e-04 1.022e+00
 6.064e-05 4.791e-06 5.172e-05 2.471e-04 1.136e-05 1.928e-04 4.191e-06
 2.163e-04 3.383e-04 1.833e-03 1.007e-04 2.631e-05 7.614e-05 1.086e-03
 2.646e+00 3.009e-02 1.478e-03 9.684e-01 3.765e-01 1.377e+00 1.229e-04
 5.513e-02 1.746e+00 9.560e-01 1.472e+00 1.180e-02 1.838e-03 1.923e+00
 4.141e-01 4.437e-02 3.959e-04 3.360e-01 4.079e-01 4.254e-04 5.935e-05
 5.872e-02 1.392e-01 1.162e+00 9.588e-02 6.731e-04 4.063e-04 1.632e-01
 2.690e+00 2.126e-03 4.852e-04 2.092e+00 1.369e-01 6.121e-01 4.646e-02
 3.330e-03 3.125e-01 1.385e-01 6.062e-02 1.448e-02 2.181e-02 1.178e-01
 1.034e-01 3.829e-01 2.017e-03 6.383e-01 4.147e-01 2.074e-03 1.394e-03
 1.328e-03 2.308e+00 1.722e+00 1.030e+00 5.492e-05 3.972e-04 3.674e-01
 1.834e-04 4.607e-06 8.949e-05 4.394e-04 2.722e+00 6.113e-06 8.992e-04
 1.147e-05 4.538e-04 4.020e-01 1.020e-03 1.265e-04 1.194e-04 2.179e-03
 6.634e-04 5.288e-05 3.430e-04 1.176e-03 3.057e+00 1.252e-04 7.885e-04
 1.415e-04 9.900e-04 8.589e-04 1.728e-04 2.153e-04 3.103e-04 5.194e-03
 3.696e-01 4.210e-04 8.175e-05 1.805e-01 1.529e-01 2.371e-04 3.593e-03
 6.136e-05 1.729e-03 4.267e-01 5.799e-01 2.072e-04 6.430e-04 3.169e-02
 4.077e-02 7.764e-02 2.020e-04 1.125e-01 3.047e+00 1.651e-02 8.033e-03
 3.454e-04 1.212e+00 1.069e-01 1.585e+00 3.144e-02 1.628e-02 2.119e-02
 1.264e-01 1.398e+00 2.136e-04 1.127e+00 1.231e+00 3.196e-03 6.293e-03
 3.254e-04 3.368e+00 3.864e-01 4.339e-01 1.576e-01 1.130e-02 5.624e-01
 8.923e-02 3.157e-04 3.306e-04 7.760e-05 1.109e-01 7.500e-05 1.536e-04
 2.248e-04 2.530e-04 1.060e-01 8.126e-02 2.152e-03 4.216e-03 1.418e-04]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9805555555555556, 0.9709424613206593)
cuda
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 38.95030310869804 clip 11.646765979092327
total norm for a microbatch 41.43773368349206 clip 12.832099025143812
total norm for a microbatch 35.49950937152788 clip 21.86182732016133
total norm for a microbatch 24.80859100931467 clip 31.60235472412892
total norm for a microbatch 39.11980170852868 clip 28.27434584058586
total norm for a microbatch 28.801421285835804 clip 27.610894345129907
total norm for a microbatch 38.67476876737611 clip 25.012506057086853
total norm for a microbatch 24.63737068033875 clip 25.50169296434716
total norm for a microbatch 20.22963890869061 clip 27.090111506074958
cuda
Objective function 20.52 = squared loss an data 18.34 + 0.5*rho*h**2 1.537989 + alpha*h 0.000000 + L2reg 0.49 + L1reg 0.15 ; SHD = 51 ; DAG False
Proportion of microbatches that were clipped  0.8438911265714635
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.7538466410862128
iteration 1 in outer loop, alpha = 1.7538466410862128, rho = 1.0, h = 1.7538466410862128
cuda
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 23.60 = squared loss an data 18.34 + 0.5*rho*h**2 1.537989 + alpha*h 3.075978 + L2reg 0.49 + L1reg 0.15 ; SHD = 51 ; DAG False
total norm for a microbatch 23.837788785245476 clip 1.3194894913699642
total norm for a microbatch 113.81576554041975 clip 1.6060315521341
total norm for a microbatch 38.3503249622529 clip 1.748764085087888
total norm for a microbatch 37.25148061373051 clip 6.867741606557044
total norm for a microbatch 99.30039097056672 clip 34.41077196774934
total norm for a microbatch 22.566474160605935 clip 34.41077196774934
total norm for a microbatch 22.435678966468416 clip 36.66799660908874
total norm for a microbatch 91.11510553297738 clip 35.00408565166857
total norm for a microbatch 105.15231689194363 clip 38.56916502927112
total norm for a microbatch 35.384022861262544 clip 37.14635109431974
cuda
Objective function 14.81 = squared loss an data 11.21 + 0.5*rho*h**2 0.629824 + alpha*h 1.968413 + L2reg 0.85 + L1reg 0.15 ; SHD = 41 ; DAG False
Proportion of microbatches that were clipped  0.8562874251497006
iteration 1 in inner loop, alpha 1.7538466410862128 rho 1.0 h 1.1223403919486046
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 20.48 = squared loss an data 11.21 + 0.5*rho*h**2 6.298240 + alpha*h 1.968413 + L2reg 0.85 + L1reg 0.15 ; SHD = 41 ; DAG False
total norm for a microbatch 65.46406276378383 clip 1.0938275778638167
total norm for a microbatch 34.62805189893817 clip 8.755562416941816
total norm for a microbatch 43.46541448331789 clip 27.49621501468779
total norm for a microbatch 60.40776651486374 clip 31.854060401846546
total norm for a microbatch 45.5806662321217 clip 49.27062847634726
total norm for a microbatch 70.91220820606208 clip 47.787776217125334
total norm for a microbatch 61.44970778096987 clip 49.27504709208374
total norm for a microbatch 44.82553118446252 clip 46.79869452975061
total norm for a microbatch 48.350854853107904 clip 49.371381696228745
cuda
Objective function 15.67 = squared loss an data 12.14 + 0.5*rho*h**2 1.420938 + alpha*h 0.934962 + L2reg 1.04 + L1reg 0.14 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.8561753478635636
iteration 2 in inner loop, alpha 1.7538466410862128 rho 10.0 h 0.5330924747516512
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 28.46 = squared loss an data 12.14 + 0.5*rho*h**2 14.209379 + alpha*h 0.934962 + L2reg 1.04 + L1reg 0.14 ; SHD = 40 ; DAG True
total norm for a microbatch 113.01739173523751 clip 1.2940664649871105
total norm for a microbatch 86.32346224346215 clip 2.82385577227029
total norm for a microbatch 52.266559124448314 clip 4.783423642450896
total norm for a microbatch 47.58326459912025 clip 54.4552187141098
total norm for a microbatch 164.54515045792266 clip 58.55241387482212
total norm for a microbatch 68.8331851891577 clip 60.230138818175796
total norm for a microbatch 51.541662578994256 clip 59.28352929848639
total norm for a microbatch 92.82898958417225 clip 57.8452814360125
total norm for a microbatch 74.48125636636975 clip 57.8452814360125
total norm for a microbatch 101.55278470269504 clip 56.23582429256854
total norm for a microbatch 64.15459295136218 clip 54.21821990152686
cuda
Objective function 16.95 = squared loss an data 13.80 + 0.5*rho*h**2 1.566222 + alpha*h 0.310408 + L2reg 1.15 + L1reg 0.12 ; SHD = 34 ; DAG True
Proportion of microbatches that were clipped  0.8673420164013507
iteration 3 in inner loop, alpha 1.7538466410862128 rho 100.0 h 0.17698713510253938
iteration 2 in outer loop, alpha = 19.45256015134015, rho = 100.0, h = 0.17698713510253938
cuda
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 20.08 = squared loss an data 13.80 + 0.5*rho*h**2 1.566222 + alpha*h 3.442853 + L2reg 1.15 + L1reg 0.12 ; SHD = 34 ; DAG True
total norm for a microbatch 83.99214216169159 clip 27.93811421563345
total norm for a microbatch 89.84320907528058 clip 33.46954100435916
total norm for a microbatch 70.38326589788039 clip 64.50479221822667
total norm for a microbatch 94.61423153657827 clip 60.87776198210076
total norm for a microbatch 71.79105467956632 clip 61.21490358923291
total norm for a microbatch 133.99918010433916 clip 59.90784971096307
total norm for a microbatch 72.14389674115051 clip 56.41072299678778
total norm for a microbatch 60.40368805367034 clip 57.96738385217153
total norm for a microbatch 82.92143173490868 clip 61.340522845318056
cuda
Objective function 18.06 = squared loss an data 13.75 + 0.5*rho*h**2 0.662651 + alpha*h 2.239413 + L2reg 1.27 + L1reg 0.13 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.866952264381885
iteration 1 in inner loop, alpha 19.45256015134015 rho 100.0 h 0.11512177228746623
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 24.02 = squared loss an data 13.75 + 0.5*rho*h**2 6.626511 + alpha*h 2.239413 + L2reg 1.27 + L1reg 0.13 ; SHD = 39 ; DAG True
total norm for a microbatch 134.685796831556 clip 1.3033700060553304
total norm for a microbatch 50.656070477921055 clip 1.553622412985671
total norm for a microbatch 90.53209295547602 clip 1.7161882334462981
total norm for a microbatch 132.71739692435617 clip 2.0410998053092286
total norm for a microbatch 104.08902105922911 clip 8.767485646731284
total norm for a microbatch 53.42106267997447 clip 10.351037010167929
total norm for a microbatch 78.09355637194436 clip 31.77902601294091
total norm for a microbatch 55.51490561350496 clip 63.80884466841701
total norm for a microbatch 77.8758184140471 clip 65.47740165570174
total norm for a microbatch 122.72348714259005 clip 68.53858952359778
total norm for a microbatch 74.64165385466777 clip 66.07759473840702
total norm for a microbatch 71.72072753343181 clip 68.85664150990128
total norm for a microbatch 86.3064169529044 clip 62.690578072962765
cuda
Objective function 18.40 = squared loss an data 14.73 + 0.5*rho*h**2 1.226078 + alpha*h 0.963276 + L2reg 1.35 + L1reg 0.13 ; SHD = 49 ; DAG True
Proportion of microbatches that were clipped  0.8683510638297872
iteration 2 in inner loop, alpha 19.45256015134015 rho 1000.0 h 0.04951924327600388
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 29.43 = squared loss an data 14.73 + 0.5*rho*h**2 12.260777 + alpha*h 0.963276 + L2reg 1.35 + L1reg 0.13 ; SHD = 49 ; DAG True
total norm for a microbatch 223.44743031887066 clip 65.7206823163498
total norm for a microbatch 121.67500332514098 clip 65.7206823163498
total norm for a microbatch 115.17504655707839 clip 77.20541641466296
total norm for a microbatch 70.46483206201478 clip 81.58533490001602
total norm for a microbatch 61.782243529712154 clip 79.72899980177169
total norm for a microbatch 108.41118968350939 clip 70.19992921211423
total norm for a microbatch 137.595627771388 clip 73.08426545041864
total norm for a microbatch 148.03374356229446 clip 74.79993106800681
total norm for a microbatch 94.68494476290773 clip 75.67638580977946
total norm for a microbatch 121.50977133633742 clip 72.07633330768547
cuda
Objective function 17.52 = squared loss an data 14.39 + 0.5*rho*h**2 1.223951 + alpha*h 0.304350 + L2reg 1.46 + L1reg 0.14 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.8699502608273687
iteration 3 in inner loop, alpha 19.45256015134015 rho 10000.0 h 0.015645771864175373
iteration 3 in outer loop, alpha = 175.91027879309388, rho = 10000.0, h = 0.015645771864175373
cuda
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 19.97 = squared loss an data 14.39 + 0.5*rho*h**2 1.223951 + alpha*h 2.752252 + L2reg 1.46 + L1reg 0.14 ; SHD = 43 ; DAG True
total norm for a microbatch 185.86504840251376 clip 1.5777083448755445
total norm for a microbatch 105.05093310660848 clip 79.7914625182563
total norm for a microbatch 106.56178957595495 clip 88.29099908269188
total norm for a microbatch 118.18637506617294 clip 87.13535378985432
total norm for a microbatch 137.88124606236053 clip 94.0052040839801
total norm for a microbatch 79.0017338633811 clip 87.73029245763558
total norm for a microbatch 78.35966672415331 clip 89.59461952794307
total norm for a microbatch 100.58934856467629 clip 83.31255195568203
total norm for a microbatch 138.66199049739782 clip 81.48746080069473
total norm for a microbatch 118.90813464398543 clip 79.138737110753
total norm for a microbatch 113.42099661387886 clip 80.00360966537681
cuda
Objective function 17.81 = squared loss an data 13.82 + 0.5*rho*h**2 0.521649 + alpha*h 1.796782 + L2reg 1.52 + L1reg 0.15 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.8728429812752417
iteration 1 in inner loop, alpha 175.91027879309388 rho 10000.0 h 0.010214195485369615
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 22.50 = squared loss an data 13.82 + 0.5*rho*h**2 5.216489 + alpha*h 1.796782 + L2reg 1.52 + L1reg 0.15 ; SHD = 46 ; DAG True
total norm for a microbatch 205.80881785247993 clip 1.0
total norm for a microbatch 344.1295379981046 clip 2.764708645463334
total norm for a microbatch 199.79647908452728 clip 170.73046781834674
total norm for a microbatch 169.68825889615178 clip 91.14649494334783
total norm for a microbatch 96.18791717148338 clip 93.21084076184977
total norm for a microbatch 104.17011158398329 clip 93.30231220451375
cuda
Objective function 17.30 = squared loss an data 14.37 + 0.5*rho*h**2 0.620937 + alpha*h 0.619912 + L2reg 1.53 + L1reg 0.15 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.8746528197077648
iteration 2 in inner loop, alpha 175.91027879309388 rho 100000.0 h 0.003524023773927354
iteration 4 in outer loop, alpha = 528.3126561858293, rho = 100000.0, h = 0.003524023773927354
cuda
noise_multiplier  0.75  noise_multiplier_b  3.75  noise_multiplier_delta  0.753778361444409
cuda
Objective function 18.54 = squared loss an data 14.37 + 0.5*rho*h**2 0.620937 + alpha*h 1.861786 + L2reg 1.53 + L1reg 0.15 ; SHD = 46 ; DAG True
total norm for a microbatch 6067.750957001518 clip 1.1774026852066053
total norm for a microbatch 378.44214228869345 clip 3.1927171618395507
total norm for a microbatch 229.9726016675046 clip 25.81189907771033
total norm for a microbatch 264.53342075822366 clip 98.08384993700066
cuda
Objective function 18.50 = squared loss an data 14.54 + 0.5*rho*h**2 0.508068 + alpha*h 1.684097 + L2reg 1.61 + L1reg 0.15 ; SHD = 44 ; DAG True
Proportion of microbatches that were clipped  0.8812751923781605
iteration 1 in inner loop, alpha 528.3126561858293 rho 100000.0 h 0.003187690347299821
iteration 5 in outer loop, alpha = 3716.0030034856504, rho = 1000000.0, h = 0.003187690347299821
Threshold 0.3
[[0.004 0.082 0.379 0.239 0.608 0.075 0.018 0.003 0.024 1.232 0.112 0.275
  0.018 0.005 0.124]
 [0.086 0.006 0.214 0.514 0.689 0.047 0.031 0.011 0.029 0.8   0.452 0.757
  0.017 0.004 0.322]
 [0.011 0.021 0.004 0.099 0.506 0.021 0.008 0.008 0.019 0.095 0.084 0.078
  0.031 0.004 0.035]
 [0.028 0.01  0.052 0.004 0.232 0.021 0.005 0.005 0.027 0.087 0.024 0.079
  0.004 0.002 0.042]
 [0.004 0.003 0.009 0.021 0.005 0.003 0.002 0.002 0.002 0.003 0.001 0.012
  0.001 0.001 0.009]
 [0.114 0.069 0.343 0.206 0.617 0.004 0.014 0.008 0.068 0.9   0.142 0.714
  0.035 0.008 0.071]
 [0.368 0.259 0.584 0.977 0.785 0.402 0.006 0.048 0.105 0.291 0.962 0.447
  0.043 0.015 0.413]
 [1.271 0.411 0.506 0.934 0.69  0.525 0.169 0.004 0.443 1.225 0.297 0.791
  0.082 0.012 0.35 ]
 [0.193 0.134 0.169 0.269 0.41  0.113 0.072 0.012 0.004 0.962 0.748 0.692
  0.013 0.001 0.113]
 [0.004 0.006 0.052 0.064 1.645 0.005 0.007 0.003 0.004 0.005 0.022 0.079
  0.004 0.002 0.056]
 [0.077 0.018 0.071 0.187 2.602 0.042 0.006 0.015 0.007 0.264 0.005 0.128
  0.004 0.002 0.117]
 [0.027 0.007 0.097 0.117 0.288 0.005 0.008 0.005 0.008 0.061 0.051 0.004
  0.008 0.001 0.028]
 [0.193 0.266 0.177 1.282 1.81  0.169 0.108 0.082 0.37  0.74  1.063 0.507
  0.004 0.016 0.445]
 [0.722 1.179 0.807 0.82  1.011 0.5   0.37  0.316 2.866 1.233 0.798 1.448
  0.289 0.006 0.733]
 [0.031 0.015 0.159 0.151 0.362 0.095 0.008 0.021 0.045 0.081 0.039 0.183
  0.006 0.006 0.005]]
[[0.    0.    0.379 0.    0.608 0.    0.    0.    0.    1.232 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.514 0.689 0.    0.    0.    0.    0.8   0.452 0.757
  0.    0.    0.322]
 [0.    0.    0.    0.    0.506 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.343 0.    0.617 0.    0.    0.    0.    0.9   0.    0.714
  0.    0.    0.   ]
 [0.368 0.    0.584 0.977 0.785 0.402 0.    0.    0.    0.    0.962 0.447
  0.    0.    0.413]
 [1.271 0.411 0.506 0.934 0.69  0.525 0.    0.    0.443 1.225 0.    0.791
  0.    0.    0.35 ]
 [0.    0.    0.    0.    0.41  0.    0.    0.    0.    0.962 0.748 0.692
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.645 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.602 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    1.282 1.81  0.    0.    0.    0.37  0.74  1.063 0.507
  0.    0.    0.445]
 [0.722 1.179 0.807 0.82  1.011 0.5   0.37  0.316 2.866 1.233 0.798 1.448
  0.    0.    0.733]
 [0.    0.    0.    0.    0.362 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.6271186440677966, 'tpr': 0.7333333333333333, 'fpr': 0.49333333333333335, 'f1': 0.49438202247191015, 'shd': 44, 'npred': 59, 'ntrue': 30}
[8.169e-02 3.788e-01 2.386e-01 6.079e-01 7.452e-02 1.843e-02 3.338e-03
 2.448e-02 1.232e+00 1.117e-01 2.752e-01 1.799e-02 4.824e-03 1.244e-01
 8.642e-02 2.143e-01 5.137e-01 6.891e-01 4.677e-02 3.137e-02 1.145e-02
 2.876e-02 8.001e-01 4.523e-01 7.571e-01 1.659e-02 4.017e-03 3.218e-01
 1.074e-02 2.132e-02 9.941e-02 5.063e-01 2.144e-02 8.376e-03 8.201e-03
 1.867e-02 9.508e-02 8.449e-02 7.814e-02 3.117e-02 3.724e-03 3.532e-02
 2.754e-02 9.576e-03 5.202e-02 2.321e-01 2.133e-02 5.383e-03 5.126e-03
 2.659e-02 8.675e-02 2.404e-02 7.873e-02 4.193e-03 2.343e-03 4.216e-02
 3.701e-03 2.694e-03 9.204e-03 2.054e-02 3.016e-03 2.316e-03 2.149e-03
 2.372e-03 2.824e-03 1.430e-03 1.179e-02 1.048e-03 7.772e-04 8.777e-03
 1.142e-01 6.930e-02 3.429e-01 2.064e-01 6.171e-01 1.354e-02 8.498e-03
 6.794e-02 9.002e-01 1.416e-01 7.141e-01 3.479e-02 8.432e-03 7.074e-02
 3.681e-01 2.594e-01 5.841e-01 9.774e-01 7.851e-01 4.022e-01 4.752e-02
 1.054e-01 2.908e-01 9.617e-01 4.473e-01 4.263e-02 1.527e-02 4.132e-01
 1.271e+00 4.105e-01 5.058e-01 9.338e-01 6.898e-01 5.246e-01 1.690e-01
 4.431e-01 1.225e+00 2.965e-01 7.909e-01 8.183e-02 1.201e-02 3.505e-01
 1.933e-01 1.338e-01 1.695e-01 2.694e-01 4.102e-01 1.128e-01 7.179e-02
 1.242e-02 9.618e-01 7.484e-01 6.915e-01 1.312e-02 1.086e-03 1.127e-01
 4.460e-03 5.804e-03 5.220e-02 6.397e-02 1.645e+00 5.108e-03 6.826e-03
 2.768e-03 4.246e-03 2.241e-02 7.900e-02 3.650e-03 1.818e-03 5.561e-02
 7.690e-02 1.758e-02 7.074e-02 1.866e-01 2.602e+00 4.157e-02 5.683e-03
 1.547e-02 6.801e-03 2.640e-01 1.285e-01 3.893e-03 2.255e-03 1.169e-01
 2.695e-02 6.618e-03 9.737e-02 1.169e-01 2.883e-01 5.397e-03 8.208e-03
 4.923e-03 7.970e-03 6.080e-02 5.113e-02 7.549e-03 1.222e-03 2.771e-02
 1.929e-01 2.661e-01 1.773e-01 1.282e+00 1.810e+00 1.691e-01 1.077e-01
 8.187e-02 3.695e-01 7.399e-01 1.063e+00 5.070e-01 1.600e-02 4.447e-01
 7.222e-01 1.179e+00 8.072e-01 8.198e-01 1.011e+00 5.004e-01 3.703e-01
 3.158e-01 2.866e+00 1.233e+00 7.980e-01 1.448e+00 2.892e-01 7.331e-01
 3.101e-02 1.452e-02 1.592e-01 1.509e-01 3.620e-01 9.485e-02 7.832e-03
 2.115e-02 4.541e-02 8.094e-02 3.946e-02 1.827e-01 6.403e-03 6.326e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8533333333333333, 0.5950595673823599)
Iterations 1110
Achieves (7.5436377211129475, 1e-05)-DP
