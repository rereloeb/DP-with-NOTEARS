samples  5000  graph  20 80 ER mlp  minibatch size  100  noise  0.6  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7605038536910733
iteration 1 in outer loop, alpha = 1.7605038536910733, rho = 1.0, h = 1.7605038536910733
cuda
iteration 1 in inner loop,alpha 1.7605038536910733 rho 1.0 h 1.174322189622707
iteration 2 in inner loop,alpha 1.7605038536910733 rho 10.0 h 0.5482301631666644
iteration 3 in inner loop,alpha 1.7605038536910733 rho 100.0 h 0.18735921368297426
iteration 2 in outer loop, alpha = 20.4964252219885, rho = 100.0, h = 0.18735921368297426
cuda
iteration 1 in inner loop,alpha 20.4964252219885 rho 100.0 h 0.11039514841012732
iteration 2 in inner loop,alpha 20.4964252219885 rho 1000.0 h 0.044404271181637256
iteration 3 in outer loop, alpha = 64.90069640362576, rho = 1000.0, h = 0.044404271181637256
cuda
iteration 1 in inner loop,alpha 64.90069640362576 rho 1000.0 h 0.023329071779293997
iteration 2 in inner loop,alpha 64.90069640362576 rho 10000.0 h 0.007601805135323048
iteration 4 in outer loop, alpha = 140.91874775685625, rho = 10000.0, h = 0.007601805135323048
cuda
iteration 1 in inner loop,alpha 140.91874775685625 rho 10000.0 h 0.004078570196483611
iteration 2 in inner loop,alpha 140.91874775685625 rho 100000.0 h 0.0013011795023842865
iteration 5 in outer loop, alpha = 271.0366979952849, rho = 100000.0, h = 0.0013011795023842865
cuda
iteration 1 in inner loop,alpha 271.0366979952849 rho 100000.0 h 0.0006987924888299801
iteration 6 in outer loop, alpha = 969.829186825265, rho = 1000000.0, h = 0.0006987924888299801
Threshold 0.3
[[0.001 3.137 0.085 0.246 1.119 1.136 0.316 0.    1.96  1.166 0.119 1.126
  0.216 0.002 0.352 0.543 0.282 1.223 1.301 0.468]
 [0.    0.002 0.025 0.461 0.546 1.67  1.826 0.    0.579 0.45  0.083 0.675
  0.321 0.001 1.211 0.425 0.345 0.868 0.545 1.933]
 [0.002 0.009 0.001 0.054 0.487 0.566 1.552 0.    0.177 0.459 0.102 0.193
  1.334 0.    1.527 0.368 0.638 0.    1.228 0.338]
 [0.    0.    0.    0.021 0.001 0.003 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.197 0.172 0.189 0.    0.001 0.732]
 [0.    0.    0.    0.555 0.004 0.318 0.199 0.    0.    0.386 0.055 0.001
  0.133 0.    0.214 0.365 1.284 0.    0.282 1.074]
 [0.    0.    0.    0.47  0.002 0.003 0.002 0.    0.    0.    0.    0.
  0.    0.    0.358 1.679 1.916 0.    0.242 0.308]
 [0.    0.    0.    0.198 0.001 0.227 0.002 0.    0.    0.009 0.    0.
  0.    0.    0.329 0.357 0.18  0.    0.165 0.204]
 [2.571 0.14  0.    0.097 0.729 0.222 0.734 0.001 0.292 0.446 0.135 0.38
  1.974 0.698 0.217 0.17  0.206 1.141 0.465 0.456]
 [0.    0.    0.002 0.357 1.523 0.922 1.301 0.    0.001 0.855 0.465 0.296
  1.761 0.    1.245 1.105 0.841 0.    1.354 0.694]
 [0.    0.    0.    0.104 0.019 2.224 0.121 0.    0.    0.008 0.001 0.002
  0.001 0.    1.184 1.294 1.795 0.    0.888 0.223]
 [0.    0.    0.    0.932 0.004 0.586 2.334 0.    0.    0.25  0.001 0.001
  0.    0.    1.529 0.496 1.043 0.    0.316 1.354]
 [0.    0.    0.    0.15  0.409 0.258 1.694 0.    0.    0.221 0.11  0.001
  0.088 0.    0.17  0.906 0.282 0.    0.128 0.234]
 [0.    0.    0.    0.104 0.004 1.411 1.175 0.    0.    0.165 2.16  0.002
  0.001 0.    0.307 0.343 0.322 0.    0.496 1.104]
 [0.15  0.037 0.032 0.735 0.507 1.666 0.295 0.001 0.279 0.344 1.733 0.532
  2.071 0.001 0.432 0.349 1.027 0.354 0.716 2.016]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.314 0.002 0.    0.001 0.002]
 [0.    0.    0.    0.003 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.001 0.005 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.002 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.131 1.155 0.004 0.    0.001 0.002]
 [0.    0.    0.031 0.053 0.746 0.302 1.412 0.    3.194 0.558 0.097 0.727
  0.061 0.    0.34  0.236 0.191 0.001 0.404 0.261]
 [0.    0.    0.    0.722 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.35  0.974 0.374 0.    0.001 0.265]
 [0.    0.    0.    0.003 0.    0.    0.001 0.    0.    0.001 0.    0.
  0.    0.    0.121 1.8   0.273 0.    0.001 0.002]]
[[0.    3.137 0.    0.    1.119 1.136 0.316 0.    1.96  1.166 0.    1.126
  0.    0.    0.352 0.543 0.    1.223 1.301 0.468]
 [0.    0.    0.    0.461 0.546 1.67  1.826 0.    0.579 0.45  0.    0.675
  0.321 0.    1.211 0.425 0.345 0.868 0.545 1.933]
 [0.    0.    0.    0.    0.487 0.566 1.552 0.    0.    0.459 0.    0.
  1.334 0.    1.527 0.368 0.638 0.    1.228 0.338]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.732]
 [0.    0.    0.    0.555 0.    0.318 0.    0.    0.    0.386 0.    0.
  0.    0.    0.    0.365 1.284 0.    0.    1.074]
 [0.    0.    0.    0.47  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.358 1.679 1.916 0.    0.    0.308]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.329 0.357 0.    0.    0.    0.   ]
 [2.571 0.    0.    0.    0.729 0.    0.734 0.    0.    0.446 0.    0.38
  1.974 0.698 0.    0.    0.    1.141 0.465 0.456]
 [0.    0.    0.    0.357 1.523 0.922 1.301 0.    0.    0.855 0.465 0.
  1.761 0.    1.245 1.105 0.841 0.    1.354 0.694]
 [0.    0.    0.    0.    0.    2.224 0.    0.    0.    0.    0.    0.
  0.    0.    1.184 1.294 1.795 0.    0.888 0.   ]
 [0.    0.    0.    0.932 0.    0.586 2.334 0.    0.    0.    0.    0.
  0.    0.    1.529 0.496 1.043 0.    0.316 1.354]
 [0.    0.    0.    0.    0.409 0.    1.694 0.    0.    0.    0.    0.
  0.    0.    0.    0.906 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.411 1.175 0.    0.    0.    2.16  0.
  0.    0.    0.307 0.343 0.322 0.    0.496 1.104]
 [0.    0.    0.    0.735 0.507 1.666 0.    0.    0.    0.344 1.733 0.532
  2.071 0.    0.432 0.349 1.027 0.354 0.716 2.016]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.314 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.131 1.155 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.746 0.302 1.412 0.    3.194 0.558 0.    0.727
  0.    0.    0.34  0.    0.    0.    0.404 0.   ]
 [0.    0.    0.    0.722 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.35  0.974 0.374 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.8   0.    0.    0.    0.   ]]
{'fdr': 0.44, 'tpr': 0.875, 'fpr': 0.5, 'f1': 0.6829268292682927, 'shd': 60, 'npred': 125, 'ntrue': 80}
[3.137e+00 8.492e-02 2.455e-01 1.119e+00 1.136e+00 3.158e-01 2.367e-04
 1.960e+00 1.166e+00 1.186e-01 1.126e+00 2.164e-01 1.526e-03 3.518e-01
 5.428e-01 2.824e-01 1.223e+00 1.301e+00 4.685e-01 4.082e-04 2.544e-02
 4.614e-01 5.455e-01 1.670e+00 1.826e+00 1.678e-04 5.791e-01 4.498e-01
 8.335e-02 6.748e-01 3.205e-01 1.289e-03 1.211e+00 4.253e-01 3.450e-01
 8.684e-01 5.449e-01 1.933e+00 2.117e-03 8.931e-03 5.430e-02 4.870e-01
 5.663e-01 1.552e+00 6.411e-05 1.769e-01 4.589e-01 1.015e-01 1.926e-01
 1.334e+00 4.532e-04 1.527e+00 3.680e-01 6.384e-01 4.647e-04 1.228e+00
 3.380e-01 7.105e-05 1.561e-05 8.936e-05 8.878e-04 3.143e-03 1.050e-03
 6.769e-05 1.011e-04 2.333e-03 4.401e-04 2.385e-04 8.092e-05 2.459e-06
 1.973e-01 1.718e-01 1.889e-01 2.237e-05 1.372e-03 7.319e-01 3.956e-06
 7.406e-06 1.244e-04 5.545e-01 3.180e-01 1.995e-01 1.729e-05 1.663e-04
 3.857e-01 5.469e-02 1.406e-03 1.331e-01 7.122e-05 2.136e-01 3.646e-01
 1.284e+00 3.844e-05 2.823e-01 1.074e+00 8.339e-05 1.949e-04 7.379e-05
 4.699e-01 2.275e-03 1.536e-03 4.708e-05 7.414e-05 4.406e-04 6.317e-05
 3.885e-04 2.481e-04 3.448e-05 3.584e-01 1.679e+00 1.916e+00 1.312e-05
 2.416e-01 3.076e-01 5.592e-05 8.934e-05 5.193e-06 1.976e-01 1.130e-03
 2.271e-01 2.912e-05 1.683e-05 8.846e-03 1.338e-04 2.158e-04 8.796e-05
 3.960e-06 3.286e-01 3.572e-01 1.800e-01 1.850e-06 1.647e-01 2.043e-01
 2.571e+00 1.396e-01 2.796e-04 9.735e-02 7.289e-01 2.218e-01 7.341e-01
 2.916e-01 4.461e-01 1.345e-01 3.801e-01 1.974e+00 6.979e-01 2.169e-01
 1.701e-01 2.060e-01 1.141e+00 4.651e-01 4.557e-01 1.063e-05 1.136e-05
 2.168e-03 3.568e-01 1.523e+00 9.219e-01 1.301e+00 1.014e-05 8.546e-01
 4.654e-01 2.961e-01 1.761e+00 2.693e-05 1.245e+00 1.105e+00 8.406e-01
 5.183e-05 1.354e+00 6.936e-01 2.823e-05 3.542e-05 1.370e-04 1.042e-01
 1.897e-02 2.224e+00 1.213e-01 5.255e-05 1.688e-04 5.737e-04 1.630e-03
 5.708e-04 1.397e-04 1.184e+00 1.294e+00 1.795e+00 3.360e-05 8.881e-01
 2.233e-01 7.679e-06 1.531e-05 6.883e-05 9.318e-01 3.717e-03 5.855e-01
 2.334e+00 2.126e-05 7.182e-05 2.499e-01 6.459e-04 2.194e-04 4.683e-05
 1.529e+00 4.958e-01 1.043e+00 1.427e-05 3.164e-01 1.354e+00 2.790e-05
 4.699e-05 3.114e-04 1.501e-01 4.087e-01 2.585e-01 1.694e+00 2.385e-05
 2.848e-04 2.208e-01 1.095e-01 8.771e-02 5.459e-05 1.702e-01 9.057e-01
 2.818e-01 1.753e-04 1.277e-01 2.341e-01 7.245e-06 1.956e-05 1.275e-04
 1.044e-01 3.764e-03 1.411e+00 1.175e+00 3.023e-05 2.394e-04 1.649e-01
 2.160e+00 2.494e-03 8.420e-05 3.068e-01 3.429e-01 3.222e-01 2.147e-05
 4.961e-01 1.104e+00 1.499e-01 3.694e-02 3.209e-02 7.354e-01 5.075e-01
 1.666e+00 2.948e-01 6.550e-04 2.794e-01 3.445e-01 1.733e+00 5.318e-01
 2.071e+00 4.317e-01 3.491e-01 1.027e+00 3.541e-01 7.160e-01 2.016e+00
 4.174e-05 6.488e-05 3.183e-05 3.743e-03 1.065e-03 6.645e-04 4.193e-04
 2.717e-05 4.848e-06 4.110e-05 8.673e-05 2.615e-04 5.819e-05 1.140e-05
 3.137e-01 2.256e-03 1.886e-06 6.132e-04 1.968e-03 2.706e-05 2.265e-05
 1.807e-05 2.685e-03 6.325e-04 1.111e-04 3.675e-04 2.594e-05 2.305e-05
 2.444e-04 8.756e-05 6.890e-05 2.354e-05 1.292e-05 7.906e-04 1.057e-03
 5.676e-06 6.890e-04 2.429e-04 4.735e-05 8.701e-05 2.745e-05 2.360e-03
 8.562e-04 5.523e-04 1.147e-03 3.734e-05 2.752e-05 5.144e-05 8.052e-05
 1.070e-04 2.794e-05 2.655e-05 1.131e+00 1.155e+00 5.128e-06 7.796e-04
 1.557e-03 9.214e-05 3.663e-06 3.067e-02 5.281e-02 7.462e-01 3.019e-01
 1.412e+00 8.286e-05 3.194e+00 5.583e-01 9.738e-02 7.266e-01 6.107e-02
 1.420e-04 3.395e-01 2.359e-01 1.910e-01 4.042e-01 2.608e-01 6.061e-05
 1.535e-05 1.493e-04 7.216e-01 1.267e-03 1.181e-03 1.125e-03 4.324e-05
 1.571e-04 5.506e-04 1.568e-04 3.372e-04 1.351e-04 2.210e-05 3.502e-01
 9.737e-01 3.744e-01 1.761e-05 2.650e-01 3.680e-05 7.684e-05 6.499e-05
 2.785e-03 2.136e-04 3.806e-04 5.628e-04 1.788e-05 7.355e-05 1.034e-03
 3.292e-04 1.486e-04 2.048e-04 6.375e-05 1.207e-01 1.800e+00 2.733e-01
 1.210e-05 6.339e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.9362916666666666, 0.8864349112551633)
cuda
4420
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
||w||^2 0.28253923600647224
exp ma of ||w||^2 58.81268691742522
||w|| 0.5315441994853036
exp ma of ||w|| 0.5805473812613043
||w||^2 0.20815607281427045
exp ma of ||w||^2 0.23182706656697438
||w|| 0.456241244096005
exp ma of ||w|| 0.4717282672858525
||w||^2 0.15405337639315336
exp ma of ||w||^2 0.22745162862708754
||w|| 0.39249633933726485
exp ma of ||w|| 0.46872695470142933
||w||^2 0.12029041086999169
exp ma of ||w||^2 0.24973694967587154
||w|| 0.3468290801965598
exp ma of ||w|| 0.4848272687943955
||w||^2 0.28696526694309554
exp ma of ||w||^2 0.24254005840792484
||w|| 0.5356913915148306
exp ma of ||w|| 0.47103522144735843
cuda
Objective function 165.43 = squared loss an data 163.14 + 0.5*rho*h**2 1.628623 + alpha*h 0.000000 + L2reg 0.41 + L1reg 0.26 ; SHD = 106 ; DAG False
Proportion of microbatches that were clipped  0.7304264799490771
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.804783937757037
iteration 1 in outer loop, alpha = 1.804783937757037, rho = 1.0, h = 1.804783937757037
cuda
4420
cuda
Objective function 168.69 = squared loss an data 163.14 + 0.5*rho*h**2 1.628623 + alpha*h 3.257245 + L2reg 0.41 + L1reg 0.26 ; SHD = 106 ; DAG False
||w||^2 354.0727959145134
exp ma of ||w||^2 64976.24954901297
||w|| 18.816822152385704
exp ma of ||w|| 78.06172736123592
||w||^2 227.7470916400779
exp ma of ||w||^2 10332.182521644707
||w|| 15.09129191421589
exp ma of ||w|| 22.677678417097237
||w||^2 0.4565648205647379
exp ma of ||w||^2 0.4063325533596706
||w|| 0.6756958047559107
exp ma of ||w|| 0.6066583115996457
||w||^2 0.2961725546711981
exp ma of ||w||^2 0.43576492423130453
||w|| 0.5442173781414905
exp ma of ||w|| 0.6248722382013443
||w||^2 0.14771374197695486
exp ma of ||w||^2 0.3952422378942718
||w|| 0.3843354550089737
exp ma of ||w|| 0.5897284876986638
cuda
Objective function 61.94 = squared loss an data 57.31 + 0.5*rho*h**2 1.011280 + alpha*h 2.566705 + L2reg 0.81 + L1reg 0.25 ; SHD = 95 ; DAG False
Proportion of microbatches that were clipped  0.7398637690561142
iteration 1 in inner loop, alpha 1.804783937757037 rho 1.0 h 1.4221675216063119
4420
cuda
Objective function 71.04 = squared loss an data 57.31 + 0.5*rho*h**2 10.112802 + alpha*h 2.566705 + L2reg 0.81 + L1reg 0.25 ; SHD = 95 ; DAG False
||w||^2 3994277.9918666
exp ma of ||w||^2 36501045.93941982
||w|| 1998.5689860163945
exp ma of ||w|| 4017.7664998698287
||w||^2 0.4520090109529941
exp ma of ||w||^2 1.3091899803150555
||w|| 0.6723161540175828
exp ma of ||w|| 0.9199003880166732
||w||^2 0.973747601602263
exp ma of ||w||^2 0.8720975518642109
||w|| 0.9867865025436166
exp ma of ||w|| 0.8788473282201458
||w||^2 0.8451985223561573
exp ma of ||w||^2 0.9419827260110365
||w|| 0.9193467911273511
exp ma of ||w|| 0.9139401433648099
||w||^2 0.7451227455408754
exp ma of ||w||^2 0.9877566513694516
||w|| 0.8632049267357522
exp ma of ||w|| 0.9309818459295887
cuda
Objective function 42.16 = squared loss an data 37.06 + 0.5*rho*h**2 2.520415 + alpha*h 1.281375 + L2reg 1.09 + L1reg 0.21 ; SHD = 85 ; DAG False
Proportion of microbatches that were clipped  0.7607978639861788
iteration 2 in inner loop, alpha 1.804783937757037 rho 10.0 h 0.7099879965031803
4420
cuda
Objective function 64.84 = squared loss an data 37.06 + 0.5*rho*h**2 25.204148 + alpha*h 1.281375 + L2reg 1.09 + L1reg 0.21 ; SHD = 85 ; DAG False
||w||^2 55322070508.27403
exp ma of ||w||^2 23322673957.62703
||w|| 235206.44231881498
exp ma of ||w|| 134843.24762297582
||w||^2 1.3451602462479835
exp ma of ||w||^2 1.4556808239847683
||w|| 1.1598104354798604
exp ma of ||w|| 1.145158662635784
||w||^2 1.8962343453873103
exp ma of ||w||^2 1.319560098086502
||w|| 1.377038251243338
exp ma of ||w|| 1.1074086806243688
||w||^2 0.28808886494018343
exp ma of ||w||^2 1.44967389835996
||w|| 0.5367391032337624
exp ma of ||w|| 1.1356427497556563
cuda
Objective function 40.96 = squared loss an data 35.94 + 0.5*rho*h**2 3.116021 + alpha*h 0.450547 + L2reg 1.28 + L1reg 0.17 ; SHD = 77 ; DAG True
Proportion of microbatches that were clipped  0.7714419621934073
iteration 3 in inner loop, alpha 1.804783937757037 rho 100.0 h 0.24964059406498862
iteration 2 in outer loop, alpha = 26.7688433442559, rho = 100.0, h = 0.24964059406498862
cuda
4420
cuda
Objective function 47.20 = squared loss an data 35.94 + 0.5*rho*h**2 3.116021 + alpha*h 6.682590 + L2reg 1.28 + L1reg 0.17 ; SHD = 77 ; DAG True
||w||^2 15899669896.76952
exp ma of ||w||^2 51236069321.71567
||w|| 126093.89317793911
exp ma of ||w|| 202423.65068743157
||w||^2 28491786643.460335
exp ma of ||w||^2 50499887012.29265
||w|| 168795.1025458391
exp ma of ||w|| 201604.37268931553
||w||^2 488799791.4419526
exp ma of ||w||^2 4544317960.546074
||w|| 22108.81705207116
exp ma of ||w|| 54225.14737974459
||w||^2 11669.371777513668
exp ma of ||w||^2 468238.43844034657
||w|| 108.02486647764795
exp ma of ||w|| 226.14840167478587
||w||^2 198.42501341851747
exp ma of ||w||^2 37759.494016518096
||w|| 14.086341378034165
exp ma of ||w|| 42.63887602334416
||w||^2 2.2760964607879832
exp ma of ||w||^2 66.18046289413253
||w|| 1.5086737423273406
exp ma of ||w|| 2.160166641719369
||w||^2 2.003517636057177
exp ma of ||w||^2 11.96502452351714
||w|| 1.4154566881601065
exp ma of ||w|| 1.6222863772950158
||w||^2 1.867755472489329
exp ma of ||w||^2 1.9465331112481687
||w|| 1.3666585061709193
exp ma of ||w|| 1.3401117242613643
||w||^2 1.124217254934525
exp ma of ||w||^2 1.8905901483348864
||w|| 1.0602911180117114
exp ma of ||w|| 1.3107698103762462
||w||^2 0.9731965432549284
exp ma of ||w||^2 1.875405272325307
||w|| 0.9865072444006321
exp ma of ||w|| 1.3077534764341887
cuda
Objective function 43.37 = squared loss an data 35.68 + 0.5*rho*h**2 1.487417 + alpha*h 4.617012 + L2reg 1.42 + L1reg 0.17 ; SHD = 70 ; DAG False
Proportion of microbatches that were clipped  0.7685653664109678
iteration 1 in inner loop, alpha 26.7688433442559 rho 100.0 h 0.17247708303736076
4420
cuda
Objective function 56.76 = squared loss an data 35.68 + 0.5*rho*h**2 14.874172 + alpha*h 4.617012 + L2reg 1.42 + L1reg 0.17 ; SHD = 70 ; DAG False
||w||^2 1842139491.3551712
exp ma of ||w||^2 11168196459.038862
||w|| 42920.15250852647
exp ma of ||w|| 89653.7456921673
||w||^2 7432.243676931068
exp ma of ||w||^2 790130.5331674734
||w|| 86.21046152834973
exp ma of ||w|| 344.62024204018337
||w||^2 13.410560868415189
exp ma of ||w||^2 97.16489473974336
||w|| 3.6620432641375484
exp ma of ||w|| 2.706482427260793
cuda
Objective function 43.71 = squared loss an data 37.51 + 0.5*rho*h**2 2.589757 + alpha*h 1.926523 + L2reg 1.54 + L1reg 0.15 ; SHD = 68 ; DAG True
Proportion of microbatches that were clipped  0.7803283915192093
iteration 2 in inner loop, alpha 26.7688433442559 rho 1000.0 h 0.07196883752534688
4420
cuda
Objective function 67.02 = squared loss an data 37.51 + 0.5*rho*h**2 25.897568 + alpha*h 1.926523 + L2reg 1.54 + L1reg 0.15 ; SHD = 68 ; DAG True
||w||^2 38456.49199332542
exp ma of ||w||^2 3937568.5579607543
||w|| 196.10326869617808
exp ma of ||w|| 938.9125410713706
||w||^2 5.000983176853777
exp ma of ||w||^2 2.8619841282968452
||w|| 2.2362878117214198
exp ma of ||w|| 1.639051606599426
||w||^2 3.102520232254756
exp ma of ||w||^2 2.8722724538412256
||w|| 1.761397238630388
exp ma of ||w|| 1.637902399127732
||w||^2 2.272380929459004
exp ma of ||w||^2 2.581720683721943
||w|| 1.5074418494452793
exp ma of ||w|| 1.5466802129754735
||w||^2 4.829730014496943
exp ma of ||w||^2 2.3462941538066775
||w|| 2.197664672896423
exp ma of ||w|| 1.469239458257364
cuda
Objective function 44.63 = squared loss an data 38.67 + 0.5*rho*h**2 3.454890 + alpha*h 0.703658 + L2reg 1.67 + L1reg 0.13 ; SHD = 66 ; DAG True
Proportion of microbatches that were clipped  0.7823348942354271
iteration 3 in inner loop, alpha 26.7688433442559 rho 10000.0 h 0.026286461084481516
iteration 3 in outer loop, alpha = 289.6334541890711, rho = 10000.0, h = 0.026286461084481516
cuda
4420
cuda
Objective function 51.53 = squared loss an data 38.67 + 0.5*rho*h**2 3.454890 + alpha*h 7.613439 + L2reg 1.67 + L1reg 0.13 ; SHD = 66 ; DAG True
||w||^2 429958968480.2347
exp ma of ||w||^2 241730125180.35587
||w|| 655712.5654433005
exp ma of ||w|| 240895.59693181422
||w||^2 132100954974.46373
exp ma of ||w||^2 229187398541.93552
||w|| 363456.9506481665
exp ma of ||w|| 426257.7525076265
||w||^2 2.6517068469740654
exp ma of ||w||^2 3.8758189883224548
||w|| 1.628406229100732
exp ma of ||w|| 1.8826424970318003
||w||^2 2.788556465315057
exp ma of ||w||^2 3.146703463798556
||w|| 1.6698971421363225
exp ma of ||w|| 1.7153317628706184
||w||^2 1.665235626461313
exp ma of ||w||^2 2.8086067384403615
||w|| 1.290440090225545
exp ma of ||w|| 1.619910373925245
cuda
Objective function 46.55 = squared loss an data 38.69 + 0.5*rho*h**2 1.289630 + alpha*h 4.651535 + L2reg 1.79 + L1reg 0.13 ; SHD = 65 ; DAG True
Proportion of microbatches that were clipped  0.7831555484795415
iteration 1 in inner loop, alpha 289.6334541890711 rho 10000.0 h 0.016060076448884075
4420
cuda
Objective function 58.16 = squared loss an data 38.69 + 0.5*rho*h**2 12.896303 + alpha*h 4.651535 + L2reg 1.79 + L1reg 0.13 ; SHD = 65 ; DAG True
||w||^2 1435012472022.752
exp ma of ||w||^2 293160070530.1604
||w|| 1197920.0607814996
exp ma of ||w|| 217189.6973687867
||w||^2 12936836462.2552
exp ma of ||w||^2 67779651751.68024
||w|| 113740.21479782426
exp ma of ||w|| 184051.01053482658
v before min max tensor([[-2.366e-04,  3.654e-03, -4.036e-05,  ..., -6.405e-07,  2.501e-05,
         -2.075e-05],
        [ 6.317e-04,  7.319e-05, -1.029e-05,  ...,  2.450e-05, -3.855e-06,
          1.515e-04],
        [ 2.008e-03, -9.845e-05, -1.011e-05,  ...,  1.178e-04,  3.792e-06,
         -3.521e-05],
        ...,
        [ 4.104e-06,  7.653e-06, -9.130e-05,  ...,  3.466e-05, -2.732e-05,
         -4.655e-04],
        [-2.489e-07, -1.072e-04, -4.555e-05,  ...,  3.188e-05,  8.742e-06,
         -2.543e-04],
        [ 1.617e-05, -7.634e-05, -2.867e-05,  ..., -1.067e-04, -1.286e-04,
          2.118e-03]], device='cuda:0')
v tensor([[1.000e-12, 3.654e-03, 1.000e-12,  ..., 1.000e-12, 2.501e-05,
         1.000e-12],
        [6.317e-04, 7.319e-05, 1.000e-12,  ..., 2.450e-05, 1.000e-12,
         1.515e-04],
        [2.008e-03, 1.000e-12, 1.000e-12,  ..., 1.178e-04, 3.792e-06,
         1.000e-12],
        ...,
        [4.104e-06, 7.653e-06, 1.000e-12,  ..., 3.466e-05, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 3.188e-05, 8.742e-06,
         1.000e-12],
        [1.617e-05, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         2.118e-03]], device='cuda:0')
v before min max tensor([-1.995e-07,  6.175e-07, -5.810e-06, -7.898e-06, -2.178e-06, -4.684e-07,
        -3.576e-06,  3.339e-06,  1.660e-05,  2.034e-04, -6.720e-07, -4.934e-07,
         3.431e-06, -2.044e-05, -1.043e-05, -4.232e-06, -1.479e-05, -1.708e-05,
         2.271e-05, -1.724e-07, -1.164e-04, -9.355e-06,  5.200e-05,  4.683e-06,
         7.451e-08, -8.527e-07, -1.776e-05, -5.369e-06, -2.565e-05, -5.162e-06,
        -2.590e-05, -3.665e-07,  2.421e-07, -1.230e-06,  3.079e-06, -1.148e-06,
        -1.602e-07, -8.492e-07,  8.513e-07, -2.092e-06,  1.364e-06, -1.883e-04,
        -6.772e-06, -1.848e-07, -1.620e-05,  1.467e-05, -5.749e-07,  3.971e-07,
        -4.798e-06, -6.723e-06, -3.400e-06, -1.418e-05, -3.585e-07,  1.867e-06,
        -1.716e-07, -1.519e-06,  8.260e-06, -1.476e-06, -4.636e-06, -4.936e-05,
         7.109e-06, -2.048e-05,  3.680e-05, -2.781e-05, -6.985e-07, -1.374e-05,
        -1.621e-05,  2.663e-05, -1.457e-08,  6.716e-05, -1.160e-07, -1.586e-06,
         1.894e-05,  9.792e-06,  1.110e-04, -5.732e-05, -2.428e-07, -3.781e-06,
        -1.175e-05, -2.766e-06, -1.266e-04,  1.448e-05, -7.316e-07, -4.923e-06,
        -5.494e-06, -2.009e-05, -2.125e-05, -1.406e-07,  2.769e-05,  1.323e-06,
        -8.503e-07,  1.463e-05, -1.844e-06, -1.767e-07, -3.391e-06,  2.832e-05,
         1.240e-05, -3.190e-06, -1.390e-06, -6.073e-06, -1.192e-05, -1.055e-06,
        -6.847e-06,  1.802e-05, -5.552e-07, -2.852e-06, -8.706e-06,  1.420e-05,
        -4.812e-05,  3.161e-07,  7.392e-06,  2.897e-05, -1.985e-06,  7.618e-07,
        -7.988e-06, -1.454e-07, -6.386e-07, -4.825e-07, -5.697e-06,  6.220e-05,
         1.717e-05, -1.976e-05, -1.689e-05, -1.463e-06, -8.841e-07, -9.364e-06,
        -2.900e-05, -1.267e-05, -2.225e-04,  1.178e-05, -5.066e-05, -3.789e-06,
        -3.083e-06,  3.347e-06,  8.603e-06, -3.427e-07, -5.475e-07, -2.133e-05,
        -6.888e-06, -7.351e-06,  2.340e-06, -4.388e-07, -1.768e-05, -6.470e-08,
        -2.780e-05, -1.349e-07, -4.997e-06, -2.189e-05, -1.390e-05, -1.397e-05,
        -2.185e-04, -1.351e-05, -4.948e-06, -9.611e-07, -3.260e-06, -4.449e-08,
         1.658e-04,  7.937e-06, -4.629e-06,  8.386e-05, -7.546e-07,  3.253e-06,
        -3.932e-06, -2.449e-05, -7.763e-06, -6.984e-07, -6.406e-05,  4.705e-05,
        -1.835e-05, -2.725e-07, -8.547e-07,  1.200e-05, -4.489e-05, -7.819e-06,
        -1.490e-05, -8.688e-08, -2.440e-05, -1.909e-05, -1.148e-06, -5.918e-06,
        -7.746e-06,  4.684e-06,  1.206e-05,  9.357e-07, -9.732e-07, -1.859e-06,
        -9.804e-06,  2.543e-04, -8.438e-06, -2.043e-06,  1.515e-04, -9.198e-06,
         8.817e-05, -1.976e-07,  5.368e-05,  2.400e-05, -3.506e-07,  2.823e-05,
        -1.088e-05, -4.546e-05], device='cuda:0')
v tensor([1.000e-12, 6.175e-07, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 3.339e-06, 1.660e-05, 2.034e-04, 1.000e-12, 1.000e-12,
        3.431e-06, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        2.271e-05, 1.000e-12, 1.000e-12, 1.000e-12, 5.200e-05, 4.683e-06,
        7.451e-08, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 2.421e-07, 1.000e-12, 3.079e-06, 1.000e-12,
        1.000e-12, 1.000e-12, 8.513e-07, 1.000e-12, 1.364e-06, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.467e-05, 1.000e-12, 3.971e-07,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.867e-06,
        1.000e-12, 1.000e-12, 8.260e-06, 1.000e-12, 1.000e-12, 1.000e-12,
        7.109e-06, 1.000e-12, 3.680e-05, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.663e-05, 1.000e-12, 6.716e-05, 1.000e-12, 1.000e-12,
        1.894e-05, 9.792e-06, 1.110e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.448e-05, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.769e-05, 1.323e-06,
        1.000e-12, 1.463e-05, 1.000e-12, 1.000e-12, 1.000e-12, 2.832e-05,
        1.240e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.802e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.420e-05,
        1.000e-12, 3.161e-07, 7.392e-06, 2.897e-05, 1.000e-12, 7.618e-07,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 6.220e-05,
        1.717e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.178e-05, 1.000e-12, 1.000e-12,
        1.000e-12, 3.347e-06, 8.603e-06, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 2.340e-06, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.658e-04, 7.937e-06, 1.000e-12, 8.386e-05, 1.000e-12, 3.253e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.705e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.200e-05, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 4.684e-06, 1.206e-05, 9.357e-07, 1.000e-12, 1.000e-12,
        1.000e-12, 2.543e-04, 1.000e-12, 1.000e-12, 1.515e-04, 1.000e-12,
        8.817e-05, 1.000e-12, 5.368e-05, 2.400e-05, 1.000e-12, 2.823e-05,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 1.318e-05],
         [ 8.643e-05],
         [ 1.391e-06],
         [-1.091e-05],
         [-1.190e-05],
         [ 9.749e-07],
         [ 4.915e-06],
         [ 4.859e-07],
         [-2.667e-05],
         [ 1.456e-06]],

        [[-1.925e-07],
         [-1.719e-06],
         [-2.344e-06],
         [-1.360e-06],
         [-4.111e-06],
         [-4.431e-05],
         [-1.719e-06],
         [-6.055e-07],
         [-7.324e-06],
         [-3.603e-06]],

        [[-4.707e-05],
         [ 1.489e-07],
         [-1.764e-06],
         [-3.348e-06],
         [-5.626e-05],
         [ 6.387e-06],
         [ 1.618e-06],
         [ 1.475e-05],
         [-6.824e-06],
         [-9.096e-06]],

        [[-1.058e-06],
         [ 3.533e-07],
         [ 1.106e-04],
         [ 1.165e-07],
         [-8.131e-06],
         [-1.655e-05],
         [-4.234e-06],
         [-1.045e-05],
         [-9.845e-06],
         [-1.106e-06]],

        [[-2.688e-05],
         [ 1.639e-04],
         [ 2.667e-06],
         [-1.449e-06],
         [-1.443e-05],
         [-1.658e-05],
         [-1.314e-05],
         [-2.406e-06],
         [ 7.916e-06],
         [-4.686e-06]],

        [[-3.430e-05],
         [ 1.519e-04],
         [ 3.716e-06],
         [-1.907e-06],
         [-3.513e-05],
         [-3.085e-06],
         [ 4.456e-08],
         [-8.189e-07],
         [-3.113e-06],
         [ 1.024e-05]],

        [[-2.172e-06],
         [ 5.396e-05],
         [ 7.560e-07],
         [-1.480e-05],
         [-3.540e-05],
         [-1.370e-05],
         [-1.055e-05],
         [-8.774e-06],
         [-2.295e-05],
         [-3.318e-06]],

        [[-1.818e-05],
         [-5.751e-05],
         [ 4.083e-05],
         [-7.927e-06],
         [ 1.776e-06],
         [ 6.219e-05],
         [-1.557e-04],
         [-3.048e-07],
         [-8.887e-07],
         [-3.536e-06]],

        [[-2.037e-06],
         [-2.535e-06],
         [-6.763e-06],
         [-4.575e-06],
         [-1.139e-06],
         [-3.521e-07],
         [-2.773e-07],
         [-8.551e-06],
         [-1.922e-06],
         [ 1.416e-05]],

        [[-2.763e-06],
         [-8.344e-06],
         [-2.740e-05],
         [-4.349e-06],
         [-1.935e-05],
         [-3.792e-05],
         [ 5.048e-08],
         [-1.229e-05],
         [-2.507e-05],
         [-1.260e-06]],

        [[-2.843e-07],
         [ 2.511e-08],
         [-2.983e-07],
         [-8.425e-06],
         [-3.018e-05],
         [-6.140e-05],
         [-4.412e-06],
         [-1.041e-06],
         [ 6.682e-05],
         [-1.085e-05]],

        [[-3.952e-05],
         [-1.069e-05],
         [-4.267e-05],
         [-8.240e-07],
         [-1.110e-05],
         [-1.422e-04],
         [-2.436e-05],
         [-1.050e-07],
         [-5.760e-05],
         [-1.126e-07]],

        [[ 2.728e-06],
         [-3.791e-05],
         [-5.483e-07],
         [ 1.516e-04],
         [ 5.437e-06],
         [-2.426e-07],
         [-5.530e-06],
         [ 1.366e-07],
         [ 4.408e-06],
         [-1.554e-05]],

        [[ 4.562e-07],
         [-6.327e-06],
         [ 2.058e-05],
         [-2.106e-05],
         [-2.240e-04],
         [ 2.600e-05],
         [ 6.583e-08],
         [-3.017e-06],
         [-4.104e-06],
         [-1.938e-05]],

        [[-5.614e-05],
         [ 3.504e-07],
         [-1.411e-05],
         [-2.010e-05],
         [ 1.403e-05],
         [-2.361e-07],
         [ 1.230e-05],
         [ 1.789e-05],
         [ 1.833e-06],
         [ 1.234e-05]],

        [[-3.136e-06],
         [-1.053e-06],
         [ 2.945e-06],
         [ 1.493e-05],
         [ 2.578e-06],
         [-7.678e-07],
         [-4.163e-05],
         [ 2.671e-06],
         [-3.150e-05],
         [-2.244e-06]],

        [[ 1.752e-06],
         [ 2.913e-06],
         [-3.048e-05],
         [-1.322e-05],
         [-1.619e-06],
         [-2.807e-07],
         [-5.289e-05],
         [ 6.412e-06],
         [ 5.274e-06],
         [ 6.442e-05]],

        [[ 3.757e-06],
         [ 2.405e-05],
         [-1.034e-06],
         [-3.344e-06],
         [ 5.782e-05],
         [-3.034e-05],
         [ 1.329e-05],
         [-1.449e-05],
         [-1.846e-07],
         [-8.598e-06]],

        [[-7.788e-06],
         [-5.622e-05],
         [-3.439e-05],
         [-1.249e-05],
         [-2.623e-08],
         [-6.320e-07],
         [-1.305e-05],
         [ 5.396e-06],
         [ 4.361e-05],
         [ 1.049e-05]],

        [[-4.444e-05],
         [-9.094e-06],
         [-1.593e-06],
         [-3.262e-06],
         [-5.101e-06],
         [ 2.845e-06],
         [ 6.143e-06],
         [-1.977e-05],
         [-4.641e-05],
         [ 1.330e-06]]], device='cuda:0')
v tensor([[[1.318e-05],
         [8.643e-05],
         [1.391e-06],
         [1.000e-12],
         [1.000e-12],
         [9.749e-07],
         [4.915e-06],
         [4.859e-07],
         [1.000e-12],
         [1.456e-06]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.489e-07],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.387e-06],
         [1.618e-06],
         [1.475e-05],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [3.533e-07],
         [1.106e-04],
         [1.165e-07],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.639e-04],
         [2.667e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.916e-06],
         [1.000e-12]],

        [[1.000e-12],
         [1.519e-04],
         [3.716e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.456e-08],
         [1.000e-12],
         [1.000e-12],
         [1.024e-05]],

        [[1.000e-12],
         [5.396e-05],
         [7.560e-07],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [4.083e-05],
         [1.000e-12],
         [1.776e-06],
         [6.219e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.416e-05]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.048e-08],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [2.511e-08],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.682e-05],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[2.728e-06],
         [1.000e-12],
         [1.000e-12],
         [1.516e-04],
         [5.437e-06],
         [1.000e-12],
         [1.000e-12],
         [1.366e-07],
         [4.408e-06],
         [1.000e-12]],

        [[4.562e-07],
         [1.000e-12],
         [2.058e-05],
         [1.000e-12],
         [1.000e-12],
         [2.600e-05],
         [6.583e-08],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [3.504e-07],
         [1.000e-12],
         [1.000e-12],
         [1.403e-05],
         [1.000e-12],
         [1.230e-05],
         [1.789e-05],
         [1.833e-06],
         [1.234e-05]],

        [[1.000e-12],
         [1.000e-12],
         [2.945e-06],
         [1.493e-05],
         [2.578e-06],
         [1.000e-12],
         [1.000e-12],
         [2.671e-06],
         [1.000e-12],
         [1.000e-12]],

        [[1.752e-06],
         [2.913e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.412e-06],
         [5.274e-06],
         [6.442e-05]],

        [[3.757e-06],
         [2.405e-05],
         [1.000e-12],
         [1.000e-12],
         [5.782e-05],
         [1.000e-12],
         [1.329e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.396e-06],
         [4.361e-05],
         [1.049e-05]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.845e-06],
         [6.143e-06],
         [1.000e-12],
         [1.000e-12],
         [1.330e-06]]], device='cuda:0')
v before min max tensor([[-3.797e-06],
        [ 3.316e-06],
        [-6.267e-07],
        [-1.008e-05],
        [-1.436e-06],
        [-1.519e-05],
        [-1.956e-05],
        [-6.370e-08],
        [-2.582e-06],
        [-1.266e-05],
        [ 8.445e-05],
        [-9.859e-05],
        [-9.424e-07],
        [-9.620e-05],
        [-4.474e-06],
        [-2.875e-06],
        [ 3.997e-07],
        [-3.903e-06],
        [-4.789e-05],
        [ 9.714e-06]], device='cuda:0')
v tensor([[1.000e-12],
        [3.316e-06],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [8.445e-05],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [3.997e-07],
        [1.000e-12],
        [1.000e-12],
        [9.714e-06]], device='cuda:0')
a after update for 1 param tensor([[-6.598e-06, -4.970e-05, -2.280e-06,  ...,  3.066e-06, -4.572e-06,
          1.838e-06],
        [ 1.503e-05, -1.389e-05,  8.654e-07,  ...,  5.122e-06,  4.000e-07,
         -1.817e-05],
        [-3.472e-05, -4.782e-06, -4.206e-06,  ..., -1.639e-05,  1.873e-06,
          6.059e-06],
        ...,
        [-3.791e-06, -6.374e-06, -8.692e-06,  ...,  1.056e-05,  2.002e-06,
         -2.180e-05],
        [ 2.645e-07, -8.755e-06,  3.586e-06,  ..., -5.759e-06,  2.240e-06,
          9.277e-06],
        [-6.529e-06,  3.430e-06, -4.732e-06,  ..., -5.430e-06, -3.320e-06,
         -3.495e-05]], device='cuda:0')
s after update for 1 param tensor([[4.096e-03, 1.943e-02, 4.729e-04,  ..., 3.791e-04, 1.582e-03,
         2.877e-04],
        [8.031e-03, 3.661e-03, 1.183e-04,  ..., 1.566e-03, 4.812e-05,
         3.974e-03],
        [1.427e-02, 4.055e-03, 2.933e-04,  ..., 3.566e-03, 6.181e-04,
         4.159e-04],
        ...,
        [6.688e-04, 9.111e-04, 1.188e-03,  ..., 1.933e-03, 3.212e-04,
         5.744e-03],
        [3.228e-06, 1.471e-03, 5.587e-04,  ..., 1.786e-03, 9.754e-04,
         2.917e-03],
        [1.448e-03, 1.337e-03, 3.305e-04,  ..., 1.231e-03, 1.491e-03,
         1.481e-02]], device='cuda:0')
b after update for 1 param tensor([[0.148, 0.322, 0.050,  ..., 0.045, 0.092, 0.039],
        [0.207, 0.140, 0.025,  ..., 0.092, 0.016, 0.146],
        [0.276, 0.147, 0.040,  ..., 0.138, 0.058, 0.047],
        ...,
        [0.060, 0.070, 0.080,  ..., 0.102, 0.041, 0.175],
        [0.004, 0.089, 0.055,  ..., 0.098, 0.072, 0.125],
        [0.088, 0.085, 0.042,  ..., 0.081, 0.089, 0.281]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 1.639e-07,  1.539e-06,  1.114e-06, -4.710e-07,  2.807e-06,  2.200e-08,
        -3.610e-07,  3.310e-06, -2.213e-06,  2.260e-05,  9.468e-07,  5.781e-07,
        -2.998e-06,  9.011e-07,  3.326e-07,  5.035e-07,  1.180e-06, -1.261e-06,
        -5.621e-06, -1.177e-06, -3.823e-06, -3.309e-06, -1.157e-05, -3.712e-06,
         2.856e-07, -1.982e-06,  2.713e-06, -3.726e-06,  1.696e-06, -1.074e-06,
        -1.943e-07, -6.215e-06,  8.047e-07, -2.004e-07, -1.740e-06, -8.066e-07,
         1.603e-07, -1.135e-07, -1.918e-06, -7.108e-07, -3.189e-06, -1.125e-05,
         7.782e-07, -2.225e-07,  3.035e-06,  5.951e-06, -1.044e-06, -1.416e-06,
         2.041e-06,  1.318e-06, -4.339e-06,  5.082e-07,  5.901e-07, -1.626e-06,
        -3.834e-07, -3.480e-07,  2.725e-06,  1.545e-06, -9.073e-07,  9.677e-07,
         1.152e-06,  4.314e-07, -1.159e-05,  3.804e-07, -1.516e-06,  2.179e-06,
         9.319e-06, -6.885e-06,  2.344e-07,  1.021e-05,  1.478e-06, -7.051e-07,
         7.661e-06,  3.393e-06, -1.398e-05, -3.625e-06, -5.190e-08,  3.985e-07,
         3.830e-07, -9.448e-07,  8.554e-06, -8.035e-06, -2.162e-07, -1.944e-06,
        -3.470e-07,  2.781e-06,  1.159e-06, -2.672e-07,  5.200e-06, -1.651e-05,
        -3.027e-06,  4.131e-06, -6.977e-07, -9.700e-07,  3.477e-06, -5.622e-06,
         4.144e-06,  2.525e-06,  2.673e-06, -4.562e-07,  5.308e-06, -9.821e-07,
        -6.476e-07, -5.608e-06, -6.379e-07,  1.042e-06, -2.556e-06, -4.307e-06,
         2.876e-07, -4.666e-07, -3.497e-06, -4.453e-06, -2.556e-06, -1.065e-06,
        -1.851e-06,  3.743e-07,  9.953e-07,  2.089e-06, -3.949e-06, -8.949e-06,
         4.773e-06, -3.076e-06,  3.886e-06, -3.469e-07, -6.735e-07,  5.197e-06,
        -3.812e-07,  1.917e-06, -1.089e-05,  5.830e-06,  4.142e-06,  1.535e-06,
        -8.152e-06, -1.499e-06, -5.259e-06,  1.501e-07,  3.964e-07, -4.153e-07,
        -1.448e-07, -5.487e-07, -3.214e-06,  4.256e-08,  2.827e-06,  8.409e-07,
        -3.417e-06,  1.708e-06, -2.739e-06, -4.383e-06, -5.991e-07,  6.667e-07,
         9.114e-06,  2.795e-06,  6.247e-06, -3.107e-07,  1.391e-06, -7.821e-07,
        -9.380e-06, -3.796e-06,  9.886e-06, -9.498e-06, -9.012e-07,  1.461e-06,
        -1.904e-06,  2.337e-06, -4.819e-06, -2.905e-07, -9.093e-06,  1.251e-05,
        -4.842e-06, -3.620e-07,  1.481e-06,  7.909e-06,  2.148e-06,  2.651e-06,
        -2.889e-06, -3.508e-07,  2.028e-07,  2.790e-06, -2.908e-07, -3.477e-06,
         1.567e-06,  2.976e-06,  4.637e-06,  8.391e-07, -1.190e-06, -6.351e-06,
         1.762e-06, -2.421e-05, -2.777e-07,  2.444e-07,  1.326e-05, -6.897e-06,
         1.288e-05,  2.457e-06,  1.521e-05,  5.705e-06,  4.190e-08, -9.898e-06,
        -1.359e-05, -1.094e-05], device='cuda:0')
s after update for 1 param tensor([2.362e-06, 2.603e-04, 6.721e-05, 1.007e-04, 5.047e-05, 6.742e-06,
        6.292e-05, 5.845e-04, 1.336e-03, 4.712e-03, 1.587e-05, 5.677e-06,
        5.925e-04, 2.373e-04, 1.225e-04, 5.669e-05, 1.716e-04, 2.932e-04,
        1.509e-03, 2.719e-05, 1.956e-03, 1.547e-04, 2.350e-03, 6.929e-04,
        8.634e-05, 3.619e-05, 2.323e-04, 1.689e-04, 3.370e-04, 6.964e-05,
        3.290e-04, 2.165e-04, 1.556e-04, 1.580e-05, 5.554e-04, 8.404e-05,
        1.882e-06, 9.826e-06, 2.938e-04, 2.490e-05, 3.974e-04, 2.183e-03,
        7.771e-05, 6.129e-06, 1.859e-04, 1.320e-03, 9.493e-06, 1.993e-04,
        5.830e-05, 1.282e-04, 3.716e-04, 2.022e-04, 4.147e-06, 4.322e-04,
        2.011e-06, 1.746e-05, 1.176e-03, 5.105e-05, 6.096e-05, 5.696e-04,
        9.626e-04, 2.455e-04, 2.069e-03, 3.786e-04, 1.713e-05, 3.853e-04,
        3.688e-04, 1.757e-03, 1.882e-06, 2.605e-03, 8.355e-06, 3.145e-05,
        1.382e-03, 9.926e-04, 3.360e-03, 6.744e-04, 3.212e-06, 5.151e-05,
        1.617e-04, 5.491e-05, 1.457e-03, 1.274e-03, 9.024e-06, 6.347e-05,
        7.484e-05, 2.630e-04, 2.878e-04, 2.530e-06, 1.666e-03, 1.392e-03,
        8.856e-05, 1.214e-03, 2.270e-05, 2.531e-05, 6.167e-05, 1.701e-03,
        1.116e-03, 1.245e-04, 3.282e-05, 7.255e-05, 1.847e-04, 1.780e-05,
        7.857e-05, 1.347e-03, 6.801e-06, 4.000e-05, 1.378e-04, 1.195e-03,
        5.993e-04, 1.778e-04, 9.125e-04, 1.725e-03, 1.351e-04, 2.763e-04,
        1.126e-04, 1.882e-06, 1.566e-05, 3.563e-05, 1.434e-04, 2.544e-03,
        1.314e-03, 2.406e-04, 2.615e-04, 2.120e-05, 1.281e-05, 1.461e-04,
        3.872e-04, 1.462e-04, 2.953e-03, 1.230e-03, 1.044e-03, 5.974e-05,
        3.600e-04, 5.788e-04, 9.363e-04, 2.923e-05, 1.039e-05, 2.468e-04,
        9.325e-05, 9.970e-05, 4.847e-04, 9.850e-06, 2.906e-04, 9.522e-06,
        3.443e-04, 4.100e-05, 1.095e-04, 3.382e-04, 2.245e-04, 1.691e-04,
        2.506e-03, 1.645e-04, 1.660e-04, 1.141e-05, 4.672e-05, 2.554e-06,
        4.135e-03, 8.924e-04, 3.561e-04, 2.916e-03, 1.253e-05, 5.714e-04,
        3.288e-04, 3.008e-04, 1.512e-04, 9.006e-06, 1.068e-03, 2.207e-03,
        4.045e-04, 4.362e-06, 2.163e-05, 1.135e-03, 5.418e-04, 1.128e-04,
        1.733e-04, 1.882e-06, 2.866e-04, 2.857e-04, 1.319e-05, 2.868e-04,
        1.095e-04, 6.863e-04, 1.101e-03, 3.064e-04, 2.505e-05, 5.929e-04,
        1.172e-04, 5.153e-03, 1.187e-04, 2.347e-05, 4.304e-03, 2.228e-04,
        2.983e-03, 1.146e-04, 2.383e-03, 1.578e-03, 4.450e-06, 1.734e-03,
        1.424e-03, 7.418e-04], device='cuda:0')
b after update for 1 param tensor([0.004, 0.037, 0.019, 0.023, 0.016, 0.006, 0.018, 0.056, 0.085, 0.159,
        0.009, 0.006, 0.056, 0.036, 0.026, 0.017, 0.030, 0.040, 0.090, 0.012,
        0.102, 0.029, 0.112, 0.061, 0.021, 0.014, 0.035, 0.030, 0.042, 0.019,
        0.042, 0.034, 0.029, 0.009, 0.055, 0.021, 0.003, 0.007, 0.040, 0.012,
        0.046, 0.108, 0.020, 0.006, 0.032, 0.084, 0.007, 0.033, 0.018, 0.026,
        0.045, 0.033, 0.005, 0.048, 0.003, 0.010, 0.079, 0.017, 0.018, 0.055,
        0.072, 0.036, 0.105, 0.045, 0.010, 0.045, 0.044, 0.097, 0.003, 0.118,
        0.007, 0.013, 0.086, 0.073, 0.134, 0.060, 0.004, 0.017, 0.029, 0.017,
        0.088, 0.083, 0.007, 0.018, 0.020, 0.038, 0.039, 0.004, 0.094, 0.086,
        0.022, 0.081, 0.011, 0.012, 0.018, 0.095, 0.077, 0.026, 0.013, 0.020,
        0.031, 0.010, 0.021, 0.085, 0.006, 0.015, 0.027, 0.080, 0.057, 0.031,
        0.070, 0.096, 0.027, 0.038, 0.025, 0.003, 0.009, 0.014, 0.028, 0.117,
        0.084, 0.036, 0.037, 0.011, 0.008, 0.028, 0.046, 0.028, 0.126, 0.081,
        0.075, 0.018, 0.044, 0.056, 0.071, 0.013, 0.007, 0.036, 0.022, 0.023,
        0.051, 0.007, 0.039, 0.007, 0.043, 0.015, 0.024, 0.043, 0.035, 0.030,
        0.116, 0.030, 0.030, 0.008, 0.016, 0.004, 0.149, 0.069, 0.044, 0.125,
        0.008, 0.055, 0.042, 0.040, 0.028, 0.007, 0.076, 0.109, 0.047, 0.005,
        0.011, 0.078, 0.054, 0.025, 0.030, 0.003, 0.039, 0.039, 0.008, 0.039,
        0.024, 0.061, 0.077, 0.040, 0.012, 0.056, 0.025, 0.166, 0.025, 0.011,
        0.152, 0.035, 0.126, 0.025, 0.113, 0.092, 0.005, 0.096, 0.087, 0.063],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 9.354e-06],
         [-1.013e-05],
         [ 1.970e-06],
         [ 3.204e-06],
         [-1.493e-06],
         [ 1.808e-06],
         [ 4.235e-06],
         [ 5.332e-07],
         [-2.511e-06],
         [-1.103e-06]],

        [[ 1.265e-06],
         [ 1.108e-06],
         [ 6.510e-07],
         [-2.099e-06],
         [ 2.999e-06],
         [ 7.170e-06],
         [ 1.871e-07],
         [-8.131e-07],
         [ 2.289e-06],
         [ 3.459e-06]],

        [[ 2.676e-07],
         [-1.647e-06],
         [-1.730e-06],
         [-4.784e-06],
         [ 3.738e-06],
         [ 3.264e-06],
         [ 7.245e-07],
         [-1.142e-05],
         [ 8.678e-07],
         [-1.410e-06]],

        [[-9.375e-07],
         [ 3.193e-06],
         [ 1.060e-05],
         [-5.236e-07],
         [ 7.817e-07],
         [ 2.369e-06],
         [ 9.933e-08],
         [ 2.702e-06],
         [-5.943e-09],
         [-1.989e-07]],

        [[-3.245e-06],
         [ 1.181e-05],
         [ 1.252e-06],
         [-1.322e-06],
         [-1.589e-07],
         [-2.363e-06],
         [-1.181e-06],
         [ 1.786e-06],
         [-2.900e-06],
         [ 1.865e-06]],

        [[ 4.178e-06],
         [-1.905e-05],
         [ 1.862e-06],
         [ 1.139e-06],
         [-4.206e-06],
         [ 2.932e-06],
         [ 8.246e-08],
         [-1.696e-06],
         [ 8.733e-07],
         [ 4.749e-06]],

        [[ 2.986e-06],
         [ 9.162e-06],
         [-4.480e-06],
         [ 9.294e-07],
         [-4.912e-06],
         [ 9.530e-09],
         [-1.278e-09],
         [ 2.312e-06],
         [-4.932e-06],
         [-9.749e-07]],

        [[-7.336e-07],
         [-4.942e-07],
         [ 7.821e-06],
         [ 4.385e-06],
         [-2.868e-06],
         [-7.544e-06],
         [-9.765e-06],
         [ 6.371e-08],
         [-6.282e-08],
         [ 2.853e-08]],

        [[-1.926e-06],
         [-2.444e-07],
         [-6.412e-06],
         [-1.341e-06],
         [ 3.118e-07],
         [ 5.271e-07],
         [-6.063e-07],
         [ 7.489e-07],
         [-1.610e-07],
         [ 3.582e-06]],

        [[-8.714e-07],
         [-1.666e-06],
         [-5.255e-07],
         [ 1.659e-06],
         [-2.170e-06],
         [ 1.725e-06],
         [-6.312e-07],
         [-4.086e-06],
         [-3.841e-06],
         [ 4.776e-07]],

        [[ 9.197e-09],
         [ 7.277e-07],
         [-2.682e-06],
         [-1.699e-06],
         [-2.228e-06],
         [ 2.408e-06],
         [ 2.519e-06],
         [-1.198e-06],
         [-1.270e-05],
         [ 5.690e-07]],

        [[ 1.121e-05],
         [ 1.197e-05],
         [-1.065e-06],
         [-1.895e-06],
         [ 3.104e-06],
         [ 1.047e-05],
         [-7.531e-07],
         [-4.493e-07],
         [-6.647e-06],
         [ 1.809e-07]],

        [[ 3.309e-06],
         [ 4.436e-06],
         [ 7.793e-07],
         [ 9.910e-06],
         [ 2.352e-06],
         [ 3.462e-07],
         [-3.517e-07],
         [-3.200e-06],
         [ 1.538e-06],
         [-1.761e-06]],

        [[-3.025e-06],
         [ 4.513e-06],
         [ 6.033e-06],
         [-6.957e-06],
         [-5.237e-06],
         [ 1.008e-05],
         [ 1.602e-06],
         [-1.612e-06],
         [-4.845e-07],
         [-1.773e-06]],

        [[ 6.031e-06],
         [ 6.343e-07],
         [ 2.049e-06],
         [ 1.173e-06],
         [ 3.126e-06],
         [-5.614e-06],
         [ 1.921e-06],
         [-5.867e-06],
         [-2.553e-06],
         [-1.101e-05]],

        [[-3.068e-06],
         [-1.428e-06],
         [ 2.472e-06],
         [ 2.923e-06],
         [-1.788e-06],
         [-9.785e-08],
         [ 2.251e-06],
         [-1.445e-06],
         [-1.052e-05],
         [ 3.659e-07]],

        [[ 9.499e-07],
         [-1.903e-06],
         [-1.108e-05],
         [-4.584e-06],
         [-1.231e-07],
         [ 5.344e-07],
         [ 8.546e-06],
         [-5.770e-06],
         [-3.222e-06],
         [-8.002e-06]],

        [[-2.296e-06],
         [-3.145e-06],
         [ 1.423e-06],
         [ 4.507e-07],
         [ 7.812e-06],
         [-1.395e-06],
         [ 7.919e-06],
         [-9.709e-07],
         [-1.676e-08],
         [ 3.208e-06]],

        [[ 1.922e-06],
         [-1.218e-06],
         [ 9.917e-07],
         [ 1.507e-06],
         [ 6.847e-07],
         [-5.634e-07],
         [-6.173e-07],
         [-4.485e-06],
         [-8.009e-06],
         [ 6.952e-06]],

        [[ 2.150e-06],
         [ 5.117e-06],
         [ 1.255e-06],
         [-1.375e-06],
         [-7.816e-07],
         [ 3.479e-06],
         [ 2.328e-06],
         [-1.099e-06],
         [-4.980e-07],
         [ 2.546e-06]]], device='cuda:0')
s after update for 1 param tensor([[[1.219e-03],
         [2.960e-03],
         [3.740e-04],
         [1.788e-04],
         [1.410e-04],
         [3.123e-04],
         [7.094e-04],
         [2.204e-04],
         [3.207e-04],
         [3.819e-04]],

        [[4.642e-05],
         [3.902e-05],
         [4.941e-05],
         [2.682e-05],
         [7.483e-05],
         [6.067e-04],
         [3.566e-05],
         [7.055e-06],
         [1.113e-04],
         [4.662e-04]],

        [[6.639e-04],
         [1.279e-04],
         [6.160e-05],
         [1.010e-04],
         [6.475e-04],
         [8.002e-04],
         [4.022e-04],
         [1.273e-03],
         [7.828e-05],
         [1.043e-04]],

        [[1.362e-04],
         [2.135e-04],
         [3.397e-03],
         [1.080e-04],
         [9.462e-05],
         [1.905e-04],
         [5.184e-05],
         [1.247e-04],
         [1.132e-04],
         [1.285e-05]],

        [[7.048e-04],
         [4.092e-03],
         [5.166e-04],
         [2.264e-05],
         [1.657e-04],
         [2.698e-04],
         [1.624e-04],
         [4.503e-05],
         [9.110e-04],
         [6.940e-05]],

        [[9.488e-04],
         [4.041e-03],
         [6.097e-04],
         [2.361e-05],
         [4.920e-04],
         [2.679e-04],
         [6.678e-05],
         [2.976e-05],
         [4.630e-05],
         [1.017e-03]],

        [[6.878e-05],
         [2.339e-03],
         [2.902e-04],
         [1.991e-04],
         [4.063e-04],
         [1.584e-04],
         [1.216e-04],
         [1.084e-04],
         [5.571e-04],
         [1.391e-04]],

        [[2.088e-04],
         [6.810e-04],
         [2.089e-03],
         [2.052e-04],
         [4.257e-04],
         [2.517e-03],
         [1.793e-03],
         [8.491e-06],
         [1.020e-05],
         [4.523e-05]],

        [[3.407e-05],
         [3.369e-05],
         [3.651e-04],
         [6.450e-05],
         [1.341e-05],
         [4.167e-06],
         [1.034e-05],
         [1.064e-04],
         [2.281e-05],
         [1.195e-03]],

        [[3.482e-05],
         [9.605e-05],
         [4.732e-04],
         [5.030e-05],
         [2.287e-04],
         [4.349e-04],
         [7.164e-05],
         [4.675e-04],
         [3.037e-04],
         [1.445e-05]],

        [[5.505e-06],
         [5.029e-05],
         [8.269e-05],
         [1.688e-04],
         [6.988e-04],
         [1.106e-03],
         [1.137e-04],
         [1.196e-05],
         [2.612e-03],
         [1.829e-04]],

        [[5.812e-04],
         [4.847e-04],
         [5.555e-04],
         [2.638e-04],
         [2.925e-04],
         [1.631e-03],
         [2.977e-04],
         [2.007e-05],
         [9.120e-04],
         [1.882e-06]],

        [[5.267e-04],
         [4.674e-04],
         [6.299e-06],
         [3.927e-03],
         [7.427e-04],
         [2.918e-06],
         [6.931e-05],
         [2.871e-04],
         [6.672e-04],
         [6.097e-04]],

        [[2.186e-04],
         [1.400e-04],
         [1.438e-03],
         [5.508e-04],
         [2.656e-03],
         [1.678e-03],
         [1.458e-04],
         [5.311e-05],
         [4.778e-05],
         [2.366e-04]],

        [[6.630e-04],
         [1.885e-04],
         [1.627e-04],
         [2.755e-04],
         [1.187e-03],
         [1.753e-04],
         [1.118e-03],
         [1.343e-03],
         [4.290e-04],
         [1.466e-03]],

        [[7.400e-05],
         [2.088e-05],
         [5.433e-04],
         [1.241e-03],
         [5.080e-04],
         [1.030e-05],
         [4.774e-04],
         [5.170e-04],
         [9.589e-04],
         [2.622e-05]],

        [[4.195e-04],
         [5.397e-04],
         [9.435e-04],
         [2.429e-04],
         [1.872e-05],
         [5.234e-06],
         [7.393e-04],
         [1.033e-03],
         [7.266e-04],
         [2.593e-03]],

        [[6.134e-04],
         [1.579e-03],
         [4.628e-05],
         [4.013e-05],
         [2.424e-03],
         [3.591e-04],
         [1.161e-03],
         [1.876e-04],
         [2.256e-06],
         [1.734e-04]],

        [[9.495e-05],
         [7.536e-04],
         [3.982e-04],
         [2.536e-04],
         [3.540e-06],
         [1.090e-05],
         [1.499e-04],
         [7.367e-04],
         [2.148e-03],
         [1.140e-03]],

        [[7.545e-04],
         [1.333e-04],
         [2.550e-05],
         [4.078e-05],
         [5.862e-05],
         [5.382e-04],
         [7.849e-04],
         [4.318e-04],
         [5.350e-04],
         [3.653e-04]]], device='cuda:0')
b after update for 1 param tensor([[[0.081],
         [0.126],
         [0.045],
         [0.031],
         [0.027],
         [0.041],
         [0.062],
         [0.034],
         [0.041],
         [0.045]],

        [[0.016],
         [0.014],
         [0.016],
         [0.012],
         [0.020],
         [0.057],
         [0.014],
         [0.006],
         [0.024],
         [0.050]],

        [[0.060],
         [0.026],
         [0.018],
         [0.023],
         [0.059],
         [0.065],
         [0.046],
         [0.083],
         [0.020],
         [0.024]],

        [[0.027],
         [0.034],
         [0.135],
         [0.024],
         [0.023],
         [0.032],
         [0.017],
         [0.026],
         [0.025],
         [0.008]],

        [[0.061],
         [0.148],
         [0.053],
         [0.011],
         [0.030],
         [0.038],
         [0.029],
         [0.016],
         [0.070],
         [0.019]],

        [[0.071],
         [0.147],
         [0.057],
         [0.011],
         [0.051],
         [0.038],
         [0.019],
         [0.013],
         [0.016],
         [0.074]],

        [[0.019],
         [0.112],
         [0.039],
         [0.033],
         [0.047],
         [0.029],
         [0.026],
         [0.024],
         [0.055],
         [0.027]],

        [[0.033],
         [0.060],
         [0.106],
         [0.033],
         [0.048],
         [0.116],
         [0.098],
         [0.007],
         [0.007],
         [0.016]],

        [[0.014],
         [0.013],
         [0.044],
         [0.019],
         [0.008],
         [0.005],
         [0.007],
         [0.024],
         [0.011],
         [0.080]],

        [[0.014],
         [0.023],
         [0.050],
         [0.016],
         [0.035],
         [0.048],
         [0.020],
         [0.050],
         [0.040],
         [0.009]],

        [[0.005],
         [0.016],
         [0.021],
         [0.030],
         [0.061],
         [0.077],
         [0.025],
         [0.008],
         [0.118],
         [0.031]],

        [[0.056],
         [0.051],
         [0.055],
         [0.038],
         [0.040],
         [0.093],
         [0.040],
         [0.010],
         [0.070],
         [0.003]],

        [[0.053],
         [0.050],
         [0.006],
         [0.145],
         [0.063],
         [0.004],
         [0.019],
         [0.039],
         [0.060],
         [0.057]],

        [[0.034],
         [0.027],
         [0.088],
         [0.054],
         [0.119],
         [0.095],
         [0.028],
         [0.017],
         [0.016],
         [0.036]],

        [[0.060],
         [0.032],
         [0.030],
         [0.038],
         [0.080],
         [0.031],
         [0.077],
         [0.085],
         [0.048],
         [0.089]],

        [[0.020],
         [0.011],
         [0.054],
         [0.081],
         [0.052],
         [0.007],
         [0.051],
         [0.053],
         [0.072],
         [0.012]],

        [[0.047],
         [0.054],
         [0.071],
         [0.036],
         [0.010],
         [0.005],
         [0.063],
         [0.074],
         [0.062],
         [0.118]],

        [[0.057],
         [0.092],
         [0.016],
         [0.015],
         [0.114],
         [0.044],
         [0.079],
         [0.032],
         [0.003],
         [0.030]],

        [[0.023],
         [0.064],
         [0.046],
         [0.037],
         [0.004],
         [0.008],
         [0.028],
         [0.063],
         [0.107],
         [0.078]],

        [[0.064],
         [0.027],
         [0.012],
         [0.015],
         [0.018],
         [0.054],
         [0.065],
         [0.048],
         [0.054],
         [0.044]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 1.245e-06],
        [ 6.915e-07],
        [ 2.597e-06],
        [ 3.567e-06],
        [ 1.170e-06],
        [-1.943e-06],
        [ 1.378e-06],
        [ 2.908e-07],
        [ 1.508e-06],
        [ 5.195e-07],
        [ 7.664e-06],
        [ 9.452e-06],
        [-4.101e-08],
        [ 2.822e-06],
        [-1.431e-06],
        [ 1.063e-07],
        [-9.329e-07],
        [-1.422e-06],
        [-4.336e-06],
        [ 3.112e-06]], device='cuda:0')
s after update for 1 param tensor([[5.376e-05],
        [5.872e-04],
        [3.123e-05],
        [1.222e-04],
        [2.343e-05],
        [1.742e-04],
        [2.669e-04],
        [1.882e-06],
        [2.106e-04],
        [4.732e-04],
        [2.909e-03],
        [1.524e-03],
        [1.291e-05],
        [1.273e-03],
        [5.269e-05],
        [3.480e-05],
        [2.000e-04],
        [8.216e-05],
        [5.813e-04],
        [1.299e-03]], device='cuda:0')
b after update for 1 param tensor([[0.017],
        [0.056],
        [0.013],
        [0.026],
        [0.011],
        [0.031],
        [0.038],
        [0.003],
        [0.034],
        [0.050],
        [0.125],
        [0.090],
        [0.008],
        [0.083],
        [0.017],
        [0.014],
        [0.033],
        [0.021],
        [0.056],
        [0.083]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 4.520298833330252
exp ma of ||w||^2 478.2241804910876
||w|| 2.126099441072842
exp ma of ||w|| 4.089433810471274
||w||^2 3.2184601321380804
exp ma of ||w||^2 4.484002744802721
||w|| 1.7940067257783847
exp ma of ||w|| 1.998008439182739
||w||^2 2.5245796506218126
exp ma of ||w||^2 3.237384099367233
||w|| 1.5888925862442094
exp ma of ||w|| 1.7487815092427637
cuda
Objective function 45.52 = squared loss an data 38.33 + 0.5*rho*h**2 2.946288 + alpha*h 2.223317 + L2reg 1.90 + L1reg 0.12 ; SHD = 58 ; DAG True
Proportion of microbatches that were clipped  0.7878837095988306
iteration 2 in inner loop, alpha 289.6334541890711 rho 100000.0 h 0.0076763120562794995
iteration 4 in outer loop, alpha = 7965.94551046857, rho = 1000000.0, h = 0.0076763120562794995
Threshold 0.3
[[0.003 1.722 0.163 0.321 0.405 0.434 0.37  0.028 0.018 0.175 0.206 0.092
  0.173 0.18  0.153 0.266 0.173 0.234 0.228 0.256]
 [0.002 0.005 0.065 0.192 0.148 0.499 0.771 0.012 0.023 0.047 0.094 0.032
  0.044 0.058 0.062 0.119 0.116 0.101 0.119 0.13 ]
 [0.04  0.046 0.004 0.232 0.259 0.152 0.36  0.07  0.05  0.178 0.123 0.086
  0.204 0.043 0.197 0.211 0.118 0.065 0.052 0.18 ]
 [0.01  0.034 0.016 0.004 0.147 0.074 0.057 0.043 0.027 0.019 0.014 0.032
  0.03  0.03  0.052 0.084 0.052 0.064 0.049 0.016]
 [0.01  0.024 0.021 0.047 0.005 0.032 0.039 0.039 0.01  0.017 0.03  0.061
  0.027 0.026 0.149 0.071 0.056 0.017 0.026 0.031]
 [0.009 0.011 0.037 0.101 0.114 0.005 0.117 0.018 0.013 0.022 0.048 0.045
  0.02  0.024 0.26  0.391 0.949 0.072 0.136 0.163]
 [0.006 0.01  0.019 0.071 0.1   0.053 0.004 0.016 0.015 0.035 0.013 0.02
  0.058 0.021 0.12  0.087 0.034 0.036 0.044 0.087]
 [0.225 0.439 0.084 0.123 0.131 0.176 0.2   0.003 0.057 0.046 0.093 0.255
  0.136 0.083 0.203 0.252 0.29  0.158 0.101 0.128]
 [0.159 0.134 0.1   0.151 0.431 0.326 0.405 0.103 0.004 0.196 0.093 0.098
  0.044 0.132 0.24  0.386 0.501 0.28  0.203 0.101]
 [0.049 0.135 0.03  0.254 0.316 0.185 0.201 0.069 0.025 0.007 0.029 0.077
  0.104 0.109 0.085 0.218 0.194 0.097 0.037 0.088]
 [0.027 0.052 0.052 0.312 0.211 0.158 0.618 0.044 0.09  0.227 0.006 0.189
  0.477 0.095 0.269 0.214 0.179 0.07  0.092 0.213]
 [0.052 0.137 0.052 0.191 0.118 0.09  0.279 0.019 0.052 0.115 0.032 0.004
  0.069 0.067 0.066 0.104 0.088 0.077 0.052 0.042]
 [0.047 0.087 0.016 0.266 0.188 0.264 0.082 0.043 0.16  0.06  0.013 0.049
  0.005 0.043 0.092 0.244 0.166 0.156 0.081 0.166]
 [0.018 0.081 0.081 0.086 0.145 0.189 0.251 0.044 0.056 0.061 0.059 0.109
  0.105 0.004 0.147 0.183 0.125 0.067 0.049 0.395]
 [0.031 0.059 0.037 0.101 0.032 0.017 0.06  0.031 0.023 0.027 0.016 0.041
  0.048 0.028 0.005 0.085 0.016 0.041 0.04  0.052]
 [0.019 0.036 0.027 0.073 0.093 0.009 0.066 0.021 0.012 0.036 0.021 0.052
  0.019 0.02  0.077 0.004 0.006 0.018 0.006 0.009]
 [0.018 0.017 0.032 0.072 0.092 0.008 0.161 0.024 0.009 0.024 0.031 0.051
  0.018 0.021 0.479 0.642 0.005 0.034 0.159 0.053]
 [0.03  0.051 0.089 0.101 0.386 0.088 0.165 0.035 0.024 0.073 0.073 0.071
  0.047 0.077 0.135 0.212 0.099 0.003 0.07  0.061]
 [0.022 0.088 0.101 0.129 0.168 0.02  0.084 0.064 0.023 0.126 0.092 0.122
  0.059 0.138 0.17  0.931 0.034 0.067 0.007 0.149]
 [0.018 0.073 0.027 0.352 0.235 0.028 0.095 0.056 0.042 0.092 0.04  0.14
  0.037 0.019 0.091 0.693 0.173 0.081 0.036 0.004]]
[[0.    1.722 0.    0.321 0.405 0.434 0.37  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.499 0.771 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.36  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.391 0.949 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.439 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.431 0.326 0.405 0.    0.    0.    0.    0.
  0.    0.    0.    0.386 0.501 0.    0.    0.   ]
 [0.    0.    0.    0.    0.316 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.312 0.    0.    0.618 0.    0.    0.    0.    0.
  0.477 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.395]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.479 0.642 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.386 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.931 0.    0.    0.    0.   ]
 [0.    0.    0.    0.352 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.693 0.    0.    0.    0.   ]]
{'fdr': 0.1111111111111111, 'tpr': 0.3, 'fpr': 0.02727272727272727, 'f1': 0.4485981308411215, 'shd': 58, 'npred': 27, 'ntrue': 80}
[1.722 0.163 0.321 0.405 0.434 0.37  0.028 0.018 0.175 0.206 0.092 0.173
 0.18  0.153 0.266 0.173 0.234 0.228 0.256 0.002 0.065 0.192 0.148 0.499
 0.771 0.012 0.023 0.047 0.094 0.032 0.044 0.058 0.062 0.119 0.116 0.101
 0.119 0.13  0.04  0.046 0.232 0.259 0.152 0.36  0.07  0.05  0.178 0.123
 0.086 0.204 0.043 0.197 0.211 0.118 0.065 0.052 0.18  0.01  0.034 0.016
 0.147 0.074 0.057 0.043 0.027 0.019 0.014 0.032 0.03  0.03  0.052 0.084
 0.052 0.064 0.049 0.016 0.01  0.024 0.021 0.047 0.032 0.039 0.039 0.01
 0.017 0.03  0.061 0.027 0.026 0.149 0.071 0.056 0.017 0.026 0.031 0.009
 0.011 0.037 0.101 0.114 0.117 0.018 0.013 0.022 0.048 0.045 0.02  0.024
 0.26  0.391 0.949 0.072 0.136 0.163 0.006 0.01  0.019 0.071 0.1   0.053
 0.016 0.015 0.035 0.013 0.02  0.058 0.021 0.12  0.087 0.034 0.036 0.044
 0.087 0.225 0.439 0.084 0.123 0.131 0.176 0.2   0.057 0.046 0.093 0.255
 0.136 0.083 0.203 0.252 0.29  0.158 0.101 0.128 0.159 0.134 0.1   0.151
 0.431 0.326 0.405 0.103 0.196 0.093 0.098 0.044 0.132 0.24  0.386 0.501
 0.28  0.203 0.101 0.049 0.135 0.03  0.254 0.316 0.185 0.201 0.069 0.025
 0.029 0.077 0.104 0.109 0.085 0.218 0.194 0.097 0.037 0.088 0.027 0.052
 0.052 0.312 0.211 0.158 0.618 0.044 0.09  0.227 0.189 0.477 0.095 0.269
 0.214 0.179 0.07  0.092 0.213 0.052 0.137 0.052 0.191 0.118 0.09  0.279
 0.019 0.052 0.115 0.032 0.069 0.067 0.066 0.104 0.088 0.077 0.052 0.042
 0.047 0.087 0.016 0.266 0.188 0.264 0.082 0.043 0.16  0.06  0.013 0.049
 0.043 0.092 0.244 0.166 0.156 0.081 0.166 0.018 0.081 0.081 0.086 0.145
 0.189 0.251 0.044 0.056 0.061 0.059 0.109 0.105 0.147 0.183 0.125 0.067
 0.049 0.395 0.031 0.059 0.037 0.101 0.032 0.017 0.06  0.031 0.023 0.027
 0.016 0.041 0.048 0.028 0.085 0.016 0.041 0.04  0.052 0.019 0.036 0.027
 0.073 0.093 0.009 0.066 0.021 0.012 0.036 0.021 0.052 0.019 0.02  0.077
 0.006 0.018 0.006 0.009 0.018 0.017 0.032 0.072 0.092 0.008 0.161 0.024
 0.009 0.024 0.031 0.051 0.018 0.021 0.479 0.642 0.034 0.159 0.053 0.03
 0.051 0.089 0.101 0.386 0.088 0.165 0.035 0.024 0.073 0.073 0.071 0.047
 0.077 0.135 0.212 0.099 0.07  0.061 0.022 0.088 0.101 0.129 0.168 0.02
 0.084 0.064 0.023 0.126 0.092 0.122 0.059 0.138 0.17  0.931 0.034 0.067
 0.149 0.018 0.073 0.027 0.352 0.235 0.028 0.095 0.056 0.042 0.092 0.04
 0.14  0.037 0.019 0.091 0.693 0.173 0.081 0.036]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.8101250000000001, 0.625844902133999)
Iterations 567
Achieves (13.598573656938171, 1e-05)-DP
