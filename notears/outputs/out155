samples  5000  graph  30 120 ER mlp  minibatch size  100  noise  1.0  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.407192360868244
iteration 1 in outer loop, alpha = 2.407192360868244, rho = 1.0, h = 2.407192360868244
cuda
iteration 1 in inner loop,alpha 2.407192360868244 rho 1.0 h 1.519847643277938
iteration 2 in inner loop,alpha 2.407192360868244 rho 10.0 h 0.6681085661998409
iteration 3 in inner loop,alpha 2.407192360868244 rho 100.0 h 0.2142382096787543
iteration 2 in outer loop, alpha = 23.831013328743673, rho = 100.0, h = 0.2142382096787543
cuda
iteration 1 in inner loop,alpha 23.831013328743673 rho 100.0 h 0.12078187772351612
iteration 2 in inner loop,alpha 23.831013328743673 rho 1000.0 h 0.04372606397762979
iteration 3 in outer loop, alpha = 67.55707730637346, rho = 1000.0, h = 0.04372606397762979
cuda
iteration 1 in inner loop,alpha 67.55707730637346 rho 1000.0 h 0.023239082362103147
iteration 2 in inner loop,alpha 67.55707730637346 rho 10000.0 h 0.007686345269199535
iteration 4 in outer loop, alpha = 144.4205299983688, rho = 10000.0, h = 0.007686345269199535
cuda
iteration 1 in inner loop,alpha 144.4205299983688 rho 10000.0 h 0.004012326571370295
iteration 2 in inner loop,alpha 144.4205299983688 rho 100000.0 h 0.0013267240677272696
iteration 5 in outer loop, alpha = 277.09293677109576, rho = 100000.0, h = 0.0013267240677272696
cuda
iteration 1 in inner loop,alpha 277.09293677109576 rho 100000.0 h 0.000637695786529946
iteration 6 in outer loop, alpha = 914.7887233010417, rho = 1000000.0, h = 0.000637695786529946
Threshold 0.3
[[0.002 0.    0.001 1.883 0.004 0.161 0.156 0.    0.    0.    0.    0.094
  0.    0.    0.    0.914 0.    0.    0.663 0.16  0.    0.    0.163 0.
  0.    0.    0.165 0.    0.204 0.   ]
 [0.392 0.003 0.02  0.091 0.202 1.539 0.186 0.    1.    0.    0.    0.123
  0.    0.    0.    0.832 0.    1.189 0.336 0.589 0.    0.003 0.996 0.001
  0.    0.    0.166 0.    0.215 0.149]
 [0.167 0.039 0.001 0.452 0.011 0.443 0.215 0.002 0.126 0.    0.    0.154
  0.    0.    0.    0.734 0.    0.253 0.05  0.076 0.    0.    0.196 0.
  0.    0.002 0.177 0.    0.233 0.004]
 [0.    0.    0.    0.003 0.    0.002 0.136 0.    0.    0.    0.    0.
  0.    0.    0.    0.799 0.    0.    0.196 0.547 0.    0.    0.172 0.
  0.    0.    0.    0.    0.171 0.   ]
 [0.079 0.006 0.066 0.212 0.003 0.079 0.332 0.    0.207 0.    0.    0.187
  0.    0.    0.    0.175 0.    0.001 0.665 0.151 0.    0.001 1.418 0.
  0.    0.    0.087 0.    0.153 0.005]
 [0.001 0.    0.    0.181 0.007 0.002 1.615 0.    0.001 0.    0.    0.
  0.    0.    0.    0.18  0.    0.    0.141 0.105 0.    0.    0.279 0.
  0.    0.    0.    0.    0.779 0.   ]
 [0.    0.    0.    0.001 0.    0.    0.004 0.    0.001 0.    0.    0.
  0.    0.    0.    0.957 0.    0.    0.005 0.001 0.    0.    0.    0.
  0.    0.    0.    0.    0.069 0.   ]
 [0.421 0.769 0.108 0.142 2.121 0.176 0.196 0.002 1.375 0.    0.    0.131
  0.    0.    0.    0.509 0.    0.301 0.165 0.687 0.    0.004 0.259 0.001
  0.    0.005 0.146 0.    0.332 0.274]
 [1.206 0.    0.002 0.251 0.002 0.111 0.101 0.    0.002 0.    0.    0.281
  0.    0.    0.    0.072 0.    0.    0.258 0.836 0.    0.    0.116 0.
  0.    0.    0.111 0.    0.114 0.   ]
 [0.313 0.957 0.167 0.23  0.278 0.354 0.213 0.319 1.047 0.001 0.    0.214
  2.239 0.093 0.007 0.29  0.001 0.393 0.23  0.095 0.002 0.478 0.452 0.084
  0.    0.164 0.241 0.002 0.325 1.846]
 [1.65  1.791 0.317 0.166 0.182 0.327 0.304 1.956 0.276 1.27  0.002 0.062
  0.169 0.276 0.004 1.067 0.    0.28  0.227 0.172 0.017 0.09  0.263 0.02
  0.    0.208 0.091 0.015 1.003 0.423]
 [0.012 0.001 0.003 0.133 0.005 0.134 0.887 0.    0.004 0.    0.    0.001
  0.    0.    0.    0.312 0.    0.    0.194 0.122 0.    0.    0.154 0.
  0.    0.    0.904 0.    0.139 0.   ]
 [0.156 1.641 0.103 0.176 2.287 0.36  0.346 1.359 0.855 0.    0.    1.731
  0.001 0.    0.    0.226 0.    0.184 0.637 0.091 0.    2.507 0.539 0.003
  0.    0.03  1.051 0.    0.346 0.264]
 [0.223 0.187 2.753 1.116 0.126 1.683 0.352 0.126 0.409 0.004 0.002 0.49
  0.299 0.001 0.    0.489 0.    1.74  0.4   0.218 0.    0.126 0.313 2.614
  0.    0.522 0.365 0.007 0.218 2.422]
 [0.231 0.836 0.07  0.251 0.252 0.238 0.422 0.282 0.26  0.028 0.019 0.676
  2.495 0.083 0.001 0.253 0.    0.176 0.185 0.167 0.358 0.619 1.023 3.569
  0.006 0.103 0.45  0.003 0.439 0.273]
 [0.    0.    0.    0.    0.001 0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    0.005 0.    0.    0.004 0.001 0.    0.    0.    0.
  0.    0.    0.    0.    0.101 0.   ]
 [0.206 0.228 0.147 0.067 0.349 1.065 0.548 0.276 0.545 0.128 0.079 0.128
  1.091 0.109 3.006 0.455 0.001 0.026 0.2   0.07  0.707 1.326 0.198 0.332
  0.002 0.053 0.137 0.004 0.267 0.142]
 [0.171 0.    0.001 0.225 0.142 0.242 0.19  0.    1.092 0.    0.    0.214
  0.    0.    0.    0.186 0.    0.001 0.688 0.235 0.    0.    0.339 0.
  0.    0.    0.688 0.    0.182 0.001]
 [0.001 0.001 0.002 0.001 0.    0.002 0.25  0.    0.001 0.    0.    0.
  0.    0.    0.    0.165 0.    0.    0.012 0.001 0.    0.    0.009 0.
  0.    0.001 0.001 0.    0.007 0.   ]
 [0.    0.    0.001 0.004 0.001 0.003 0.253 0.001 0.    0.    0.    0.001
  0.    0.    0.    0.233 0.    0.    0.693 0.004 0.    0.    0.208 0.
  0.    0.    0.    0.    0.385 0.   ]
 [0.191 0.535 0.214 0.393 1.823 1.008 0.266 1.103 0.183 0.083 0.027 0.205
  0.223 3.741 0.001 0.134 0.001 0.215 0.222 0.06  0.001 1.695 0.636 0.075
  0.    2.439 0.223 0.002 0.22  0.158]
 [0.084 0.183 2.458 1.061 1.499 0.216 0.588 0.234 0.446 0.    0.    0.363
  0.    0.    0.    0.258 0.    2.282 0.285 0.224 0.    0.001 0.506 0.
  0.    0.003 0.285 0.    0.508 2.097]
 [0.001 0.001 0.001 0.002 0.    0.001 1.053 0.    0.001 0.    0.    0.
  0.    0.    0.    0.191 0.    0.    0.512 0.003 0.    0.    0.005 0.
  0.    0.    0.001 0.    0.083 0.   ]
 [2.093 0.278 0.128 0.19  0.22  0.149 0.988 0.133 0.516 0.003 0.    2.154
  0.249 0.    0.    0.754 0.    0.087 0.773 0.764 0.    0.259 0.449 0.002
  0.    0.013 1.525 0.    0.12  0.126]
 [0.229 0.447 0.695 0.129 0.167 0.525 0.579 0.282 0.116 1.144 4.368 0.149
  0.191 2.14  0.04  0.147 0.01  0.393 0.164 0.064 0.033 0.101 0.879 0.108
  0.001 2.782 0.113 0.003 0.262 0.902]
 [0.228 1.713 0.18  0.071 0.186 0.245 0.186 0.116 0.19  0.003 0.004 0.103
  0.018 0.    0.    0.12  0.    0.116 0.336 0.163 0.    0.127 1.331 0.037
  0.    0.003 0.065 0.    0.089 0.143]
 [0.002 0.    0.001 1.52  0.009 1.769 0.358 0.    0.001 0.    0.    0.
  0.    0.    0.    0.733 0.    0.    0.8   0.182 0.    0.    1.298 0.
  0.    0.    0.003 0.    0.74  0.   ]
 [2.067 0.355 0.188 0.264 0.124 0.222 0.088 0.1   0.247 0.013 0.007 0.978
  0.005 0.017 0.021 0.188 0.018 0.121 0.166 0.073 0.001 0.051 0.895 0.033
  0.003 2.995 0.137 0.    0.123 0.089]
 [0.002 0.001 0.001 0.012 0.005 0.002 0.033 0.002 0.001 0.    0.    0.001
  0.    0.    0.    0.081 0.    0.    0.122 0.014 0.    0.    0.008 0.
  0.    0.    0.002 0.    0.011 0.   ]
 [0.268 0.004 0.236 0.383 0.143 0.244 0.224 0.002 0.555 0.    0.    1.962
  0.    0.    0.    0.19  0.    0.312 0.603 0.199 0.    0.    0.163 0.
  0.    0.001 0.689 0.    0.109 0.002]]
[[0.    0.    0.    1.883 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.914 0.    0.    0.663 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.392 0.    0.    0.    0.    1.539 0.    0.    1.    0.    0.    0.
  0.    0.    0.    0.832 0.    1.189 0.336 0.589 0.    0.    0.996 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.452 0.    0.443 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.734 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.799 0.    0.    0.    0.547 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.332 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.665 0.    0.    0.    1.418 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.615 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.779 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.957 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.421 0.769 0.    0.    2.121 0.    0.    0.    1.375 0.    0.    0.
  0.    0.    0.    0.509 0.    0.301 0.    0.687 0.    0.    0.    0.
  0.    0.    0.    0.    0.332 0.   ]
 [1.206 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.836 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.313 0.957 0.    0.    0.    0.354 0.    0.319 1.047 0.    0.    0.
  2.239 0.    0.    0.    0.    0.393 0.    0.    0.    0.478 0.452 0.
  0.    0.    0.    0.    0.325 1.846]
 [1.65  1.791 0.317 0.    0.    0.327 0.304 1.956 0.    1.27  0.    0.
  0.    0.    0.    1.067 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.003 0.423]
 [0.    0.    0.    0.    0.    0.    0.887 0.    0.    0.    0.    0.
  0.    0.    0.    0.312 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.904 0.    0.    0.   ]
 [0.    1.641 0.    0.    2.287 0.36  0.346 1.359 0.855 0.    0.    1.731
  0.    0.    0.    0.    0.    0.    0.637 0.    0.    2.507 0.539 0.
  0.    0.    1.051 0.    0.346 0.   ]
 [0.    0.    2.753 1.116 0.    1.683 0.352 0.    0.409 0.    0.    0.49
  0.    0.    0.    0.489 0.    1.74  0.4   0.    0.    0.    0.313 2.614
  0.    0.522 0.365 0.    0.    2.422]
 [0.    0.836 0.    0.    0.    0.    0.422 0.    0.    0.    0.    0.676
  2.495 0.    0.    0.    0.    0.    0.    0.    0.358 0.619 1.023 3.569
  0.    0.    0.45  0.    0.439 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.349 1.065 0.548 0.    0.545 0.    0.    0.
  1.091 0.    3.006 0.455 0.    0.    0.    0.    0.707 1.326 0.    0.332
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.092 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.688 0.    0.    0.    0.339 0.
  0.    0.    0.688 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.693 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.385 0.   ]
 [0.    0.535 0.    0.393 1.823 1.008 0.    1.103 0.    0.    0.    0.
  0.    3.741 0.    0.    0.    0.    0.    0.    0.    1.695 0.636 0.
  0.    2.439 0.    0.    0.    0.   ]
 [0.    0.    2.458 1.061 1.499 0.    0.588 0.    0.446 0.    0.    0.363
  0.    0.    0.    0.    0.    2.282 0.    0.    0.    0.    0.506 0.
  0.    0.    0.    0.    0.508 2.097]
 [0.    0.    0.    0.    0.    0.    1.053 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.512 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [2.093 0.    0.    0.    0.    0.    0.988 0.    0.516 0.    0.    2.154
  0.    0.    0.    0.754 0.    0.    0.773 0.764 0.    0.    0.449 0.
  0.    0.    1.525 0.    0.    0.   ]
 [0.    0.447 0.695 0.    0.    0.525 0.579 0.    0.    1.144 4.368 0.
  0.    2.14  0.    0.    0.    0.393 0.    0.    0.    0.    0.879 0.
  0.    2.782 0.    0.    0.    0.902]
 [0.    1.713 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.336 0.    0.    0.    1.331 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.52  0.    1.769 0.358 0.    0.    0.    0.    0.
  0.    0.    0.    0.733 0.    0.    0.8   0.    0.    0.    1.298 0.
  0.    0.    0.    0.    0.74  0.   ]
 [2.067 0.355 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.978
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.895 0.
  0.    2.995 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.383 0.    0.    0.    0.    0.555 0.    0.    1.962
  0.    0.    0.    0.    0.    0.312 0.603 0.    0.    0.    0.    0.
  0.    0.    0.689 0.    0.    0.   ]]
{'fdr': 0.3235294117647059, 'tpr': 0.9583333333333334, 'fpr': 0.1746031746031746, 'f1': 0.7931034482758621, 'shd': 60, 'npred': 170, 'ntrue': 120}
[2.742e-04 8.636e-04 1.883e+00 4.131e-03 1.612e-01 1.563e-01 2.559e-05
 4.319e-04 2.048e-05 1.846e-05 9.361e-02 1.420e-05 2.572e-05 3.680e-05
 9.139e-01 2.256e-05 6.886e-05 6.635e-01 1.597e-01 7.071e-07 4.064e-05
 1.633e-01 2.023e-04 2.176e-06 1.138e-04 1.647e-01 2.700e-05 2.035e-01
 1.128e-04 3.920e-01 1.958e-02 9.135e-02 2.020e-01 1.539e+00 1.855e-01
 3.002e-04 9.997e-01 7.185e-05 3.445e-05 1.227e-01 2.110e-04 1.359e-04
 1.006e-04 8.325e-01 4.921e-05 1.189e+00 3.364e-01 5.889e-01 4.955e-05
 3.246e-03 9.964e-01 6.399e-04 1.061e-06 2.944e-04 1.657e-01 2.243e-05
 2.145e-01 1.487e-01 1.674e-01 3.938e-02 4.524e-01 1.131e-02 4.434e-01
 2.146e-01 2.433e-03 1.258e-01 2.544e-06 1.240e-05 1.536e-01 2.531e-05
 1.822e-04 4.446e-06 7.344e-01 5.887e-06 2.532e-01 4.952e-02 7.645e-02
 1.742e-05 1.065e-04 1.962e-01 3.398e-04 1.802e-05 2.317e-03 1.775e-01
 1.036e-04 2.328e-01 3.626e-03 1.072e-04 1.617e-04 1.434e-04 3.673e-04
 1.544e-03 1.360e-01 5.817e-05 1.417e-04 5.154e-05 6.069e-05 5.599e-05
 1.059e-04 1.953e-05 2.121e-06 7.987e-01 1.889e-05 4.185e-05 1.960e-01
 5.469e-01 6.406e-06 1.306e-05 1.718e-01 1.857e-05 1.126e-05 2.389e-05
 1.423e-04 1.226e-05 1.712e-01 5.646e-05 7.927e-02 5.863e-03 6.612e-02
 2.121e-01 7.855e-02 3.322e-01 3.968e-04 2.067e-01 2.825e-05 1.748e-05
 1.866e-01 6.280e-05 1.135e-05 1.650e-05 1.749e-01 1.537e-06 1.097e-03
 6.654e-01 1.506e-01 4.110e-05 6.835e-04 1.418e+00 3.232e-04 8.627e-07
 4.899e-04 8.683e-02 7.379e-05 1.527e-01 4.701e-03 7.419e-04 4.758e-04
 7.687e-05 1.809e-01 6.626e-03 1.615e+00 3.663e-04 1.429e-03 2.374e-05
 1.747e-05 7.249e-05 6.494e-05 8.976e-06 6.951e-06 1.803e-01 3.077e-06
 2.173e-04 1.412e-01 1.052e-01 2.769e-06 1.196e-05 2.785e-01 3.116e-05
 1.611e-06 3.793e-05 1.380e-04 5.979e-06 7.793e-01 4.863e-05 2.356e-04
 1.504e-04 1.274e-04 9.708e-04 7.729e-05 3.855e-04 1.169e-04 8.062e-04
 2.002e-06 4.779e-05 8.054e-05 3.270e-05 1.216e-05 1.474e-05 9.574e-01
 1.115e-05 3.400e-05 4.895e-03 8.059e-04 1.990e-06 1.880e-05 4.521e-04
 2.497e-04 3.712e-06 6.501e-05 6.180e-05 3.052e-06 6.892e-02 1.241e-05
 4.208e-01 7.686e-01 1.083e-01 1.424e-01 2.121e+00 1.764e-01 1.960e-01
 1.375e+00 4.229e-05 2.297e-05 1.308e-01 1.956e-04 1.338e-04 1.981e-05
 5.090e-01 1.468e-05 3.015e-01 1.653e-01 6.865e-01 5.433e-06 3.739e-03
 2.595e-01 1.428e-03 5.591e-06 5.037e-03 1.459e-01 1.420e-04 3.322e-01
 2.744e-01 1.206e+00 4.672e-04 1.593e-03 2.509e-01 1.672e-03 1.110e-01
 1.011e-01 8.798e-05 6.905e-05 1.513e-05 2.808e-01 3.155e-05 1.830e-05
 1.211e-05 7.250e-02 1.203e-05 1.740e-04 2.580e-01 8.359e-01 4.150e-06
 6.512e-05 1.164e-01 1.955e-04 6.318e-07 1.030e-04 1.106e-01 1.324e-05
 1.144e-01 1.394e-04 3.127e-01 9.568e-01 1.666e-01 2.299e-01 2.778e-01
 3.536e-01 2.134e-01 3.190e-01 1.047e+00 9.524e-05 2.143e-01 2.239e+00
 9.265e-02 6.614e-03 2.901e-01 5.602e-04 3.926e-01 2.305e-01 9.531e-02
 1.838e-03 4.776e-01 4.520e-01 8.362e-02 5.153e-05 1.637e-01 2.409e-01
 1.733e-03 3.251e-01 1.846e+00 1.650e+00 1.791e+00 3.165e-01 1.663e-01
 1.817e-01 3.275e-01 3.043e-01 1.956e+00 2.756e-01 1.270e+00 6.228e-02
 1.692e-01 2.759e-01 4.441e-03 1.067e+00 1.301e-04 2.804e-01 2.268e-01
 1.725e-01 1.668e-02 8.969e-02 2.628e-01 2.014e-02 2.472e-04 2.076e-01
 9.098e-02 1.531e-02 1.003e+00 4.228e-01 1.171e-02 1.020e-03 2.662e-03
 1.333e-01 5.283e-03 1.341e-01 8.870e-01 3.715e-04 3.926e-03 2.483e-05
 2.430e-05 3.752e-05 1.033e-05 3.596e-06 3.121e-01 5.065e-06 1.380e-04
 1.944e-01 1.221e-01 2.342e-06 4.047e-05 1.540e-01 5.241e-05 4.251e-06
 4.926e-04 9.039e-01 5.330e-05 1.392e-01 2.221e-04 1.562e-01 1.641e+00
 1.032e-01 1.756e-01 2.287e+00 3.604e-01 3.456e-01 1.359e+00 8.551e-01
 1.562e-04 3.392e-05 1.731e+00 1.149e-04 7.237e-05 2.258e-01 2.286e-05
 1.843e-01 6.371e-01 9.115e-02 3.875e-05 2.507e+00 5.389e-01 3.197e-03
 7.566e-06 3.047e-02 1.051e+00 3.484e-04 3.457e-01 2.644e-01 2.228e-01
 1.866e-01 2.753e+00 1.116e+00 1.255e-01 1.683e+00 3.522e-01 1.259e-01
 4.094e-01 4.462e-03 1.766e-03 4.897e-01 2.986e-01 8.546e-05 4.892e-01
 1.493e-04 1.740e+00 4.002e-01 2.182e-01 7.674e-05 1.263e-01 3.135e-01
 2.614e+00 2.586e-04 5.218e-01 3.651e-01 6.657e-03 2.179e-01 2.422e+00
 2.309e-01 8.357e-01 6.997e-02 2.509e-01 2.520e-01 2.384e-01 4.215e-01
 2.817e-01 2.604e-01 2.783e-02 1.940e-02 6.759e-01 2.495e+00 8.339e-02
 2.530e-01 2.554e-04 1.757e-01 1.855e-01 1.672e-01 3.583e-01 6.193e-01
 1.023e+00 3.569e+00 5.857e-03 1.027e-01 4.502e-01 2.588e-03 4.394e-01
 2.725e-01 8.834e-05 4.268e-04 3.401e-04 3.216e-04 5.407e-04 2.418e-04
 1.376e-03 4.520e-05 5.175e-05 2.499e-05 4.052e-05 1.061e-04 5.196e-05
 1.551e-05 5.389e-06 9.212e-06 1.250e-04 4.133e-03 5.947e-04 3.414e-06
 1.753e-05 2.746e-04 3.354e-05 2.920e-06 1.357e-04 2.296e-04 2.487e-05
 1.007e-01 2.955e-05 2.064e-01 2.281e-01 1.466e-01 6.673e-02 3.491e-01
 1.065e+00 5.483e-01 2.758e-01 5.452e-01 1.285e-01 7.864e-02 1.281e-01
 1.091e+00 1.088e-01 3.006e+00 4.554e-01 2.595e-02 2.004e-01 7.001e-02
 7.072e-01 1.326e+00 1.978e-01 3.323e-01 2.240e-03 5.299e-02 1.365e-01
 3.955e-03 2.670e-01 1.421e-01 1.712e-01 4.764e-04 6.666e-04 2.248e-01
 1.420e-01 2.423e-01 1.901e-01 9.561e-05 1.092e+00 5.459e-06 1.829e-05
 2.141e-01 3.911e-05 1.126e-04 4.882e-06 1.858e-01 3.694e-07 6.875e-01
 2.349e-01 1.996e-05 1.236e-04 3.389e-01 9.995e-05 6.817e-06 1.298e-04
 6.879e-01 9.730e-06 1.818e-01 9.526e-04 1.210e-03 5.518e-04 1.819e-03
 7.932e-04 3.810e-04 2.198e-03 2.505e-01 7.468e-05 5.524e-04 2.262e-05
 2.454e-05 4.572e-04 6.260e-05 6.021e-05 7.258e-05 1.646e-01 5.745e-05
 3.011e-04 5.053e-04 5.062e-06 9.968e-05 9.172e-03 3.173e-04 1.149e-06
 5.237e-04 1.129e-03 1.892e-05 7.079e-03 1.503e-04 4.231e-04 3.676e-04
 6.708e-04 4.417e-03 1.215e-03 2.869e-03 2.532e-01 5.634e-04 2.323e-04
 7.971e-06 1.445e-05 6.650e-04 2.801e-05 6.672e-05 6.336e-05 2.330e-01
 3.706e-05 6.759e-05 6.927e-01 1.989e-05 5.136e-05 2.082e-01 2.633e-04
 1.984e-05 1.971e-04 1.015e-04 2.928e-05 3.851e-01 1.722e-04 1.912e-01
 5.353e-01 2.138e-01 3.931e-01 1.823e+00 1.008e+00 2.661e-01 1.103e+00
 1.832e-01 8.259e-02 2.702e-02 2.047e-01 2.230e-01 3.741e+00 7.477e-04
 1.344e-01 6.654e-04 2.154e-01 2.215e-01 6.002e-02 1.695e+00 6.365e-01
 7.477e-02 1.710e-04 2.439e+00 2.231e-01 1.552e-03 2.205e-01 1.583e-01
 8.384e-02 1.833e-01 2.458e+00 1.061e+00 1.499e+00 2.159e-01 5.876e-01
 2.336e-01 4.461e-01 9.699e-06 4.001e-06 3.629e-01 3.466e-05 2.134e-05
 1.965e-05 2.576e-01 6.013e-06 2.282e+00 2.847e-01 2.238e-01 5.713e-05
 5.058e-01 2.539e-04 1.202e-06 3.341e-03 2.850e-01 5.063e-05 5.079e-01
 2.097e+00 6.309e-04 1.141e-03 6.393e-04 1.686e-03 3.079e-04 9.429e-04
 1.053e+00 3.218e-05 1.434e-03 7.099e-06 9.307e-06 2.635e-04 2.716e-05
 2.186e-05 3.998e-06 1.908e-01 2.988e-06 1.284e-04 5.120e-01 2.680e-03
 3.010e-06 6.395e-05 2.408e-05 8.083e-06 1.342e-04 6.505e-04 4.176e-05
 8.274e-02 8.368e-05 2.093e+00 2.778e-01 1.278e-01 1.902e-01 2.205e-01
 1.491e-01 9.882e-01 1.332e-01 5.165e-01 3.011e-03 6.491e-05 2.154e+00
 2.489e-01 9.041e-05 1.858e-04 7.536e-01 6.139e-05 8.730e-02 7.732e-01
 7.638e-01 5.865e-06 2.592e-01 4.494e-01 4.152e-05 1.348e-02 1.525e+00
 1.888e-05 1.197e-01 1.257e-01 2.288e-01 4.472e-01 6.954e-01 1.292e-01
 1.674e-01 5.247e-01 5.795e-01 2.817e-01 1.156e-01 1.144e+00 4.368e+00
 1.492e-01 1.909e-01 2.140e+00 4.046e-02 1.471e-01 1.045e-02 3.927e-01
 1.643e-01 6.402e-02 3.341e-02 1.006e-01 8.786e-01 1.078e-01 2.782e+00
 1.134e-01 2.980e-03 2.617e-01 9.020e-01 2.276e-01 1.713e+00 1.796e-01
 7.086e-02 1.863e-01 2.446e-01 1.862e-01 1.159e-01 1.901e-01 2.946e-03
 4.163e-03 1.028e-01 1.755e-02 3.583e-04 1.353e-04 1.204e-01 8.019e-05
 1.160e-01 3.357e-01 1.628e-01 3.052e-05 1.266e-01 1.331e+00 3.742e-02
 3.801e-04 6.529e-02 7.895e-05 8.906e-02 1.433e-01 1.526e-03 1.951e-04
 7.464e-04 1.520e+00 8.829e-03 1.769e+00 3.575e-01 3.180e-04 8.248e-04
 5.796e-06 1.441e-05 3.377e-04 4.800e-05 7.360e-06 5.405e-06 7.326e-01
 7.053e-06 1.617e-04 8.000e-01 1.823e-01 3.714e-06 8.866e-05 1.298e+00
 3.607e-05 1.152e-06 2.878e-04 1.398e-05 7.405e-01 1.017e-04 2.067e+00
 3.546e-01 1.877e-01 2.643e-01 1.235e-01 2.220e-01 8.795e-02 9.962e-02
 2.473e-01 1.314e-02 6.824e-03 9.781e-01 5.164e-03 1.662e-02 2.150e-02
 1.882e-01 1.843e-02 1.213e-01 1.664e-01 7.311e-02 1.008e-03 5.142e-02
 8.947e-01 3.273e-02 3.313e-03 2.995e+00 1.371e-01 1.227e-01 8.853e-02
 2.256e-03 6.331e-04 5.770e-04 1.245e-02 5.488e-03 1.619e-03 3.338e-02
 2.118e-03 1.258e-03 1.533e-05 1.861e-04 8.047e-04 6.521e-05 2.216e-05
 2.748e-05 8.051e-02 3.154e-05 4.127e-04 1.220e-01 1.417e-02 2.719e-05
 1.630e-04 7.591e-03 1.790e-04 8.717e-05 4.850e-05 1.701e-03 2.295e-05
 2.765e-04 2.683e-01 3.862e-03 2.355e-01 3.832e-01 1.426e-01 2.439e-01
 2.241e-01 1.827e-03 5.553e-01 4.483e-05 3.224e-05 1.962e+00 2.129e-05
 1.077e-04 7.009e-06 1.897e-01 5.386e-06 3.117e-01 6.031e-01 1.987e-01
 1.967e-05 1.378e-04 1.628e-01 3.420e-04 2.305e-05 8.278e-04 6.894e-01
 6.769e-05 1.086e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.9850777777777777, 0.9567360984318124)
cuda
9630
cuda
Objective function 1220.30 = squared loss an data 964.29 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 397 ; DAG False
||w||^2 0.32779174958435797
exp ma of ||w||^2 7.099955369685766
||w|| 0.5725310031643335
exp ma of ||w|| 0.5337622036986776
||w||^2 0.2219362095812293
exp ma of ||w||^2 0.21756066796198936
||w|| 0.4711010609001314
exp ma of ||w|| 0.4598828909944511
||w||^2 0.2147672153422196
exp ma of ||w||^2 0.2162780980188458
||w|| 0.46342983864034865
exp ma of ||w|| 0.4560860050081166
||w||^2 0.18457943676865424
exp ma of ||w||^2 0.2080687670695684
||w|| 0.4296270903570377
exp ma of ||w|| 0.4496721460942113
||w||^2 0.16376022092290415
exp ma of ||w||^2 0.21219270187965486
||w|| 0.40467298022341963
exp ma of ||w|| 0.449378883092086
||w||^2 0.1500122917894284
exp ma of ||w||^2 0.20975968193797032
||w|| 0.3873142029275823
exp ma of ||w|| 0.44582604841386064
||w||^2 0.1960185618410548
exp ma of ||w||^2 0.2161404283278015
||w|| 0.4427398353898763
exp ma of ||w|| 0.45285258403953393
cuda
Objective function 400.41 = squared loss an data 395.34 + 0.5*rho*h**2 3.962546 + alpha*h 0.000000 + L2reg 0.57 + L1reg 0.54 ; SHD = 174 ; DAG False
Proportion of microbatches that were clipped  0.7348364560177835
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.8151539768986567
iteration 1 in outer loop, alpha = 2.8151539768986567, rho = 1.0, h = 2.8151539768986567
cuda
9630
cuda
Objective function 408.33 = squared loss an data 395.34 + 0.5*rho*h**2 3.962546 + alpha*h 7.925092 + L2reg 0.57 + L1reg 0.54 ; SHD = 174 ; DAG False
||w||^2 7851.641718756911
exp ma of ||w||^2 1485544.6215865794
||w|| 88.60949000393192
exp ma of ||w|| 353.60612668689305
||w||^2 3373.0291911386553
exp ma of ||w||^2 857647.9709677283
||w|| 58.07778569417618
exp ma of ||w|| 235.18296107953336
||w||^2 157.3574615990623
exp ma of ||w||^2 35745.2766310521
||w|| 12.544220246753575
exp ma of ||w|| 29.45100273586192
||w||^2 0.4461964166913212
exp ma of ||w||^2 0.4025199041184509
||w|| 0.6679793534917986
exp ma of ||w|| 0.6130498172789333
||w||^2 0.16417577721781634
exp ma of ||w||^2 0.4058479408076796
||w|| 0.4051861019554056
exp ma of ||w|| 0.6154334722937739
cuda
Objective function 185.14 = squared loss an data 173.44 + 0.5*rho*h**2 3.142427 + alpha*h 7.057482 + L2reg 0.98 + L1reg 0.53 ; SHD = 165 ; DAG False
Proportion of microbatches that were clipped  0.7455784520525718
iteration 1 in inner loop, alpha 2.8151539768986567 rho 1.0 h 2.5069612752923263
9630
cuda
Objective function 213.42 = squared loss an data 173.44 + 0.5*rho*h**2 31.424274 + alpha*h 7.057482 + L2reg 0.98 + L1reg 0.53 ; SHD = 165 ; DAG False
||w||^2 0.43534137709221354
exp ma of ||w||^2 0.8265358903769224
||w|| 0.6598040444648802
exp ma of ||w|| 0.8812334268515428
||w||^2 0.6783257947603173
exp ma of ||w||^2 0.9102417897584902
||w|| 0.8236053634844284
exp ma of ||w|| 0.930650676051824
||w||^2 0.6880643054028901
exp ma of ||w||^2 0.8952847804107713
||w|| 0.8294964167510852
exp ma of ||w|| 0.9126081570473704
cuda
Objective function 120.73 = squared loss an data 108.73 + 0.5*rho*h**2 6.956280 + alpha*h 3.320517 + L2reg 1.27 + L1reg 0.46 ; SHD = 139 ; DAG False
Proportion of microbatches that were clipped  0.7643758765778401
iteration 2 in inner loop, alpha 2.8151539768986567 rho 10.0 h 1.1795151386907392
9630
cuda
Objective function 183.33 = squared loss an data 108.73 + 0.5*rho*h**2 69.562798 + alpha*h 3.320517 + L2reg 1.27 + L1reg 0.46 ; SHD = 139 ; DAG False
||w||^2 596821942145.0828
exp ma of ||w||^2 186167934019.68094
||w|| 772542.5180176705
exp ma of ||w|| 250290.86448639954
||w||^2 305796497603.49536
exp ma of ||w||^2 221522302344.3696
||w|| 552988.695728489
exp ma of ||w|| 305585.1710686465
||w||^2 1242394031.40972
exp ma of ||w||^2 24275298356.983936
||w|| 35247.61029360317
exp ma of ||w|| 124221.61992366721
||w||^2 233483.78064874327
exp ma of ||w||^2 16241927.608253142
||w|| 483.2015942117154
exp ma of ||w|| 1571.1044471843593
||w||^2 8712.36630536446
exp ma of ||w||^2 403571.4642263011
||w|| 93.34005734605299
exp ma of ||w|| 145.86300106343435
||w||^2 1.3836096462973344
exp ma of ||w||^2 142.36686857400213
||w|| 1.1762693765874102
exp ma of ||w|| 2.5612336385640524
||w||^2 1.4862395044930174
exp ma of ||w||^2 24.380338864776924
||w|| 1.2191142294686816
exp ma of ||w|| 1.7101625515894068
||w||^2 1.360763461566414
exp ma of ||w||^2 6.617463262214415
||w|| 1.1665176644896613
exp ma of ||w|| 1.4375074680050837
||w||^2 0.8836163763125147
exp ma of ||w||^2 1.980731852458402
||w|| 0.9400087107641687
exp ma of ||w|| 1.3610496682118887
cuda
Objective function 112.81 = squared loss an data 96.95 + 0.5*rho*h**2 12.604284 + alpha*h 1.413436 + L2reg 1.45 + L1reg 0.39 ; SHD = 130 ; DAG True
Proportion of microbatches that were clipped  0.7765940780197399
iteration 3 in inner loop, alpha 2.8151539768986567 rho 100.0 h 0.5020813470989047
iteration 2 in outer loop, alpha = 53.02328868678913, rho = 100.0, h = 0.5020813470989047
cuda
9630
cuda
Objective function 138.02 = squared loss an data 96.95 + 0.5*rho*h**2 12.604284 + alpha*h 26.622004 + L2reg 1.45 + L1reg 0.39 ; SHD = 130 ; DAG True
||w||^2 3854050263.067301
exp ma of ||w||^2 18301708983.555054
||w|| 62080.99760045179
exp ma of ||w|| 105243.29881765042
||w||^2 21225.929961712154
exp ma of ||w||^2 5373206.591543342
||w|| 145.69121442870932
exp ma of ||w|| 596.7360530478879
||w||^2 3.9753758139368203
exp ma of ||w||^2 384.8591476227849
||w|| 1.9938344499824503
exp ma of ||w|| 3.8598329369716398
||w||^2 2.6825378032902156
exp ma of ||w||^2 288.9261708200025
||w|| 1.6378454760111576
exp ma of ||w|| 3.449702695364335
||w||^2 3.6450649062592224
exp ma of ||w||^2 102.80404603771889
||w|| 1.9092053075191318
exp ma of ||w|| 2.5788375100720144
||w||^2 2.003529790365123
exp ma of ||w||^2 12.19745001773786
||w|| 1.4154609815763637
exp ma of ||w|| 1.8089375187141996
||w||^2 2.3690221416000115
exp ma of ||w||^2 3.2734501916975125
||w|| 1.5391628054237834
exp ma of ||w|| 1.7087210988661288
||w||^2 2.1613757206112916
exp ma of ||w||^2 2.7791760332245263
||w|| 1.4701618008271373
exp ma of ||w|| 1.607069548028728
||w||^2 1.645308518334648
exp ma of ||w||^2 2.8255327295656723
||w|| 1.2826958011682459
exp ma of ||w|| 1.607350668203103
cuda
Objective function 122.19 = squared loss an data 94.62 + 0.5*rho*h**2 6.499230 + alpha*h 19.116687 + L2reg 1.59 + L1reg 0.37 ; SHD = 126 ; DAG True
Proportion of microbatches that were clipped  0.7771270538474052
iteration 1 in inner loop, alpha 53.02328868678913 rho 100.0 h 0.3605337832329667
9630
cuda
Objective function 180.68 = squared loss an data 94.62 + 0.5*rho*h**2 64.992304 + alpha*h 19.116687 + L2reg 1.59 + L1reg 0.37 ; SHD = 126 ; DAG True
||w||^2 131630.2689941033
exp ma of ||w||^2 18489433.340853766
||w|| 362.80886013726746
exp ma of ||w|| 1223.7153161036374
||w||^2 184.78152537194464
exp ma of ||w||^2 31902.770981303445
||w|| 13.59343684915425
exp ma of ||w|| 23.893145714804575
||w||^2 10.483281108123492
exp ma of ||w||^2 7.984760126596368
||w|| 3.237789540430862
exp ma of ||w|| 2.209354872576775
v before min max tensor([[-1.050e+03, -8.052e+02, -7.315e+02,  ..., -2.395e+02, -6.763e+02,
         -1.019e+03],
        [ 3.840e+03, -5.374e+02, -9.516e+02,  ...,  9.042e+02, -4.075e+02,
          7.670e+03],
        [ 5.276e+03, -8.561e+02, -1.666e+01,  ..., -7.348e+02,  3.434e+02,
          2.613e+03],
        ...,
        [-9.419e+01, -7.102e+02, -8.959e+02,  ...,  8.189e+02,  4.367e+02,
          2.680e+04],
        [-9.667e+02, -6.947e+02,  3.676e+02,  ...,  1.595e+02, -8.993e+02,
          1.367e+03],
        [-1.159e+03, -8.559e+02, -2.366e+02,  ...,  2.065e+03,  9.085e+02,
          1.762e+03]], device='cuda:0')
v tensor([[1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        ...,
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01]], device='cuda:0')
v before min max tensor([-4.814e+02, -3.176e+02,  3.137e+03, -4.210e+02, -8.186e+02, -5.917e+02,
         7.538e+02, -2.058e+02,  1.416e+03, -5.509e+02,  4.322e+03, -1.030e+03,
        -8.607e+02, -8.090e+01, -1.430e-01,  4.866e+03, -7.158e+02, -9.230e+02,
         2.217e+02, -9.786e+02, -4.045e+02, -5.497e+02, -7.294e+02, -2.035e+02,
        -9.332e+02, -5.216e+02,  3.653e+02, -1.661e+02, -5.000e+02, -4.037e+02,
        -5.082e+01,  1.291e+02, -8.725e+02, -3.213e+02, -9.305e+02,  1.741e+03,
         3.132e+03,  6.073e+02, -6.577e+02,  7.121e+02, -2.027e+01, -1.013e+03,
        -1.355e+00, -1.427e+02, -5.932e+01, -7.172e+02,  5.308e+01, -8.326e+02,
         1.127e+04, -2.973e+02,  2.539e+03, -9.774e+02, -8.869e+02, -6.000e+02,
         2.099e+03, -2.613e+02,  8.650e+02, -4.145e+02, -7.858e+02, -7.974e+02,
        -6.435e+02, -5.828e+02,  5.595e+03,  1.001e+03, -5.323e+02, -2.053e+02,
         2.687e+02, -1.318e+02,  1.110e+03,  1.036e+03, -6.214e+02, -7.377e+02,
        -4.319e+02,  7.380e+02, -4.679e+02,  9.965e+02, -4.954e+01, -5.549e+02,
        -8.665e+02, -5.034e+02,  2.061e+01, -7.426e+02, -3.814e+02, -5.941e+02,
        -3.458e+02, -7.476e+02, -7.208e+02, -7.635e+02,  6.866e+02, -3.703e+02,
        -1.014e+03,  3.400e+03,  2.287e+02, -7.716e+02,  1.226e+02, -1.081e+03,
        -9.884e+02, -2.435e+02,  7.454e+02, -8.173e+02,  4.094e+03,  3.571e+03,
         1.318e+03, -6.540e+02,  3.948e+02,  3.331e+03, -3.699e+02, -6.978e+02,
         9.371e+01,  7.256e+02,  7.534e+02, -6.906e+02, -9.363e+02, -9.198e+02,
         2.075e+03, -8.885e+02,  2.250e+03,  2.711e+02,  4.722e+02,  1.355e+03,
        -5.387e+02, -1.033e+03, -1.422e+01,  1.904e+02, -6.729e+02, -9.006e+02,
         8.255e+01, -7.275e+02,  3.358e+01,  2.105e+02, -4.182e+02, -8.062e+02,
        -8.533e+02,  1.242e+03, -8.710e+02, -7.715e+02,  2.821e+03, -1.079e+03,
         2.827e+03, -2.100e+02, -4.303e+02, -8.520e+02,  7.731e+02,  8.827e+02,
        -6.377e+02, -4.840e+02, -9.049e+02, -1.245e+02,  2.099e+03, -3.975e+02,
        -8.874e+02,  4.244e+02,  2.211e+02,  6.003e+03, -1.396e+02,  3.819e+02,
        -5.438e+02, -7.413e+02, -8.198e+02,  2.478e+02,  3.476e+02, -1.160e+02,
        -7.931e+02, -8.473e+02, -4.046e+02,  6.002e+02, -3.348e+02,  2.382e+02,
        -6.057e+02,  6.454e+02, -6.828e+02,  2.011e+03, -9.372e+02, -4.834e+02,
         3.390e+03,  1.470e+03, -6.755e+02, -5.311e+02,  3.690e+02, -6.621e+02,
        -6.576e+02, -8.865e+02,  6.628e+02,  3.262e+03,  3.053e+03, -7.841e+02,
        -1.021e+03,  1.120e+03, -3.566e+02,  3.741e+02, -8.375e+02, -6.080e+02,
        -4.598e+02, -8.526e+02,  1.080e+03,  2.934e+02,  3.566e+03, -4.154e+02,
         9.746e+01, -1.047e+03, -6.753e+02, -2.911e+02, -7.217e+02, -9.378e+02,
        -5.520e+02,  1.510e+03, -7.743e+02, -9.697e+02, -4.258e+02, -6.257e+02,
         3.140e+02, -8.739e+02, -8.950e+02,  1.509e+03, -1.920e+02,  6.097e+03,
        -6.422e+02,  7.128e+02, -6.492e+02, -4.847e+02,  7.163e+01, -7.714e+02,
        -7.224e+02, -1.116e+03, -8.137e+02, -6.132e+02,  3.449e+03,  2.943e+03,
        -3.910e+02, -8.187e+02,  1.291e+03, -7.588e+02, -1.278e+02, -4.728e+02,
        -9.380e+02,  4.679e+02, -2.003e+02,  2.064e+02, -7.784e+02, -8.376e+02,
        -1.941e+02, -7.971e+02,  2.835e+03, -7.220e+02, -7.965e+02, -5.786e+02,
         1.406e+03, -5.459e+02, -5.389e+02, -7.192e+02, -4.004e+02, -6.137e+02,
         4.537e+02,  8.857e+02, -5.612e+02, -2.006e+02,  2.059e+02, -8.834e+02,
        -6.911e+02,  9.766e+02, -7.201e+02, -4.840e+01, -9.813e+02, -8.370e+02,
        -7.682e+02, -2.094e+02, -1.031e+03, -8.326e+02, -6.700e+02, -8.247e+02,
         9.348e+03, -2.163e+02, -1.078e+03,  1.027e+03, -2.785e+02, -5.313e+02,
        -9.125e+02, -2.739e+02,  1.714e+02,  1.439e+01, -2.764e+02, -3.243e+02,
        -4.985e+02, -4.857e+02, -4.113e+02,  1.181e+03,  8.767e+02,  3.744e+03,
        -9.924e+02, -4.395e+02, -1.092e+03, -7.718e+02, -6.194e+02, -9.668e+02,
        -7.096e+02,  6.091e+02,  1.110e+03, -3.478e+02,  2.584e+03, -4.731e+02],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 3.447e+03],
         [-8.384e+02],
         [ 1.995e+03],
         [-6.851e+02],
         [ 8.940e+00],
         [ 1.501e+03],
         [-1.117e+03],
         [ 4.546e+02],
         [-9.182e+02],
         [-8.382e-01]],

        [[-4.954e+02],
         [-1.156e+03],
         [-7.762e+02],
         [-8.659e+02],
         [ 2.492e+02],
         [ 4.539e+02],
         [ 9.741e+02],
         [-4.755e+02],
         [ 5.005e+02],
         [ 1.824e+03]],

        [[-9.263e+02],
         [-2.250e+02],
         [-3.696e+02],
         [-1.009e+03],
         [-6.407e+02],
         [-7.889e+02],
         [-2.451e+02],
         [-7.517e+02],
         [ 1.033e+03],
         [-5.990e+02]],

        [[-1.071e+03],
         [ 7.566e+02],
         [ 9.063e+02],
         [ 2.619e+03],
         [-2.642e+01],
         [ 4.416e+03],
         [-5.258e+02],
         [-7.876e+02],
         [ 1.509e+03],
         [ 2.150e+03]],

        [[-2.412e+01],
         [ 3.169e+00],
         [ 2.806e+02],
         [ 1.214e+03],
         [ 8.968e+02],
         [-1.183e+03],
         [-7.631e+02],
         [-1.065e+03],
         [-6.926e+02],
         [-7.480e+02]],

        [[ 1.124e+03],
         [-8.828e+02],
         [-4.627e+02],
         [-5.127e+02],
         [-8.437e+02],
         [-5.238e+02],
         [ 3.456e+02],
         [-8.287e+02],
         [-9.741e+02],
         [ 4.200e+03]],

        [[ 4.005e+03],
         [-6.956e+02],
         [-1.012e+03],
         [ 6.596e+02],
         [-1.646e+02],
         [-2.444e+02],
         [-4.218e+02],
         [-7.981e+02],
         [-9.366e+02],
         [ 6.242e+02]],

        [[-6.394e+02],
         [-7.817e+02],
         [ 1.891e+03],
         [-4.979e+02],
         [ 3.631e+02],
         [ 2.076e+03],
         [-2.265e+02],
         [-4.503e+02],
         [-3.587e+02],
         [ 7.001e+02]],

        [[-2.621e+02],
         [-8.465e+02],
         [ 2.249e+02],
         [-7.476e+02],
         [-6.311e+02],
         [-5.881e+02],
         [ 2.053e+02],
         [-1.442e+02],
         [ 5.735e+03],
         [ 1.157e+01]],

        [[-1.044e+03],
         [-5.893e+02],
         [ 2.501e+02],
         [-8.728e+02],
         [-5.197e+02],
         [-3.720e+02],
         [ 3.967e+02],
         [-7.055e+02],
         [-4.053e+02],
         [ 2.468e+03]],

        [[-8.062e+02],
         [-7.476e+02],
         [-5.265e+02],
         [-8.236e+02],
         [ 1.158e+02],
         [ 1.591e+02],
         [ 7.810e+01],
         [ 2.960e+02],
         [-6.622e+02],
         [ 1.987e+02]],

        [[-3.708e+02],
         [-8.333e+02],
         [-6.814e+02],
         [ 7.821e+02],
         [-1.145e+02],
         [-5.782e+02],
         [ 2.308e+02],
         [-1.006e+01],
         [-4.792e+02],
         [-8.849e+02]],

        [[ 7.664e+02],
         [-5.437e+02],
         [ 3.280e+02],
         [-2.146e+02],
         [-7.081e+02],
         [ 2.545e+03],
         [-2.572e+01],
         [ 7.104e+02],
         [-7.887e+02],
         [ 8.672e+02]],

        [[-8.961e+02],
         [ 2.888e+03],
         [-5.013e+02],
         [-6.439e+02],
         [-5.446e+02],
         [-7.284e+02],
         [-8.311e+02],
         [-7.088e+02],
         [-9.347e+02],
         [-5.405e+02]],

        [[-9.112e+02],
         [-3.271e+02],
         [ 2.163e+03],
         [-3.923e+02],
         [-1.733e+01],
         [-4.663e+02],
         [-3.074e+01],
         [-6.695e+02],
         [ 2.228e+02],
         [ 2.171e+02]],

        [[-8.248e+02],
         [-9.754e+02],
         [-1.129e+03],
         [ 1.961e+03],
         [-7.592e+02],
         [-4.687e+02],
         [-1.153e+02],
         [ 7.516e+02],
         [-8.141e+02],
         [-8.293e+02]],

        [[-7.054e+02],
         [-9.062e+02],
         [-3.848e+02],
         [ 2.708e+02],
         [-9.111e+02],
         [-1.014e+03],
         [ 3.284e+02],
         [-5.703e+02],
         [ 1.233e+02],
         [-8.161e+02]],

        [[-9.785e+02],
         [-6.233e+02],
         [ 5.365e+02],
         [-5.674e+02],
         [ 3.944e+03],
         [-5.971e+02],
         [-6.366e+02],
         [-9.123e+02],
         [ 2.479e+02],
         [-1.069e+03]],

        [[ 1.447e+03],
         [-9.485e+02],
         [ 4.819e+02],
         [ 3.258e+02],
         [-4.609e+02],
         [-7.180e+02],
         [-2.357e+02],
         [-8.040e+02],
         [ 2.099e+03],
         [ 1.920e+03]],

        [[ 1.817e+03],
         [-6.953e+02],
         [-8.168e+02],
         [-7.378e+02],
         [ 1.956e+03],
         [-1.087e+03],
         [ 1.622e+02],
         [-5.586e+02],
         [ 4.910e+02],
         [-3.676e+02]],

        [[-4.387e+02],
         [-5.257e+02],
         [ 3.275e+02],
         [-4.255e+02],
         [ 5.185e+02],
         [-5.946e+02],
         [ 6.409e+02],
         [-1.001e+03],
         [-8.574e+02],
         [-6.399e+00]],

        [[-4.251e+02],
         [-5.658e+02],
         [ 5.830e+02],
         [ 7.165e+02],
         [-8.097e+02],
         [-6.705e+02],
         [-8.517e+02],
         [-3.249e+02],
         [-9.030e+02],
         [-8.363e+02]],

        [[-7.732e+02],
         [-6.620e+02],
         [ 8.104e+02],
         [-8.100e+02],
         [-6.918e+02],
         [-6.969e+02],
         [ 5.761e+01],
         [-3.700e+02],
         [-2.804e+01],
         [-1.231e+03]],

        [[-7.018e+02],
         [-8.149e+02],
         [-7.309e+02],
         [-3.983e+02],
         [-6.888e+02],
         [-8.006e+02],
         [-8.703e+02],
         [-6.855e+02],
         [-6.132e+02],
         [ 2.796e+03]],

        [[-6.368e+02],
         [-7.658e+02],
         [-5.300e+02],
         [ 1.248e+03],
         [-1.048e+03],
         [-2.419e+02],
         [-9.425e+02],
         [-8.338e+02],
         [ 8.845e+02],
         [ 1.164e+03]],

        [[-5.714e+02],
         [-5.744e+02],
         [ 9.405e+03],
         [-1.050e+03],
         [-4.881e+02],
         [-4.433e+02],
         [-1.089e+03],
         [-7.819e+02],
         [-5.184e+02],
         [-3.491e+02]],

        [[ 1.906e+03],
         [-2.005e+02],
         [ 1.154e+03],
         [-7.107e+02],
         [ 5.437e+02],
         [-9.200e+02],
         [ 3.625e+02],
         [-5.327e+02],
         [-8.027e+02],
         [ 6.429e+01]],

        [[-5.012e+02],
         [ 6.303e+02],
         [-6.255e+02],
         [-1.305e+02],
         [ 3.814e+03],
         [-5.830e+02],
         [-3.622e+02],
         [-4.647e+02],
         [-7.684e+02],
         [ 2.058e+03]],

        [[-4.816e+02],
         [-9.836e+00],
         [-6.222e+02],
         [-8.001e+01],
         [-8.562e+02],
         [ 1.417e+02],
         [-2.584e+02],
         [-3.112e+02],
         [-7.371e+02],
         [ 4.284e+01]],

        [[ 3.101e+02],
         [-6.705e+02],
         [-7.217e+00],
         [-8.581e+02],
         [-9.001e+02],
         [-8.681e+02],
         [-6.553e+02],
         [-5.349e+02],
         [-3.093e+02],
         [-8.337e+02]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [8.940e+00],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [3.169e+00],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-400.467],
        [-832.671],
        [-324.828],
        [-928.315],
        [-766.571],
        [-912.491],
        [-378.452],
        [-478.334],
        [-428.728],
        [-325.499],
        [-367.446],
        [-558.010],
        [-713.794],
        [-445.120],
        [-738.211],
        [-593.030],
        [ 373.293],
        [-825.482],
        [ -91.340],
        [-465.201],
        [-215.257],
        [-761.986],
        [-697.122],
        [-901.616],
        [1400.876],
        [-465.157],
        [2270.459],
        [-483.200],
        [-466.727],
        [-937.844]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.145, -0.022,  0.008,  ...,  0.051, -0.147,  0.129],
        [-0.012, -0.155, -0.008,  ...,  0.277, -0.128,  0.080],
        [ 0.137,  0.078,  0.048,  ..., -0.065, -0.184, -0.040],
        ...,
        [ 0.171,  0.029, -0.077,  ..., -0.214,  0.001,  0.151],
        [-0.064, -0.217, -0.056,  ..., -0.105,  0.165,  0.016],
        [-0.094, -0.304,  0.003,  ...,  0.011,  0.057, -0.128]],
       device='cuda:0')
s after update for 1 param tensor([[2.190, 1.708, 1.420,  ..., 1.882, 1.710, 1.908],
        [2.087, 1.635, 1.752,  ..., 1.934, 1.597, 2.342],
        [2.080, 1.669, 2.063,  ..., 1.347, 2.175, 1.830],
        ...,
        [1.880, 1.355, 1.889,  ..., 1.889, 1.616, 2.482],
        [1.801, 1.584, 1.993,  ..., 2.103, 1.810, 2.270],
        [2.113, 1.793, 2.016,  ..., 2.286, 1.980, 2.073]], device='cuda:0')
b after update for 1 param tensor([[189.428, 167.250, 152.525,  ..., 175.611, 167.365, 176.807],
        [184.906, 163.643, 169.432,  ..., 177.987, 161.738, 195.859],
        [184.591, 165.344, 183.842,  ..., 148.573, 188.778, 173.122],
        ...,
        [175.472, 148.986, 175.894,  ..., 175.918, 162.691, 201.642],
        [171.775, 161.108, 180.714,  ..., 185.603, 172.188, 192.833],
        [186.047, 171.399, 181.713,  ..., 193.514, 180.115, 184.276]],
       device='cuda:0')
clipping threshold 1.785079240384182
a after update for 1 param tensor([ 5.453e-02, -2.933e-01, -8.604e-02,  1.195e-01, -1.245e-01,  1.021e-01,
         1.470e-01,  1.264e-01,  2.812e-03,  8.855e-04, -1.253e-01, -1.638e-01,
         8.801e-02,  1.339e-01, -2.728e-02,  1.741e-02, -8.326e-02, -2.033e-01,
         3.302e-01,  2.006e-01,  1.649e-01,  8.931e-03,  2.513e-03,  2.503e-02,
        -2.130e-01, -1.187e-02,  7.278e-02, -2.075e-01, -1.741e-03, -1.526e-01,
        -8.266e-02, -1.560e-01, -1.235e-01, -1.992e-01,  1.774e-01, -3.679e-02,
         2.690e-01,  5.209e-02,  1.083e-01,  1.008e-01, -1.434e-01,  6.595e-02,
        -8.033e-02, -2.025e-01,  1.777e-02,  1.392e-01, -9.221e-02,  1.861e-01,
         2.899e-03, -2.128e-02,  1.041e-01, -7.574e-02, -9.292e-02, -1.493e-01,
         6.962e-03, -6.445e-02, -1.721e-01, -6.923e-03,  1.465e-01,  1.598e-01,
        -8.533e-02, -2.381e-01, -4.311e-01,  1.625e-01,  9.145e-02, -7.422e-02,
         3.241e-01,  4.223e-01,  4.316e-03, -2.087e-01,  6.087e-02,  2.838e-02,
         6.323e-02, -4.056e-01, -3.053e-01, -6.433e-02, -4.529e-02, -4.757e-02,
        -4.624e-01, -1.466e-01,  3.102e-02,  2.173e-02,  5.513e-03,  9.076e-02,
         5.076e-02,  1.349e-01, -4.281e-01, -4.943e-02,  2.367e-01,  1.309e-01,
         6.780e-03,  1.670e-01, -3.217e-02, -8.316e-02,  4.462e-02, -6.238e-02,
         2.414e-01, -1.125e-01, -4.303e-01,  9.367e-02,  1.361e-01, -4.577e-01,
         1.803e-02,  8.164e-02,  5.786e-02,  3.117e-01,  1.947e-02, -7.362e-02,
        -8.160e-02, -2.731e-02,  2.018e-01,  1.064e-01,  2.241e-01, -4.288e-02,
        -2.045e-01, -1.313e-01,  3.558e-02, -2.377e-02, -4.749e-01,  6.204e-02,
        -9.581e-02, -3.965e-01,  1.430e-01, -6.603e-02, -1.802e-01,  2.338e-01,
        -3.662e-02,  3.050e-01,  3.001e-01,  1.426e-01,  5.411e-02,  6.808e-02,
         1.498e-01, -3.262e-01,  1.170e-01,  8.642e-02, -1.469e-01,  3.334e-02,
         4.032e-02,  3.538e-01,  7.782e-02, -6.461e-02, -7.541e-02,  2.344e-01,
        -2.727e-01, -4.086e-02, -2.551e-02,  1.155e-02,  5.478e-02,  1.845e-01,
        -2.332e-02,  1.140e-01, -1.558e-01, -9.357e-02,  9.697e-02,  2.087e-02,
         1.852e-01, -1.035e-02, -1.203e-01, -2.852e-02,  6.486e-02, -7.378e-02,
         2.670e-01,  5.442e-01, -1.899e-02, -5.133e-02,  6.411e-02, -6.529e-02,
        -1.131e-01,  1.082e-01,  1.763e-02,  1.852e-01, -1.575e-01,  4.726e-01,
        -2.455e-01,  5.286e-02, -1.250e-01,  2.571e-01, -2.585e-01,  5.098e-02,
        -2.923e-01,  5.724e-02,  6.257e-02,  2.643e-02,  1.244e-01, -2.821e-01,
        -3.017e-01, -2.648e-01,  6.050e-02,  6.040e-03,  2.133e-02, -2.128e-01,
        -7.221e-02,  2.197e-02,  7.900e-02, -8.692e-02, -2.807e-01,  1.756e-02,
         1.564e-02, -7.242e-02, -5.710e-04,  9.192e-02, -3.991e-02, -1.650e-01,
        -1.639e-01, -6.036e-02, -1.923e-01,  5.333e-02,  2.982e-01,  1.386e-01,
        -2.293e-02, -2.699e-01,  1.233e-01, -1.053e-01,  2.803e-02, -7.051e-02,
         1.864e-01, -9.603e-02, -4.778e-02,  6.031e-02,  1.317e-01,  1.441e-02,
        -4.777e-02, -1.163e-01, -1.547e-01,  1.767e-01, -1.033e-01, -5.870e-01,
         1.755e-01,  7.225e-04, -1.805e-01, -2.430e-02,  6.130e-02, -4.652e-02,
         4.728e-02, -8.365e-02, -9.155e-02,  3.917e-01, -1.970e-01, -2.464e-01,
        -3.409e-02, -4.825e-02,  1.285e-01,  7.094e-03,  7.549e-02, -8.636e-02,
        -2.434e-01, -4.817e-01, -1.897e-01, -2.242e-02,  1.731e-02, -3.522e-01,
        -8.086e-02,  1.608e-01, -1.507e-01,  7.490e-02, -1.037e-01, -1.744e-01,
         1.660e-03, -1.099e-01, -1.015e-01, -3.843e-01,  8.062e-02, -1.025e-02,
        -2.958e-01, -2.750e-02,  1.871e-02,  7.855e-02,  1.536e-01, -2.241e-01,
         6.253e-02, -1.879e-01,  6.852e-02,  2.843e-01,  7.941e-02,  9.646e-04,
         2.028e-01, -9.894e-02,  3.751e-02,  2.771e-01,  1.284e-01, -4.448e-01,
         1.536e-02, -2.111e-01, -2.025e-01, -1.856e-01, -6.149e-02,  9.958e-02,
        -4.007e-02, -4.451e-02,  3.602e-01,  1.719e-01, -2.309e-01, -5.747e-01,
        -1.877e-01, -1.830e-01, -8.957e-02, -1.999e-01, -4.106e-01, -6.829e-03],
       device='cuda:0')
s after update for 1 param tensor([1.717, 2.009, 2.102, 1.667, 1.583, 1.592, 1.669, 1.846, 2.084, 1.654,
        1.957, 1.884, 1.923, 1.314, 1.550, 1.648, 1.495, 1.756, 1.786, 2.117,
        1.698, 1.032, 1.381, 1.637, 1.718, 0.950, 2.205, 1.651, 1.766, 1.510,
        2.015, 1.585, 1.693, 1.942, 1.697, 2.057, 1.675, 1.221, 1.412, 2.197,
        1.215, 1.998, 1.660, 1.970, 1.057, 1.433, 1.998, 1.675, 2.113, 0.922,
        2.255, 1.782, 1.881, 1.346, 2.062, 1.833, 2.025, 1.750, 2.062, 1.526,
        1.532, 1.860, 2.136, 1.867, 1.665, 1.793, 1.907, 1.819, 2.207, 1.400,
        1.224, 1.390, 1.390, 1.738, 1.309, 1.890, 2.028, 1.420, 1.631, 1.025,
        1.831, 1.361, 0.757, 1.083, 1.633, 1.418, 1.559, 1.539, 1.844, 1.927,
        1.847, 2.011, 2.371, 1.475, 1.842, 1.982, 1.876, 1.731, 2.167, 1.775,
        1.841, 1.983, 1.894, 1.201, 1.723, 1.984, 1.451, 1.272, 1.426, 1.238,
        1.263, 1.488, 1.848, 1.675, 2.213, 1.655, 2.014, 1.594, 2.289, 1.643,
        1.940, 2.041, 1.450, 1.498, 1.309, 1.964, 1.716, 1.998, 1.833, 2.085,
        1.004, 1.470, 1.663, 1.721, 1.590, 1.406, 1.873, 1.974, 1.623, 1.761,
        0.954, 1.553, 2.237, 1.915, 1.852, 1.842, 1.658, 0.517, 1.752, 1.422,
        1.616, 1.796, 1.741, 2.283, 1.551, 2.004, 1.284, 1.352, 1.625, 1.689,
        1.382, 1.650, 1.804, 1.543, 1.659, 1.527, 1.416, 1.405, 1.493, 2.072,
        1.299, 1.941, 1.834, 1.349, 1.667, 1.822, 1.446, 1.421, 1.831, 1.213,
        1.301, 1.891, 1.801, 1.755, 1.465, 1.627, 1.882, 1.774, 1.078, 2.116,
        1.835, 1.828, 1.609, 1.712, 2.053, 1.646, 1.643, 1.627, 1.567, 1.925,
        1.594, 1.407, 1.532, 1.793, 1.280, 1.870, 1.443, 1.768, 2.042, 1.957,
        1.272, 1.688, 1.829, 1.702, 0.529, 1.924, 1.170, 2.126, 1.405, 1.712,
        1.828, 2.048, 1.318, 2.086, 1.671, 1.287, 2.051, 2.277, 1.766, 1.493,
        1.824, 1.550, 1.397, 1.334, 1.710, 2.018, 1.144, 1.929, 1.442, 1.770,
        1.328, 1.487, 2.016, 1.323, 1.487, 1.095, 1.819, 1.823, 1.788, 1.386,
        1.091, 2.093, 2.188, 1.729, 1.908, 1.774, 1.669, 1.972, 1.349, 1.775,
        1.354, 2.056, 2.097, 1.887, 1.466, 0.625, 1.892, 1.522, 1.957, 1.535,
        1.676, 1.775, 1.997, 2.144, 1.096, 1.002, 1.666, 1.165, 1.397, 1.971,
        1.458, 2.308, 1.365, 1.681, 1.560, 1.989, 1.758, 1.405, 2.006, 1.123,
        2.005, 1.610, 1.201, 1.823, 1.294, 2.072, 1.883, 1.705, 1.787, 1.791],
       device='cuda:0')
b after update for 1 param tensor([167.713, 181.421, 185.565, 165.260, 161.030, 161.477, 165.329, 173.922,
        184.788, 164.619, 179.069, 175.665, 177.492, 146.709, 159.340, 164.287,
        156.489, 169.622, 171.053, 186.235, 166.801, 130.000, 150.431, 163.785,
        167.787, 124.755, 190.058, 164.443, 170.074, 157.265, 181.672, 161.133,
        166.546, 178.355, 166.732, 183.557, 165.666, 141.452, 152.064, 189.694,
        141.096, 180.915, 164.897, 179.643, 131.570, 153.231, 180.934, 165.632,
        186.072, 122.882, 192.190, 170.872, 175.535, 148.479, 183.788, 173.281,
        182.114, 169.338, 183.783, 158.098, 158.442, 174.559, 187.064, 174.908,
        165.149, 171.406, 176.728, 172.601, 190.140, 151.441, 141.630, 150.914,
        150.898, 168.747, 146.431, 175.975, 182.256, 152.522, 163.483, 129.593,
        173.207, 149.303, 111.385, 133.184, 163.559, 152.409, 159.819, 158.780,
        173.826, 177.674, 173.966, 181.496, 197.089, 155.447, 173.713, 180.194,
        175.320, 168.375, 188.394, 170.542, 173.647, 180.245, 176.157, 140.287,
        168.031, 180.272, 154.194, 144.344, 152.839, 142.384, 143.840, 156.112,
        173.986, 165.674, 190.384, 164.656, 181.647, 161.582, 193.632, 164.073,
        178.293, 182.873, 154.104, 156.670, 146.456, 179.356, 167.663, 180.904,
        173.279, 184.797, 128.248, 155.195, 165.061, 167.905, 161.368, 151.785,
        175.173, 179.812, 163.082, 169.847, 124.983, 159.505, 191.424, 177.121,
        174.196, 173.725, 164.832,  92.014, 169.393, 152.607, 162.725, 171.507,
        168.890, 193.390, 159.419, 181.198, 145.031, 148.816, 163.170, 166.343,
        150.453, 164.426, 171.919, 159.004, 164.844, 158.152, 152.294, 151.714,
        156.396, 184.252, 145.870, 178.318, 173.326, 148.664, 165.245, 172.775,
        153.934, 152.568, 173.187, 140.949, 145.978, 176.011, 171.758, 169.552,
        154.912, 163.269, 175.585, 170.493, 132.879, 186.175, 173.367, 173.067,
        162.350, 167.468, 183.373, 164.219, 164.067, 163.252, 160.208, 177.582,
        161.608, 151.810, 158.440, 171.403, 144.794, 175.011, 153.762, 170.192,
        182.893, 179.038, 144.342, 166.283, 173.076, 166.999,  93.135, 177.514,
        138.427, 186.619, 151.708, 167.482, 173.049, 183.175, 146.915, 184.857,
        165.455, 145.175, 183.313, 193.122, 170.112, 156.391, 172.859, 159.338,
        151.274, 147.819, 167.378, 181.843, 136.873, 177.788, 153.702, 170.275,
        147.481, 156.061, 181.729, 147.243, 156.054, 133.953, 172.601, 172.822,
        171.127, 150.696, 133.716, 185.147, 189.307, 168.317, 176.774, 170.461,
        165.343, 179.745, 148.681, 170.545, 148.957, 183.512, 185.355, 175.826,
        154.956, 101.177, 176.058, 157.899, 179.040, 158.567, 165.709, 170.539,
        180.862, 187.406, 134.011, 128.143, 165.187, 138.158, 151.275, 179.674,
        154.545, 194.428, 149.542, 165.949, 159.860, 180.514, 169.727, 151.687,
        181.267, 135.628, 181.226, 162.416, 140.288, 172.811, 145.584, 184.217,
        175.657, 167.142, 171.077, 171.314], device='cuda:0')
clipping threshold 1.785079240384182
a after update for 1 param tensor([[[ 0.010],
         [ 0.146],
         [-0.396],
         [ 0.170],
         [ 0.050],
         [ 0.042],
         [-0.135],
         [ 0.291],
         [-0.180],
         [-0.331]],

        [[ 0.026],
         [ 0.461],
         [ 0.173],
         [ 0.079],
         [ 0.556],
         [-0.081],
         [ 0.029],
         [-0.078],
         [ 0.103],
         [ 0.005]],

        [[ 0.219],
         [ 0.071],
         [ 0.027],
         [-0.046],
         [ 0.304],
         [-0.194],
         [-0.095],
         [-0.196],
         [ 0.162],
         [-0.033]],

        [[-0.016],
         [ 0.096],
         [ 0.147],
         [ 0.183],
         [-0.138],
         [-0.134],
         [-0.111],
         [-0.127],
         [ 0.012],
         [ 0.174]],

        [[ 0.167],
         [ 0.023],
         [ 0.084],
         [ 0.073],
         [-0.131],
         [-0.084],
         [-0.041],
         [-0.122],
         [-0.133],
         [-0.123]],

        [[-0.134],
         [-0.106],
         [-0.142],
         [-0.152],
         [ 0.094],
         [ 0.198],
         [ 0.159],
         [-0.102],
         [-0.304],
         [ 0.165]],

        [[-0.229],
         [ 0.186],
         [ 0.349],
         [-0.061],
         [-0.092],
         [-0.380],
         [ 0.121],
         [ 0.010],
         [-0.059],
         [ 0.086]],

        [[ 0.237],
         [-0.111],
         [-0.033],
         [-0.158],
         [ 0.258],
         [-0.077],
         [-0.005],
         [-0.353],
         [-0.160],
         [-0.143]],

        [[-0.035],
         [-0.020],
         [ 0.301],
         [ 0.053],
         [-0.193],
         [ 0.266],
         [ 0.029],
         [ 0.034],
         [ 0.077],
         [ 0.035]],

        [[ 0.221],
         [-0.038],
         [ 0.049],
         [-0.056],
         [-0.021],
         [ 0.210],
         [ 0.274],
         [-0.263],
         [-0.033],
         [ 0.042]],

        [[ 0.285],
         [-0.302],
         [ 0.122],
         [ 0.035],
         [-0.117],
         [ 0.190],
         [-0.371],
         [-0.166],
         [-0.182],
         [-0.105]],

        [[ 0.031],
         [-0.132],
         [-0.089],
         [-0.090],
         [-0.048],
         [ 0.011],
         [ 0.200],
         [-0.102],
         [ 0.085],
         [-0.208]],

        [[-0.233],
         [ 0.199],
         [ 0.362],
         [ 0.018],
         [ 0.273],
         [ 0.279],
         [ 0.033],
         [ 0.063],
         [ 0.046],
         [-0.368]],

        [[-0.143],
         [-0.120],
         [-0.036],
         [ 0.222],
         [-0.080],
         [-0.107],
         [ 0.164],
         [ 0.119],
         [-0.123],
         [-0.031]],

        [[ 0.006],
         [-0.062],
         [-0.085],
         [-0.105],
         [-0.002],
         [-0.023],
         [ 0.115],
         [ 0.137],
         [-0.143],
         [-0.043]],

        [[-0.231],
         [-0.015],
         [ 0.058],
         [ 0.126],
         [ 0.065],
         [-0.203],
         [-0.160],
         [-0.035],
         [-0.237],
         [ 0.002]],

        [[-0.054],
         [-0.032],
         [-0.077],
         [-0.033],
         [-0.027],
         [ 0.140],
         [-0.148],
         [ 0.038],
         [ 0.264],
         [ 0.146]],

        [[ 0.064],
         [ 0.037],
         [ 0.136],
         [ 0.063],
         [-0.015],
         [ 0.147],
         [ 0.013],
         [ 0.005],
         [-0.307],
         [ 0.086]],

        [[ 0.231],
         [ 0.292],
         [ 0.060],
         [ 0.313],
         [-0.051],
         [-0.022],
         [-0.025],
         [-0.040],
         [ 0.042],
         [-0.033]],

        [[ 0.438],
         [ 0.057],
         [ 0.331],
         [-0.063],
         [ 0.007],
         [-0.425],
         [ 0.302],
         [-0.157],
         [-0.076],
         [-0.093]],

        [[ 0.001],
         [ 0.226],
         [ 0.165],
         [ 0.096],
         [ 0.003],
         [-0.128],
         [ 0.003],
         [-0.010],
         [ 0.075],
         [ 0.208]],

        [[ 0.012],
         [-0.059],
         [-0.003],
         [ 0.357],
         [-0.058],
         [-0.028],
         [-0.048],
         [ 0.088],
         [ 0.189],
         [-0.428]],

        [[-0.202],
         [ 0.089],
         [-0.089],
         [ 0.202],
         [-0.002],
         [ 0.268],
         [-0.073],
         [-0.013],
         [ 0.053],
         [-0.236]],

        [[ 0.208],
         [-0.098],
         [-0.065],
         [-0.006],
         [ 0.148],
         [ 0.183],
         [ 0.210],
         [-0.191],
         [-0.102],
         [ 0.047]],

        [[-0.124],
         [-0.008],
         [-0.111],
         [ 0.092],
         [-0.012],
         [-0.047],
         [-0.388],
         [-0.154],
         [ 0.115],
         [ 0.139]],

        [[-0.107],
         [-0.054],
         [-0.469],
         [-0.060],
         [-0.016],
         [-0.177],
         [ 0.007],
         [-0.110],
         [-0.061],
         [-0.064]],

        [[ 0.142],
         [ 0.234],
         [-0.098],
         [-0.008],
         [ 0.141],
         [-0.052],
         [ 0.007],
         [-0.008],
         [ 0.239],
         [ 0.136]],

        [[-0.055],
         [-0.183],
         [ 0.085],
         [ 0.099],
         [-0.086],
         [ 0.117],
         [ 0.182],
         [-0.065],
         [ 0.234],
         [-0.004]],

        [[ 0.174],
         [ 0.229],
         [-0.028],
         [-0.143],
         [ 0.031],
         [-0.120],
         [ 0.100],
         [ 0.229],
         [ 0.363],
         [ 0.015]],

        [[ 0.061],
         [-0.287],
         [ 0.170],
         [ 0.168],
         [-0.132],
         [-0.087],
         [ 0.116],
         [ 0.053],
         [-0.187],
         [ 0.383]]], device='cuda:0')
s after update for 1 param tensor([[[2.057],
         [1.759],
         [2.343],
         [1.607],
         [1.511],
         [1.481],
         [2.035],
         [2.546],
         [1.827],
         [1.822]],

        [[1.378],
         [2.265],
         [1.640],
         [1.591],
         [1.841],
         [2.010],
         [1.862],
         [1.185],
         [1.765],
         [1.554]],

        [[1.938],
         [1.907],
         [1.471],
         [2.013],
         [1.355],
         [1.941],
         [1.293],
         [2.108],
         [1.777],
         [1.232]],

        [[1.952],
         [1.993],
         [1.796],
         [1.673],
         [0.981],
         [1.729],
         [0.976],
         [1.514],
         [1.810],
         [2.240]],

        [[1.493],
         [1.708],
         [1.573],
         [1.884],
         [1.754],
         [2.165],
         [1.424],
         [2.047],
         [2.246],
         [1.736]],

        [[2.080],
         [1.616],
         [1.157],
         [1.300],
         [1.575],
         [1.366],
         [2.138],
         [1.526],
         [2.077],
         [1.971]],

        [[2.092],
         [1.324],
         [2.087],
         [1.812],
         [1.011],
         [1.769],
         [1.473],
         [1.485],
         [1.788],
         [1.571]],

        [[1.266],
         [1.425],
         [1.838],
         [1.458],
         [1.861],
         [2.181],
         [1.010],
         [1.833],
         [1.403],
         [1.937]],

        [[0.811],
         [1.582],
         [1.561],
         [1.536],
         [1.623],
         [1.736],
         [1.433],
         [1.153],
         [2.271],
         [1.261]],

        [[2.049],
         [1.153],
         [1.827],
         [1.590],
         [0.981],
         [1.776],
         [1.296],
         [1.569],
         [1.475],
         [1.624]],

        [[1.641],
         [1.398],
         [1.566],
         [1.851],
         [1.968],
         [1.434],
         [2.188],
         [2.120],
         [1.561],
         [1.747]],

        [[1.620],
         [1.602],
         [1.315],
         [1.818],
         [1.119],
         [1.140],
         [2.054],
         [1.637],
         [1.086],
         [1.612]],

        [[2.220],
         [1.683],
         [1.810],
         [0.802],
         [1.323],
         [1.808],
         [1.013],
         [1.860],
         [1.444],
         [2.398]],

        [[1.739],
         [1.923],
         [1.046],
         [1.864],
         [1.511],
         [1.421],
         [2.243],
         [1.298],
         [1.714],
         [1.618]],

        [[1.710],
         [1.550],
         [1.662],
         [1.853],
         [0.044],
         [1.412],
         [0.832],
         [1.474],
         [1.850],
         [1.894]],

        [[1.576],
         [1.821],
         [2.122],
         [1.807],
         [1.500],
         [1.944],
         [1.336],
         [1.988],
         [1.760],
         [1.582]],

        [[1.420],
         [1.869],
         [1.303],
         [1.162],
         [1.723],
         [1.965],
         [2.216],
         [1.286],
         [2.389],
         [1.627]],

        [[1.844],
         [1.244],
         [1.972],
         [1.553],
         [2.028],
         [1.092],
         [1.415],
         [1.760],
         [1.948],
         [2.008]],

        [[1.758],
         [1.990],
         [1.786],
         [1.919],
         [0.839],
         [1.890],
         [1.520],
         [1.578],
         [1.798],
         [1.269]],

        [[2.068],
         [1.547],
         [1.496],
         [1.661],
         [2.098],
         [2.001],
         [1.807],
         [1.062],
         [1.537],
         [0.904]],

        [[1.081],
         [1.757],
         [1.905],
         [1.562],
         [1.830],
         [1.312],
         [1.430],
         [2.124],
         [1.688],
         [1.579]],

        [[0.930],
         [1.762],
         [1.329],
         [1.984],
         [1.722],
         [1.372],
         [2.099],
         [1.420],
         [1.961],
         [1.755]],

        [[1.568],
         [1.215],
         [1.934],
         [1.504],
         [1.454],
         [1.548],
         [2.183],
         [1.386],
         [1.066],
         [2.247]],

        [[1.920],
         [1.487],
         [1.331],
         [1.671],
         [1.265],
         [1.818],
         [2.014],
         [1.849],
         [1.117],
         [2.009]],

        [[1.258],
         [1.871],
         [1.164],
         [1.888],
         [1.980],
         [1.353],
         [2.071],
         [1.544],
         [1.668],
         [1.783]],

        [[1.633],
         [1.459],
         [1.565],
         [2.099],
         [1.264],
         [1.365],
         [1.989],
         [1.949],
         [2.122],
         [0.981]],

        [[2.239],
         [0.919],
         [1.647],
         [1.334],
         [1.877],
         [1.700],
         [1.618],
         [1.003],
         [1.540],
         [1.691]],

        [[1.631],
         [1.459],
         [1.995],
         [1.500],
         [1.683],
         [1.400],
         [1.732],
         [1.322],
         [1.965],
         [1.504]],

        [[1.417],
         [1.841],
         [1.325],
         [2.014],
         [1.600],
         [1.851],
         [1.653],
         [1.454],
         [1.367],
         [1.542]],

        [[1.440],
         [1.519],
         [1.665],
         [1.615],
         [1.795],
         [1.641],
         [1.405],
         [1.497],
         [1.090],
         [1.598]]], device='cuda:0')
b after update for 1 param tensor([[[183.549],
         [169.730],
         [195.912],
         [162.238],
         [157.315],
         [155.784],
         [182.568],
         [204.235],
         [173.009],
         [172.784]],

        [[150.231],
         [192.639],
         [163.902],
         [161.466],
         [173.680],
         [181.459],
         [174.654],
         [139.352],
         [170.060],
         [159.565]],

        [[178.172],
         [176.744],
         [155.249],
         [181.586],
         [148.980],
         [178.327],
         [145.523],
         [185.812],
         [170.608],
         [142.054]],

        [[178.809],
         [180.702],
         [171.524],
         [165.550],
         [126.769],
         [168.296],
         [126.478],
         [157.483],
         [172.216],
         [191.577]],

        [[156.386],
         [167.277],
         [160.552],
         [175.662],
         [169.510],
         [188.325],
         [152.751],
         [183.139],
         [191.826],
         [168.620]],

        [[184.582],
         [162.689],
         [137.700],
         [145.948],
         [160.610],
         [149.570],
         [187.154],
         [158.122],
         [184.455],
         [179.704]],

        [[185.132],
         [147.302],
         [184.890],
         [172.311],
         [128.719],
         [170.238],
         [155.330],
         [155.949],
         [171.123],
         [160.432]],

        [[143.998],
         [152.784],
         [173.538],
         [154.563],
         [174.583],
         [189.027],
         [128.658],
         [173.297],
         [151.579],
         [178.113]],

        [[115.245],
         [160.976],
         [159.893],
         [158.611],
         [163.079],
         [168.660],
         [153.206],
         [137.427],
         [192.864],
         [143.754]],

        [[183.191],
         [137.448],
         [172.987],
         [161.389],
         [126.747],
         [170.593],
         [145.723],
         [160.299],
         [155.471],
         [163.103]],

        [[163.971],
         [151.339],
         [160.170],
         [174.142],
         [179.552],
         [153.278],
         [189.343],
         [186.376],
         [159.902],
         [169.185]],

        [[162.916],
         [162.000],
         [146.781],
         [172.568],
         [135.416],
         [136.635],
         [183.457],
         [163.741],
         [133.397],
         [162.516]],

        [[190.725],
         [166.059],
         [172.192],
         [114.613],
         [147.222],
         [172.103],
         [128.800],
         [174.544],
         [153.808],
         [198.186]],

        [[168.807],
         [177.510],
         [130.883],
         [174.731],
         [157.306],
         [152.574],
         [191.689],
         [145.814],
         [167.576],
         [162.809]],

        [[167.376],
         [159.331],
         [164.998],
         [174.242],
         [ 26.810],
         [152.076],
         [116.762],
         [155.418],
         [174.097],
         [176.142]],

        [[160.687],
         [172.704],
         [186.451],
         [172.035],
         [156.743],
         [178.455],
         [147.937],
         [180.484],
         [169.819],
         [160.964]],

        [[152.535],
         [175.000],
         [146.125],
         [137.955],
         [168.031],
         [179.405],
         [190.538],
         [145.156],
         [197.819],
         [163.280]],

        [[173.823],
         [142.779],
         [179.752],
         [159.485],
         [182.275],
         [133.721],
         [152.229],
         [169.821],
         [178.648],
         [181.369]],

        [[169.698],
         [180.553],
         [171.033],
         [177.323],
         [117.269],
         [175.944],
         [157.782],
         [160.792],
         [171.605],
         [144.203]],

        [[184.082],
         [159.209],
         [156.550],
         [164.942],
         [185.385],
         [181.044],
         [172.071],
         [131.904],
         [158.685],
         [121.726]],

        [[133.102],
         [169.636],
         [176.665],
         [159.978],
         [173.138],
         [146.581],
         [153.042],
         [186.534],
         [166.292],
         [160.852]],

        [[123.450],
         [169.879],
         [147.531],
         [180.293],
         [167.935],
         [149.937],
         [185.425],
         [152.541],
         [179.238],
         [169.562]],

        [[160.296],
         [141.108],
         [177.985],
         [156.966],
         [154.338],
         [159.251],
         [189.095],
         [150.676],
         [132.155],
         [191.866]],

        [[177.355],
         [156.051],
         [147.684],
         [165.464],
         [143.961],
         [172.589],
         [181.653],
         [174.052],
         [135.287],
         [181.415]],

        [[143.541],
         [175.072],
         [138.117],
         [175.878],
         [180.105],
         [148.905],
         [184.195],
         [159.056],
         [165.295],
         [170.922]],

        [[163.548],
         [154.580],
         [160.116],
         [185.439],
         [143.892],
         [149.511],
         [180.495],
         [178.673],
         [186.454],
         [126.776]],

        [[191.534],
         [122.707],
         [164.271],
         [147.855],
         [175.359],
         [166.894],
         [162.791],
         [128.167],
         [158.848],
         [166.434]],

        [[163.452],
         [154.582],
         [180.771],
         [156.771],
         [166.031],
         [151.420],
         [168.453],
         [147.188],
         [179.419],
         [156.979]],

        [[152.364],
         [173.649],
         [147.308],
         [181.651],
         [161.887],
         [174.133],
         [164.575],
         [154.316],
         [149.660],
         [158.953]],

        [[153.592],
         [157.764],
         [165.138],
         [162.652],
         [171.470],
         [163.946],
         [151.715],
         [156.611],
         [133.653],
         [161.814]]], device='cuda:0')
clipping threshold 1.785079240384182
a after update for 1 param tensor([[-0.055],
        [ 0.098],
        [ 0.266],
        [ 0.138],
        [-0.050],
        [ 0.268],
        [ 0.178],
        [ 0.106],
        [-0.206],
        [ 0.123],
        [-0.152],
        [ 0.086],
        [-0.029],
        [ 0.032],
        [ 0.017],
        [ 0.068],
        [-0.088],
        [ 0.129],
        [ 0.180],
        [ 0.111],
        [-0.301],
        [ 0.261],
        [ 0.066],
        [ 0.015],
        [ 0.291],
        [ 0.191],
        [-0.234],
        [ 0.071],
        [ 0.238],
        [ 0.084]], device='cuda:0')
s after update for 1 param tensor([[1.179],
        [1.629],
        [1.654],
        [1.815],
        [1.831],
        [1.715],
        [1.920],
        [1.654],
        [1.627],
        [1.774],
        [1.505],
        [2.167],
        [1.389],
        [1.570],
        [1.653],
        [1.804],
        [1.491],
        [1.662],
        [0.889],
        [1.117],
        [1.843],
        [1.468],
        [1.382],
        [1.656],
        [2.019],
        [1.674],
        [2.303],
        [1.322],
        [1.471],
        [1.832]], device='cuda:0')
b after update for 1 param tensor([[138.967],
        [163.368],
        [164.589],
        [172.445],
        [173.211],
        [167.611],
        [177.346],
        [164.610],
        [163.275],
        [170.488],
        [156.995],
        [188.396],
        [150.829],
        [160.376],
        [164.568],
        [171.914],
        [156.265],
        [165.003],
        [120.672],
        [135.300],
        [173.741],
        [155.082],
        [150.442],
        [164.719],
        [181.868],
        [165.622],
        [194.249],
        [147.150],
        [155.252],
        [173.217]], device='cuda:0')
clipping threshold 1.785079240384182
||w||^2 3.870252755929215
exp ma of ||w||^2 4.892856703549626
||w|| 1.9672957977714522
exp ma of ||w|| 2.0652890278319953
||w||^2 3.84675327595295
exp ma of ||w||^2 3.3364182590892897
||w|| 1.9613141706399182
exp ma of ||w|| 1.7822614473626748
cuda
Objective function 120.85 = squared loss an data 96.78 + 0.5*rho*h**2 13.399239 + alpha*h 8.680040 + L2reg 1.67 + L1reg 0.32 ; SHD = 123 ; DAG True
Proportion of microbatches that were clipped  0.7869327325394271
iteration 2 in inner loop, alpha 53.02328868678913 rho 1000.0 h 0.16370240784478085
9630
cuda
Objective function 241.44 = squared loss an data 96.78 + 0.5*rho*h**2 133.992392 + alpha*h 8.680040 + L2reg 1.67 + L1reg 0.32 ; SHD = 123 ; DAG True
||w||^2 68.49661108131652
exp ma of ||w||^2 16253.467784655784
||w|| 8.276267944026252
exp ma of ||w|| 16.903154408275498
||w||^2 2.769700379843185
exp ma of ||w||^2 5.188143761079756
||w|| 1.6642416831227325
exp ma of ||w|| 2.209187616888283
||w||^2 4.534849268591662
exp ma of ||w||^2 4.686079779081755
||w|| 2.129518553239596
exp ma of ||w|| 2.0958916485424464
||w||^2 6.524535297690962
exp ma of ||w||^2 4.697978283610552
||w|| 2.554316992405399
exp ma of ||w|| 2.0768197908084076
||w||^2 6.661382386083586
exp ma of ||w||^2 5.124140439627272
||w|| 2.5809653980794836
exp ma of ||w|| 2.173997261292729
cuda
Objective function 126.57 = squared loss an data 101.92 + 0.5*rho*h**2 19.354390 + alpha*h 3.298917 + L2reg 1.73 + L1reg 0.27 ; SHD = 118 ; DAG True
Proportion of microbatches that were clipped  0.7938277589271288
iteration 3 in inner loop, alpha 53.02328868678913 rho 10000.0 h 0.06221637972882377
iteration 3 in outer loop, alpha = 675.1870859750268, rho = 10000.0, h = 0.06221637972882377
cuda
9630
cuda
Objective function 165.28 = squared loss an data 101.92 + 0.5*rho*h**2 19.354390 + alpha*h 42.007696 + L2reg 1.73 + L1reg 0.27 ; SHD = 118 ; DAG True
||w||^2 36.814725560717676
exp ma of ||w||^2 4286.018076556388
||w|| 6.067513952247467
exp ma of ||w|| 9.121086336724753
||w||^2 19.06007768154423
exp ma of ||w||^2 1046.5668952792732
||w|| 4.365784887227522
exp ma of ||w|| 5.721197679107091
||w||^2 5.413502883280581
exp ma of ||w||^2 106.94402433538008
||w|| 2.3266935516480425
exp ma of ||w|| 3.63191843577041
cuda
Objective function 138.65 = squared loss an data 99.63 + 0.5*rho*h**2 8.742221 + alpha*h 28.232545 + L2reg 1.79 + L1reg 0.25 ; SHD = 125 ; DAG True
Proportion of microbatches that were clipped  0.7990773146675151
iteration 1 in inner loop, alpha 675.1870859750268 rho 10000.0 h 0.04181440284180127
9630
cuda
Objective function 217.33 = squared loss an data 99.63 + 0.5*rho*h**2 87.422214 + alpha*h 28.232545 + L2reg 1.79 + L1reg 0.25 ; SHD = 125 ; DAG True
||w||^2 10280232880581.256
exp ma of ||w||^2 6270502926766.825
||w|| 3206280.224899448
exp ma of ||w|| 2203339.9886392266
||w||^2 674.45536080853
exp ma of ||w||^2 1328536.854656054
||w|| 25.970278412225966
exp ma of ||w|| 75.17301828540008
||w||^2 7.744189631484205
exp ma of ||w||^2 9.352843154414375
||w|| 2.7828384127513055
exp ma of ||w|| 2.994047740493882
cuda
Objective function 136.19 = squared loss an data 103.37 + 0.5*rho*h**2 17.954917 + alpha*h 12.794721 + L2reg 1.85 + L1reg 0.22 ; SHD = 121 ; DAG True
Proportion of microbatches that were clipped  0.7975728155339806
iteration 2 in inner loop, alpha 675.1870859750268 rho 100000.0 h 0.018949890434157624
iteration 4 in outer loop, alpha = 19625.07752013265, rho = 1000000.0, h = 0.018949890434157624
Threshold 0.3
[[0.005 0.092 0.042 0.042 0.142 0.038 0.017 0.069 0.053 0.037 0.08  0.064
  0.036 0.136 0.022 0.033 0.129 0.063 0.433 0.029 0.116 0.047 0.033 0.104
  0.052 0.033 0.082 0.029 0.023 0.063]
 [0.074 0.006 0.06  0.068 0.124 0.043 0.115 0.032 0.102 0.052 0.074 0.07
  0.017 0.088 0.034 0.085 0.057 0.046 0.107 0.053 0.146 0.129 0.085 0.185
  0.06  0.064 0.155 0.139 0.075 0.079]
 [0.132 0.066 0.003 0.072 0.1   0.038 0.089 0.036 0.099 0.056 0.114 0.14
  0.05  0.052 0.098 0.137 0.192 0.104 0.136 0.058 0.111 0.058 0.128 0.085
  0.141 0.068 0.12  0.071 0.106 0.084]
 [0.12  0.047 0.089 0.002 0.127 0.034 0.046 0.076 0.166 0.082 0.05  0.16
  0.045 0.237 0.067 0.155 0.119 0.051 0.206 0.043 0.08  0.078 0.047 0.25
  0.149 0.044 0.063 0.124 0.062 0.053]
 [0.041 0.035 0.058 0.04  0.004 0.035 0.024 0.013 0.017 0.035 0.041 0.064
  0.01  0.036 0.024 0.041 0.029 0.034 0.055 0.02  0.059 0.023 0.059 0.042
  0.059 0.036 0.05  0.082 0.087 0.018]
 [0.191 0.128 0.18  0.222 0.209 0.007 0.202 0.095 0.333 0.111 0.208 0.336
  0.092 0.22  0.132 0.179 0.302 0.13  0.143 0.11  0.173 0.142 0.057 0.207
  0.097 0.084 0.064 0.1   0.26  0.098]
 [0.23  0.065 0.088 0.137 0.227 0.029 0.006 0.155 0.248 0.304 0.177 0.198
  0.049 0.346 0.032 0.242 0.069 0.082 0.45  0.037 0.244 0.084 0.15  0.264
  0.26  0.112 0.077 0.189 0.077 0.232]
 [0.095 0.2   0.113 0.072 0.417 0.061 0.037 0.005 0.147 0.062 0.089 0.087
  0.105 0.211 0.043 0.089 0.177 0.072 0.214 0.073 0.094 0.129 0.093 0.113
  0.08  0.025 0.076 0.056 0.178 0.065]
 [0.14  0.077 0.09  0.037 0.26  0.018 0.029 0.033 0.005 0.062 0.046 0.094
  0.029 0.078 0.031 0.105 0.221 0.064 0.168 0.026 0.043 0.133 0.067 0.068
  0.066 0.04  0.068 0.09  0.024 0.085]
 [0.193 0.145 0.102 0.057 0.178 0.043 0.026 0.096 0.071 0.005 0.166 0.121
  0.043 0.113 0.049 0.135 0.098 0.072 0.188 0.06  0.24  0.189 0.07  0.186
  0.069 0.023 0.052 0.064 0.087 0.052]
 [0.071 0.072 0.061 0.069 0.14  0.021 0.036 0.077 0.129 0.056 0.004 0.088
  0.035 0.192 0.031 0.044 0.075 0.07  0.128 0.071 0.072 0.067 0.095 0.08
  0.037 0.009 0.037 0.066 0.109 0.069]
 [0.056 0.082 0.048 0.043 0.096 0.016 0.03  0.07  0.044 0.045 0.044 0.006
  0.017 0.128 0.027 0.141 0.08  0.04  0.07  0.034 0.053 0.026 0.06  0.072
  0.049 0.031 0.068 0.035 0.033 0.016]
 [0.143 0.302 0.131 0.16  0.459 0.075 0.129 0.072 0.12  0.116 0.129 0.427
  0.005 0.178 0.175 0.11  0.082 0.083 0.28  0.141 0.203 0.191 0.132 0.217
  0.093 0.05  0.186 0.098 0.133 0.163]
 [0.047 0.046 0.181 0.033 0.172 0.029 0.011 0.029 0.108 0.082 0.022 0.083
  0.046 0.004 0.039 0.071 0.054 0.033 0.115 0.068 0.137 0.072 0.07  0.071
  0.047 0.048 0.072 0.06  0.087 0.058]
 [0.304 0.214 0.055 0.178 0.176 0.038 0.188 0.094 0.194 0.169 0.112 0.159
  0.041 0.163 0.003 0.073 0.242 0.104 0.571 0.19  0.215 0.099 0.122 0.657
  0.067 0.046 0.113 0.294 0.303 0.09 ]
 [0.143 0.079 0.06  0.041 0.185 0.048 0.028 0.07  0.06  0.05  0.134 0.044
  0.049 0.06  0.075 0.004 0.084 0.036 0.125 0.023 0.063 0.042 0.04  0.087
  0.036 0.026 0.046 0.101 0.086 0.08 ]
 [0.057 0.077 0.035 0.086 0.207 0.022 0.051 0.039 0.033 0.067 0.074 0.08
  0.033 0.091 0.021 0.051 0.004 0.045 0.199 0.049 0.097 0.045 0.048 0.075
  0.066 0.036 0.051 0.062 0.056 0.03 ]
 [0.127 0.185 0.076 0.146 0.158 0.054 0.068 0.103 0.098 0.106 0.079 0.175
  0.064 0.148 0.067 0.133 0.136 0.002 0.201 0.073 0.078 0.089 0.141 0.212
  0.109 0.049 0.173 0.153 0.068 0.249]
 [0.019 0.052 0.045 0.024 0.109 0.034 0.013 0.029 0.037 0.018 0.041 0.084
  0.025 0.048 0.009 0.045 0.025 0.032 0.005 0.015 0.1   0.032 0.036 0.296
  0.038 0.04  0.041 0.042 0.037 0.045]
 [0.231 0.135 0.139 0.094 0.257 0.07  0.134 0.062 0.285 0.075 0.08  0.248
  0.044 0.071 0.058 0.203 0.074 0.107 0.315 0.004 0.391 0.159 0.15  0.514
  0.079 0.052 0.116 0.133 0.675 0.1  ]
 [0.054 0.049 0.055 0.06  0.118 0.02  0.016 0.067 0.142 0.025 0.101 0.13
  0.022 0.049 0.027 0.086 0.074 0.057 0.056 0.02  0.005 0.05  0.04  0.081
  0.038 0.02  0.057 0.047 0.059 0.057]
 [0.143 0.024 0.1   0.076 0.291 0.054 0.048 0.052 0.035 0.056 0.1   0.206
  0.042 0.07  0.06  0.109 0.114 0.073 0.149 0.032 0.089 0.003 0.087 0.112
  0.058 0.031 0.046 0.073 0.078 0.061]
 [0.124 0.037 0.057 0.087 0.117 0.067 0.044 0.051 0.066 0.1   0.075 0.062
  0.046 0.101 0.037 0.132 0.114 0.056 0.137 0.049 0.148 0.098 0.005 0.217
  0.074 0.036 0.065 0.054 0.084 0.046]
 [0.056 0.05  0.044 0.024 0.11  0.022 0.018 0.055 0.087 0.022 0.069 0.085
  0.026 0.084 0.007 0.061 0.058 0.019 0.027 0.014 0.094 0.053 0.025 0.005
  0.032 0.022 0.044 0.03  0.018 0.04 ]
 [0.15  0.105 0.045 0.047 0.109 0.065 0.021 0.07  0.11  0.114 0.137 0.097
  0.06  0.171 0.1   0.099 0.076 0.05  0.086 0.057 0.156 0.078 0.061 0.161
  0.005 0.089 0.124 0.156 0.169 0.067]
 [0.213 0.102 0.145 0.176 0.089 0.078 0.06  0.199 0.179 0.22  0.733 0.133
  0.127 0.112 0.125 0.22  0.149 0.091 0.115 0.074 0.261 0.185 0.119 0.197
  0.086 0.004 0.136 0.288 0.316 0.122]
 [0.061 0.046 0.052 0.114 0.134 0.075 0.076 0.065 0.115 0.073 0.21  0.071
  0.033 0.093 0.047 0.109 0.113 0.048 0.227 0.051 0.1   0.108 0.094 0.138
  0.04  0.038 0.005 0.132 0.152 0.07 ]
 [0.128 0.043 0.081 0.061 0.049 0.079 0.033 0.091 0.1   0.076 0.082 0.184
  0.039 0.131 0.018 0.057 0.101 0.029 0.119 0.059 0.167 0.114 0.114 0.137
  0.052 0.017 0.036 0.003 0.063 0.049]
 [0.162 0.084 0.058 0.057 0.078 0.032 0.055 0.052 0.068 0.051 0.111 0.18
  0.039 0.055 0.021 0.036 0.071 0.097 0.178 0.009 0.102 0.073 0.073 0.288
  0.03  0.017 0.044 0.115 0.006 0.063]
 [0.093 0.053 0.09  0.123 0.274 0.068 0.018 0.111 0.072 0.132 0.131 0.254
  0.029 0.118 0.057 0.077 0.14  0.031 0.087 0.068 0.151 0.11  0.105 0.11
  0.09  0.043 0.093 0.11  0.089 0.007]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.433 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.333 0.    0.    0.336
  0.    0.    0.    0.    0.302 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.304 0.    0.
  0.    0.346 0.    0.    0.    0.    0.45  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.417 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.302 0.    0.    0.459 0.    0.    0.    0.    0.    0.    0.427
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.304 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.571 0.    0.    0.    0.    0.657
  0.    0.    0.    0.    0.303 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.315 0.    0.391 0.    0.    0.514
  0.    0.    0.    0.    0.675 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.733 0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.316 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5714285714285714, 'tpr': 0.075, 'fpr': 0.0380952380952381, 'f1': 0.12765957446808512, 'shd': 121, 'npred': 21, 'ntrue': 120}
[0.092 0.042 0.042 0.142 0.038 0.017 0.069 0.053 0.037 0.08  0.064 0.036
 0.136 0.022 0.033 0.129 0.063 0.433 0.029 0.116 0.047 0.033 0.104 0.052
 0.033 0.082 0.029 0.023 0.063 0.074 0.06  0.068 0.124 0.043 0.115 0.032
 0.102 0.052 0.074 0.07  0.017 0.088 0.034 0.085 0.057 0.046 0.107 0.053
 0.146 0.129 0.085 0.185 0.06  0.064 0.155 0.139 0.075 0.079 0.132 0.066
 0.072 0.1   0.038 0.089 0.036 0.099 0.056 0.114 0.14  0.05  0.052 0.098
 0.137 0.192 0.104 0.136 0.058 0.111 0.058 0.128 0.085 0.141 0.068 0.12
 0.071 0.106 0.084 0.12  0.047 0.089 0.127 0.034 0.046 0.076 0.166 0.082
 0.05  0.16  0.045 0.237 0.067 0.155 0.119 0.051 0.206 0.043 0.08  0.078
 0.047 0.25  0.149 0.044 0.063 0.124 0.062 0.053 0.041 0.035 0.058 0.04
 0.035 0.024 0.013 0.017 0.035 0.041 0.064 0.01  0.036 0.024 0.041 0.029
 0.034 0.055 0.02  0.059 0.023 0.059 0.042 0.059 0.036 0.05  0.082 0.087
 0.018 0.191 0.128 0.18  0.222 0.209 0.202 0.095 0.333 0.111 0.208 0.336
 0.092 0.22  0.132 0.179 0.302 0.13  0.143 0.11  0.173 0.142 0.057 0.207
 0.097 0.084 0.064 0.1   0.26  0.098 0.23  0.065 0.088 0.137 0.227 0.029
 0.155 0.248 0.304 0.177 0.198 0.049 0.346 0.032 0.242 0.069 0.082 0.45
 0.037 0.244 0.084 0.15  0.264 0.26  0.112 0.077 0.189 0.077 0.232 0.095
 0.2   0.113 0.072 0.417 0.061 0.037 0.147 0.062 0.089 0.087 0.105 0.211
 0.043 0.089 0.177 0.072 0.214 0.073 0.094 0.129 0.093 0.113 0.08  0.025
 0.076 0.056 0.178 0.065 0.14  0.077 0.09  0.037 0.26  0.018 0.029 0.033
 0.062 0.046 0.094 0.029 0.078 0.031 0.105 0.221 0.064 0.168 0.026 0.043
 0.133 0.067 0.068 0.066 0.04  0.068 0.09  0.024 0.085 0.193 0.145 0.102
 0.057 0.178 0.043 0.026 0.096 0.071 0.166 0.121 0.043 0.113 0.049 0.135
 0.098 0.072 0.188 0.06  0.24  0.189 0.07  0.186 0.069 0.023 0.052 0.064
 0.087 0.052 0.071 0.072 0.061 0.069 0.14  0.021 0.036 0.077 0.129 0.056
 0.088 0.035 0.192 0.031 0.044 0.075 0.07  0.128 0.071 0.072 0.067 0.095
 0.08  0.037 0.009 0.037 0.066 0.109 0.069 0.056 0.082 0.048 0.043 0.096
 0.016 0.03  0.07  0.044 0.045 0.044 0.017 0.128 0.027 0.141 0.08  0.04
 0.07  0.034 0.053 0.026 0.06  0.072 0.049 0.031 0.068 0.035 0.033 0.016
 0.143 0.302 0.131 0.16  0.459 0.075 0.129 0.072 0.12  0.116 0.129 0.427
 0.178 0.175 0.11  0.082 0.083 0.28  0.141 0.203 0.191 0.132 0.217 0.093
 0.05  0.186 0.098 0.133 0.163 0.047 0.046 0.181 0.033 0.172 0.029 0.011
 0.029 0.108 0.082 0.022 0.083 0.046 0.039 0.071 0.054 0.033 0.115 0.068
 0.137 0.072 0.07  0.071 0.047 0.048 0.072 0.06  0.087 0.058 0.304 0.214
 0.055 0.178 0.176 0.038 0.188 0.094 0.194 0.169 0.112 0.159 0.041 0.163
 0.073 0.242 0.104 0.571 0.19  0.215 0.099 0.122 0.657 0.067 0.046 0.113
 0.294 0.303 0.09  0.143 0.079 0.06  0.041 0.185 0.048 0.028 0.07  0.06
 0.05  0.134 0.044 0.049 0.06  0.075 0.084 0.036 0.125 0.023 0.063 0.042
 0.04  0.087 0.036 0.026 0.046 0.101 0.086 0.08  0.057 0.077 0.035 0.086
 0.207 0.022 0.051 0.039 0.033 0.067 0.074 0.08  0.033 0.091 0.021 0.051
 0.045 0.199 0.049 0.097 0.045 0.048 0.075 0.066 0.036 0.051 0.062 0.056
 0.03  0.127 0.185 0.076 0.146 0.158 0.054 0.068 0.103 0.098 0.106 0.079
 0.175 0.064 0.148 0.067 0.133 0.136 0.201 0.073 0.078 0.089 0.141 0.212
 0.109 0.049 0.173 0.153 0.068 0.249 0.019 0.052 0.045 0.024 0.109 0.034
 0.013 0.029 0.037 0.018 0.041 0.084 0.025 0.048 0.009 0.045 0.025 0.032
 0.015 0.1   0.032 0.036 0.296 0.038 0.04  0.041 0.042 0.037 0.045 0.231
 0.135 0.139 0.094 0.257 0.07  0.134 0.062 0.285 0.075 0.08  0.248 0.044
 0.071 0.058 0.203 0.074 0.107 0.315 0.391 0.159 0.15  0.514 0.079 0.052
 0.116 0.133 0.675 0.1   0.054 0.049 0.055 0.06  0.118 0.02  0.016 0.067
 0.142 0.025 0.101 0.13  0.022 0.049 0.027 0.086 0.074 0.057 0.056 0.02
 0.05  0.04  0.081 0.038 0.02  0.057 0.047 0.059 0.057 0.143 0.024 0.1
 0.076 0.291 0.054 0.048 0.052 0.035 0.056 0.1   0.206 0.042 0.07  0.06
 0.109 0.114 0.073 0.149 0.032 0.089 0.087 0.112 0.058 0.031 0.046 0.073
 0.078 0.061 0.124 0.037 0.057 0.087 0.117 0.067 0.044 0.051 0.066 0.1
 0.075 0.062 0.046 0.101 0.037 0.132 0.114 0.056 0.137 0.049 0.148 0.098
 0.217 0.074 0.036 0.065 0.054 0.084 0.046 0.056 0.05  0.044 0.024 0.11
 0.022 0.018 0.055 0.087 0.022 0.069 0.085 0.026 0.084 0.007 0.061 0.058
 0.019 0.027 0.014 0.094 0.053 0.025 0.032 0.022 0.044 0.03  0.018 0.04
 0.15  0.105 0.045 0.047 0.109 0.065 0.021 0.07  0.11  0.114 0.137 0.097
 0.06  0.171 0.1   0.099 0.076 0.05  0.086 0.057 0.156 0.078 0.061 0.161
 0.089 0.124 0.156 0.169 0.067 0.213 0.102 0.145 0.176 0.089 0.078 0.06
 0.199 0.179 0.22  0.733 0.133 0.127 0.112 0.125 0.22  0.149 0.091 0.115
 0.074 0.261 0.185 0.119 0.197 0.086 0.136 0.288 0.316 0.122 0.061 0.046
 0.052 0.114 0.134 0.075 0.076 0.065 0.115 0.073 0.21  0.071 0.033 0.093
 0.047 0.109 0.113 0.048 0.227 0.051 0.1   0.108 0.094 0.138 0.04  0.038
 0.132 0.152 0.07  0.128 0.043 0.081 0.061 0.049 0.079 0.033 0.091 0.1
 0.076 0.082 0.184 0.039 0.131 0.018 0.057 0.101 0.029 0.119 0.059 0.167
 0.114 0.114 0.137 0.052 0.017 0.036 0.063 0.049 0.162 0.084 0.058 0.057
 0.078 0.032 0.055 0.052 0.068 0.051 0.111 0.18  0.039 0.055 0.021 0.036
 0.071 0.097 0.178 0.009 0.102 0.073 0.073 0.288 0.03  0.017 0.044 0.115
 0.063 0.093 0.053 0.09  0.123 0.274 0.068 0.018 0.111 0.072 0.132 0.131
 0.254 0.029 0.118 0.057 0.077 0.14  0.031 0.087 0.068 0.151 0.11  0.105
 0.11  0.09  0.043 0.093 0.11  0.089]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
  0. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.5279222222222222, 0.19380527398697894)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
