samples  5000  graph  20 80 ER mlp  minibatch size  100  noise  1.0  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7577874243840341
iteration 1 in outer loop, alpha = 1.7577874243840341, rho = 1.0, h = 1.7577874243840341
cuda
iteration 1 in inner loop,alpha 1.7577874243840341 rho 1.0 h 1.175806803077986
iteration 2 in inner loop,alpha 1.7577874243840341 rho 10.0 h 0.5492398526752922
iteration 3 in inner loop,alpha 1.7577874243840341 rho 100.0 h 0.18559104761474643
iteration 2 in outer loop, alpha = 20.316892185858677, rho = 100.0, h = 0.18559104761474643
cuda
iteration 1 in inner loop,alpha 20.316892185858677 rho 100.0 h 0.10791703687588239
iteration 2 in inner loop,alpha 20.316892185858677 rho 1000.0 h 0.04331167106514755
iteration 3 in outer loop, alpha = 63.62856325100623, rho = 1000.0, h = 0.04331167106514755
cuda
iteration 1 in inner loop,alpha 63.62856325100623 rho 1000.0 h 0.02298919281323819
iteration 2 in inner loop,alpha 63.62856325100623 rho 10000.0 h 0.008093176195941965
iteration 4 in outer loop, alpha = 144.56032521042587, rho = 10000.0, h = 0.008093176195941965
cuda
iteration 1 in inner loop,alpha 144.56032521042587 rho 10000.0 h 0.004032383854379873
iteration 2 in inner loop,alpha 144.56032521042587 rho 100000.0 h 0.0014331718319375852
iteration 5 in outer loop, alpha = 287.87750840418437, rho = 100000.0, h = 0.0014331718319375852
cuda
iteration 1 in inner loop,alpha 287.87750840418437 rho 100000.0 h 0.0006666248676481246
iteration 6 in outer loop, alpha = 954.5023760523089, rho = 1000000.0, h = 0.0006666248676481246
Threshold 0.3
[[0.001 3.157 0.074 0.24  1.108 1.127 0.324 0.    1.974 1.163 0.102 1.153
  0.243 0.002 0.361 0.482 0.299 1.231 1.311 0.468]
 [0.    0.001 0.021 0.473 0.554 1.677 1.814 0.    0.551 0.451 0.078 0.679
  0.33  0.001 1.213 0.301 0.317 0.863 0.503 1.926]
 [0.003 0.01  0.    0.053 0.485 0.568 1.535 0.    0.203 0.49  0.098 0.186
  1.318 0.001 1.51  0.345 0.648 0.    1.207 0.29 ]
 [0.    0.    0.    0.021 0.001 0.003 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.208 0.198 0.147 0.    0.001 0.697]
 [0.    0.    0.    0.549 0.004 0.307 0.207 0.    0.    0.398 0.042 0.001
  0.087 0.    0.221 0.332 1.29  0.    0.32  1.06 ]
 [0.    0.    0.    0.465 0.002 0.002 0.001 0.    0.    0.    0.    0.
  0.    0.    0.367 1.72  1.846 0.    0.289 0.357]
 [0.    0.    0.    0.21  0.002 0.227 0.002 0.    0.    0.009 0.    0.
  0.    0.    0.356 0.299 0.252 0.    0.173 0.192]
 [2.571 0.134 0.001 0.096 0.748 0.221 0.757 0.001 0.308 0.458 0.138 0.399
  2.005 0.714 0.219 0.19  0.174 1.145 0.487 0.406]
 [0.    0.    0.002 0.354 1.566 0.956 1.317 0.    0.001 0.841 0.397 0.289
  1.794 0.    1.226 1.265 0.875 0.    1.355 0.572]
 [0.    0.    0.    0.097 0.017 2.213 0.119 0.    0.    0.008 0.    0.001
  0.    0.    1.196 1.372 1.834 0.    0.848 0.262]
 [0.    0.    0.    0.92  0.008 0.582 2.348 0.    0.    0.256 0.001 0.
  0.    0.    1.533 0.529 1.037 0.    0.293 1.367]
 [0.    0.    0.    0.142 0.41  0.252 1.688 0.    0.    0.243 0.119 0.001
  0.126 0.    0.165 0.903 0.29  0.    0.123 0.257]
 [0.    0.    0.    0.109 0.008 1.41  1.105 0.    0.    0.176 2.147 0.001
  0.001 0.    0.314 0.381 0.304 0.    0.491 1.102]
 [0.137 0.035 0.03  0.725 0.516 1.667 0.319 0.001 0.293 0.35  1.713 0.534
  2.064 0.001 0.434 0.426 1.102 0.35  0.714 2.048]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.286 0.002 0.    0.001 0.002]
 [0.    0.    0.    0.003 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.001 0.006 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.003 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.144 1.276 0.004 0.    0.001 0.001]
 [0.    0.    0.033 0.047 0.754 0.295 1.405 0.    3.193 0.558 0.073 0.74
  0.075 0.    0.357 0.183 0.194 0.001 0.426 0.203]
 [0.    0.    0.    0.712 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.339 1.161 0.364 0.    0.001 0.2  ]
 [0.    0.    0.    0.003 0.    0.    0.001 0.    0.    0.001 0.    0.
  0.    0.    0.12  1.836 0.278 0.    0.001 0.002]]
[[0.    3.157 0.    0.    1.108 1.127 0.324 0.    1.974 1.163 0.    1.153
  0.    0.    0.361 0.482 0.    1.231 1.311 0.468]
 [0.    0.    0.    0.473 0.554 1.677 1.814 0.    0.551 0.451 0.    0.679
  0.33  0.    1.213 0.301 0.317 0.863 0.503 1.926]
 [0.    0.    0.    0.    0.485 0.568 1.535 0.    0.    0.49  0.    0.
  1.318 0.    1.51  0.345 0.648 0.    1.207 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.697]
 [0.    0.    0.    0.549 0.    0.307 0.    0.    0.    0.398 0.    0.
  0.    0.    0.    0.332 1.29  0.    0.32  1.06 ]
 [0.    0.    0.    0.465 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.367 1.72  1.846 0.    0.    0.357]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.356 0.    0.    0.    0.    0.   ]
 [2.571 0.    0.    0.    0.748 0.    0.757 0.    0.308 0.458 0.    0.399
  2.005 0.714 0.    0.    0.    1.145 0.487 0.406]
 [0.    0.    0.    0.354 1.566 0.956 1.317 0.    0.    0.841 0.397 0.
  1.794 0.    1.226 1.265 0.875 0.    1.355 0.572]
 [0.    0.    0.    0.    0.    2.213 0.    0.    0.    0.    0.    0.
  0.    0.    1.196 1.372 1.834 0.    0.848 0.   ]
 [0.    0.    0.    0.92  0.    0.582 2.348 0.    0.    0.    0.    0.
  0.    0.    1.533 0.529 1.037 0.    0.    1.367]
 [0.    0.    0.    0.    0.41  0.    1.688 0.    0.    0.    0.    0.
  0.    0.    0.    0.903 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.41  1.105 0.    0.    0.    2.147 0.
  0.    0.    0.314 0.381 0.304 0.    0.491 1.102]
 [0.    0.    0.    0.725 0.516 1.667 0.319 0.    0.    0.35  1.713 0.534
  2.064 0.    0.434 0.426 1.102 0.35  0.714 2.048]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.144 1.276 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.754 0.    1.405 0.    3.193 0.558 0.    0.74
  0.    0.    0.357 0.    0.    0.    0.426 0.   ]
 [0.    0.    0.    0.712 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.339 1.161 0.364 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.836 0.    0.    0.    0.   ]]
{'fdr': 0.43089430894308944, 'tpr': 0.875, 'fpr': 0.4818181818181818, 'f1': 0.689655172413793, 'shd': 58, 'npred': 123, 'ntrue': 80}
[3.157e+00 7.360e-02 2.403e-01 1.108e+00 1.127e+00 3.240e-01 2.395e-04
 1.974e+00 1.163e+00 1.015e-01 1.153e+00 2.431e-01 1.655e-03 3.607e-01
 4.816e-01 2.986e-01 1.231e+00 1.311e+00 4.678e-01 3.856e-04 2.135e-02
 4.730e-01 5.543e-01 1.677e+00 1.814e+00 1.621e-04 5.506e-01 4.514e-01
 7.802e-02 6.792e-01 3.301e-01 1.409e-03 1.213e+00 3.009e-01 3.172e-01
 8.630e-01 5.029e-01 1.926e+00 2.598e-03 1.034e-02 5.263e-02 4.847e-01
 5.679e-01 1.535e+00 9.633e-05 2.031e-01 4.901e-01 9.760e-02 1.856e-01
 1.318e+00 7.714e-04 1.510e+00 3.448e-01 6.480e-01 3.835e-04 1.207e+00
 2.897e-01 8.721e-05 1.426e-05 8.773e-05 7.071e-04 3.194e-03 9.799e-04
 6.638e-05 9.525e-05 2.323e-03 4.972e-04 2.535e-04 8.105e-05 2.260e-06
 2.076e-01 1.985e-01 1.474e-01 2.002e-05 1.347e-03 6.968e-01 3.223e-06
 1.604e-06 1.223e-05 5.486e-01 3.065e-01 2.066e-01 1.613e-05 1.478e-04
 3.978e-01 4.211e-02 1.298e-03 8.666e-02 7.290e-05 2.206e-01 3.317e-01
 1.290e+00 2.075e-05 3.204e-01 1.060e+00 7.858e-05 1.862e-04 6.751e-05
 4.652e-01 2.145e-03 1.474e-03 4.508e-05 7.392e-05 4.267e-04 5.151e-05
 3.228e-04 2.387e-04 4.064e-05 3.666e-01 1.720e+00 1.846e+00 1.215e-05
 2.890e-01 3.572e-01 5.355e-05 8.940e-05 4.436e-06 2.101e-01 1.602e-03
 2.273e-01 2.737e-05 7.893e-06 9.063e-03 1.210e-04 1.966e-04 5.998e-05
 4.325e-06 3.558e-01 2.992e-01 2.525e-01 1.808e-06 1.729e-01 1.924e-01
 2.571e+00 1.344e-01 1.010e-03 9.589e-02 7.476e-01 2.206e-01 7.569e-01
 3.084e-01 4.581e-01 1.383e-01 3.993e-01 2.005e+00 7.141e-01 2.187e-01
 1.905e-01 1.744e-01 1.145e+00 4.872e-01 4.064e-01 1.138e-05 3.511e-06
 1.500e-03 3.544e-01 1.566e+00 9.557e-01 1.317e+00 9.904e-06 8.412e-01
 3.966e-01 2.894e-01 1.794e+00 2.931e-05 1.226e+00 1.265e+00 8.754e-01
 4.824e-05 1.355e+00 5.725e-01 2.758e-05 3.028e-05 9.115e-05 9.697e-02
 1.702e-02 2.213e+00 1.192e-01 5.173e-05 1.550e-04 4.638e-04 1.230e-03
 4.842e-04 1.194e-04 1.196e+00 1.372e+00 1.834e+00 3.240e-05 8.476e-01
 2.619e-01 7.204e-06 7.165e-06 6.295e-05 9.198e-01 7.674e-03 5.823e-01
 2.348e+00 2.058e-05 6.749e-05 2.560e-01 4.003e-04 2.098e-04 4.321e-05
 1.533e+00 5.285e-01 1.037e+00 1.354e-05 2.928e-01 1.367e+00 2.663e-05
 4.805e-05 2.796e-04 1.416e-01 4.104e-01 2.523e-01 1.688e+00 1.576e-05
 2.373e-04 2.429e-01 1.185e-01 1.256e-01 5.806e-05 1.655e-01 9.032e-01
 2.903e-01 1.698e-04 1.232e-01 2.573e-01 5.040e-06 1.795e-06 1.700e-04
 1.090e-01 7.651e-03 1.410e+00 1.105e+00 2.841e-05 2.159e-04 1.763e-01
 2.147e+00 1.219e-03 7.860e-05 3.141e-01 3.812e-01 3.041e-01 2.035e-05
 4.906e-01 1.102e+00 1.370e-01 3.547e-02 3.046e-02 7.249e-01 5.162e-01
 1.667e+00 3.194e-01 6.131e-04 2.929e-01 3.499e-01 1.713e+00 5.340e-01
 2.064e+00 4.340e-01 4.256e-01 1.102e+00 3.505e-01 7.142e-01 2.048e+00
 3.966e-05 7.901e-05 2.858e-05 3.995e-03 9.940e-04 6.697e-04 3.840e-04
 2.619e-05 5.065e-06 4.175e-05 7.778e-05 2.088e-04 5.437e-05 2.192e-05
 2.865e-01 2.344e-03 2.314e-06 6.125e-04 1.785e-03 2.359e-05 2.077e-05
 2.664e-05 2.745e-03 5.458e-04 9.964e-05 3.691e-04 2.086e-05 1.799e-05
 2.264e-04 7.515e-05 6.137e-05 1.660e-05 1.134e-05 8.673e-04 9.861e-04
 4.850e-06 5.175e-04 2.249e-04 4.593e-05 8.462e-05 2.562e-05 3.276e-03
 8.223e-04 5.727e-04 9.388e-04 3.671e-05 2.461e-05 5.408e-05 7.261e-05
 9.178e-05 2.680e-05 2.877e-05 1.144e+00 1.276e+00 4.921e-06 6.823e-04
 1.125e-03 1.173e-04 1.315e-05 3.256e-02 4.716e-02 7.539e-01 2.946e-01
 1.405e+00 8.049e-05 3.193e+00 5.578e-01 7.271e-02 7.401e-01 7.460e-02
 1.223e-04 3.571e-01 1.828e-01 1.943e-01 4.262e-01 2.025e-01 6.131e-05
 1.525e-05 1.391e-04 7.121e-01 9.872e-04 7.465e-04 8.947e-04 4.202e-05
 1.440e-04 5.524e-04 2.121e-04 2.858e-04 1.315e-04 2.045e-05 3.394e-01
 1.161e+00 3.642e-01 1.606e-05 2.000e-01 3.573e-05 7.474e-05 6.148e-05
 2.991e-03 2.207e-04 2.232e-04 5.616e-04 1.760e-05 6.834e-05 8.689e-04
 3.010e-04 1.242e-04 1.924e-04 5.941e-05 1.202e-01 1.836e+00 2.782e-01
 1.515e-05 7.648e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.9369583333333333, 0.8885729146958601)
cuda
4420
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
||w||^2 0.23047102906544725
exp ma of ||w||^2 63.413894995658104
||w|| 0.4800739829083089
exp ma of ||w|| 0.5736486860562041
||w||^2 0.18228381075386121
exp ma of ||w||^2 0.2128005570539218
||w|| 0.4269470819128071
exp ma of ||w|| 0.4520930267233622
||w||^2 0.13054427018438294
exp ma of ||w||^2 0.20654763957680253
||w|| 0.3613091061465002
exp ma of ||w|| 0.44673523104829915
||w||^2 0.09771162108242612
exp ma of ||w||^2 0.19159433102135276
||w|| 0.31258858117728183
exp ma of ||w|| 0.4275194992749482
||w||^2 0.19494747289503989
exp ma of ||w||^2 0.18780612673345348
||w|| 0.4415285640760288
exp ma of ||w|| 0.4190418582169087
cuda
Objective function 180.65 = squared loss an data 178.00 + 0.5*rho*h**2 1.993706 + alpha*h 0.000000 + L2reg 0.38 + L1reg 0.27 ; SHD = 120 ; DAG False
Proportion of microbatches that were clipped  0.727243793761935
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.9968507151307406
iteration 1 in outer loop, alpha = 1.9968507151307406, rho = 1.0, h = 1.9968507151307406
cuda
4420
cuda
Objective function 184.64 = squared loss an data 178.00 + 0.5*rho*h**2 1.993706 + alpha*h 3.987413 + L2reg 0.38 + L1reg 0.27 ; SHD = 120 ; DAG False
||w||^2 13.042661709474904
exp ma of ||w||^2 8585.282592897182
||w|| 3.61146254438211
exp ma of ||w|| 16.87459015097861
||w||^2 16.003726496002532
exp ma of ||w||^2 1354.9045602380218
||w|| 4.0004657848808725
exp ma of ||w|| 5.757761683825216
||w||^2 0.38636229502168357
exp ma of ||w||^2 0.32144976085055865
||w|| 0.621580481532105
exp ma of ||w|| 0.5380972766591751
||w||^2 0.1383349055127517
exp ma of ||w||^2 0.3213960934080547
||w|| 0.37193400693234774
exp ma of ||w|| 0.5351427786676068
||w||^2 0.131345266775464
exp ma of ||w||^2 0.30060586360512187
||w|| 0.3624158754462392
exp ma of ||w|| 0.5152314022055494
cuda
Objective function 78.24 = squared loss an data 72.87 + 0.5*rho*h**2 1.250069 + alpha*h 3.157386 + L2reg 0.71 + L1reg 0.26 ; SHD = 110 ; DAG False
Proportion of microbatches that were clipped  0.7327278624716186
iteration 1 in inner loop, alpha 1.9968507151307406 rho 1.0 h 1.5811825834032618
4420
cuda
Objective function 89.50 = squared loss an data 72.87 + 0.5*rho*h**2 12.500692 + alpha*h 3.157386 + L2reg 0.71 + L1reg 0.26 ; SHD = 110 ; DAG False
||w||^2 164705.95841285647
exp ma of ||w||^2 4524940.108448317
||w|| 405.8398186635418
exp ma of ||w|| 952.3197555887988
||w||^2 0.3932429505237761
exp ma of ||w||^2 0.7413345705384152
||w|| 0.6270908630523778
exp ma of ||w|| 0.7851463924787967
||w||^2 0.6802055990713579
exp ma of ||w||^2 0.6822479809882792
||w|| 0.8247457784501585
exp ma of ||w|| 0.7691230322406559
||w||^2 0.5927894372539251
exp ma of ||w||^2 0.7193842368986781
||w|| 0.769928202661732
exp ma of ||w|| 0.7816388847669299
||w||^2 0.6793222285735491
exp ma of ||w||^2 0.77631192435187
||w|| 0.8242100633779893
exp ma of ||w|| 0.8058683851958232
cuda
Objective function 54.12 = squared loss an data 48.64 + 0.5*rho*h**2 2.843275 + alpha*h 1.505810 + L2reg 0.91 + L1reg 0.22 ; SHD = 90 ; DAG False
Proportion of microbatches that were clipped  0.7534160515156274
iteration 2 in inner loop, alpha 1.9968507151307406 rho 10.0 h 0.7540921774858802
4420
cuda
Objective function 79.71 = squared loss an data 48.64 + 0.5*rho*h**2 28.432751 + alpha*h 1.505810 + L2reg 0.91 + L1reg 0.22 ; SHD = 90 ; DAG False
||w||^2 16808178510.299463
exp ma of ||w||^2 9222334447.851372
||w|| 129646.35941783889
exp ma of ||w|| 77693.12566646673
||w||^2 1.0661801425618844
exp ma of ||w||^2 1.1873288154136297
||w|| 1.0325599946549762
exp ma of ||w|| 1.0355126559471066
||w||^2 1.4897287379931128
exp ma of ||w||^2 1.113774484609337
||w|| 1.2205444432682953
exp ma of ||w|| 1.0059714150514363
||w||^2 0.18091709742553413
exp ma of ||w||^2 1.1639427168786218
||w|| 0.4253435052114163
exp ma of ||w|| 1.005069367240582
cuda
Objective function 49.34 = squared loss an data 43.75 + 0.5*rho*h**2 3.794702 + alpha*h 0.550110 + L2reg 1.07 + L1reg 0.18 ; SHD = 79 ; DAG True
Proportion of microbatches that were clipped  0.7628495547570692
iteration 3 in inner loop, alpha 1.9968507151307406 rho 100.0 h 0.27548873307848964
iteration 2 in outer loop, alpha = 29.545724022979705, rho = 100.0, h = 0.27548873307848964
cuda
4420
cuda
Objective function 56.93 = squared loss an data 43.75 + 0.5*rho*h**2 3.794702 + alpha*h 8.139514 + L2reg 1.07 + L1reg 0.18 ; SHD = 79 ; DAG True
||w||^2 3492325704.8473024
exp ma of ||w||^2 21280068905.813663
||w|| 59095.9026062493
exp ma of ||w|| 123020.54000545906
||w||^2 14031734792.416815
exp ma of ||w||^2 20626665968.49665
||w|| 118455.62372642683
exp ma of ||w|| 121751.38633047312
||w||^2 77958567.66881171
exp ma of ||w||^2 1163983110.972625
||w|| 8829.414910899346
exp ma of ||w|| 24137.658095215385
||w||^2 608.9831461507428
exp ma of ||w||^2 57621.58172588366
||w|| 24.677583879925173
exp ma of ||w|| 41.033667044598154
||w||^2 16.060366928792178
exp ma of ||w||^2 4601.591414168613
||w|| 4.007538761982493
exp ma of ||w|| 10.252384634963247
||w||^2 1.3075120758548637
exp ma of ||w||^2 9.546873070944796
||w|| 1.1434649429933843
exp ma of ||w|| 1.4215659425801461
||w||^2 1.3492402296060926
exp ma of ||w||^2 2.9289386749754454
||w|| 1.1615680047272705
exp ma of ||w|| 1.2950162401111085
||w||^2 1.4897057613365214
exp ma of ||w||^2 1.6765776620166837
||w|| 1.2205350307699168
exp ma of ||w|| 1.2328821178844787
||w||^2 0.8312115106826177
exp ma of ||w||^2 1.6911623371967992
||w|| 0.9117080183274784
exp ma of ||w|| 1.2201363201036433
||w||^2 0.7132463523589339
exp ma of ||w||^2 1.634164738369059
||w|| 0.8445391360730028
exp ma of ||w|| 1.2060421775334342
cuda
Objective function 51.21 = squared loss an data 41.99 + 0.5*rho*h**2 1.978488 + alpha*h 5.877279 + L2reg 1.19 + L1reg 0.17 ; SHD = 75 ; DAG True
Proportion of microbatches that were clipped  0.7631793700016322
iteration 1 in inner loop, alpha 29.545724022979705 rho 100.0 h 0.1989214805427153
4420
cuda
Objective function 69.02 = squared loss an data 41.99 + 0.5*rho*h**2 19.784878 + alpha*h 5.877279 + L2reg 1.19 + L1reg 0.17 ; SHD = 75 ; DAG True
||w||^2 286871958.16263884
exp ma of ||w||^2 3590881364.1696115
||w|| 16937.29488916807
exp ma of ||w|| 45251.555459355855
||w||^2 369.37241738025756
exp ma of ||w||^2 90426.59811484576
||w|| 19.21906390489031
exp ma of ||w|| 60.268394844228354
||w||^2 10.881786801726642
exp ma of ||w||^2 13.656041536708104
||w|| 3.2987553412956596
exp ma of ||w|| 1.8654654028119633
cuda
Objective function 50.53 = squared loss an data 42.73 + 0.5*rho*h**2 3.788154 + alpha*h 2.571719 + L2reg 1.29 + L1reg 0.15 ; SHD = 72 ; DAG True
Proportion of microbatches that were clipped  0.7774589510600989
iteration 2 in inner loop, alpha 29.545724022979705 rho 1000.0 h 0.08704199214233199
4420
cuda
Objective function 84.62 = squared loss an data 42.73 + 0.5*rho*h**2 37.881542 + alpha*h 2.571719 + L2reg 1.29 + L1reg 0.15 ; SHD = 72 ; DAG True
||w||^2 3339.6820389685977
exp ma of ||w||^2 856534.7112129665
||w|| 57.78998216792075
exp ma of ||w|| 228.12218546632684
||w||^2 4.423708661388762
exp ma of ||w||^2 2.866617614641932
||w|| 2.1032614343891636
exp ma of ||w|| 1.6238290104302755
||w||^2 3.1653627944113425
exp ma of ||w||^2 2.7859302892329993
||w|| 1.7791466478093767
exp ma of ||w|| 1.6033259961581308
||w||^2 2.4651025640389848
exp ma of ||w||^2 2.5387975845830195
||w|| 1.5700645095151298
exp ma of ||w|| 1.5310059302580588
||w||^2 4.606282163942417
exp ma of ||w||^2 2.254910238901037
||w|| 2.1462250962893936
exp ma of ||w|| 1.4375780671032634
cuda
Objective function 53.06 = squared loss an data 45.36 + 0.5*rho*h**2 5.231759 + alpha*h 0.955726 + L2reg 1.38 + L1reg 0.13 ; SHD = 72 ; DAG True
Proportion of microbatches that were clipped  0.7805586953011464
iteration 3 in inner loop, alpha 29.545724022979705 rho 10000.0 h 0.03234736161811824
iteration 3 in outer loop, alpha = 353.0193402041621, rho = 10000.0, h = 0.03234736161811824
cuda
4420
cuda
Objective function 63.52 = squared loss an data 45.36 + 0.5*rho*h**2 5.231759 + alpha*h 11.419244 + L2reg 1.38 + L1reg 0.13 ; SHD = 72 ; DAG True
||w||^2 388728081730.245
exp ma of ||w||^2 211431329008.65903
||w|| 623480.6185682479
exp ma of ||w|| 225871.60989319443
||w||^2 59165128322.50179
exp ma of ||w||^2 163345580702.12042
||w|| 243238.82980005842
exp ma of ||w|| 347395.570732157
||w||^2 3.579695527200787
exp ma of ||w||^2 3.92424935569542
||w|| 1.8920083316943366
exp ma of ||w|| 1.9191805863322775
||w||^2 2.401591451659013
exp ma of ||w||^2 3.0835285372653924
||w|| 1.5497068921763926
exp ma of ||w|| 1.7060470165420278
||w||^2 1.6550660182770618
exp ma of ||w||^2 2.8109441261541934
||w|| 1.2864936915030178
exp ma of ||w|| 1.6245433402506115
cuda
Objective function 55.52 = squared loss an data 45.26 + 0.5*rho*h**2 1.854673 + alpha*h 6.799033 + L2reg 1.49 + L1reg 0.12 ; SHD = 74 ; DAG True
Proportion of microbatches that were clipped  0.7825187072122274
iteration 1 in inner loop, alpha 353.0193402041621 rho 10000.0 h 0.019259662590997095
4420
cuda
Objective function 72.21 = squared loss an data 45.26 + 0.5*rho*h**2 18.546730 + alpha*h 6.799033 + L2reg 1.49 + L1reg 0.12 ; SHD = 74 ; DAG True
||w||^2 1626955696436.9402
exp ma of ||w||^2 340385226116.8957
||w|| 1275521.7349919758
exp ma of ||w|| 234671.1568477465
||w||^2 4966691845.17331
exp ma of ||w||^2 47659077631.63623
||w|| 70474.76034136838
exp ma of ||w|| 132372.403590297
v before min max tensor([[-1.558e-02,  7.443e-03, -2.831e-03,  ..., -8.808e-04,  7.474e-03,
         -1.187e-04],
        [-1.269e-03, -1.577e-03, -6.197e-04,  ...,  9.173e-05, -3.707e-05,
          7.299e-04],
        [ 1.215e-02, -1.456e-02, -3.937e-04,  ...,  2.002e-05,  2.316e-04,
         -9.319e-05],
        ...,
        [ 2.048e-04, -3.307e-05, -6.340e-03,  ...,  4.126e-03, -1.528e-03,
         -1.802e-02],
        [-2.363e-04, -4.392e-03, -3.663e-03,  ...,  3.113e-04, -4.296e-04,
          6.242e-03],
        [ 1.145e-03,  2.069e-04, -2.372e-03,  ..., -4.872e-03, -9.769e-03,
          2.508e-02]], device='cuda:0')
v tensor([[1.000e-12, 7.443e-03, 1.000e-12,  ..., 1.000e-12, 7.474e-03,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 9.173e-05, 1.000e-12,
         7.299e-04],
        [1.215e-02, 1.000e-12, 1.000e-12,  ..., 2.002e-05, 2.316e-04,
         1.000e-12],
        ...,
        [2.048e-04, 1.000e-12, 1.000e-12,  ..., 4.126e-03, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 3.113e-04, 1.000e-12,
         6.242e-03],
        [1.145e-03, 2.069e-04, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         2.508e-02]], device='cuda:0')
v before min max tensor([-2.904e-06, -2.900e-06, -1.964e-04, -1.166e-03, -1.193e-04, -1.077e-05,
        -1.843e-04,  1.363e-04,  1.110e-03,  1.741e-02, -2.085e-05, -1.023e-05,
         1.362e-04, -1.261e-03, -7.454e-04, -2.074e-04, -4.433e-04, -6.328e-04,
         9.408e-04, -1.146e-05, -1.136e-02, -5.734e-04,  3.598e-03,  2.462e-04,
         1.056e-06, -3.177e-05, -9.990e-04, -3.485e-04, -1.813e-03, -2.411e-04,
        -1.869e-03,  1.883e-04,  4.966e-08, -1.584e-06,  1.156e-04, -8.413e-05,
        -5.558e-05, -2.152e-05,  5.898e-06, -1.449e-04,  5.867e-05, -1.891e-02,
        -3.268e-04, -3.514e-06, -9.689e-04,  1.038e-03, -1.503e-05,  7.775e-06,
        -2.870e-04, -3.429e-04, -3.969e-04, -1.515e-03, -9.883e-06,  1.253e-04,
        -3.985e-06, -8.631e-05,  8.820e-04, -1.958e-05, -4.178e-04, -4.470e-03,
         6.831e-04, -1.276e-03,  2.856e-03, -3.051e-03, -1.813e-05, -9.708e-04,
        -9.410e-04,  2.093e-03,  1.845e-04,  6.011e-03, -4.760e-06, -8.161e-05,
         1.039e-03,  4.581e-04,  8.694e-03, -4.467e-03, -4.376e-06, -1.894e-04,
        -6.562e-04, -9.161e-05, -1.197e-02,  1.206e-03, -1.633e-05, -2.185e-04,
        -2.340e-04, -1.290e-03, -1.260e-03, -1.112e-06,  1.576e-03,  8.733e-05,
        -5.404e-05,  1.054e-04, -1.630e-04, -1.312e-05, -4.745e-05,  4.583e-03,
         1.901e-03, -1.372e-04, -3.473e-04, -2.173e-04, -5.564e-04, -5.695e-05,
        -3.154e-04,  2.704e-03, -1.615e-05, -1.003e-04, -6.479e-04,  7.830e-04,
        -3.062e-03,  1.198e-06,  4.695e-04,  1.732e-03, -9.271e-05,  1.751e-05,
        -4.990e-04, -1.777e-06, -2.327e-05, -1.814e-05, -2.754e-04,  3.967e-03,
         2.349e-03, -1.279e-03, -2.329e-03, -4.941e-05, -1.168e-06, -4.682e-04,
        -2.280e-03, -6.213e-04, -2.111e-02,  6.681e-04, -4.304e-03, -2.067e-04,
        -2.124e-04,  1.165e-04,  4.256e-04, -1.680e-05, -2.292e-05, -1.384e-03,
        -3.198e-04, -3.047e-04,  1.187e-04, -3.377e-05, -1.361e-03, -1.933e-06,
        -3.542e-03, -9.280e-06, -1.952e-04, -2.847e-03, -8.838e-04, -3.871e-04,
        -1.913e-02, -8.914e-04, -3.953e-04, -2.666e-05, -1.633e-04, -2.696e-05,
         1.426e-02,  3.676e-04, -1.693e-04,  7.018e-03, -8.554e-06,  1.412e-04,
        -2.973e-04, -1.391e-03, -5.431e-04, -2.732e-05, -4.651e-03,  2.764e-03,
        -1.268e-03, -4.185e-06, -3.754e-05,  8.258e-04, -3.621e-03, -4.689e-04,
        -9.557e-04, -2.495e-06, -1.795e-03, -1.438e-03, -3.119e-05, -4.412e-04,
        -3.880e-04,  2.019e-04,  4.001e-04,  8.145e-07, -1.660e-05, -9.850e-05,
        -5.202e-04,  2.013e-02, -4.089e-04, -6.685e-05,  1.376e-02, -1.070e-03,
         5.716e-03,  2.195e-05,  3.278e-03,  1.045e-03, -7.324e-06,  2.167e-03,
        -1.463e-03, -3.987e-03], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.363e-04, 1.110e-03, 1.741e-02, 1.000e-12, 1.000e-12,
        1.362e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        9.408e-04, 1.000e-12, 1.000e-12, 1.000e-12, 3.598e-03, 2.462e-04,
        1.056e-06, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.883e-04, 4.966e-08, 1.000e-12, 1.156e-04, 1.000e-12,
        1.000e-12, 1.000e-12, 5.898e-06, 1.000e-12, 5.867e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.038e-03, 1.000e-12, 7.775e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.253e-04,
        1.000e-12, 1.000e-12, 8.820e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        6.831e-04, 1.000e-12, 2.856e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.093e-03, 1.845e-04, 6.011e-03, 1.000e-12, 1.000e-12,
        1.039e-03, 4.581e-04, 8.694e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.206e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.576e-03, 8.733e-05,
        1.000e-12, 1.054e-04, 1.000e-12, 1.000e-12, 1.000e-12, 4.583e-03,
        1.901e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.704e-03, 1.000e-12, 1.000e-12, 1.000e-12, 7.830e-04,
        1.000e-12, 1.198e-06, 4.695e-04, 1.732e-03, 1.000e-12, 1.751e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.967e-03,
        2.349e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 6.681e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.165e-04, 4.256e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.187e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.426e-02, 3.676e-04, 1.000e-12, 7.018e-03, 1.000e-12, 1.412e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.764e-03,
        1.000e-12, 1.000e-12, 1.000e-12, 8.258e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.019e-04, 4.001e-04, 8.145e-07, 1.000e-12, 1.000e-12,
        1.000e-12, 2.013e-02, 1.000e-12, 1.000e-12, 1.376e-02, 1.000e-12,
        5.716e-03, 2.195e-05, 3.278e-03, 1.045e-03, 1.000e-12, 2.167e-03,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 6.564e-04],
         [ 5.351e-03],
         [ 2.450e-05],
         [-5.881e-04],
         [-1.529e-04],
         [ 1.086e-05],
         [ 1.220e-04],
         [ 1.482e-04],
         [-2.017e-03],
         [ 5.254e-05]],

        [[-1.674e-05],
         [-1.227e-04],
         [-8.781e-05],
         [-3.138e-04],
         [-1.313e-04],
         [-2.965e-03],
         [-6.340e-04],
         [-6.427e-04],
         [-3.054e-04],
         [-2.273e-04]],

        [[-4.112e-03],
         [ 6.077e-06],
         [-7.557e-05],
         [-1.950e-04],
         [-4.486e-03],
         [ 2.801e-04],
         [ 6.032e-05],
         [ 1.444e-03],
         [-3.327e-04],
         [-5.742e-04]],

        [[-8.721e-05],
         [ 9.193e-05],
         [ 7.944e-03],
         [-1.551e-04],
         [-3.921e-04],
         [-1.001e-03],
         [-1.292e-04],
         [-2.041e-04],
         [-3.439e-04],
         [-2.647e-05]],

        [[-1.103e-03],
         [ 8.095e-03],
         [ 5.397e-05],
         [-2.033e-05],
         [-2.778e-04],
         [-1.824e-03],
         [-6.949e-04],
         [-4.700e-05],
         [ 5.290e-04],
         [-3.248e-04]],

        [[-2.477e-03],
         [ 8.512e-03],
         [ 8.221e-05],
         [-9.424e-05],
         [-2.604e-03],
         [-2.048e-04],
         [ 1.689e-06],
         [-4.098e-05],
         [-1.018e-04],
         [ 3.895e-04]],

        [[-2.841e-04],
         [ 2.305e-03],
         [ 4.848e-05],
         [-4.210e-04],
         [-2.025e-03],
         [-6.859e-04],
         [-6.817e-06],
         [-4.025e-04],
         [-1.923e-03],
         [-1.726e-04]],

        [[-9.772e-04],
         [-4.272e-03],
         [ 2.603e-03],
         [-4.858e-04],
         [ 1.078e-04],
         [ 4.197e-03],
         [-1.327e-02],
         [-5.823e-06],
         [-3.270e-05],
         [-1.263e-04]],

        [[-5.069e-05],
         [-4.420e-05],
         [-4.948e-04],
         [-2.410e-04],
         [-3.508e-05],
         [-5.828e-06],
         [-1.155e-05],
         [-3.829e-04],
         [-3.217e-05],
         [ 6.769e-04]],

        [[-6.963e-04],
         [-5.183e-04],
         [-1.728e-04],
         [-1.008e-05],
         [-8.182e-04],
         [-2.641e-03],
         [ 1.764e-03],
         [-4.299e-04],
         [-1.431e-03],
         [-1.245e-04]],

        [[ 2.471e-04],
         [ 1.013e-07],
         [ 1.286e-04],
         [-5.128e-04],
         [-1.838e-03],
         [-7.396e-03],
         [-1.372e-04],
         [-3.403e-05],
         [ 1.888e-03],
         [-9.128e-04]],

        [[-2.949e-03],
         [-7.639e-04],
         [-2.789e-03],
         [-7.317e-05],
         [-9.661e-04],
         [-1.108e-02],
         [-2.531e-03],
         [-2.840e-06],
         [-5.682e-03],
         [-2.073e-06]],

        [[ 2.057e-05],
         [-2.082e-03],
         [-9.554e-06],
         [ 1.105e-02],
         [ 2.296e-04],
         [-3.754e-06],
         [-7.142e-05],
         [ 1.296e-04],
         [ 1.690e-04],
         [-5.223e-04]],

        [[ 7.115e-06],
         [-7.822e-04],
         [ 1.181e-03],
         [-1.315e-03],
         [-2.001e-02],
         [ 2.790e-03],
         [ 1.536e-05],
         [-1.162e-04],
         [-4.512e-06],
         [-8.340e-04]],

        [[-2.084e-03],
         [ 1.516e-05],
         [-7.695e-04],
         [-3.323e-04],
         [ 3.655e-04],
         [-9.465e-05],
         [ 9.875e-04],
         [ 7.777e-04],
         [ 1.378e-04],
         [ 1.517e-03]],

        [[-1.604e-04],
         [-1.372e-04],
         [ 4.205e-05],
         [ 1.444e-03],
         [ 6.462e-05],
         [-1.887e-05],
         [-2.973e-03],
         [ 5.486e-05],
         [-2.525e-03],
         [-2.722e-04]],

        [[ 1.508e-05],
         [ 6.698e-05],
         [-1.896e-03],
         [-8.157e-04],
         [-1.069e-04],
         [-2.259e-07],
         [-4.149e-03],
         [ 4.301e-04],
         [ 7.993e-05],
         [ 4.367e-03]],

        [[ 1.542e-04],
         [ 2.446e-03],
         [-8.435e-05],
         [-1.156e-04],
         [ 4.623e-03],
         [-1.967e-03],
         [ 7.504e-04],
         [-7.995e-04],
         [-2.947e-06],
         [-4.652e-04]],

        [[-4.683e-04],
         [-5.182e-03],
         [-3.214e-04],
         [-6.874e-04],
         [-1.952e-06],
         [-1.477e-06],
         [-4.421e-04],
         [ 1.314e-04],
         [ 4.075e-03],
         [ 2.230e-04]],

        [[-3.343e-03],
         [-4.673e-04],
         [-1.815e-04],
         [-7.582e-05],
         [-9.716e-05],
         [-2.658e-06],
         [ 7.882e-06],
         [-1.763e-03],
         [-3.827e-03],
         [-1.440e-05]]], device='cuda:0')
v tensor([[[6.564e-04],
         [5.351e-03],
         [2.450e-05],
         [1.000e-12],
         [1.000e-12],
         [1.086e-05],
         [1.220e-04],
         [1.482e-04],
         [1.000e-12],
         [5.254e-05]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [6.077e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.801e-04],
         [6.032e-05],
         [1.444e-03],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [9.193e-05],
         [7.944e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [8.095e-03],
         [5.397e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.290e-04],
         [1.000e-12]],

        [[1.000e-12],
         [8.512e-03],
         [8.221e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.689e-06],
         [1.000e-12],
         [1.000e-12],
         [3.895e-04]],

        [[1.000e-12],
         [2.305e-03],
         [4.848e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [2.603e-03],
         [1.000e-12],
         [1.078e-04],
         [4.197e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.769e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.764e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[2.471e-04],
         [1.013e-07],
         [1.286e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.888e-03],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[2.057e-05],
         [1.000e-12],
         [1.000e-12],
         [1.105e-02],
         [2.296e-04],
         [1.000e-12],
         [1.000e-12],
         [1.296e-04],
         [1.690e-04],
         [1.000e-12]],

        [[7.115e-06],
         [1.000e-12],
         [1.181e-03],
         [1.000e-12],
         [1.000e-12],
         [2.790e-03],
         [1.536e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.516e-05],
         [1.000e-12],
         [1.000e-12],
         [3.655e-04],
         [1.000e-12],
         [9.875e-04],
         [7.777e-04],
         [1.378e-04],
         [1.517e-03]],

        [[1.000e-12],
         [1.000e-12],
         [4.205e-05],
         [1.444e-03],
         [6.462e-05],
         [1.000e-12],
         [1.000e-12],
         [5.486e-05],
         [1.000e-12],
         [1.000e-12]],

        [[1.508e-05],
         [6.698e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.301e-04],
         [7.993e-05],
         [4.367e-03]],

        [[1.542e-04],
         [2.446e-03],
         [1.000e-12],
         [1.000e-12],
         [4.623e-03],
         [1.000e-12],
         [7.504e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.314e-04],
         [4.075e-03],
         [2.230e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.882e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-2.000e-04],
        [-1.377e-04],
        [-2.164e-05],
        [-1.065e-04],
        [-2.803e-05],
        [-6.127e-04],
        [-7.187e-05],
        [-1.457e-06],
        [-1.204e-04],
        [-7.729e-04],
        [ 2.174e-03],
        [-7.482e-03],
        [-2.897e-04],
        [-7.161e-03],
        [-2.651e-05],
        [-2.127e-04],
        [ 6.150e-06],
        [-1.363e-04],
        [-3.318e-03],
        [-8.323e-04]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [2.174e-03],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [6.150e-06],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-8.081e-05, -6.646e-05, -2.032e-05,  ...,  1.263e-06, -8.742e-05,
          2.335e-06],
        [-2.055e-04, -3.413e-05,  5.782e-06,  ...,  1.121e-05, -3.778e-06,
         -3.936e-05],
        [-1.136e-04, -9.909e-05, -2.925e-05,  ..., -1.768e-05,  1.673e-05,
          2.302e-06],
        ...,
        [-3.857e-05, -1.984e-06, -4.793e-05,  ...,  1.052e-04,  3.995e-06,
         -7.498e-05],
        [ 1.828e-05, -7.355e-06,  1.872e-05,  ..., -2.103e-05,  2.202e-05,
          7.964e-05],
        [-6.592e-05,  5.258e-05, -5.741e-05,  ..., -4.465e-06, -3.175e-05,
         -1.412e-04]], device='cuda:0')
s after update for 1 param tensor([[1.512e-02, 2.759e-02, 2.670e-03,  ..., 1.084e-03, 2.742e-02,
         1.378e-04],
        [1.500e-02, 1.779e-03, 5.958e-04,  ..., 3.029e-03, 3.579e-05,
         8.559e-03],
        [3.524e-02, 1.538e-02, 1.337e-03,  ..., 1.448e-03, 4.815e-03,
         8.895e-05],
        ...,
        [4.976e-03, 3.134e-05, 6.358e-03,  ..., 2.070e-02, 1.541e-03,
         1.811e-02],
        [2.756e-04, 4.210e-03, 3.465e-03,  ..., 5.580e-03, 2.019e-03,
         2.552e-02],
        [1.246e-02, 5.983e-03, 2.617e-03,  ..., 4.842e-03, 9.212e-03,
         5.014e-02]], device='cuda:0')
b after update for 1 param tensor([[0.642, 0.867, 0.270,  ..., 0.172, 0.864, 0.061],
        [0.639, 0.220, 0.127,  ..., 0.287, 0.031, 0.483],
        [0.980, 0.647, 0.191,  ..., 0.199, 0.362, 0.049],
        ...,
        [0.368, 0.029, 0.416,  ..., 0.751, 0.205, 0.702],
        [0.087, 0.339, 0.307,  ..., 0.390, 0.234, 0.834],
        [0.582, 0.404, 0.267,  ..., 0.363, 0.501, 1.168]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 7.751e-07,  1.210e-05,  6.174e-06, -2.036e-06,  1.796e-05, -1.477e-07,
        -6.591e-07,  2.283e-05, -2.384e-05,  2.026e-04,  4.254e-06,  1.986e-06,
        -1.864e-05,  1.824e-06,  3.246e-06,  6.921e-07,  5.534e-06, -9.483e-06,
        -3.716e-05, -9.193e-06, -2.217e-05, -2.403e-05, -9.049e-05, -2.727e-05,
         1.266e-06, -1.072e-05,  1.668e-05, -2.797e-05,  1.411e-05, -7.954e-06,
        -5.832e-06, -4.332e-05,  2.372e-06,  4.358e-08, -1.005e-05, -1.159e-06,
        -8.930e-06, -1.136e-06, -9.459e-06, -3.224e-06, -2.211e-05, -9.293e-05,
         3.080e-06, -1.623e-06,  1.937e-05,  5.074e-05, -5.088e-06, -5.904e-06,
         1.551e-05,  8.531e-06, -3.802e-05,  1.018e-05,  2.408e-06, -1.627e-05,
        -1.438e-06, -2.760e-06,  4.226e-05,  3.467e-06, -3.955e-06,  7.134e-06,
         2.543e-05,  9.067e-06, -9.196e-05, -1.980e-06, -6.625e-06,  2.643e-05,
         6.303e-05, -6.789e-05,  4.489e-05,  9.566e-05,  7.995e-06, -6.068e-06,
         5.516e-05,  2.277e-05, -1.207e-04, -2.464e-05, -2.562e-07,  2.152e-06,
         6.714e-06, -6.573e-06,  7.689e-05, -7.298e-05, -2.009e-06, -1.332e-05,
        -4.002e-06,  1.917e-05,  1.861e-06, -1.960e-06,  3.970e-05, -1.463e-04,
        -1.823e-05,  1.065e-05, -1.207e-06, -5.608e-06,  1.168e-05, -7.590e-05,
         5.447e-05,  2.091e-05,  3.101e-05,  2.722e-07,  3.525e-05, -6.687e-06,
        -4.112e-06, -7.461e-05, -2.786e-06,  8.999e-06, -1.529e-05, -3.090e-05,
         5.563e-06, -1.290e-06, -3.105e-05, -3.960e-05, -1.943e-05, -5.489e-06,
        -1.376e-05,  1.188e-06,  5.936e-06,  1.175e-05, -2.829e-05, -7.524e-05,
         5.507e-05, -2.227e-05,  4.487e-05, -2.681e-06, -1.142e-06,  3.613e-05,
        -6.765e-06,  1.279e-05, -6.587e-05,  4.997e-05,  2.225e-05,  1.169e-05,
        -5.992e-05, -9.750e-06, -3.487e-05,  2.246e-06,  3.140e-06, -1.413e-06,
        -1.158e-06, -4.500e-06, -2.265e-05, -5.760e-07,  2.414e-05,  4.666e-06,
        -3.640e-05,  1.325e-05, -1.670e-05, -4.298e-05, -3.230e-06,  5.068e-06,
         6.929e-05,  2.153e-05,  4.531e-05,  6.543e-07,  8.279e-06, -1.069e-05,
        -1.038e-04, -2.673e-05,  8.031e-05, -8.496e-05, -3.574e-06,  1.144e-05,
        -2.434e-05,  1.314e-05, -3.841e-05, -1.637e-06, -6.720e-05,  9.174e-05,
        -4.472e-05, -1.863e-06,  8.696e-06,  6.126e-05,  1.286e-05,  1.992e-05,
        -2.051e-05, -1.891e-06,  1.756e-06,  2.510e-05, -1.478e-06, -3.245e-05,
         1.508e-05,  1.834e-05,  2.658e-05,  1.365e-06, -7.860e-06, -5.630e-05,
         1.028e-05, -2.026e-04, -3.444e-06,  2.443e-06,  1.352e-04, -5.727e-05,
         9.735e-05,  2.110e-05,  1.195e-04,  4.380e-05,  2.031e-06, -8.014e-05,
        -1.266e-04, -7.750e-05], device='cuda:0')
s after update for 1 param tensor([3.062e-06, 3.070e-04, 1.862e-04, 1.184e-03, 2.086e-04, 1.204e-05,
        2.821e-04, 3.712e-03, 1.069e-02, 4.279e-02, 4.014e-05, 9.817e-06,
        3.708e-03, 1.205e-03, 7.273e-04, 2.293e-04, 4.245e-04, 8.830e-04,
        9.704e-03, 6.901e-05, 1.574e-02, 7.707e-04, 1.925e-02, 4.986e-03,
        3.250e-04, 1.043e-04, 1.080e-03, 8.769e-04, 1.963e-03, 2.658e-04,
        1.978e-03, 4.434e-03, 7.050e-05, 1.882e-06, 3.401e-03, 3.701e-04,
        1.078e-04, 2.232e-05, 7.743e-04, 1.366e-04, 2.468e-03, 1.802e-02,
        3.084e-04, 1.122e-05, 9.141e-04, 1.063e-02, 2.080e-05, 8.818e-04,
        2.912e-04, 5.451e-04, 2.258e-03, 1.785e-03, 9.564e-06, 3.542e-03,
        3.880e-06, 8.175e-05, 1.123e-02, 3.595e-05, 4.335e-04, 4.249e-03,
        8.945e-03, 1.255e-03, 1.763e-02, 3.403e-03, 3.343e-05, 2.291e-03,
        1.747e-03, 1.512e-02, 4.455e-03, 2.459e-02, 2.153e-05, 1.328e-04,
        1.021e-02, 6.775e-03, 2.962e-02, 4.322e-03, 4.745e-06, 2.124e-04,
        7.452e-04, 1.466e-04, 1.133e-02, 1.135e-02, 1.769e-05, 2.348e-04,
        2.640e-04, 1.384e-03, 1.412e-03, 3.289e-06, 1.256e-02, 1.058e-02,
        3.541e-04, 3.248e-03, 1.637e-04, 8.490e-05, 6.586e-05, 2.167e-02,
        1.382e-02, 4.466e-04, 6.991e-04, 2.145e-04, 7.670e-04, 6.751e-05,
        2.975e-04, 1.654e-02, 1.525e-05, 1.324e-04, 7.671e-04, 8.858e-03,
        3.095e-03, 3.461e-04, 7.034e-03, 1.323e-02, 5.237e-04, 1.324e-03,
        5.791e-04, 1.882e-06, 4.529e-05, 9.073e-05, 5.754e-04, 2.009e-02,
        1.537e-02, 1.278e-03, 3.048e-03, 5.962e-05, 2.202e-06, 6.601e-04,
        2.434e-03, 5.958e-04, 2.306e-02, 8.625e-03, 7.326e-03, 2.654e-04,
        2.116e-03, 3.413e-03, 6.546e-03, 1.000e-04, 3.616e-05, 1.318e-03,
        3.554e-04, 3.421e-04, 3.449e-03, 5.746e-05, 1.864e-03, 1.798e-05,
        3.646e-03, 1.716e-04, 3.536e-04, 3.485e-03, 1.161e-03, 3.862e-04,
        1.805e-02, 8.980e-04, 1.069e-03, 2.576e-05, 1.971e-04, 1.019e-04,
        3.809e-02, 6.067e-03, 2.206e-03, 2.659e-02, 1.307e-05, 3.759e-03,
        1.777e-03, 1.408e-03, 8.854e-04, 2.944e-05, 6.378e-03, 1.674e-02,
        2.287e-03, 6.131e-06, 7.306e-05, 9.256e-03, 3.578e-03, 5.477e-04,
        9.159e-04, 4.642e-06, 1.731e-03, 1.774e-03, 2.951e-05, 1.870e-03,
        4.912e-04, 4.495e-03, 6.329e-03, 2.854e-04, 6.770e-05, 3.486e-03,
        5.202e-04, 4.536e-02, 4.908e-04, 6.405e-05, 3.980e-02, 1.642e-03,
        2.395e-02, 1.593e-03, 1.833e-02, 1.030e-02, 1.501e-05, 1.499e-02,
        1.023e-02, 5.007e-03], device='cuda:0')
b after update for 1 param tensor([0.009, 0.091, 0.071, 0.180, 0.075, 0.018, 0.088, 0.318, 0.539, 1.079,
        0.033, 0.016, 0.318, 0.181, 0.141, 0.079, 0.108, 0.155, 0.514, 0.043,
        0.655, 0.145, 0.724, 0.368, 0.094, 0.053, 0.171, 0.155, 0.231, 0.085,
        0.232, 0.347, 0.044, 0.007, 0.304, 0.100, 0.054, 0.025, 0.145, 0.061,
        0.259, 0.700, 0.092, 0.017, 0.158, 0.538, 0.024, 0.155, 0.089, 0.122,
        0.248, 0.220, 0.016, 0.311, 0.010, 0.047, 0.553, 0.031, 0.109, 0.340,
        0.493, 0.185, 0.693, 0.304, 0.030, 0.250, 0.218, 0.642, 0.348, 0.818,
        0.024, 0.060, 0.527, 0.429, 0.898, 0.343, 0.011, 0.076, 0.142, 0.063,
        0.555, 0.556, 0.022, 0.080, 0.085, 0.194, 0.196, 0.009, 0.585, 0.537,
        0.098, 0.297, 0.067, 0.048, 0.042, 0.768, 0.613, 0.110, 0.138, 0.076,
        0.145, 0.043, 0.090, 0.671, 0.020, 0.060, 0.145, 0.491, 0.290, 0.097,
        0.438, 0.600, 0.119, 0.190, 0.126, 0.007, 0.035, 0.050, 0.125, 0.740,
        0.647, 0.187, 0.288, 0.040, 0.008, 0.134, 0.257, 0.127, 0.792, 0.485,
        0.447, 0.085, 0.240, 0.305, 0.422, 0.052, 0.031, 0.189, 0.098, 0.097,
        0.306, 0.040, 0.225, 0.022, 0.315, 0.068, 0.098, 0.308, 0.178, 0.103,
        0.701, 0.156, 0.171, 0.026, 0.073, 0.053, 1.018, 0.406, 0.245, 0.851,
        0.019, 0.320, 0.220, 0.196, 0.155, 0.028, 0.417, 0.675, 0.250, 0.013,
        0.045, 0.502, 0.312, 0.122, 0.158, 0.011, 0.217, 0.220, 0.028, 0.226,
        0.116, 0.350, 0.415, 0.088, 0.043, 0.308, 0.119, 1.111, 0.116, 0.042,
        1.041, 0.211, 0.808, 0.208, 0.706, 0.530, 0.020, 0.639, 0.528, 0.369],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 7.155e-05],
         [-8.076e-05],
         [ 1.049e-05],
         [ 1.846e-05],
         [-7.148e-06],
         [ 7.030e-06],
         [ 2.933e-05],
         [ 1.345e-05],
         [-2.139e-05],
         [-6.727e-06]],

        [[ 7.755e-06],
         [ 8.695e-06],
         [ 4.756e-06],
         [-1.545e-05],
         [ 1.480e-05],
         [ 4.619e-05],
         [ 4.056e-06],
         [ 1.364e-06],
         [ 1.329e-05],
         [ 3.768e-05]],

        [[ 7.304e-06],
         [-9.190e-06],
         [-1.182e-05],
         [-3.370e-05],
         [ 2.163e-05],
         [ 2.151e-05],
         [ 4.753e-06],
         [-9.871e-05],
         [ 3.604e-06],
         [-1.048e-05]],

        [[-2.552e-06],
         [ 1.830e-05],
         [ 9.588e-05],
         [ 1.779e-06],
         [ 4.689e-06],
         [ 1.721e-05],
         [ 2.287e-06],
         [ 1.349e-05],
         [ 1.801e-06],
         [ 3.385e-06]],

        [[-2.803e-05],
         [ 9.237e-05],
         [ 6.232e-06],
         [-4.984e-06],
         [ 7.284e-07],
         [-2.155e-05],
         [-5.088e-06],
         [ 7.905e-06],
         [-3.112e-05],
         [ 1.413e-05]],

        [[ 2.942e-05],
         [-1.299e-04],
         [ 1.019e-05],
         [ 4.953e-06],
         [-2.428e-05],
         [ 1.919e-05],
         [ 1.745e-06],
         [-9.076e-06],
         [ 9.249e-06],
         [ 2.894e-05]],

        [[ 2.607e-05],
         [ 6.130e-05],
         [-2.958e-05],
         [ 6.386e-06],
         [-2.710e-05],
         [ 2.556e-06],
         [ 3.145e-07],
         [ 1.378e-05],
         [-3.752e-05],
         [-7.566e-06]],

        [[-3.899e-06],
         [-2.113e-06],
         [ 6.677e-05],
         [ 3.177e-05],
         [-2.031e-05],
         [-6.838e-05],
         [-6.560e-05],
         [ 9.065e-07],
         [-7.640e-07],
         [ 7.656e-09]],

        [[-7.415e-06],
         [ 1.013e-07],
         [-5.750e-05],
         [-9.876e-06],
         [ 1.666e-06],
         [ 3.382e-06],
         [-1.754e-06],
         [ 3.736e-06],
         [ 7.810e-07],
         [ 2.434e-05]],

        [[ 5.334e-07],
         [-1.562e-05],
         [ 4.072e-06],
         [ 4.198e-06],
         [-1.254e-05],
         [ 1.436e-05],
         [-7.456e-05],
         [-2.669e-05],
         [-3.145e-05],
         [-1.959e-06]],

        [[-1.936e-05],
         [ 1.979e-06],
         [-2.679e-05],
         [-2.840e-05],
         [-3.371e-05],
         [ 2.489e-05],
         [ 1.121e-05],
         [-6.412e-06],
         [-6.674e-05],
         [ 6.848e-06]],

        [[ 8.567e-05],
         [ 9.284e-05],
         [-1.157e-05],
         [-1.949e-05],
         [ 2.835e-05],
         [ 7.202e-05],
         [-4.852e-06],
         [-3.668e-06],
         [-6.949e-05],
         [ 7.004e-07]],

        [[ 5.815e-06],
         [ 2.452e-05],
         [-2.587e-07],
         [ 9.763e-05],
         [ 1.847e-05],
         [ 9.219e-07],
         [-5.724e-07],
         [-2.485e-05],
         [ 1.039e-05],
         [-2.561e-05]],

        [[-1.669e-05],
         [ 4.148e-05],
         [ 4.604e-05],
         [-5.028e-05],
         [-3.687e-05],
         [ 1.009e-04],
         [ 1.323e-05],
         [-8.978e-06],
         [-4.668e-07],
         [-9.445e-06]],

        [[ 2.313e-05],
         [ 1.020e-05],
         [ 1.200e-05],
         [ 1.225e-06],
         [ 1.650e-05],
         [-4.349e-05],
         [ 2.270e-05],
         [-3.885e-05],
         [-2.811e-05],
         [-1.051e-04]],

        [[-2.099e-05],
         [-1.047e-05],
         [ 1.086e-05],
         [ 2.994e-05],
         [-9.393e-06],
         [-2.381e-06],
         [ 1.114e-05],
         [-6.768e-06],
         [-7.937e-05],
         [-2.869e-06]],

        [[ 3.310e-06],
         [-8.763e-06],
         [-7.269e-05],
         [-2.912e-05],
         [ 3.225e-07],
         [ 4.069e-06],
         [ 5.835e-05],
         [-5.535e-05],
         [-1.204e-05],
         [-7.386e-05]],

        [[-1.383e-05],
         [-3.792e-05],
         [ 1.009e-05],
         [ 3.330e-06],
         [ 7.402e-05],
         [-7.168e-06],
         [ 5.786e-05],
         [-1.062e-05],
         [-5.002e-08],
         [ 1.944e-05]],

        [[ 9.481e-06],
         [-7.517e-06],
         [-1.497e-06],
         [ 1.107e-05],
         [ 8.193e-08],
         [-9.503e-07],
         [-3.697e-06],
         [-1.865e-05],
         [-7.807e-05],
         [ 3.796e-05]],

        [[ 5.024e-07],
         [ 2.515e-05],
         [ 5.233e-06],
         [-8.653e-06],
         [-6.038e-06],
         [ 1.259e-05],
         [ 2.442e-06],
         [-3.236e-06],
         [-9.992e-06],
         [ 1.383e-05]]], device='cuda:0')
s after update for 1 param tensor([[[8.487e-03],
         [2.320e-02],
         [1.567e-03],
         [7.053e-04],
         [1.519e-04],
         [1.042e-03],
         [3.526e-03],
         [3.851e-03],
         [1.995e-03],
         [2.293e-03]],

        [[1.429e-04],
         [2.317e-04],
         [1.493e-04],
         [4.171e-04],
         [1.903e-04],
         [3.292e-03],
         [8.164e-04],
         [6.330e-04],
         [3.706e-04],
         [2.651e-03]],

        [[4.738e-03],
         [7.849e-04],
         [2.117e-04],
         [4.993e-04],
         [4.249e-03],
         [5.295e-03],
         [2.456e-03],
         [1.238e-02],
         [3.138e-04],
         [5.415e-04]],

        [[1.733e-04],
         [3.041e-03],
         [2.841e-02],
         [1.525e-04],
         [3.719e-04],
         [9.958e-04],
         [1.219e-04],
         [2.271e-04],
         [3.270e-04],
         [5.284e-05]],

        [[2.355e-03],
         [2.855e-02],
         [2.323e-03],
         [2.826e-05],
         [2.626e-04],
         [2.392e-03],
         [7.070e-04],
         [6.974e-05],
         [7.359e-03],
         [4.047e-04]],

        [[4.130e-03],
         [2.954e-02],
         [2.867e-03],
         [8.898e-05],
         [2.890e-03],
         [7.858e-04],
         [4.110e-04],
         [9.967e-05],
         [1.474e-04],
         [6.247e-03]],

        [[6.972e-04],
         [1.521e-02],
         [2.237e-03],
         [4.620e-04],
         [1.910e-03],
         [6.513e-04],
         [6.759e-06],
         [4.061e-04],
         [3.749e-03],
         [6.296e-04]],

        [[9.218e-04],
         [4.141e-03],
         [1.640e-02],
         [1.014e-03],
         [3.293e-03],
         [2.057e-02],
         [1.254e-02],
         [1.542e-05],
         [3.138e-05],
         [1.345e-04]],

        [[6.382e-05],
         [4.544e-05],
         [2.340e-03],
         [2.820e-04],
         [3.415e-05],
         [6.599e-06],
         [2.181e-05],
         [3.887e-04],
         [3.328e-05],
         [8.239e-03]],

        [[7.003e-04],
         [4.909e-04],
         [2.171e-04],
         [1.145e-05],
         [7.946e-04],
         [2.491e-03],
         [1.351e-02],
         [1.264e-03],
         [1.435e-03],
         [1.174e-04]],

        [[4.973e-03],
         [1.007e-04],
         [3.621e-03],
         [1.286e-03],
         [4.638e-03],
         [1.010e-02],
         [2.228e-04],
         [3.377e-05],
         [1.376e-02],
         [1.649e-03]],

        [[3.593e-03],
         [2.873e-03],
         [2.973e-03],
         [1.316e-03],
         [2.204e-03],
         [1.045e-02],
         [2.527e-03],
         [4.311e-05],
         [7.344e-03],
         [2.681e-06]],

        [[1.434e-03],
         [2.057e-03],
         [1.213e-05],
         [3.339e-02],
         [4.825e-03],
         [3.766e-06],
         [7.388e-05],
         [3.751e-03],
         [4.117e-03],
         [2.426e-03]],

        [[8.606e-04],
         [1.525e-03],
         [1.088e-02],
         [2.702e-03],
         [1.951e-02],
         [1.715e-02],
         [1.377e-03],
         [1.599e-04],
         [4.353e-06],
         [8.300e-04]],

        [[2.014e-03],
         [1.235e-03],
         [7.397e-04],
         [3.676e-04],
         [6.047e-03],
         [8.510e-04],
         [1.001e-02],
         [8.828e-03],
         [3.719e-03],
         [1.389e-02]],

        [[2.839e-04],
         [1.611e-04],
         [2.051e-03],
         [1.210e-02],
         [2.542e-03],
         [1.868e-05],
         [2.834e-03],
         [2.342e-03],
         [5.603e-03],
         [2.886e-04]],

        [[1.231e-03],
         [2.588e-03],
         [4.565e-03],
         [1.160e-03],
         [1.043e-04],
         [7.577e-06],
         [4.658e-03],
         [7.767e-03],
         [2.827e-03],
         [2.113e-02]],

        [[3.928e-03],
         [1.581e-02],
         [1.941e-04],
         [1.121e-04],
         [2.161e-02],
         [1.894e-03],
         [8.692e-03],
         [8.766e-04],
         [2.833e-06],
         [7.016e-04]],

        [[4.976e-04],
         [5.461e-03],
         [3.093e-04],
         [1.012e-03],
         [1.882e-06],
         [1.882e-06],
         [4.229e-04],
         [3.625e-03],
         [2.048e-02],
         [5.138e-03]],

        [[5.438e-03],
         [4.766e-04],
         [1.741e-04],
         [1.013e-04],
         [1.101e-04],
         [9.531e-05],
         [8.878e-04],
         [2.543e-03],
         [3.615e-03],
         [1.028e-04]]], device='cuda:0')
b after update for 1 param tensor([[[0.481],
         [0.795],
         [0.207],
         [0.139],
         [0.064],
         [0.168],
         [0.310],
         [0.324],
         [0.233],
         [0.250]],

        [[0.062],
         [0.079],
         [0.064],
         [0.107],
         [0.072],
         [0.299],
         [0.149],
         [0.131],
         [0.100],
         [0.269]],

        [[0.359],
         [0.146],
         [0.076],
         [0.117],
         [0.340],
         [0.380],
         [0.259],
         [0.580],
         [0.092],
         [0.121]],

        [[0.069],
         [0.288],
         [0.879],
         [0.064],
         [0.101],
         [0.165],
         [0.058],
         [0.079],
         [0.094],
         [0.038]],

        [[0.253],
         [0.882],
         [0.252],
         [0.028],
         [0.085],
         [0.255],
         [0.139],
         [0.044],
         [0.448],
         [0.105]],

        [[0.335],
         [0.897],
         [0.279],
         [0.049],
         [0.281],
         [0.146],
         [0.106],
         [0.052],
         [0.063],
         [0.412]],

        [[0.138],
         [0.644],
         [0.247],
         [0.112],
         [0.228],
         [0.133],
         [0.014],
         [0.105],
         [0.319],
         [0.131]],

        [[0.158],
         [0.336],
         [0.668],
         [0.166],
         [0.299],
         [0.748],
         [0.584],
         [0.020],
         [0.029],
         [0.061]],

        [[0.042],
         [0.035],
         [0.252],
         [0.088],
         [0.030],
         [0.013],
         [0.024],
         [0.103],
         [0.030],
         [0.474]],

        [[0.138],
         [0.116],
         [0.077],
         [0.018],
         [0.147],
         [0.260],
         [0.606],
         [0.186],
         [0.198],
         [0.057]],

        [[0.368],
         [0.052],
         [0.314],
         [0.187],
         [0.355],
         [0.524],
         [0.078],
         [0.030],
         [0.612],
         [0.212]],

        [[0.313],
         [0.280],
         [0.285],
         [0.189],
         [0.245],
         [0.533],
         [0.262],
         [0.034],
         [0.447],
         [0.009]],

        [[0.198],
         [0.237],
         [0.018],
         [0.953],
         [0.362],
         [0.010],
         [0.045],
         [0.320],
         [0.335],
         [0.257]],

        [[0.153],
         [0.204],
         [0.544],
         [0.271],
         [0.729],
         [0.683],
         [0.194],
         [0.066],
         [0.011],
         [0.150]],

        [[0.234],
         [0.183],
         [0.142],
         [0.100],
         [0.406],
         [0.152],
         [0.522],
         [0.490],
         [0.318],
         [0.615]],

        [[0.088],
         [0.066],
         [0.236],
         [0.574],
         [0.263],
         [0.023],
         [0.278],
         [0.253],
         [0.391],
         [0.089]],

        [[0.183],
         [0.265],
         [0.353],
         [0.178],
         [0.053],
         [0.014],
         [0.356],
         [0.460],
         [0.277],
         [0.758]],

        [[0.327],
         [0.656],
         [0.073],
         [0.055],
         [0.767],
         [0.227],
         [0.486],
         [0.154],
         [0.009],
         [0.138]],

        [[0.116],
         [0.386],
         [0.092],
         [0.166],
         [0.007],
         [0.007],
         [0.107],
         [0.314],
         [0.747],
         [0.374]],

        [[0.385],
         [0.114],
         [0.069],
         [0.053],
         [0.055],
         [0.051],
         [0.155],
         [0.263],
         [0.314],
         [0.053]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 3.040e-06],
        [ 3.489e-05],
        [ 1.341e-05],
        [ 1.746e-05],
        [ 5.221e-06],
        [-7.075e-06],
        [ 6.810e-07],
        [ 9.179e-07],
        [ 1.790e-05],
        [ 5.952e-06],
        [ 4.046e-05],
        [ 6.238e-05],
        [-8.052e-06],
        [ 2.498e-05],
        [-2.957e-06],
        [-2.419e-06],
        [-6.083e-06],
        [-1.291e-05],
        [-3.761e-05],
        [ 3.544e-05]], device='cuda:0')
s after update for 1 param tensor([[3.216e-04],
        [3.638e-03],
        [8.398e-05],
        [1.531e-04],
        [3.556e-05],
        [5.783e-04],
        [7.928e-05],
        [1.882e-06],
        [1.037e-03],
        [2.306e-03],
        [1.475e-02],
        [9.555e-03],
        [2.829e-04],
        [7.849e-03],
        [2.844e-05],
        [2.009e-04],
        [7.882e-04],
        [2.961e-04],
        [3.546e-03],
        [5.753e-03]], device='cuda:0')
b after update for 1 param tensor([[0.094],
        [0.315],
        [0.048],
        [0.065],
        [0.031],
        [0.125],
        [0.046],
        [0.007],
        [0.168],
        [0.251],
        [0.634],
        [0.510],
        [0.088],
        [0.462],
        [0.028],
        [0.074],
        [0.146],
        [0.090],
        [0.311],
        [0.396]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 4.637001538689175
exp ma of ||w||^2 178.02982266642644
||w|| 2.153369810016193
exp ma of ||w|| 3.1852079300067855
||w||^2 3.5792268097688247
exp ma of ||w||^2 4.664891107230907
||w|| 1.8918844599416806
exp ma of ||w|| 2.093411548415504
||w||^2 2.712034956550494
exp ma of ||w||^2 3.7228984357224126
||w|| 1.6468257213653466
exp ma of ||w|| 1.8777559689255803
cuda
Objective function 56.57 = squared loss an data 45.95 + 0.5*rho*h**2 5.290115 + alpha*h 3.631166 + L2reg 1.58 + L1reg 0.11 ; SHD = 68 ; DAG True
Proportion of microbatches that were clipped  0.7896702939743382
iteration 2 in inner loop, alpha 353.0193402041621 rho 100000.0 h 0.010286024437885288
iteration 4 in outer loop, alpha = 10639.04377808945, rho = 1000000.0, h = 0.010286024437885288
Threshold 0.3
[[0.005 1.052 0.148 0.105 0.258 0.284 0.205 0.105 0.021 0.153 0.094 0.089
  0.16  0.19  0.185 0.243 0.156 0.152 0.041 0.037]
 [0.009 0.006 0.085 0.043 0.183 0.475 0.545 0.017 0.039 0.045 0.012 0.04
  0.083 0.062 0.092 0.099 0.148 0.115 0.039 0.023]
 [0.05  0.061 0.005 0.073 0.206 0.174 0.234 0.068 0.03  0.147 0.094 0.074
  0.172 0.038 0.116 0.178 0.121 0.052 0.02  0.045]
 [0.093 0.141 0.07  0.004 0.361 0.138 0.172 0.159 0.048 0.101 0.07  0.107
  0.097 0.135 0.08  0.054 0.097 0.111 0.109 0.024]
 [0.016 0.023 0.026 0.019 0.006 0.072 0.072 0.068 0.02  0.033 0.039 0.087
  0.088 0.122 0.11  0.072 0.051 0.035 0.033 0.026]
 [0.014 0.014 0.042 0.029 0.086 0.004 0.126 0.029 0.016 0.024 0.045 0.055
  0.082 0.043 0.221 0.114 0.833 0.062 0.061 0.048]
 [0.013 0.012 0.025 0.072 0.09  0.056 0.005 0.041 0.009 0.044 0.03  0.063
  0.082 0.025 0.21  0.078 0.047 0.078 0.035 0.009]
 [0.075 0.236 0.094 0.059 0.092 0.183 0.107 0.005 0.056 0.046 0.06  0.194
  0.101 0.107 0.125 0.09  0.249 0.136 0.093 0.038]
 [0.207 0.125 0.16  0.143 0.37  0.365 0.36  0.108 0.004 0.19  0.134 0.137
  0.09  0.157 0.1   0.305 0.497 0.307 0.092 0.059]
 [0.048 0.155 0.031 0.057 0.175 0.252 0.185 0.113 0.031 0.006 0.034 0.124
  0.148 0.187 0.086 0.172 0.19  0.045 0.042 0.053]
 [0.064 0.269 0.085 0.072 0.192 0.193 0.268 0.094 0.078 0.203 0.005 0.243
  0.391 0.217 0.144 0.154 0.14  0.041 0.081 0.026]
 [0.07  0.07  0.061 0.05  0.082 0.108 0.09  0.029 0.059 0.083 0.028 0.004
  0.043 0.099 0.033 0.083 0.059 0.045 0.051 0.045]
 [0.068 0.071 0.019 0.056 0.08  0.074 0.066 0.069 0.072 0.058 0.011 0.094
  0.006 0.044 0.041 0.116 0.136 0.072 0.052 0.022]
 [0.019 0.077 0.108 0.052 0.051 0.141 0.231 0.044 0.05  0.022 0.029 0.078
  0.121 0.004 0.046 0.093 0.12  0.042 0.044 0.023]
 [0.031 0.062 0.063 0.062 0.063 0.019 0.051 0.053 0.038 0.049 0.053 0.116
  0.143 0.133 0.005 0.123 0.025 0.049 0.061 0.057]
 [0.035 0.099 0.047 0.088 0.111 0.057 0.121 0.041 0.027 0.057 0.041 0.082
  0.053 0.069 0.065 0.005 0.043 0.052 0.01  0.01 ]
 [0.019 0.034 0.047 0.042 0.099 0.008 0.133 0.032 0.009 0.028 0.046 0.079
  0.058 0.035 0.31  0.235 0.005 0.032 0.11  0.03 ]
 [0.035 0.047 0.121 0.077 0.279 0.106 0.124 0.045 0.024 0.179 0.146 0.108
  0.072 0.075 0.12  0.107 0.117 0.004 0.054 0.047]
 [0.145 0.25  0.219 0.052 0.14  0.114 0.171 0.079 0.053 0.137 0.095 0.133
  0.066 0.197 0.123 0.785 0.091 0.101 0.006 0.039]
 [0.245 0.42  0.13  0.367 0.353 0.178 0.463 0.191 0.119 0.169 0.293 0.201
  0.329 0.202 0.077 0.606 0.203 0.182 0.198 0.004]]
[[0.    1.052 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.475 0.545 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.361 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.833 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.37  0.365 0.36  0.    0.    0.    0.    0.
  0.    0.    0.    0.305 0.497 0.307 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.391 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.31  0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.785 0.    0.    0.    0.   ]
 [0.    0.42  0.    0.367 0.353 0.    0.463 0.    0.    0.    0.    0.
  0.329 0.    0.    0.606 0.    0.    0.    0.   ]]
{'fdr': 0.35, 'tpr': 0.1625, 'fpr': 0.06363636363636363, 'f1': 0.26, 'shd': 68, 'npred': 20, 'ntrue': 80}
[1.052 0.148 0.105 0.258 0.284 0.205 0.105 0.021 0.153 0.094 0.089 0.16
 0.19  0.185 0.243 0.156 0.152 0.041 0.037 0.009 0.085 0.043 0.183 0.475
 0.545 0.017 0.039 0.045 0.012 0.04  0.083 0.062 0.092 0.099 0.148 0.115
 0.039 0.023 0.05  0.061 0.073 0.206 0.174 0.234 0.068 0.03  0.147 0.094
 0.074 0.172 0.038 0.116 0.178 0.121 0.052 0.02  0.045 0.093 0.141 0.07
 0.361 0.138 0.172 0.159 0.048 0.101 0.07  0.107 0.097 0.135 0.08  0.054
 0.097 0.111 0.109 0.024 0.016 0.023 0.026 0.019 0.072 0.072 0.068 0.02
 0.033 0.039 0.087 0.088 0.122 0.11  0.072 0.051 0.035 0.033 0.026 0.014
 0.014 0.042 0.029 0.086 0.126 0.029 0.016 0.024 0.045 0.055 0.082 0.043
 0.221 0.114 0.833 0.062 0.061 0.048 0.013 0.012 0.025 0.072 0.09  0.056
 0.041 0.009 0.044 0.03  0.063 0.082 0.025 0.21  0.078 0.047 0.078 0.035
 0.009 0.075 0.236 0.094 0.059 0.092 0.183 0.107 0.056 0.046 0.06  0.194
 0.101 0.107 0.125 0.09  0.249 0.136 0.093 0.038 0.207 0.125 0.16  0.143
 0.37  0.365 0.36  0.108 0.19  0.134 0.137 0.09  0.157 0.1   0.305 0.497
 0.307 0.092 0.059 0.048 0.155 0.031 0.057 0.175 0.252 0.185 0.113 0.031
 0.034 0.124 0.148 0.187 0.086 0.172 0.19  0.045 0.042 0.053 0.064 0.269
 0.085 0.072 0.192 0.193 0.268 0.094 0.078 0.203 0.243 0.391 0.217 0.144
 0.154 0.14  0.041 0.081 0.026 0.07  0.07  0.061 0.05  0.082 0.108 0.09
 0.029 0.059 0.083 0.028 0.043 0.099 0.033 0.083 0.059 0.045 0.051 0.045
 0.068 0.071 0.019 0.056 0.08  0.074 0.066 0.069 0.072 0.058 0.011 0.094
 0.044 0.041 0.116 0.136 0.072 0.052 0.022 0.019 0.077 0.108 0.052 0.051
 0.141 0.231 0.044 0.05  0.022 0.029 0.078 0.121 0.046 0.093 0.12  0.042
 0.044 0.023 0.031 0.062 0.063 0.062 0.063 0.019 0.051 0.053 0.038 0.049
 0.053 0.116 0.143 0.133 0.123 0.025 0.049 0.061 0.057 0.035 0.099 0.047
 0.088 0.111 0.057 0.121 0.041 0.027 0.057 0.041 0.082 0.053 0.069 0.065
 0.043 0.052 0.01  0.01  0.019 0.034 0.047 0.042 0.099 0.008 0.133 0.032
 0.009 0.028 0.046 0.079 0.058 0.035 0.31  0.235 0.032 0.11  0.03  0.035
 0.047 0.121 0.077 0.279 0.106 0.124 0.045 0.024 0.179 0.146 0.108 0.072
 0.075 0.12  0.107 0.117 0.054 0.047 0.145 0.25  0.219 0.052 0.14  0.114
 0.171 0.079 0.053 0.137 0.095 0.133 0.066 0.197 0.123 0.785 0.091 0.101
 0.039 0.245 0.42  0.13  0.367 0.353 0.178 0.463 0.191 0.119 0.169 0.293
 0.201 0.329 0.202 0.077 0.606 0.203 0.182 0.198]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.6174583333333333, 0.4003780740600839)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
