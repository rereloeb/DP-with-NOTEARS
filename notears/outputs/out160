samples  5000  graph  20 40 ER mlp  minibatch size  50  noise  0.8  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617290166446
iteration 1 in outer loop, alpha = 1.5634617290166446, rho = 1.0, h = 1.5634617290166446
cuda
iteration 1 in inner loop,alpha 1.5634617290166446 rho 1.0 h 1.0381693524821998
iteration 2 in inner loop,alpha 1.5634617290166446 rho 10.0 h 0.4486130544339133
iteration 3 in inner loop,alpha 1.5634617290166446 rho 100.0 h 0.13645699227387453
iteration 2 in outer loop, alpha = 15.209160956404098, rho = 100.0, h = 0.13645699227387453
cuda
iteration 1 in inner loop,alpha 15.209160956404098 rho 100.0 h 0.07242523875783746
iteration 2 in inner loop,alpha 15.209160956404098 rho 1000.0 h 0.02272817333826893
iteration 3 in outer loop, alpha = 37.93733429467303, rho = 1000.0, h = 0.02272817333826893
cuda
iteration 1 in inner loop,alpha 37.93733429467303 rho 1000.0 h 0.010195562783067658
iteration 2 in inner loop,alpha 37.93733429467303 rho 10000.0 h 0.00340231470086394
iteration 4 in outer loop, alpha = 71.96048130331243, rho = 10000.0, h = 0.00340231470086394
cuda
iteration 1 in inner loop,alpha 71.96048130331243 rho 10000.0 h 0.0015203021867513655
iteration 2 in inner loop,alpha 71.96048130331243 rho 100000.0 h 0.0005097088441843312
iteration 5 in outer loop, alpha = 122.93136572174555, rho = 100000.0, h = 0.0005097088441843312
cuda
iteration 1 in inner loop,alpha 122.93136572174555 rho 100000.0 h 0.0002538805382776843
iteration 6 in outer loop, alpha = 376.8119039994299, rho = 1000000.0, h = 0.0002538805382776843
Threshold 0.3
[[0.    0.011 0.042 2.312 0.034 0.031 0.042 0.002 1.995 0.015 1.851 0.001
  0.101 0.004 0.206 0.948 0.002 0.001 0.037 0.221]
 [0.032 0.005 0.218 0.057 0.39  0.109 0.029 0.002 3.052 0.048 0.016 0.01
  0.601 0.001 1.813 1.576 0.288 0.    0.226 0.305]
 [0.001 0.    0.003 0.069 0.127 1.182 0.001 0.    2.294 0.077 0.061 0.
  2.296 0.    1.363 0.879 0.    0.    3.747 2.25 ]
 [0.    0.    0.    0.003 0.001 0.002 0.    0.    0.    0.    0.    0.
  0.    0.    0.156 1.497 0.    0.    0.    0.001]
 [0.    0.    0.    1.678 0.001 0.205 0.001 0.    0.001 0.031 0.017 0.
  0.375 0.    0.087 0.344 0.001 0.    0.034 0.18 ]
 [0.    0.    0.    0.111 0.002 0.001 0.    0.    0.    0.037 0.067 0.
  0.234 0.    1.91  0.445 0.    0.    0.001 0.11 ]
 [0.001 0.016 0.576 0.057 0.227 0.104 0.005 0.    0.035 0.    0.017 0.
  0.037 0.    0.067 1.637 0.007 0.    0.064 0.027]
 [0.008 0.013 0.674 0.047 0.187 0.094 4.464 0.001 0.149 0.009 0.007 0.05
  0.009 0.004 0.122 0.229 0.063 0.007 0.063 0.053]
 [0.    0.    0.    0.061 0.992 0.123 0.001 0.    0.002 0.049 0.045 0.
  0.283 0.    0.3   0.533 0.    0.    0.017 0.267]
 [0.    0.    0.    0.094 0.    0.    0.    0.    0.    0.001 0.833 0.
  0.    0.    0.178 0.25  0.    0.    0.    0.194]
 [0.    0.    0.    0.096 0.001 0.    0.    0.    0.002 0.    0.002 0.
  0.    0.    1.348 0.282 0.    0.    0.    2.298]
 [0.006 0.003 2.755 0.067 0.021 0.218 3.597 0.004 0.218 0.016 0.016 0.001
  0.227 0.002 0.585 0.356 0.008 0.025 0.208 0.156]
 [0.    0.    0.    0.109 0.    0.001 0.    0.    0.001 3.206 0.126 0.
  0.002 0.    0.377 0.535 0.001 0.    0.    0.418]
 [0.005 0.009 0.017 0.053 0.031 0.122 0.005 0.012 0.095 0.029 0.077 0.029
  0.275 0.    0.171 0.178 0.007 0.029 2.631 0.238]
 [0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.005 0.281 0.    0.    0.    0.004]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.006 0.008 0.    0.    0.    0.   ]
 [0.009 0.005 0.337 0.064 0.336 0.197 0.03  0.003 0.268 0.048 0.056 0.011
  0.4   0.001 1.855 1.511 0.003 0.001 0.328 0.451]
 [0.006 4.27  0.006 0.051 0.209 0.003 0.011 0.002 0.592 0.007 0.006 0.003
  1.392 0.001 0.143 0.255 2.997 0.001 1.414 1.004]
 [0.    0.001 0.    0.063 0.014 1.225 0.    0.    0.083 0.05  0.018 0.
  1.036 0.    0.354 1.342 0.    0.    0.003 0.445]
 [0.    0.    0.    2.089 0.    0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.223 0.337 0.    0.    0.    0.003]]
[[0.    0.    0.    2.312 0.    0.    0.    0.    1.995 0.    1.851 0.
  0.    0.    0.    0.948 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.39  0.    0.    0.    3.052 0.    0.    0.
  0.601 0.    1.813 1.576 0.    0.    0.    0.305]
 [0.    0.    0.    0.    0.    1.182 0.    0.    2.294 0.    0.    0.
  2.296 0.    1.363 0.879 0.    0.    3.747 2.25 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.497 0.    0.    0.    0.   ]
 [0.    0.    0.    1.678 0.    0.    0.    0.    0.    0.    0.    0.
  0.375 0.    0.    0.344 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.91  0.445 0.    0.    0.    0.   ]
 [0.    0.    0.576 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.637 0.    0.    0.    0.   ]
 [0.    0.    0.674 0.    0.    0.    4.464 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.992 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.533 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.833 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.348 0.    0.    0.    0.    2.298]
 [0.    0.    2.755 0.    0.    0.    3.597 0.    0.    0.    0.    0.
  0.    0.    0.585 0.356 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.206 0.    0.
  0.    0.    0.377 0.535 0.    0.    0.    0.418]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.631 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.337 0.    0.336 0.    0.    0.    0.    0.    0.    0.
  0.4   0.    1.855 1.511 0.    0.    0.328 0.451]
 [0.    4.27  0.    0.    0.    0.    0.    0.    0.592 0.    0.    0.
  1.392 0.    0.    0.    2.997 0.    1.414 1.004]
 [0.    0.    0.    0.    0.    1.225 0.    0.    0.    0.    0.    0.
  1.036 0.    0.354 1.342 0.    0.    0.    0.445]
 [0.    0.    0.    2.089 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.337 0.    0.    0.    0.   ]]
{'fdr': 0.3442622950819672, 'tpr': 1.0, 'fpr': 0.14, 'f1': 0.7920792079207921, 'shd': 21, 'npred': 61, 'ntrue': 40}
[1.073e-02 4.229e-02 2.312e+00 3.350e-02 3.093e-02 4.171e-02 1.605e-03
 1.995e+00 1.465e-02 1.851e+00 1.033e-03 1.005e-01 3.555e-03 2.059e-01
 9.478e-01 2.060e-03 1.077e-03 3.723e-02 2.208e-01 3.237e-02 2.185e-01
 5.744e-02 3.896e-01 1.089e-01 2.914e-02 2.279e-03 3.052e+00 4.766e-02
 1.555e-02 9.996e-03 6.010e-01 1.018e-03 1.813e+00 1.576e+00 2.881e-01
 3.739e-04 2.261e-01 3.051e-01 5.886e-04 2.497e-04 6.862e-02 1.265e-01
 1.182e+00 5.279e-04 5.837e-05 2.294e+00 7.654e-02 6.078e-02 2.309e-04
 2.296e+00 3.631e-04 1.363e+00 8.794e-01 3.751e-04 6.737e-05 3.747e+00
 2.250e+00 9.636e-06 1.438e-04 1.911e-04 5.551e-04 1.572e-03 1.522e-04
 3.605e-05 4.652e-04 3.223e-05 6.183e-05 8.960e-05 5.965e-05 1.971e-04
 1.561e-01 1.497e+00 2.562e-04 8.952e-05 2.789e-04 1.030e-03 7.567e-06
 1.086e-04 1.618e-04 1.678e+00 2.048e-01 8.792e-04 9.991e-06 7.058e-04
 3.086e-02 1.715e-02 6.389e-05 3.754e-01 3.867e-04 8.700e-02 3.440e-01
 9.004e-04 3.149e-05 3.409e-02 1.804e-01 2.904e-04 3.296e-04 8.125e-05
 1.106e-01 1.981e-03 1.207e-04 1.134e-05 4.259e-04 3.708e-02 6.719e-02
 3.175e-05 2.340e-01 5.410e-05 1.910e+00 4.446e-01 1.855e-04 1.804e-05
 7.005e-04 1.099e-01 6.291e-04 1.645e-02 5.758e-01 5.667e-02 2.266e-01
 1.042e-01 1.716e-04 3.517e-02 4.962e-04 1.684e-02 1.506e-05 3.651e-02
 4.410e-04 6.708e-02 1.637e+00 7.336e-03 4.203e-04 6.414e-02 2.737e-02
 7.622e-03 1.298e-02 6.743e-01 4.690e-02 1.868e-01 9.380e-02 4.464e+00
 1.494e-01 8.698e-03 6.884e-03 4.994e-02 8.791e-03 4.052e-03 1.217e-01
 2.293e-01 6.272e-02 6.526e-03 6.259e-02 5.293e-02 4.165e-05 2.168e-04
 3.511e-04 6.080e-02 9.920e-01 1.232e-01 6.106e-04 2.209e-05 4.913e-02
 4.499e-02 8.358e-05 2.827e-01 3.021e-04 3.000e-01 5.330e-01 3.315e-04
 3.718e-05 1.707e-02 2.673e-01 1.740e-04 5.343e-05 4.632e-05 9.395e-02
 1.027e-04 4.551e-04 1.311e-04 8.767e-06 1.688e-04 8.327e-01 2.157e-05
 1.412e-04 2.826e-05 1.782e-01 2.496e-01 1.051e-04 1.229e-05 5.408e-05
 1.938e-01 5.251e-05 7.578e-05 3.397e-05 9.637e-02 5.114e-04 2.829e-04
 4.599e-04 5.244e-06 1.687e-03 1.083e-04 1.135e-05 5.679e-05 1.813e-05
 1.348e+00 2.822e-01 1.975e-04 7.093e-05 5.855e-05 2.298e+00 5.834e-03
 3.364e-03 2.755e+00 6.726e-02 2.077e-02 2.175e-01 3.597e+00 3.993e-03
 2.176e-01 1.555e-02 1.576e-02 2.267e-01 1.992e-03 5.846e-01 3.565e-01
 8.155e-03 2.548e-02 2.084e-01 1.558e-01 1.284e-04 7.352e-05 1.932e-04
 1.087e-01 1.342e-04 1.368e-03 3.911e-05 3.382e-05 7.473e-04 3.206e+00
 1.265e-01 4.581e-05 3.691e-05 3.770e-01 5.351e-01 5.038e-04 3.976e-05
 2.139e-04 4.177e-01 4.575e-03 8.858e-03 1.675e-02 5.328e-02 3.077e-02
 1.220e-01 4.786e-03 1.173e-02 9.547e-02 2.920e-02 7.707e-02 2.928e-02
 2.752e-01 1.712e-01 1.785e-01 7.160e-03 2.913e-02 2.631e+00 2.384e-01
 1.278e-05 4.339e-04 9.290e-05 3.746e-03 4.448e-04 1.301e-04 1.339e-04
 1.080e-05 4.269e-04 1.650e-04 1.738e-04 3.170e-05 2.633e-05 1.081e-05
 2.805e-01 2.778e-04 7.700e-05 5.831e-05 4.047e-03 2.086e-05 2.107e-04
 4.867e-05 1.380e-04 1.190e-04 2.575e-04 4.119e-04 3.355e-05 3.927e-05
 8.807e-05 3.808e-05 9.885e-06 4.327e-05 9.167e-06 5.635e-03 1.670e-04
 3.009e-05 3.755e-05 1.131e-04 9.005e-03 5.378e-03 3.374e-01 6.417e-02
 3.355e-01 1.974e-01 2.962e-02 2.588e-03 2.678e-01 4.817e-02 5.575e-02
 1.052e-02 3.998e-01 6.981e-04 1.855e+00 1.511e+00 5.287e-04 3.285e-01
 4.509e-01 5.739e-03 4.270e+00 5.580e-03 5.117e-02 2.093e-01 3.005e-03
 1.055e-02 1.754e-03 5.918e-01 6.776e-03 6.303e-03 2.772e-03 1.392e+00
 1.275e-03 1.433e-01 2.548e-01 2.997e+00 1.414e+00 1.004e+00 2.343e-04
 9.334e-04 4.123e-04 6.333e-02 1.444e-02 1.225e+00 1.218e-04 2.582e-05
 8.344e-02 5.031e-02 1.821e-02 9.266e-05 1.036e+00 1.042e-05 3.541e-01
 1.342e+00 1.779e-04 1.008e-04 4.454e-01 5.529e-05 3.460e-05 1.754e-04
 2.089e+00 3.321e-04 1.411e-03 9.103e-05 2.760e-05 1.975e-04 1.798e-05
 8.165e-05 6.671e-05 1.084e-04 2.403e-05 2.226e-01 3.374e-01 3.121e-04
 1.143e-04 1.235e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9992647058823529, 0.9934713912844813)
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
total norm for a microbatch 50.54510490787813 clip 49.82476132060303
total norm for a microbatch 71.79331718028631 clip 48.66145727268887
total norm for a microbatch 70.07347748630397 clip 54.97623521602846
total norm for a microbatch 51.81766524417816 clip 58.966861869347234
total norm for a microbatch 61.25226642512104 clip 61.58922587688947
total norm for a microbatch 150.01242194248087 clip 73.31116955713931
total norm for a microbatch 94.09706950437969 clip 82.88785013074317
total norm for a microbatch 121.61800511538213 clip 78.87697315764733
total norm for a microbatch 91.30357941319431 clip 79.0042012225377
total norm for a microbatch 213.29437165340553 clip 81.07060562212693
total norm for a microbatch 81.30801679931488 clip 82.33726923444863
total norm for a microbatch 73.0412613564077 clip 83.92713953281458
cuda
Objective function 40.52 = squared loss an data 32.25 + 0.5*rho*h**2 6.825576 + alpha*h 0.000000 + L2reg 1.14 + L1reg 0.29 ; SHD = 119 ; DAG False
Proportion of microbatches that were clipped  0.8153771603941204
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 3.6947465806367177
iteration 1 in outer loop, alpha = 3.6947465806367177, rho = 1.0, h = 3.6947465806367177
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 54.17 = squared loss an data 32.25 + 0.5*rho*h**2 6.825576 + alpha*h 13.651152 + L2reg 1.14 + L1reg 0.29 ; SHD = 119 ; DAG False
total norm for a microbatch 93.67660464210292 clip 1.2048967501798744
total norm for a microbatch 217.72746401733153 clip 5.734361006279214
total norm for a microbatch 93.84320898894474 clip 12.067532816689715
total norm for a microbatch 164.95003335024694 clip 110.22110971576804
total norm for a microbatch 151.404621769579 clip 101.62851557241912
total norm for a microbatch 111.38341523880423 clip 115.1616400067627
total norm for a microbatch 160.78283238424285 clip 114.54387895395004
total norm for a microbatch 131.22027708387603 clip 110.96662458950212
total norm for a microbatch 111.46891636855403 clip 104.9506055595474
total norm for a microbatch 105.95149027556758 clip 107.63879387610926
total norm for a microbatch 118.24296071769317 clip 109.94218315299557
total norm for a microbatch 155.1594703530279 clip 110.44899060545725
cuda
Objective function 36.54 = squared loss an data 23.38 + 0.5*rho*h**2 2.684972 + alpha*h 8.561887 + L2reg 1.64 + L1reg 0.27 ; SHD = 108 ; DAG False
Proportion of microbatches that were clipped  0.8181226084832696
iteration 1 in inner loop, alpha 3.6947465806367177 rho 1.0 h 2.3173138409777465
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 60.70 = squared loss an data 23.38 + 0.5*rho*h**2 26.849717 + alpha*h 8.561887 + L2reg 1.64 + L1reg 0.27 ; SHD = 108 ; DAG False
total norm for a microbatch 204.79130379278507 clip 6.535292006935008
total norm for a microbatch 281.0282575502575 clip 117.36945622703136
total norm for a microbatch 137.92475784968863 clip 140.7162198680901
total norm for a microbatch 275.8499141597527 clip 151.293821695774
total norm for a microbatch 185.38550644528004 clip 150.3215162491383
total norm for a microbatch 305.98101058682715 clip 146.24768434264195
total norm for a microbatch 158.86526889744303 clip 148.71685614370978
total norm for a microbatch 108.22449097821385 clip 135.98578702738084
total norm for a microbatch 238.52690256925132 clip 132.37263992956645
total norm for a microbatch 148.69688317147686 clip 136.341071534348
total norm for a microbatch 123.30355058653214 clip 132.75837767775656
cuda
Objective function 37.60 = squared loss an data 25.20 + 0.5*rho*h**2 6.131007 + alpha*h 4.091340 + L2reg 1.93 + L1reg 0.26 ; SHD = 90 ; DAG False
Proportion of microbatches that were clipped  0.8223382884692647
iteration 2 in inner loop, alpha 3.6947465806367177 rho 10.0 h 1.107339770586215
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 92.78 = squared loss an data 25.20 + 0.5*rho*h**2 61.310068 + alpha*h 4.091340 + L2reg 1.93 + L1reg 0.26 ; SHD = 90 ; DAG False
total norm for a microbatch 333.9807446701845 clip 2.948653418152897
total norm for a microbatch 257.5512107711949 clip 92.30126067140115
total norm for a microbatch 208.772227523576 clip 180.744423474406
total norm for a microbatch 314.920645477532 clip 162.88162178502327
total norm for a microbatch 294.85496967373234 clip 162.00707778710975
total norm for a microbatch 159.75925997550178 clip 154.64391873392233
total norm for a microbatch 231.5590075616716 clip 166.47638333472895
total norm for a microbatch 153.52996668296765 clip 154.90306443100886
cuda
Objective function 39.91 = squared loss an data 27.54 + 0.5*rho*h**2 8.413589 + alpha*h 1.515620 + L2reg 2.19 + L1reg 0.25 ; SHD = 102 ; DAG True
Proportion of microbatches that were clipped  0.8250284044797922
iteration 3 in inner loop, alpha 3.6947465806367177 rho 100.0 h 0.410209425153937
iteration 2 in outer loop, alpha = 44.71568909603042, rho = 100.0, h = 0.410209425153937
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 56.74 = squared loss an data 27.54 + 0.5*rho*h**2 8.413589 + alpha*h 18.342797 + L2reg 2.19 + L1reg 0.25 ; SHD = 102 ; DAG True
total norm for a microbatch 205.55217985997822 clip 1.8892737511837898
total norm for a microbatch 215.18512662646927 clip 4.232688379914981
total norm for a microbatch 154.42839778197808 clip 12.870221525249661
total norm for a microbatch 320.7593918847328 clip 22.389119785841025
total norm for a microbatch 158.85602641869093 clip 179.6673973318705
total norm for a microbatch 286.31111880475225 clip 174.58669340771112
total norm for a microbatch 157.88334855990593 clip 172.83416399017236
total norm for a microbatch 165.6203693975097 clip 170.49983365618783
total norm for a microbatch 231.9022515989402 clip 178.15591982068764
total norm for a microbatch 163.1007810733066 clip 177.64377708314922
total norm for a microbatch 243.45663093022898 clip 168.63861524708958
total norm for a microbatch 173.18775526406787 clip 179.00467912848777
total norm for a microbatch 281.30901208758854 clip 167.1572638754554
total norm for a microbatch 168.82399540902622 clip 157.416062951128
total norm for a microbatch 199.61816640036082 clip 156.71867026308453
cuda
Objective function 43.74 = squared loss an data 26.23 + 0.5*rho*h**2 3.329349 + alpha*h 11.538639 + L2reg 2.39 + L1reg 0.24 ; SHD = 88 ; DAG True
Proportion of microbatches that were clipped  0.8252144359928791
iteration 1 in inner loop, alpha 44.71568909603042 rho 100.0 h 0.2580445394812507
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 73.70 = squared loss an data 26.23 + 0.5*rho*h**2 33.293492 + alpha*h 11.538639 + L2reg 2.39 + L1reg 0.24 ; SHD = 88 ; DAG True
total norm for a microbatch 372.3323403094661 clip 13.748719700092343
total norm for a microbatch 221.7946489246872 clip 31.423795728973765
total norm for a microbatch 163.95598505474732 clip 184.63016859226857
total norm for a microbatch 191.38881492777182 clip 178.98413344304615
total norm for a microbatch 207.44991589587232 clip 185.3683644712288
total norm for a microbatch 188.87419921397264 clip 166.0793717082982
cuda
Objective function 40.58 = squared loss an data 26.15 + 0.5*rho*h**2 6.452408 + alpha*h 5.079674 + L2reg 2.66 + L1reg 0.25 ; SHD = 89 ; DAG True
Proportion of microbatches that were clipped  0.8277260796945837
iteration 2 in inner loop, alpha 44.71568909603042 rho 1000.0 h 0.11359936198910958
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 98.65 = squared loss an data 26.15 + 0.5*rho*h**2 64.524075 + alpha*h 5.079674 + L2reg 2.66 + L1reg 0.25 ; SHD = 89 ; DAG True
total norm for a microbatch 1034.58255680738 clip 1.1023523130427815
total norm for a microbatch 300.12978856002263 clip 1.724667458972577
total norm for a microbatch 212.59188485608138 clip 5.069901930891112
total norm for a microbatch 253.8012836460938 clip 24.459772588059636
total norm for a microbatch 295.2498396265998 clip 291.02331964581253
total norm for a microbatch 305.3594061929783 clip 304.5203826684427
total norm for a microbatch 318.46374487047086 clip 276.58957939002994
total norm for a microbatch 324.20327583438285 clip 199.66394018553075
total norm for a microbatch 204.1014144891083 clip 200.68077344671224
total norm for a microbatch 211.92017811001116 clip 203.10586584822738
total norm for a microbatch 232.22405539743664 clip 204.3901538808949
total norm for a microbatch 294.49428785407 clip 190.84769241636664
cuda
Objective function 37.16 = squared loss an data 25.08 + 0.5*rho*h**2 7.239199 + alpha*h 1.701454 + L2reg 2.88 + L1reg 0.26 ; SHD = 76 ; DAG True
Proportion of microbatches that were clipped  0.8273387032458729
iteration 3 in inner loop, alpha 44.71568909603042 rho 10000.0 h 0.03805049056452958
iteration 3 in outer loop, alpha = 425.22059474132624, rho = 10000.0, h = 0.03805049056452958
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 51.64 = squared loss an data 25.08 + 0.5*rho*h**2 7.239199 + alpha*h 16.179852 + L2reg 2.88 + L1reg 0.26 ; SHD = 76 ; DAG True
total norm for a microbatch 327.9465945440785 clip 25.04474568717045
total norm for a microbatch 410.5715523805364 clip 61.003513761105026
total norm for a microbatch 376.2704583044736 clip 167.70596421678044
total norm for a microbatch 449.7525958649196 clip 432.35847864782636
total norm for a microbatch 341.30886202397517 clip 299.48497491530054
total norm for a microbatch 256.2440451008213 clip 270.2521031136747
total norm for a microbatch 226.75032117961175 clip 239.91360871327385
total norm for a microbatch 213.962701486322 clip 197.38981582670405
total norm for a microbatch 233.02057428311207 clip 197.38981582670405
total norm for a microbatch 259.1185928106476 clip 198.72056082177662
total norm for a microbatch 220.84847878677468 clip 218.95029835944916
total norm for a microbatch 200.85993843520112 clip 215.79884238580863
cuda
Objective function 42.05 = squared loss an data 24.91 + 0.5*rho*h**2 3.163591 + alpha*h 10.695952 + L2reg 3.02 + L1reg 0.26 ; SHD = 82 ; DAG True
Proportion of microbatches that were clipped  0.828343949044586
iteration 1 in inner loop, alpha 425.22059474132624 rho 10000.0 h 0.02515388887303871
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 70.52 = squared loss an data 24.91 + 0.5*rho*h**2 31.635906 + alpha*h 10.695952 + L2reg 3.02 + L1reg 0.26 ; SHD = 82 ; DAG True
total norm for a microbatch 998.2752903868512 clip 7.1683676437658574
total norm for a microbatch 1200.5731349013952 clip 73.62502321720677
total norm for a microbatch 1397.2447715231965 clip 169.08077693103417
total norm for a microbatch 2898.871123884567 clip 2823.532579136949
total norm for a microbatch 1047.1908978500512 clip 928.5223297555805
total norm for a microbatch 697.3856776152467 clip 590.013537721883
total norm for a microbatch 204.96892622596147 clip 185.51029558586188
total norm for a microbatch 321.66332994910687 clip 180.7720417099404
total norm for a microbatch 175.44787867421027 clip 184.1977303216724
total norm for a microbatch 190.00061926655616 clip 176.77457600563127
total norm for a microbatch 266.0677268343618 clip 175.69908999580676
cuda
Objective function 35.11 = squared loss an data 29.19 + 0.5*rho*h**2 0.811173 + alpha*h 1.712719 + L2reg 3.13 + L1reg 0.26 ; SHD = 88 ; DAG True
Proportion of microbatches that were clipped  0.826031031433395
iteration 2 in inner loop, alpha 425.22059474132624 rho 100000.0 h 0.00402783581742483
iteration 4 in outer loop, alpha = 828.0041764838093, rho = 100000.0, h = 0.00402783581742483
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 36.73 = squared loss an data 29.19 + 0.5*rho*h**2 0.811173 + alpha*h 3.335065 + L2reg 3.13 + L1reg 0.26 ; SHD = 88 ; DAG True
total norm for a microbatch 967.8230981752052 clip 14.910992334882119
total norm for a microbatch 918.2583729118353 clip 16.170435013909188
total norm for a microbatch 890.3431495871946 clip 23.28079197829579
total norm for a microbatch 1020.7751285712955 clip 28.18538658198441
total norm for a microbatch 2472.262771937332 clip 678.2505316234354
total norm for a microbatch 2884.14516981234 clip 1263.4750714756124
total norm for a microbatch 2925.6126334613396 clip 2546.8132274931436
total norm for a microbatch 883.7842891355751 clip 875.814746449446
total norm for a microbatch 449.5780549655134 clip 417.88991018795804
total norm for a microbatch 376.95220103744083 clip 220.3224447333375
total norm for a microbatch 251.21082672765388 clip 194.18561866866958
cuda
Objective function 38.48 = squared loss an data 32.09 + 0.5*rho*h**2 0.440119 + alpha*h 2.456590 + L2reg 3.22 + L1reg 0.26 ; SHD = 92 ; DAG True
Proportion of microbatches that were clipped  0.826382774353638
iteration 1 in inner loop, alpha 828.0041764838093 rho 100000.0 h 0.002966881658021947
iteration 5 in outer loop, alpha = 3794.8858345057565, rho = 1000000.0, h = 0.002966881658021947
Threshold 0.3
[[0.003 0.026 0.004 0.338 0.015 0.01  0.062 0.004 0.407 0.402 0.009 0.025
  0.005 0.021 0.427 0.185 0.011 0.003 0.434 0.015]
 [0.095 0.007 0.003 0.633 0.114 0.068 0.077 0.01  1.669 0.369 0.019 0.105
  0.014 0.114 0.866 0.319 0.121 0.001 0.43  0.05 ]
 [0.821 0.638 0.003 1.22  1.263 0.658 1.166 0.216 1.999 1.304 0.376 0.634
  0.249 1.094 1.999 0.917 0.318 0.037 2.444 0.477]
 [0.011 0.005 0.001 0.003 0.005 0.006 0.056 0.001 0.081 0.017 0.004 0.008
  0.003 0.007 0.023 0.045 0.011 0.001 0.031 0.002]
 [0.186 0.061 0.002 0.71  0.003 0.009 0.113 0.003 0.286 0.232 0.014 0.072
  0.014 0.16  0.234 0.36  0.059 0.004 0.242 0.026]
 [0.31  0.056 0.009 0.556 0.296 0.003 0.201 0.006 0.78  0.374 0.042 0.104
  0.032 0.444 0.837 0.471 0.029 0.003 0.719 0.04 ]
 [0.059 0.027 0.002 0.089 0.032 0.018 0.004 0.    0.246 0.114 0.009 0.014
  0.005 0.023 0.163 0.132 0.008 0.003 0.107 0.01 ]
 [0.797 0.452 0.015 1.558 0.842 0.612 3.989 0.003 1.131 1.163 0.309 0.322
  0.253 0.498 1.13  1.486 0.649 0.027 1.048 0.545]
 [0.009 0.002 0.001 0.065 0.014 0.003 0.015 0.002 0.003 0.013 0.002 0.002
  0.003 0.005 0.03  0.011 0.005 0.001 0.05  0.003]
 [0.007 0.006 0.001 0.207 0.016 0.008 0.035 0.002 0.218 0.003 0.003 0.004
  0.001 0.008 0.071 0.027 0.003 0.001 0.032 0.005]
 [0.335 0.162 0.009 0.698 0.27  0.095 0.426 0.016 1.029 0.957 0.004 0.185
  0.196 0.234 0.582 0.177 0.02  0.008 0.457 0.129]
 [0.142 0.036 0.006 0.354 0.067 0.035 0.258 0.008 0.815 0.567 0.018 0.002
  0.028 0.081 0.636 0.346 0.014 0.004 0.418 0.027]
 [0.548 0.299 0.029 0.968 0.168 0.075 0.534 0.013 0.598 1.766 0.023 0.161
  0.002 0.126 0.399 0.323 0.075 0.01  0.386 0.162]
 [0.176 0.034 0.002 0.509 0.014 0.007 0.192 0.007 0.384 0.538 0.013 0.047
  0.016 0.002 0.314 0.098 0.012 0.003 0.285 0.025]
 [0.01  0.003 0.001 0.21  0.011 0.005 0.028 0.002 0.177 0.083 0.005 0.004
  0.006 0.011 0.004 0.02  0.005 0.001 0.047 0.006]
 [0.03  0.013 0.003 0.144 0.011 0.005 0.05  0.001 0.382 0.13  0.016 0.008
  0.007 0.041 0.267 0.004 0.007 0.002 0.158 0.01 ]
 [0.224 0.148 0.015 0.362 0.099 0.147 0.359 0.004 0.449 1.04  0.12  0.169
  0.043 0.283 0.767 0.605 0.003 0.001 0.338 0.039]
 [0.858 2.391 0.14  1.261 0.69  0.646 0.869 0.106 1.093 1.203 0.331 0.69
  0.331 0.552 0.979 1.283 1.439 0.002 1.118 0.524]
 [0.007 0.007 0.001 0.14  0.013 0.005 0.039 0.002 0.11  0.119 0.007 0.01
  0.011 0.013 0.111 0.013 0.008 0.002 0.004 0.003]
 [0.257 0.097 0.008 1.521 0.161 0.121 0.236 0.006 1.101 0.514 0.028 0.144
  0.048 0.141 0.439 0.231 0.128 0.005 0.714 0.004]]
[[0.    0.    0.    0.338 0.    0.    0.    0.    0.407 0.402 0.    0.
  0.    0.    0.427 0.    0.    0.    0.434 0.   ]
 [0.    0.    0.    0.633 0.    0.    0.    0.    1.669 0.369 0.    0.
  0.    0.    0.866 0.319 0.    0.    0.43  0.   ]
 [0.821 0.638 0.    1.22  1.263 0.658 1.166 0.    1.999 1.304 0.376 0.634
  0.    1.094 1.999 0.917 0.318 0.    2.444 0.477]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.71  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.36  0.    0.    0.    0.   ]
 [0.31  0.    0.    0.556 0.    0.    0.    0.    0.78  0.374 0.    0.
  0.    0.444 0.837 0.471 0.    0.    0.719 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.797 0.452 0.    1.558 0.842 0.612 3.989 0.    1.131 1.163 0.309 0.322
  0.    0.498 1.13  1.486 0.649 0.    1.048 0.545]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.335 0.    0.    0.698 0.    0.    0.426 0.    1.029 0.957 0.    0.
  0.    0.    0.582 0.    0.    0.    0.457 0.   ]
 [0.    0.    0.    0.354 0.    0.    0.    0.    0.815 0.567 0.    0.
  0.    0.    0.636 0.346 0.    0.    0.418 0.   ]
 [0.548 0.    0.    0.968 0.    0.    0.534 0.    0.598 1.766 0.    0.
  0.    0.    0.399 0.323 0.    0.    0.386 0.   ]
 [0.    0.    0.    0.509 0.    0.    0.    0.    0.384 0.538 0.    0.
  0.    0.    0.314 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.382 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.362 0.    0.    0.359 0.    0.449 1.04  0.    0.
  0.    0.    0.767 0.605 0.    0.    0.338 0.   ]
 [0.858 2.391 0.    1.261 0.69  0.646 0.869 0.    1.093 1.203 0.331 0.69
  0.331 0.552 0.979 1.283 1.439 0.    1.118 0.524]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.521 0.    0.    0.    0.    1.101 0.514 0.    0.
  0.    0.    0.439 0.    0.    0.    0.714 0.   ]]
{'fdr': 0.7592592592592593, 'tpr': 0.65, 'fpr': 0.5466666666666666, 'f1': 0.3513513513513513, 'shd': 92, 'npred': 108, 'ntrue': 40}
[2.607e-02 3.703e-03 3.385e-01 1.544e-02 9.526e-03 6.241e-02 3.910e-03
 4.066e-01 4.021e-01 9.333e-03 2.470e-02 4.932e-03 2.064e-02 4.265e-01
 1.850e-01 1.110e-02 3.252e-03 4.341e-01 1.508e-02 9.475e-02 3.022e-03
 6.334e-01 1.136e-01 6.766e-02 7.722e-02 9.904e-03 1.669e+00 3.690e-01
 1.888e-02 1.053e-01 1.354e-02 1.135e-01 8.656e-01 3.195e-01 1.209e-01
 8.350e-04 4.296e-01 5.013e-02 8.213e-01 6.385e-01 1.220e+00 1.263e+00
 6.579e-01 1.166e+00 2.162e-01 1.999e+00 1.304e+00 3.756e-01 6.341e-01
 2.490e-01 1.094e+00 1.999e+00 9.169e-01 3.180e-01 3.663e-02 2.444e+00
 4.772e-01 1.111e-02 5.304e-03 1.303e-03 5.217e-03 5.546e-03 5.564e-02
 1.372e-03 8.070e-02 1.702e-02 3.746e-03 8.116e-03 2.860e-03 6.787e-03
 2.265e-02 4.462e-02 1.132e-02 1.158e-03 3.072e-02 2.375e-03 1.865e-01
 6.067e-02 2.044e-03 7.097e-01 9.433e-03 1.131e-01 3.296e-03 2.861e-01
 2.323e-01 1.352e-02 7.187e-02 1.360e-02 1.603e-01 2.336e-01 3.598e-01
 5.914e-02 4.262e-03 2.416e-01 2.645e-02 3.104e-01 5.587e-02 8.647e-03
 5.564e-01 2.958e-01 2.012e-01 6.080e-03 7.798e-01 3.735e-01 4.213e-02
 1.044e-01 3.219e-02 4.441e-01 8.368e-01 4.712e-01 2.859e-02 3.389e-03
 7.194e-01 4.015e-02 5.940e-02 2.721e-02 1.707e-03 8.875e-02 3.158e-02
 1.812e-02 4.322e-04 2.462e-01 1.141e-01 9.406e-03 1.403e-02 5.088e-03
 2.301e-02 1.634e-01 1.315e-01 7.813e-03 2.507e-03 1.069e-01 9.986e-03
 7.972e-01 4.523e-01 1.500e-02 1.558e+00 8.415e-01 6.122e-01 3.989e+00
 1.131e+00 1.163e+00 3.094e-01 3.216e-01 2.526e-01 4.979e-01 1.130e+00
 1.486e+00 6.493e-01 2.748e-02 1.048e+00 5.445e-01 9.346e-03 2.266e-03
 9.175e-04 6.498e-02 1.398e-02 3.351e-03 1.530e-02 1.932e-03 1.265e-02
 2.022e-03 1.864e-03 3.183e-03 5.173e-03 3.021e-02 1.059e-02 4.829e-03
 5.040e-04 5.004e-02 2.827e-03 7.155e-03 6.187e-03 1.265e-03 2.067e-01
 1.573e-02 8.093e-03 3.493e-02 2.074e-03 2.176e-01 2.989e-03 3.818e-03
 1.187e-03 8.182e-03 7.092e-02 2.746e-02 2.738e-03 1.489e-03 3.166e-02
 4.977e-03 3.355e-01 1.619e-01 9.125e-03 6.979e-01 2.695e-01 9.500e-02
 4.262e-01 1.586e-02 1.029e+00 9.565e-01 1.847e-01 1.955e-01 2.337e-01
 5.819e-01 1.766e-01 2.001e-02 8.419e-03 4.572e-01 1.291e-01 1.424e-01
 3.582e-02 5.638e-03 3.540e-01 6.733e-02 3.529e-02 2.579e-01 8.477e-03
 8.145e-01 5.671e-01 1.811e-02 2.786e-02 8.143e-02 6.359e-01 3.460e-01
 1.376e-02 3.587e-03 4.178e-01 2.656e-02 5.477e-01 2.994e-01 2.909e-02
 9.676e-01 1.681e-01 7.541e-02 5.345e-01 1.274e-02 5.978e-01 1.766e+00
 2.257e-02 1.609e-01 1.258e-01 3.994e-01 3.229e-01 7.519e-02 1.014e-02
 3.856e-01 1.619e-01 1.758e-01 3.442e-02 2.184e-03 5.092e-01 1.428e-02
 6.876e-03 1.922e-01 6.610e-03 3.843e-01 5.379e-01 1.329e-02 4.730e-02
 1.630e-02 3.145e-01 9.767e-02 1.157e-02 2.975e-03 2.848e-01 2.503e-02
 9.767e-03 3.225e-03 1.015e-03 2.099e-01 1.073e-02 4.672e-03 2.795e-02
 2.264e-03 1.768e-01 8.310e-02 5.126e-03 3.958e-03 6.449e-03 1.084e-02
 1.953e-02 5.424e-03 7.344e-04 4.665e-02 6.324e-03 2.966e-02 1.261e-02
 2.679e-03 1.444e-01 1.148e-02 4.668e-03 5.007e-02 1.296e-03 3.821e-01
 1.299e-01 1.552e-02 8.038e-03 6.528e-03 4.093e-02 2.671e-01 7.262e-03
 1.533e-03 1.576e-01 1.006e-02 2.243e-01 1.478e-01 1.460e-02 3.623e-01
 9.864e-02 1.466e-01 3.591e-01 4.274e-03 4.490e-01 1.040e+00 1.203e-01
 1.688e-01 4.284e-02 2.825e-01 7.671e-01 6.046e-01 1.084e-03 3.383e-01
 3.885e-02 8.578e-01 2.391e+00 1.402e-01 1.261e+00 6.896e-01 6.459e-01
 8.688e-01 1.064e-01 1.093e+00 1.203e+00 3.308e-01 6.904e-01 3.306e-01
 5.523e-01 9.791e-01 1.283e+00 1.439e+00 1.118e+00 5.245e-01 6.941e-03
 6.915e-03 1.265e-03 1.402e-01 1.251e-02 4.795e-03 3.934e-02 2.006e-03
 1.096e-01 1.186e-01 6.617e-03 9.885e-03 1.112e-02 1.322e-02 1.112e-01
 1.316e-02 8.214e-03 2.232e-03 3.222e-03 2.566e-01 9.739e-02 7.870e-03
 1.521e+00 1.614e-01 1.213e-01 2.357e-01 6.399e-03 1.101e+00 5.139e-01
 2.829e-02 1.445e-01 4.786e-02 1.411e-01 4.386e-01 2.313e-01 1.280e-01
 4.778e-03 7.135e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7598529411764706, 0.4211668097912299)
Iterations 2500
Achieves (6.086687062773559, 1e-05)-DP
