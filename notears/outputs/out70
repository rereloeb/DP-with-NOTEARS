samples  5000  graph  15 30 ER mlp  minibatch size  100  noise  0.5  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006583550827196305
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0011537970975332712
iteration 4 in outer loop, alpha = 156.00431709949223, rho = 100000.0, h = 0.0011537970975332712
cuda
iteration 1 in inner loop,alpha 156.00431709949223 rho 100000.0 h 0.00033575511004180214
iteration 5 in outer loop, alpha = 491.7594271412944, rho = 1000000.0, h = 0.00033575511004180214
Threshold 0.3
[[0.002 0.    0.    0.005 0.194 0.    0.    0.    0.    2.734 0.179 0.001
  0.001 0.    0.006]
 [2.52  0.001 0.002 0.262 0.215 0.01  0.01  0.001 0.002 2.189 0.199 1.342
  0.001 0.001 0.098]
 [0.027 0.068 0.001 0.147 0.166 0.035 0.137 0.001 0.067 0.125 0.135 0.162
  0.001 0.001 0.006]
 [0.153 0.    0.001 0.001 0.204 0.    0.    0.    0.    0.214 0.335 0.001
  0.    0.    1.022]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    0.001]
 [2.646 0.03  0.001 0.968 0.376 0.001 1.377 0.    0.055 1.746 0.956 1.472
  0.012 0.002 1.923]
 [0.414 0.044 0.    0.336 0.408 0.    0.001 0.    0.059 0.139 1.162 0.096
  0.001 0.    0.163]
 [2.69  0.002 0.    2.092 0.137 0.612 0.046 0.    0.003 0.313 0.139 0.061
  0.014 0.022 0.118]
 [0.103 0.383 0.002 0.638 0.415 0.002 0.001 0.001 0.003 2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.001 0.    0.    0.002 0.402 0.001
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.057 0.    0.001 0.    0.001 0.001 0.015 0.
  0.    0.    0.005]
 [0.37  0.    0.    0.18  0.153 0.    0.004 0.    0.002 0.427 0.58  0.001
  0.    0.001 0.032]
 [0.041 0.078 0.    0.112 3.047 0.017 0.008 0.    1.212 0.107 1.585 0.031
  0.001 0.016 0.021]
 [0.126 1.398 0.    1.127 1.231 0.003 0.006 0.    3.368 0.386 0.434 0.158
  0.011 0.002 0.562]
 [0.089 0.    0.    0.    0.111 0.    0.    0.    0.    0.106 0.081 0.002
  0.004 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.734 0.    0.
  0.    0.    0.   ]
 [2.52  0.    0.    0.    0.    0.    0.    0.    0.    2.189 0.    1.342
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.335 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.968 0.376 0.    1.377 0.    0.    1.746 0.956 1.472
  0.    0.    1.923]
 [0.414 0.    0.    0.336 0.408 0.    0.    0.    0.    0.    1.162 0.
  0.    0.    0.   ]
 [2.69  0.    0.    2.092 0.    0.612 0.    0.    0.    0.313 0.    0.
  0.    0.    0.   ]
 [0.    0.383 0.    0.638 0.415 0.    0.    0.    0.    2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.    0.    0.    0.    0.402 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.057 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.427 0.58  0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.047 0.    0.    0.    1.212 0.    1.585 0.
  0.    0.    0.   ]
 [0.    1.398 0.    1.127 1.231 0.    0.    0.    3.368 0.386 0.434 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[7.166e-05 3.031e-04 4.737e-03 1.942e-01 6.655e-05 2.207e-04 4.993e-05
 2.937e-04 2.734e+00 1.793e-01 5.105e-04 1.050e-03 1.242e-04 5.740e-03
 2.520e+00 1.954e-03 2.620e-01 2.147e-01 9.724e-03 9.801e-03 9.328e-04
 1.620e-03 2.189e+00 1.994e-01 1.342e+00 5.488e-04 9.886e-04 9.781e-02
 2.679e-02 6.849e-02 1.472e-01 1.660e-01 3.508e-02 1.368e-01 5.716e-04
 6.687e-02 1.254e-01 1.352e-01 1.619e-01 8.477e-04 1.272e-03 6.092e-03
 1.532e-01 1.509e-04 9.882e-04 2.036e-01 4.044e-05 3.897e-04 2.489e-04
 6.858e-05 2.135e-01 3.353e-01 1.155e-03 4.106e-04 1.293e-04 1.022e+00
 6.064e-05 4.791e-06 5.172e-05 2.471e-04 1.136e-05 1.928e-04 4.191e-06
 2.163e-04 3.383e-04 1.833e-03 1.007e-04 2.631e-05 7.614e-05 1.086e-03
 2.646e+00 3.009e-02 1.478e-03 9.684e-01 3.765e-01 1.377e+00 1.229e-04
 5.513e-02 1.746e+00 9.560e-01 1.472e+00 1.180e-02 1.838e-03 1.923e+00
 4.141e-01 4.437e-02 3.959e-04 3.360e-01 4.079e-01 4.254e-04 5.935e-05
 5.872e-02 1.392e-01 1.162e+00 9.588e-02 6.731e-04 4.063e-04 1.632e-01
 2.690e+00 2.126e-03 4.852e-04 2.092e+00 1.369e-01 6.121e-01 4.646e-02
 3.330e-03 3.125e-01 1.385e-01 6.062e-02 1.448e-02 2.181e-02 1.178e-01
 1.034e-01 3.829e-01 2.017e-03 6.383e-01 4.147e-01 2.074e-03 1.394e-03
 1.328e-03 2.308e+00 1.722e+00 1.030e+00 5.492e-05 3.972e-04 3.674e-01
 1.834e-04 4.607e-06 8.949e-05 4.394e-04 2.722e+00 6.113e-06 8.992e-04
 1.147e-05 4.538e-04 4.020e-01 1.020e-03 1.265e-04 1.194e-04 2.179e-03
 6.634e-04 5.288e-05 3.430e-04 1.176e-03 3.057e+00 1.252e-04 7.885e-04
 1.415e-04 9.900e-04 8.589e-04 1.728e-04 2.153e-04 3.103e-04 5.194e-03
 3.696e-01 4.210e-04 8.175e-05 1.805e-01 1.529e-01 2.371e-04 3.593e-03
 6.136e-05 1.729e-03 4.267e-01 5.799e-01 2.072e-04 6.430e-04 3.169e-02
 4.077e-02 7.764e-02 2.020e-04 1.125e-01 3.047e+00 1.651e-02 8.033e-03
 3.454e-04 1.212e+00 1.069e-01 1.585e+00 3.144e-02 1.628e-02 2.119e-02
 1.264e-01 1.398e+00 2.136e-04 1.127e+00 1.231e+00 3.196e-03 6.293e-03
 3.254e-04 3.368e+00 3.864e-01 4.339e-01 1.576e-01 1.130e-02 5.624e-01
 8.923e-02 3.157e-04 3.306e-04 7.760e-05 1.109e-01 7.500e-05 1.536e-04
 2.248e-04 2.530e-04 1.060e-01 8.126e-02 2.152e-03 4.216e-03 1.418e-04]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9805555555555556, 0.9709424613206593)
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 58.55091749524359 clip 2.301154480791764
total norm for a microbatch 38.44375536607758 clip 3.6335650159454
total norm for a microbatch 31.99090605508873 clip 28.691528776909347
total norm for a microbatch 37.3398277427851 clip 31.237008965568812
total norm for a microbatch 34.35213369572985 clip 31.889522980475853
total norm for a microbatch 26.63037462326803 clip 27.69412260214732
cuda
Objective function 34.16 = squared loss an data 32.17 + 0.5*rho*h**2 1.498514 + alpha*h 0.000000 + L2reg 0.35 + L1reg 0.15 ; SHD = 56 ; DAG False
Proportion of microbatches that were clipped  0.9008937120970316
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.7311924007861492
iteration 1 in outer loop, alpha = 1.7311924007861492, rho = 1.0, h = 1.7311924007861492
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 37.16 = squared loss an data 32.17 + 0.5*rho*h**2 1.498514 + alpha*h 2.997027 + L2reg 0.35 + L1reg 0.15 ; SHD = 56 ; DAG False
total norm for a microbatch 39.228399384957 clip 4.554978865441029
total norm for a microbatch 51.06044102495421 clip 5.98011779677846
total norm for a microbatch 138.0902575560358 clip 5.98011779677846
total norm for a microbatch 31.93999112917094 clip 7.9500404460471925
total norm for a microbatch 54.10751522454244 clip 21.3806140689559
total norm for a microbatch 25.77959392078535 clip 30.580683619104068
total norm for a microbatch 26.050267368706177 clip 30.580683619104068
cuda
Objective function 16.19 = squared loss an data 13.26 + 0.5*rho*h**2 0.448479 + alpha*h 1.639575 + L2reg 0.70 + L1reg 0.14 ; SHD = 40 ; DAG False
Proportion of microbatches that were clipped  0.9117647058823529
iteration 1 in inner loop, alpha 1.7311924007861492 rho 1.0 h 0.9470783062767438
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 20.23 = squared loss an data 13.26 + 0.5*rho*h**2 4.484787 + alpha*h 1.639575 + L2reg 0.70 + L1reg 0.14 ; SHD = 40 ; DAG False
total norm for a microbatch 40.75229174339663 clip 1.7127507768024548
total norm for a microbatch 27.822742795538066 clip 4.264841689888263
total norm for a microbatch 87.86377817615083 clip 17.450129935586336
total norm for a microbatch 56.118428692893474 clip 27.592943805137516
total norm for a microbatch 48.30699751442557 clip 42.51111776942658
total norm for a microbatch 49.54497444833473 clip 42.318973203763576
cuda
Objective function 14.77 = squared loss an data 12.21 + 0.5*rho*h**2 0.815205 + alpha*h 0.699027 + L2reg 0.92 + L1reg 0.13 ; SHD = 33 ; DAG True
Proportion of microbatches that were clipped  0.924776680771039
iteration 2 in inner loop, alpha 1.7311924007861492 rho 10.0 h 0.4037832709299156
iteration 2 in outer loop, alpha = 5.769025110085305, rho = 10.0, h = 0.4037832709299156
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 16.41 = squared loss an data 12.21 + 0.5*rho*h**2 0.815205 + alpha*h 2.329436 + L2reg 0.92 + L1reg 0.13 ; SHD = 33 ; DAG True
total norm for a microbatch 117.48310655511172 clip 15.020579517072397
total norm for a microbatch 29.77360152415972 clip 47.442442187855995
total norm for a microbatch 99.36756443334225 clip 47.4922278725621
cuda
Objective function 15.44 = squared loss an data 12.28 + 0.5*rho*h**2 0.387470 + alpha*h 1.605967 + L2reg 1.04 + L1reg 0.13 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.9293767616661447
iteration 1 in inner loop, alpha 5.769025110085305 rho 10.0 h 0.27837753551096256
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 18.93 = squared loss an data 12.28 + 0.5*rho*h**2 3.874703 + alpha*h 1.605967 + L2reg 1.04 + L1reg 0.13 ; SHD = 36 ; DAG True
total norm for a microbatch 64.31117138071237 clip 2.6816129656819547
total norm for a microbatch 109.00095267043615 clip 4.637624057169128
total norm for a microbatch 90.16960758461288 clip 10.360116268678821
total norm for a microbatch 81.83262238115194 clip 14.716890398629449
total norm for a microbatch 50.49880381596031 clip 20.72602495076353
total norm for a microbatch 55.902067294311486 clip 32.81102339716593
cuda
Objective function 16.01 = squared loss an data 13.01 + 0.5*rho*h**2 0.941100 + alpha*h 0.791472 + L2reg 1.14 + L1reg 0.13 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.9341463414634147
iteration 2 in inner loop, alpha 5.769025110085305 rho 100.0 h 0.13719331270310064
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 24.48 = squared loss an data 13.01 + 0.5*rho*h**2 9.411003 + alpha*h 0.791472 + L2reg 1.14 + L1reg 0.13 ; SHD = 37 ; DAG True
total norm for a microbatch 100.63454536280246 clip 1.0
total norm for a microbatch 89.65459369588396 clip 1.5660775951591195
total norm for a microbatch 93.0632531330262 clip 3.3066864048390743
total norm for a microbatch 70.60625571277609 clip 9.155735673332364
total norm for a microbatch 79.13464017373286 clip 11.00081104225364
cuda
Objective function 16.15 = squared loss an data 13.64 + 0.5*rho*h**2 0.921374 + alpha*h 0.247648 + L2reg 1.21 + L1reg 0.13 ; SHD = 31 ; DAG True
Proportion of microbatches that were clipped  0.9325896414342629
iteration 3 in inner loop, alpha 5.769025110085305 rho 1000.0 h 0.042927245480484544
iteration 3 in outer loop, alpha = 48.69627059056985, rho = 1000.0, h = 0.042927245480484544
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 18.00 = squared loss an data 13.64 + 0.5*rho*h**2 0.921374 + alpha*h 2.090397 + L2reg 1.21 + L1reg 0.13 ; SHD = 31 ; DAG True
total norm for a microbatch 78.98593877753311 clip 1.8832828303503548
total norm for a microbatch 59.2181635525153 clip 3.309663241988424
total norm for a microbatch 72.02024022610004 clip 3.309663241988424
total norm for a microbatch 59.49661227200777 clip 26.519579569524577
total norm for a microbatch 144.60016526318873 clip 31.991320006842397
cuda
Objective function 16.70 = squared loss an data 13.66 + 0.5*rho*h**2 0.338156 + alpha*h 1.266396 + L2reg 1.29 + L1reg 0.14 ; SHD = 34 ; DAG True
Proportion of microbatches that were clipped  0.9338389543327417
iteration 1 in inner loop, alpha 48.69627059056985 rho 1000.0 h 0.02600601560849114
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 19.74 = squared loss an data 13.66 + 0.5*rho*h**2 3.381564 + alpha*h 1.266396 + L2reg 1.29 + L1reg 0.14 ; SHD = 34 ; DAG True
total norm for a microbatch 108.06753440944878 clip 1.3012637762245307
total norm for a microbatch 168.55352874216368 clip 1.428728413180174
total norm for a microbatch 55.77024431970505 clip 5.343526481691496
total norm for a microbatch 137.41576047956195 clip 10.840452323959955
total norm for a microbatch 50.8148347772266 clip 12.939628995728423
total norm for a microbatch 59.092359561090824 clip 51.34812484534141
total norm for a microbatch 85.47120084139529 clip 57.17431311471757
total norm for a microbatch 84.89658190455897 clip 60.57863597368921
total norm for a microbatch 92.42339578055402 clip 64.50065040534056
total norm for a microbatch 127.3103200450775 clip 65.66052919042514
total norm for a microbatch 75.79921362109967 clip 65.66052919042514
cuda
Objective function 16.06 = squared loss an data 13.50 + 0.5*rho*h**2 0.557640 + alpha*h 0.514266 + L2reg 1.35 + L1reg 0.14 ; SHD = 33 ; DAG True
Proportion of microbatches that were clipped  0.9401330376940134
iteration 2 in inner loop, alpha 48.69627059056985 rho 10000.0 h 0.010560681643891456
iteration 4 in outer loop, alpha = 154.3030870294844, rho = 10000.0, h = 0.010560681643891456
cuda
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 17.18 = squared loss an data 13.50 + 0.5*rho*h**2 0.557640 + alpha*h 1.629546 + L2reg 1.35 + L1reg 0.14 ; SHD = 33 ; DAG True
total norm for a microbatch 68.03353178715237 clip 4.090724270103214
total norm for a microbatch 109.85118583287408 clip 10.73436651970534
total norm for a microbatch 76.4227187056736 clip 77.08279963267945
total norm for a microbatch 130.00899260609194 clip 68.32282183277792
cuda
Objective function 16.83 = squared loss an data 13.54 + 0.5*rho*h**2 0.394177 + alpha*h 1.370047 + L2reg 1.38 + L1reg 0.14 ; SHD = 35 ; DAG True
Proportion of microbatches that were clipped  0.9433349569735346
iteration 1 in inner loop, alpha 154.3030870294844 rho 10000.0 h 0.008878935376044339
noise_multiplier  0.5  noise_multiplier_b  5.0  noise_multiplier_delta  0.5006261743217588
cuda
Objective function 20.37 = squared loss an data 13.54 + 0.5*rho*h**2 3.941775 + alpha*h 1.370047 + L2reg 1.38 + L1reg 0.14 ; SHD = 35 ; DAG True
total norm for a microbatch 102.06270302357187 clip 4.6033377489028515
total norm for a microbatch 92.38864891634158 clip 5.0685099640970925
total norm for a microbatch 159.1027262525147 clip 5.5444093994644
total norm for a microbatch 114.55001954131508 clip 5.5444093994644
total norm for a microbatch 80.38689355930812 clip 7.862692712887023
cuda
Objective function 16.93 = squared loss an data 13.82 + 0.5*rho*h**2 0.912658 + alpha*h 0.659240 + L2reg 1.40 + L1reg 0.14 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.9556516170349023
iteration 2 in inner loop, alpha 154.3030870294844 rho 100000.0 h 0.004272372474950714
iteration 5 in outer loop, alpha = 4426.675561980199, rho = 1000000.0, h = 0.004272372474950714
Threshold 0.3
[[0.005 0.025 0.16  0.433 0.363 0.013 0.017 0.003 0.04  1.109 0.119 0.258
  0.027 0.012 0.158]
 [0.254 0.006 0.614 0.693 0.348 0.191 0.104 0.021 0.023 0.896 0.319 0.755
  0.112 0.004 0.45 ]
 [0.04  0.011 0.005 0.158 0.228 0.03  0.022 0.019 0.017 0.033 0.028 0.117
  0.023 0.009 0.2  ]
 [0.015 0.01  0.032 0.005 0.111 0.016 0.007 0.005 0.006 0.036 0.03  0.054
  0.006 0.006 0.334]
 [0.005 0.005 0.025 0.033 0.004 0.008 0.003 0.004 0.005 0.003 0.005 0.017
  0.003 0.002 0.069]
 [0.502 0.041 0.184 0.335 0.497 0.005 0.013 0.026 0.08  0.526 0.209 0.565
  0.014 0.013 0.251]
 [0.211 0.058 0.362 0.658 0.473 0.38  0.007 0.06  0.109 0.361 0.987 0.379
  0.131 0.015 0.381]
 [1.455 0.352 0.291 1.162 0.388 0.177 0.108 0.004 0.297 0.573 0.299 0.613
  0.131 0.014 0.39 ]
 [0.177 0.215 0.274 0.368 0.579 0.063 0.049 0.018 0.005 0.943 0.931 0.763
  0.017 0.002 0.267]
 [0.005 0.007 0.116 0.128 1.833 0.009 0.014 0.005 0.007 0.01  0.028 0.051
  0.011 0.004 0.14 ]
 [0.042 0.012 0.185 0.181 2.29  0.013 0.007 0.014 0.007 0.206 0.007 0.056
  0.005 0.004 0.205]
 [0.025 0.007 0.077 0.123 0.25  0.014 0.02  0.01  0.009 0.085 0.104 0.004
  0.011 0.003 0.084]
 [0.273 0.056 0.275 0.77  1.286 0.371 0.042 0.027 0.291 0.313 1.231 0.335
  0.004 0.031 0.691]
 [0.431 0.967 0.395 0.748 0.624 0.278 0.353 0.244 2.866 0.749 0.685 0.904
  0.186 0.006 0.669]
 [0.02  0.012 0.047 0.019 0.055 0.029 0.007 0.007 0.019 0.044 0.019 0.092
  0.007 0.005 0.004]]
[[0.    0.    0.    0.433 0.363 0.    0.    0.    0.    1.109 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.614 0.693 0.348 0.    0.    0.    0.    0.896 0.319 0.755
  0.    0.    0.45 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.334]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.502 0.    0.    0.335 0.497 0.    0.    0.    0.    0.526 0.    0.565
  0.    0.    0.   ]
 [0.    0.    0.362 0.658 0.473 0.38  0.    0.    0.    0.361 0.987 0.379
  0.    0.    0.381]
 [1.455 0.352 0.    1.162 0.388 0.    0.    0.    0.    0.573 0.    0.613
  0.    0.    0.39 ]
 [0.    0.    0.    0.368 0.579 0.    0.    0.    0.    0.943 0.931 0.763
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.833 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.29  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.77  1.286 0.371 0.    0.    0.    0.313 1.231 0.335
  0.    0.    0.691]
 [0.431 0.967 0.395 0.748 0.624 0.    0.353 0.    2.866 0.749 0.685 0.904
  0.    0.    0.669]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.5892857142857143, 'tpr': 0.7666666666666667, 'fpr': 0.44, 'f1': 0.5348837209302325, 'shd': 39, 'npred': 56, 'ntrue': 30}
[2.457e-02 1.599e-01 4.329e-01 3.633e-01 1.288e-02 1.710e-02 3.135e-03
 3.954e-02 1.109e+00 1.187e-01 2.582e-01 2.686e-02 1.216e-02 1.580e-01
 2.544e-01 6.139e-01 6.928e-01 3.481e-01 1.912e-01 1.037e-01 2.061e-02
 2.344e-02 8.965e-01 3.190e-01 7.549e-01 1.119e-01 3.598e-03 4.496e-01
 4.031e-02 1.082e-02 1.576e-01 2.282e-01 3.049e-02 2.240e-02 1.887e-02
 1.748e-02 3.327e-02 2.837e-02 1.168e-01 2.253e-02 9.192e-03 1.999e-01
 1.550e-02 1.002e-02 3.215e-02 1.114e-01 1.639e-02 7.042e-03 4.895e-03
 5.883e-03 3.627e-02 2.950e-02 5.395e-02 6.011e-03 5.546e-03 3.344e-01
 4.525e-03 5.158e-03 2.472e-02 3.274e-02 8.045e-03 3.215e-03 3.815e-03
 4.656e-03 3.110e-03 4.659e-03 1.658e-02 3.200e-03 2.278e-03 6.891e-02
 5.025e-01 4.126e-02 1.840e-01 3.354e-01 4.969e-01 1.289e-02 2.581e-02
 8.002e-02 5.260e-01 2.090e-01 5.647e-01 1.447e-02 1.284e-02 2.515e-01
 2.115e-01 5.798e-02 3.617e-01 6.581e-01 4.727e-01 3.804e-01 5.987e-02
 1.088e-01 3.608e-01 9.866e-01 3.791e-01 1.311e-01 1.508e-02 3.808e-01
 1.455e+00 3.524e-01 2.915e-01 1.162e+00 3.881e-01 1.767e-01 1.078e-01
 2.971e-01 5.728e-01 2.992e-01 6.131e-01 1.307e-01 1.391e-02 3.904e-01
 1.771e-01 2.151e-01 2.741e-01 3.683e-01 5.789e-01 6.293e-02 4.932e-02
 1.776e-02 9.428e-01 9.312e-01 7.630e-01 1.656e-02 2.202e-03 2.673e-01
 4.756e-03 6.560e-03 1.160e-01 1.280e-01 1.833e+00 9.423e-03 1.420e-02
 5.012e-03 6.647e-03 2.797e-02 5.071e-02 1.096e-02 3.557e-03 1.402e-01
 4.164e-02 1.178e-02 1.846e-01 1.810e-01 2.290e+00 1.281e-02 6.782e-03
 1.369e-02 6.656e-03 2.064e-01 5.554e-02 4.503e-03 4.184e-03 2.050e-01
 2.458e-02 6.963e-03 7.661e-02 1.230e-01 2.501e-01 1.384e-02 1.958e-02
 9.598e-03 8.786e-03 8.458e-02 1.042e-01 1.113e-02 2.774e-03 8.450e-02
 2.725e-01 5.637e-02 2.752e-01 7.703e-01 1.286e+00 3.707e-01 4.211e-02
 2.671e-02 2.909e-01 3.134e-01 1.231e+00 3.347e-01 3.057e-02 6.911e-01
 4.312e-01 9.671e-01 3.954e-01 7.482e-01 6.244e-01 2.778e-01 3.528e-01
 2.440e-01 2.866e+00 7.493e-01 6.854e-01 9.038e-01 1.864e-01 6.692e-01
 2.035e-02 1.186e-02 4.695e-02 1.932e-02 5.528e-02 2.896e-02 7.290e-03
 6.526e-03 1.919e-02 4.370e-02 1.860e-02 9.177e-02 6.755e-03 5.264e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8946296296296297, 0.7345662848175852)
Iterations 630
Achieves (23.639217916927258, 1e-05)-DP
