samples  5000  graph  13 26 ER mlp  minibatch size  75  noise  0.5  minibatches per NN training  111 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3261408551088962
iteration 1 in outer loop, alpha = 1.3261408551088962, rho = 1.0, h = 1.3261408551088962
cuda
iteration 1 in inner loop,alpha 1.3261408551088962 rho 1.0 h 0.8735352591375847
iteration 2 in inner loop,alpha 1.3261408551088962 rho 10.0 h 0.401177931240678
iteration 3 in inner loop,alpha 1.3261408551088962 rho 100.0 h 0.1440955735845204
iteration 2 in outer loop, alpha = 15.735698213560937, rho = 100.0, h = 0.1440955735845204
cuda
iteration 1 in inner loop,alpha 15.735698213560937 rho 100.0 h 0.09229178119352355
iteration 2 in inner loop,alpha 15.735698213560937 rho 1000.0 h 0.03613185652054618
iteration 3 in inner loop,alpha 15.735698213560937 rho 10000.0 h 0.009823201610750232
iteration 3 in outer loop, alpha = 113.96771432106326, rho = 10000.0, h = 0.009823201610750232
cuda
iteration 1 in inner loop,alpha 113.96771432106326 rho 10000.0 h 0.004275171907316633
iteration 2 in inner loop,alpha 113.96771432106326 rho 100000.0 h 0.0014694906596393054
iteration 4 in outer loop, alpha = 260.9167802849938, rho = 100000.0, h = 0.0014694906596393054
cuda
iteration 1 in inner loop,alpha 260.9167802849938 rho 100000.0 h 0.0007784744775616304
iteration 5 in outer loop, alpha = 1039.3912578466243, rho = 1000000.0, h = 0.0007784744775616304
Threshold 0.3
[[0.001 0.    0.178 2.749 0.052 0.007 0.023 0.    0.    0.327 0.    1.781
  0.   ]
 [0.61  0.001 0.337 0.015 0.017 0.003 0.039 0.    0.    2.057 0.012 0.217
  0.   ]
 [0.    0.    0.002 0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.659 0.001 0.003 0.001 0.089 0.    0.    1.683 0.    0.297
  0.   ]
 [0.024 0.037 0.245 0.121 0.002 0.    0.056 0.    0.025 0.25  0.    0.381
  0.   ]
 [0.08  0.019 0.333 0.026 0.383 0.001 0.02  0.    0.135 2.345 0.032 0.132
  0.014]
 [0.01  0.011 1.069 0.024 0.024 0.017 0.027 0.001 0.002 0.961 0.001 0.393
  0.004]
 [0.012 0.017 0.017 0.038 0.995 3.712 0.057 0.001 0.591 0.406 0.009 0.158
  0.02 ]
 [0.035 1.502 2.206 0.014 0.017 0.003 0.358 0.    0.    1.278 0.011 0.146
  0.012]
 [0.    0.    1.119 0.    0.003 0.    0.001 0.    0.    0.004 0.    2.286
  0.   ]
 [1.725 0.014 2.501 0.057 0.245 0.014 0.015 0.001 0.006 1.054 0.    0.081
  0.008]
 [0.    0.    0.944 0.    0.003 0.    0.001 0.    0.    0.    0.    0.001
  0.   ]
 [0.016 0.992 2.295 0.026 0.554 0.006 0.085 0.022 0.015 2.28  0.01  0.496
  0.   ]]
[[0.    0.    0.    2.749 0.    0.    0.    0.    0.    0.327 0.    1.781
  0.   ]
 [0.61  0.    0.337 0.    0.    0.    0.    0.    0.    2.057 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.659 0.    0.    0.    0.    0.    0.    1.683 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.381
  0.   ]
 [0.    0.    0.333 0.    0.383 0.    0.    0.    0.    2.345 0.    0.
  0.   ]
 [0.    0.    1.069 0.    0.    0.    0.    0.    0.    0.961 0.    0.393
  0.   ]
 [0.    0.    0.    0.    0.995 3.712 0.    0.    0.591 0.406 0.    0.
  0.   ]
 [0.    1.502 2.206 0.    0.    0.    0.358 0.    0.    1.278 0.    0.
  0.   ]
 [0.    0.    1.119 0.    0.    0.    0.    0.    0.    0.    0.    2.286
  0.   ]
 [1.725 0.    2.501 0.    0.    0.    0.    0.    0.    1.054 0.    0.
  0.   ]
 [0.    0.    0.944 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.992 2.295 0.    0.554 0.    0.    0.    0.    2.28  0.    0.496
  0.   ]]
{'fdr': 0.47058823529411764, 'tpr': 0.6923076923076923, 'fpr': 0.3076923076923077, 'f1': 0.5999999999999999, 'shd': 18, 'npred': 34, 'ntrue': 26}
[2.675e-04 1.782e-01 2.749e+00 5.164e-02 6.702e-03 2.258e-02 1.466e-04
 1.159e-04 3.274e-01 4.006e-05 1.781e+00 6.587e-05 6.097e-01 3.366e-01
 1.479e-02 1.663e-02 2.606e-03 3.904e-02 3.907e-04 1.417e-04 2.057e+00
 1.193e-02 2.169e-01 1.581e-04 3.356e-05 1.408e-05 9.345e-05 3.433e-04
 1.034e-05 1.040e-03 5.319e-06 2.384e-05 1.674e-04 5.889e-07 2.030e-05
 2.086e-05 1.172e-04 6.464e-05 2.659e+00 2.505e-03 1.088e-03 8.934e-02
 2.081e-04 9.341e-05 1.683e+00 9.833e-06 2.972e-01 4.505e-05 2.409e-02
 3.702e-02 2.452e-01 1.212e-01 2.328e-04 5.625e-02 5.089e-05 2.519e-02
 2.505e-01 2.772e-04 3.812e-01 1.030e-04 7.972e-02 1.865e-02 3.333e-01
 2.571e-02 3.828e-01 1.993e-02 1.173e-04 1.350e-01 2.345e+00 3.237e-02
 1.320e-01 1.371e-02 9.864e-03 1.109e-02 1.069e+00 2.432e-02 2.425e-02
 1.690e-02 1.350e-03 2.447e-03 9.612e-01 1.376e-03 3.933e-01 4.344e-03
 1.209e-02 1.667e-02 1.733e-02 3.757e-02 9.948e-01 3.712e+00 5.719e-02
 5.912e-01 4.063e-01 9.262e-03 1.582e-01 2.019e-02 3.481e-02 1.502e+00
 2.206e+00 1.416e-02 1.650e-02 3.330e-03 3.577e-01 3.881e-04 1.278e+00
 1.053e-02 1.460e-01 1.155e-02 3.748e-05 8.834e-05 1.119e+00 1.119e-04
 3.496e-03 1.919e-04 1.229e-03 4.246e-05 1.577e-05 7.869e-06 2.286e+00
 4.296e-05 1.725e+00 1.357e-02 2.501e+00 5.713e-02 2.448e-01 1.396e-02
 1.457e-02 9.135e-04 5.961e-03 1.054e+00 8.112e-02 7.823e-03 2.483e-06
 3.087e-06 9.445e-01 6.714e-06 2.743e-03 7.073e-06 9.258e-04 1.064e-05
 3.441e-06 3.690e-05 3.535e-07 1.524e-06 1.592e-02 9.917e-01 2.295e+00
 2.574e-02 5.536e-01 5.821e-03 8.518e-02 2.182e-02 1.521e-02 2.280e+00
 1.023e-02 4.955e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.828698224852071, 0.7252972970900552)
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 480.43 = squared loss an data 285.64 + 0.5*rho*h**2 194.315677 + alpha*h 0.000000 + L2reg 0.24 + L1reg 0.24 ; SHD = 91 ; DAG False
total norm for a microbatch 55.631582095649975 clip 5.91078294838156
total norm for a microbatch 55.631108296942614 clip 5.91078294838156
total norm for a microbatch 63.14540589780845 clip 15.947856607571815
total norm for a microbatch 55.52713073196148 clip 44.599712397261136
total norm for a microbatch 50.30609409917551 clip 40.20462670659809
total norm for a microbatch 50.82483035645543 clip 34.785814571998685
total norm for a microbatch 36.0734725936924 clip 35.38492461356325
cuda
Objective function 23.57 = squared loss an data 21.71 + 0.5*rho*h**2 1.374523 + alpha*h 0.000000 + L2reg 0.38 + L1reg 0.12 ; SHD = 30 ; DAG False
Proportion of microbatches that were clipped  0.8533105718814779
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.6580244464483656
iteration 1 in outer loop, alpha = 1.6580244464483656, rho = 1.0, h = 1.6580244464483656
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 26.32 = squared loss an data 21.71 + 0.5*rho*h**2 1.374523 + alpha*h 2.749045 + L2reg 0.38 + L1reg 0.12 ; SHD = 30 ; DAG False
total norm for a microbatch 44.983679881704866 clip 1.2045336682656211
total norm for a microbatch 102.02309991800767 clip 45.138947487767496
total norm for a microbatch 36.29760631062359 clip 45.540896850806426
total norm for a microbatch 52.42814575069342 clip 47.206466072158165
total norm for a microbatch 75.68140721081332 clip 45.71391597991361
cuda
Objective function 13.47 = squared loss an data 10.09 + 0.5*rho*h**2 0.663349 + alpha*h 1.909753 + L2reg 0.69 + L1reg 0.11 ; SHD = 35 ; DAG False
Proportion of microbatches that were clipped  0.8592814371257484
iteration 1 in inner loop, alpha 1.6580244464483656 rho 1.0 h 1.1518241793328663
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 19.44 = squared loss an data 10.09 + 0.5*rho*h**2 6.633495 + alpha*h 1.909753 + L2reg 0.69 + L1reg 0.11 ; SHD = 35 ; DAG False
total norm for a microbatch 46.10644619939852 clip 3.116333396677827
total norm for a microbatch 80.10921403306685 clip 3.7476515514652977
total norm for a microbatch 65.98477295909538 clip 8.456955391907453
total norm for a microbatch 79.10941538393267 clip 29.79884036242661
total norm for a microbatch 94.74033410136009 clip 60.472897204754275
total norm for a microbatch 143.31287652790908 clip 60.472897204754275
total norm for a microbatch 68.70174703622018 clip 61.46068447320106
total norm for a microbatch 91.06948850839083 clip 65.69813074073717
total norm for a microbatch 48.93373975713013 clip 63.273785072819166
cuda
Objective function 13.95 = squared loss an data 10.91 + 0.5*rho*h**2 1.248198 + alpha*h 0.828414 + L2reg 0.86 + L1reg 0.11 ; SHD = 34 ; DAG False
Proportion of microbatches that were clipped  0.8652499692912419
iteration 2 in inner loop, alpha 1.6580244464483656 rho 10.0 h 0.49963940379089955
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 25.19 = squared loss an data 10.91 + 0.5*rho*h**2 12.481977 + alpha*h 0.828414 + L2reg 0.86 + L1reg 0.11 ; SHD = 34 ; DAG False
total norm for a microbatch 87.41805250934675 clip 2.269253936683588
total norm for a microbatch 69.98705096625699 clip 11.811882655247478
total norm for a microbatch 96.55370579388129 clip 22.239561728830108
total norm for a microbatch 195.02937124946808 clip 74.84440896110458
cuda
Objective function 15.30 = squared loss an data 12.55 + 0.5*rho*h**2 1.408929 + alpha*h 0.278324 + L2reg 0.96 + L1reg 0.10 ; SHD = 32 ; DAG True
Proportion of microbatches that were clipped  0.8700217233888486
iteration 3 in inner loop, alpha 1.6580244464483656 rho 100.0 h 0.16786478108989833
iteration 2 in outer loop, alpha = 18.4445025554382, rho = 100.0, h = 0.16786478108989833
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 18.11 = squared loss an data 12.55 + 0.5*rho*h**2 1.408929 + alpha*h 3.096182 + L2reg 0.96 + L1reg 0.10 ; SHD = 32 ; DAG True
total norm for a microbatch 119.29361220037116 clip 64.39309625613961
total norm for a microbatch 63.49843186345322 clip 69.77893795363087
total norm for a microbatch 99.41668654278531 clip 80.35532181358495
total norm for a microbatch 94.39914500205559 clip 87.32971629837175
total norm for a microbatch 148.10846460123972 clip 79.72583938828144
total norm for a microbatch 95.61407852155489 clip 85.00836508109428
cuda
Objective function 16.83 = squared loss an data 13.45 + 0.5*rho*h**2 0.472292 + alpha*h 1.792616 + L2reg 1.01 + L1reg 0.11 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.8722703428083445
iteration 1 in inner loop, alpha 18.4445025554382 rho 100.0 h 0.09718969457854776
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 21.08 = squared loss an data 13.45 + 0.5*rho*h**2 4.722918 + alpha*h 1.792616 + L2reg 1.01 + L1reg 0.11 ; SHD = 36 ; DAG True
total norm for a microbatch 176.26963269126153 clip 1.320342084796965
total norm for a microbatch 137.20931709282158 clip 1.320342084796965
total norm for a microbatch 120.06430695331694 clip 11.808125105698963
total norm for a microbatch 115.77281786763686 clip 14.051790520478773
total norm for a microbatch 131.3609823106292 clip 32.07501343111023
total norm for a microbatch 73.33290171697092 clip 83.82953666353964
total norm for a microbatch 167.06940493681344 clip 86.33357683189465
total norm for a microbatch 110.30373334886798 clip 83.83915919026035
total norm for a microbatch 166.03019538447768 clip 81.98990891682489
total norm for a microbatch 217.86377886561695 clip 85.0912142757366
total norm for a microbatch 132.45005751896957 clip 85.08156064526568
total norm for a microbatch 101.65172113125327 clip 85.08156064526568
cuda
Objective function 16.85 = squared loss an data 14.07 + 0.5*rho*h**2 0.836478 + alpha*h 0.754413 + L2reg 1.08 + L1reg 0.11 ; SHD = 32 ; DAG True
Proportion of microbatches that were clipped  0.8726809748999637
iteration 2 in inner loop, alpha 18.4445025554382 rho 1000.0 h 0.040901778020431934
iteration 3 in outer loop, alpha = 59.34628057587013, rho = 1000.0, h = 0.040901778020431934
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 18.52 = squared loss an data 14.07 + 0.5*rho*h**2 0.836478 + alpha*h 2.427368 + L2reg 1.08 + L1reg 0.11 ; SHD = 32 ; DAG True
total norm for a microbatch 158.41735049913976 clip 76.37407464576795
total norm for a microbatch 75.93068184470603 clip 82.96184228253837
cuda
Objective function 17.08 = squared loss an data 14.16 + 0.5*rho*h**2 0.274732 + alpha*h 1.391114 + L2reg 1.14 + L1reg 0.12 ; SHD = 32 ; DAG True
Proportion of microbatches that were clipped  0.871088558656518
iteration 1 in inner loop, alpha 59.34628057587013 rho 1000.0 h 0.023440632442639497
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 19.55 = squared loss an data 14.16 + 0.5*rho*h**2 2.747316 + alpha*h 1.391114 + L2reg 1.14 + L1reg 0.12 ; SHD = 32 ; DAG True
total norm for a microbatch 112.98042898458901 clip 1.0
total norm for a microbatch 150.00390620715984 clip 1.0
total norm for a microbatch 110.11763470400976 clip 1.0
total norm for a microbatch 108.51991605178567 clip 28.552465975959517
total norm for a microbatch 146.62700268800748 clip 75.57209310245914
total norm for a microbatch 150.31862723886772 clip 75.57209310245914
total norm for a microbatch 101.46543780512883 clip 88.40445689571074
total norm for a microbatch 88.73152840586194 clip 85.05514710033414
total norm for a microbatch 119.32109579605321 clip 89.14356040097012
total norm for a microbatch 59.5302991080434 clip 86.54002672181205
total norm for a microbatch 106.32586454486368 clip 85.86611008496476
total norm for a microbatch 242.85105555668494 clip 84.42797052739904
total norm for a microbatch 106.6332164529635 clip 86.01224193491818
cuda
Objective function 16.72 = squared loss an data 14.24 + 0.5*rho*h**2 0.567505 + alpha*h 0.632257 + L2reg 1.16 + L1reg 0.12 ; SHD = 29 ; DAG True
Proportion of microbatches that were clipped  0.8729187071498531
iteration 2 in inner loop, alpha 59.34628057587013 rho 10000.0 h 0.010653685223175557
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 21.83 = squared loss an data 14.24 + 0.5*rho*h**2 5.675050 + alpha*h 0.632257 + L2reg 1.16 + L1reg 0.12 ; SHD = 29 ; DAG True
total norm for a microbatch 214.82142401636435 clip 1.7358946476187347
total norm for a microbatch 138.38234847569643 clip 19.56490289855683
total norm for a microbatch 100.30262063272467 clip 68.70129163460396
total norm for a microbatch 111.22162761004664 clip 101.81141961144053
cuda
Objective function 16.20 = squared loss an data 14.04 + 0.5*rho*h**2 0.617483 + alpha*h 0.208555 + L2reg 1.22 + L1reg 0.12 ; SHD = 31 ; DAG True
Proportion of microbatches that were clipped  0.8733260948244662
iteration 3 in inner loop, alpha 59.34628057587013 rho 100000.0 h 0.0035142076227980823
iteration 4 in outer loop, alpha = 410.76704285567837, rho = 100000.0, h = 0.0035142076227980823
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 17.44 = squared loss an data 14.04 + 0.5*rho*h**2 0.617483 + alpha*h 1.443521 + L2reg 1.22 + L1reg 0.12 ; SHD = 31 ; DAG True
total norm for a microbatch 171.9077576403679 clip 3.3037301891008504
total norm for a microbatch 153.84400524296333 clip 5.219837571956456
total norm for a microbatch 126.65637354372582 clip 10.035988813062866
total norm for a microbatch 108.17219264781686 clip 17.751656258203454
total norm for a microbatch 116.84147207760893 clip 27.775939301987556
total norm for a microbatch 91.26894516076561 clip 125.37938381620641
total norm for a microbatch 110.31131279092067 clip 101.03681256934846
total norm for a microbatch 83.45163981030512 clip 99.21726588203519
cuda
Objective function 16.57 = squared loss an data 14.09 + 0.5*rho*h**2 0.229541 + alpha*h 0.880117 + L2reg 1.25 + L1reg 0.12 ; SHD = 29 ; DAG True
Proportion of microbatches that were clipped  0.8724627048178039
iteration 1 in inner loop, alpha 410.76704285567837 rho 100000.0 h 0.0021426182834574092
iteration 5 in outer loop, alpha = 2553.3853263130877, rho = 1000000.0, h = 0.0021426182834574092
Threshold 0.3
[[0.002 0.304 0.719 1.348 0.739 0.507 0.226 0.111 0.029 0.638 0.048 0.478
  0.063]
 [0.017 0.004 0.136 0.028 0.193 0.066 0.041 0.005 0.003 0.689 0.019 0.042
  0.005]
 [0.002 0.049 0.004 0.004 0.269 0.046 0.046 0.005 0.003 0.577 0.004 0.029
  0.004]
 [0.003 0.094 1.637 0.006 0.292 0.153 0.165 0.004 0.018 0.469 0.014 0.111
  0.018]
 [0.005 0.019 0.016 0.011 0.006 0.024 0.006 0.006 0.005 0.092 0.005 0.004
  0.004]
 [0.006 0.092 0.075 0.041 0.247 0.005 0.042 0.001 0.007 1.434 0.016 0.051
  0.013]
 [0.025 0.113 0.116 0.044 0.54  0.157 0.005 0.008 0.005 0.71  0.028 0.322
  0.011]
 [0.054 0.552 0.759 0.81  0.906 3.441 0.364 0.003 0.025 0.886 0.334 0.688
  0.195]
 [0.162 1.575 1.335 0.263 0.819 0.422 0.906 0.21  0.006 1.016 0.085 0.445
  0.384]
 [0.006 0.005 0.008 0.004 0.06  0.002 0.01  0.001 0.002 0.008 0.007 0.032
  0.001]
 [0.113 0.245 0.789 0.274 0.731 0.402 0.189 0.015 0.067 0.532 0.004 0.185
  0.035]
 [0.007 0.13  0.2   0.028 0.971 0.118 0.017 0.006 0.009 0.243 0.025 0.004
  0.005]
 [0.075 0.833 1.375 0.203 0.991 0.318 0.446 0.027 0.016 1.998 0.134 0.743
  0.003]]
[[0.    0.304 0.719 1.348 0.739 0.507 0.    0.    0.    0.638 0.    0.478
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.689 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.577 0.    0.
  0.   ]
 [0.    0.    1.637 0.    0.    0.    0.    0.    0.    0.469 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.434 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.54  0.    0.    0.    0.    0.71  0.    0.322
  0.   ]
 [0.    0.552 0.759 0.81  0.906 3.441 0.364 0.    0.    0.886 0.334 0.688
  0.   ]
 [0.    1.575 1.335 0.    0.819 0.422 0.906 0.    0.    1.016 0.    0.445
  0.384]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.789 0.    0.731 0.402 0.    0.    0.    0.532 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.971 0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.833 1.375 0.    0.991 0.318 0.446 0.    0.    1.998 0.    0.743
  0.   ]]
{'fdr': 0.5681818181818182, 'tpr': 0.7307692307692307, 'fpr': 0.4807692307692308, 'f1': 0.5428571428571428, 'shd': 29, 'npred': 44, 'ntrue': 26}
[3.040e-01 7.193e-01 1.348e+00 7.391e-01 5.066e-01 2.257e-01 1.108e-01
 2.942e-02 6.378e-01 4.820e-02 4.784e-01 6.318e-02 1.698e-02 1.360e-01
 2.790e-02 1.934e-01 6.590e-02 4.145e-02 5.407e-03 2.598e-03 6.886e-01
 1.919e-02 4.155e-02 5.133e-03 2.034e-03 4.907e-02 3.531e-03 2.690e-01
 4.587e-02 4.627e-02 4.787e-03 2.986e-03 5.774e-01 4.299e-03 2.907e-02
 3.580e-03 3.454e-03 9.361e-02 1.637e+00 2.915e-01 1.530e-01 1.654e-01
 4.082e-03 1.818e-02 4.692e-01 1.415e-02 1.105e-01 1.766e-02 4.946e-03
 1.869e-02 1.552e-02 1.145e-02 2.447e-02 6.290e-03 5.563e-03 4.706e-03
 9.165e-02 5.350e-03 3.755e-03 4.128e-03 5.964e-03 9.187e-02 7.512e-02
 4.059e-02 2.465e-01 4.226e-02 8.121e-04 7.017e-03 1.434e+00 1.604e-02
 5.103e-02 1.279e-02 2.457e-02 1.133e-01 1.157e-01 4.442e-02 5.403e-01
 1.573e-01 7.990e-03 4.697e-03 7.099e-01 2.765e-02 3.223e-01 1.142e-02
 5.399e-02 5.523e-01 7.594e-01 8.097e-01 9.061e-01 3.441e+00 3.643e-01
 2.537e-02 8.860e-01 3.338e-01 6.885e-01 1.948e-01 1.619e-01 1.575e+00
 1.335e+00 2.627e-01 8.195e-01 4.217e-01 9.058e-01 2.104e-01 1.016e+00
 8.534e-02 4.453e-01 3.839e-01 5.535e-03 5.466e-03 8.477e-03 3.941e-03
 6.041e-02 2.245e-03 1.022e-02 6.000e-04 2.258e-03 6.631e-03 3.182e-02
 1.499e-03 1.129e-01 2.446e-01 7.890e-01 2.743e-01 7.306e-01 4.018e-01
 1.893e-01 1.462e-02 6.666e-02 5.319e-01 1.847e-01 3.516e-02 7.190e-03
 1.298e-01 1.997e-01 2.780e-02 9.714e-01 1.177e-01 1.653e-02 5.921e-03
 8.622e-03 2.426e-01 2.486e-02 5.291e-03 7.460e-02 8.327e-01 1.375e+00
 2.030e-01 9.912e-01 3.176e-01 4.463e-01 2.727e-02 1.576e-02 1.998e+00
 1.342e-01 7.433e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.8301775147928995, 0.6652612784532825)
Iterations 1110
Achieves (23.82233853610087, 1e-05)-DP
