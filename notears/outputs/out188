samples  5000  graph  20 80 ER mlp  minibatch size  50  noise  1.0  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7622777646747636
iteration 1 in outer loop, alpha = 1.7622777646747636, rho = 1.0, h = 1.7622777646747636
cuda
iteration 1 in inner loop,alpha 1.7622777646747636 rho 1.0 h 1.1769941079911774
iteration 2 in inner loop,alpha 1.7622777646747636 rho 10.0 h 0.546699080645368
iteration 3 in inner loop,alpha 1.7622777646747636 rho 100.0 h 0.18576399615164618
iteration 2 in outer loop, alpha = 20.338677379839382, rho = 100.0, h = 0.18576399615164618
cuda
iteration 1 in inner loop,alpha 20.338677379839382 rho 100.0 h 0.1095608426621908
iteration 2 in inner loop,alpha 20.338677379839382 rho 1000.0 h 0.04370983218316127
iteration 3 in outer loop, alpha = 64.04850956300065, rho = 1000.0, h = 0.04370983218316127
cuda
iteration 1 in inner loop,alpha 64.04850956300065 rho 1000.0 h 0.023592627798716848
iteration 2 in inner loop,alpha 64.04850956300065 rho 10000.0 h 0.008006307647868027
iteration 4 in outer loop, alpha = 144.11158604168094, rho = 10000.0, h = 0.008006307647868027
cuda
iteration 1 in inner loop,alpha 144.11158604168094 rho 10000.0 h 0.0041674095898649455
iteration 2 in inner loop,alpha 144.11158604168094 rho 100000.0 h 0.0014209538164706714
iteration 5 in outer loop, alpha = 286.2069676887481, rho = 100000.0, h = 0.0014209538164706714
cuda
iteration 1 in inner loop,alpha 286.2069676887481 rho 100000.0 h 0.0006536102144423239
iteration 6 in outer loop, alpha = 939.8171821310721, rho = 1000000.0, h = 0.0006536102144423239
Threshold 0.3
[[0.001 3.147 0.081 0.246 0.908 1.118 0.317 0.    1.946 1.114 0.125 1.121
  0.274 0.001 0.368 0.433 0.298 1.209 1.297 0.475]
 [0.    0.001 0.025 0.508 0.487 1.652 1.787 0.    0.572 0.469 0.094 0.677
  0.318 0.001 1.177 0.299 0.366 0.862 0.48  1.889]
 [0.002 0.008 0.001 0.067 0.44  0.551 1.518 0.    0.186 0.474 0.094 0.19
  1.301 0.002 1.491 0.319 0.601 0.002 1.19  0.329]
 [0.    0.    0.    0.02  0.    0.002 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.206 0.239 0.193 0.    0.001 0.724]
 [0.    0.    0.    0.624 0.004 0.324 0.22  0.    0.    0.416 0.002 0.001
  0.003 0.    0.212 0.347 1.201 0.    0.349 1.057]
 [0.    0.    0.    0.55  0.002 0.003 0.001 0.    0.    0.    0.    0.
  0.    0.    0.348 1.651 1.791 0.    0.297 0.303]
 [0.    0.    0.    0.194 0.001 0.225 0.002 0.    0.    0.01  0.    0.
  0.    0.    0.327 0.353 0.298 0.    0.167 0.196]
 [2.484 0.131 0.003 0.111 0.709 0.223 0.719 0.001 0.301 0.455 0.134 0.381
  1.948 0.688 0.221 0.181 0.182 1.129 0.494 0.447]
 [0.    0.    0.002 0.358 1.592 0.946 1.294 0.    0.001 0.824 0.439 0.297
  1.767 0.    1.211 1.249 0.811 0.    1.332 0.69 ]
 [0.    0.    0.    0.112 0.016 2.203 0.119 0.    0.    0.008 0.001 0.001
  0.    0.    1.174 1.418 1.737 0.    0.817 0.228]
 [0.    0.    0.    1.011 0.11  0.592 2.319 0.    0.    0.258 0.001 0.
  0.    0.    1.507 0.477 1.022 0.    0.276 1.296]
 [0.    0.    0.    0.158 0.442 0.262 1.665 0.    0.    0.251 0.127 0.001
  0.113 0.    0.163 0.818 0.279 0.    0.121 0.218]
 [0.    0.    0.    0.099 0.138 1.383 1.076 0.    0.    0.172 2.141 0.002
  0.001 0.    0.302 0.322 0.291 0.    0.483 1.077]
 [0.156 0.038 0.033 0.798 0.337 1.627 0.324 0.001 0.287 0.357 1.709 0.533
  2.058 0.001 0.436 0.449 1.012 0.355 0.731 1.95 ]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.252 0.003 0.    0.001 0.002]
 [0.    0.    0.    0.003 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.001 0.005 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.002 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.097 1.185 0.005 0.    0.001 0.002]
 [0.    0.    0.032 0.051 0.454 0.292 1.386 0.    3.186 0.543 0.095 0.727
  0.082 0.    0.342 0.186 0.185 0.001 0.422 0.243]
 [0.    0.    0.    0.787 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.332 1.052 0.395 0.    0.001 0.258]
 [0.    0.    0.    0.003 0.    0.    0.001 0.    0.    0.001 0.    0.
  0.    0.    0.116 1.753 0.275 0.    0.001 0.002]]
[[0.    3.147 0.    0.    0.908 1.118 0.317 0.    1.946 1.114 0.    1.121
  0.    0.    0.368 0.433 0.    1.209 1.297 0.475]
 [0.    0.    0.    0.508 0.487 1.652 1.787 0.    0.572 0.469 0.    0.677
  0.318 0.    1.177 0.    0.366 0.862 0.48  1.889]
 [0.    0.    0.    0.    0.44  0.551 1.518 0.    0.    0.474 0.    0.
  1.301 0.    1.491 0.319 0.601 0.    1.19  0.329]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.724]
 [0.    0.    0.    0.624 0.    0.324 0.    0.    0.    0.416 0.    0.
  0.    0.    0.    0.347 1.201 0.    0.349 1.057]
 [0.    0.    0.    0.55  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.348 1.651 1.791 0.    0.    0.303]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.327 0.353 0.    0.    0.    0.   ]
 [2.484 0.    0.    0.    0.709 0.    0.719 0.    0.301 0.455 0.    0.381
  1.948 0.688 0.    0.    0.    1.129 0.494 0.447]
 [0.    0.    0.    0.358 1.592 0.946 1.294 0.    0.    0.824 0.439 0.
  1.767 0.    1.211 1.249 0.811 0.    1.332 0.69 ]
 [0.    0.    0.    0.    0.    2.203 0.    0.    0.    0.    0.    0.
  0.    0.    1.174 1.418 1.737 0.    0.817 0.   ]
 [0.    0.    0.    1.011 0.    0.592 2.319 0.    0.    0.    0.    0.
  0.    0.    1.507 0.477 1.022 0.    0.    1.296]
 [0.    0.    0.    0.    0.442 0.    1.665 0.    0.    0.    0.    0.
  0.    0.    0.    0.818 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.383 1.076 0.    0.    0.    2.141 0.
  0.    0.    0.302 0.322 0.    0.    0.483 1.077]
 [0.    0.    0.    0.798 0.337 1.627 0.324 0.    0.    0.357 1.709 0.533
  2.058 0.    0.436 0.449 1.012 0.355 0.731 1.95 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.097 1.185 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.454 0.    1.386 0.    3.186 0.543 0.    0.727
  0.    0.    0.342 0.    0.    0.    0.422 0.   ]
 [0.    0.    0.    0.787 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.332 1.052 0.395 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.753 0.    0.    0.    0.   ]]
{'fdr': 0.43089430894308944, 'tpr': 0.875, 'fpr': 0.4818181818181818, 'f1': 0.689655172413793, 'shd': 58, 'npred': 123, 'ntrue': 80}
[3.147e+00 8.083e-02 2.460e-01 9.083e-01 1.118e+00 3.168e-01 2.674e-04
 1.946e+00 1.114e+00 1.246e-01 1.121e+00 2.742e-01 1.376e-03 3.679e-01
 4.333e-01 2.975e-01 1.209e+00 1.297e+00 4.745e-01 4.054e-04 2.457e-02
 5.076e-01 4.874e-01 1.652e+00 1.787e+00 1.819e-04 5.720e-01 4.692e-01
 9.395e-02 6.772e-01 3.177e-01 1.177e-03 1.177e+00 2.989e-01 3.665e-01
 8.621e-01 4.802e-01 1.889e+00 2.203e-03 8.308e-03 6.743e-02 4.400e-01
 5.511e-01 1.518e+00 2.377e-04 1.860e-01 4.738e-01 9.446e-02 1.904e-01
 1.301e+00 2.297e-03 1.491e+00 3.191e-01 6.008e-01 2.303e-03 1.190e+00
 3.288e-01 7.199e-05 1.872e-05 7.850e-05 4.841e-04 2.261e-03 1.129e-03
 6.654e-05 8.666e-05 1.614e-03 3.733e-04 2.640e-04 7.937e-05 2.142e-06
 2.061e-01 2.390e-01 1.929e-01 1.753e-05 1.113e-03 7.238e-01 2.912e-06
 6.699e-06 1.372e-05 6.244e-01 3.240e-01 2.203e-01 1.895e-05 1.503e-04
 4.162e-01 2.401e-03 1.177e-03 2.905e-03 7.241e-05 2.117e-01 3.471e-01
 1.201e+00 3.078e-05 3.491e-01 1.057e+00 8.636e-05 1.853e-04 7.731e-05
 5.501e-01 2.029e-03 1.462e-03 5.252e-05 7.726e-05 4.244e-04 6.867e-05
 3.330e-04 2.645e-04 3.712e-05 3.476e-01 1.651e+00 1.791e+00 1.303e-05
 2.974e-01 3.033e-01 5.928e-05 4.674e-05 2.954e-05 1.940e-01 1.381e-03
 2.253e-01 3.242e-05 1.793e-05 9.577e-03 1.363e-04 2.179e-04 7.084e-05
 4.116e-06 3.274e-01 3.529e-01 2.982e-01 1.872e-06 1.667e-01 1.961e-01
 2.484e+00 1.307e-01 2.806e-03 1.109e-01 7.092e-01 2.232e-01 7.188e-01
 3.014e-01 4.554e-01 1.340e-01 3.812e-01 1.948e+00 6.878e-01 2.215e-01
 1.813e-01 1.816e-01 1.129e+00 4.944e-01 4.475e-01 1.097e-05 3.457e-05
 1.860e-03 3.578e-01 1.592e+00 9.465e-01 1.294e+00 1.111e-05 8.243e-01
 4.389e-01 2.969e-01 1.767e+00 2.755e-05 1.211e+00 1.249e+00 8.108e-01
 5.050e-05 1.332e+00 6.901e-01 3.106e-05 3.868e-05 1.221e-04 1.125e-01
 1.584e-02 2.203e+00 1.186e-01 5.534e-05 1.736e-04 6.659e-04 1.210e-03
 3.908e-04 1.298e-04 1.174e+00 1.418e+00 1.737e+00 3.677e-05 8.167e-01
 2.281e-01 7.845e-06 3.395e-05 6.917e-05 1.011e+00 1.096e-01 5.918e-01
 2.319e+00 2.336e-05 8.438e-05 2.575e-01 4.749e-04 2.169e-04 4.735e-05
 1.507e+00 4.775e-01 1.022e+00 1.417e-05 2.759e-01 1.296e+00 1.902e-05
 4.836e-05 3.669e-04 1.583e-01 4.416e-01 2.616e-01 1.665e+00 2.503e-05
 2.221e-04 2.514e-01 1.273e-01 1.134e-01 9.341e-05 1.632e-01 8.180e-01
 2.789e-01 1.719e-04 1.209e-01 2.176e-01 5.597e-06 2.038e-05 1.836e-04
 9.950e-02 1.378e-01 1.383e+00 1.076e+00 3.274e-05 2.486e-04 1.720e-01
 2.141e+00 1.769e-03 8.376e-05 3.021e-01 3.221e-01 2.915e-01 2.474e-05
 4.834e-01 1.077e+00 1.563e-01 3.768e-02 3.312e-02 7.983e-01 3.374e-01
 1.627e+00 3.239e-01 6.816e-04 2.873e-01 3.565e-01 1.709e+00 5.333e-01
 2.058e+00 4.362e-01 4.486e-01 1.012e+00 3.548e-01 7.308e-01 1.950e+00
 4.638e-05 7.325e-05 3.146e-05 3.029e-03 1.271e-03 8.169e-04 3.702e-04
 3.201e-05 8.252e-06 4.601e-05 9.115e-05 2.405e-04 6.234e-05 4.760e-06
 2.524e-01 2.837e-03 2.160e-06 6.183e-04 2.095e-03 2.842e-05 2.333e-05
 3.312e-05 2.564e-03 6.030e-04 5.181e-05 3.493e-04 2.606e-05 2.326e-05
 3.010e-04 9.464e-05 6.774e-05 2.252e-05 1.388e-05 1.185e-03 1.275e-03
 5.675e-06 5.864e-04 2.706e-04 5.401e-05 1.128e-04 1.268e-05 2.127e-03
 9.787e-04 6.410e-04 8.580e-04 4.560e-05 2.783e-05 5.077e-05 7.555e-05
 8.963e-05 2.750e-05 3.119e-05 1.097e+00 1.185e+00 5.596e-06 6.570e-04
 1.565e-03 8.968e-05 2.166e-05 3.226e-02 5.095e-02 4.543e-01 2.916e-01
 1.386e+00 9.442e-05 3.186e+00 5.433e-01 9.507e-02 7.269e-01 8.159e-02
 1.141e-04 3.423e-01 1.861e-01 1.847e-01 4.222e-01 2.432e-01 6.874e-05
 1.538e-05 1.552e-04 7.872e-01 1.004e-03 7.431e-04 9.950e-04 4.916e-05
 1.620e-04 5.573e-04 1.794e-04 2.946e-04 1.533e-04 2.241e-05 3.321e-01
 1.052e+00 3.951e-01 1.319e-05 2.580e-01 3.906e-05 7.786e-05 7.081e-05
 2.714e-03 2.648e-04 3.211e-04 5.967e-04 2.050e-05 7.199e-05 9.269e-04
 3.466e-04 1.372e-04 2.314e-04 6.356e-05 1.157e-01 1.753e+00 2.747e-01
 1.577e-05 5.984e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.9380416666666666, 0.8893654169764621)
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
total norm for a microbatch 47.98997918601048 clip 49.82476132060305
total norm for a microbatch 55.287279147842995 clip 51.464244498843854
total norm for a microbatch 56.930421667723685 clip 51.567983488012075
total norm for a microbatch 59.12445332536903 clip 49.05664362841173
total norm for a microbatch 49.307684473772056 clip 54.18948850668389
total norm for a microbatch 85.69373286788056 clip 57.20910011725554
total norm for a microbatch 118.9245873648122 clip 63.14846181468247
total norm for a microbatch 51.65235603170711 clip 62.046824682403994
total norm for a microbatch 107.8732588479021 clip 63.6564739213145
total norm for a microbatch 88.49001004925499 clip 70.19804413593938
total norm for a microbatch 73.38859391857035 clip 72.44472456547192
total norm for a microbatch 150.0187472198772 clip 76.24478623713934
cuda
Objective function 42.70 = squared loss an data 34.53 + 0.5*rho*h**2 6.630765 + alpha*h 0.000000 + L2reg 1.23 + L1reg 0.31 ; SHD = 139 ; DAG False
Proportion of microbatches that were clipped  0.8141657244387014
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 3.6416384889111875
iteration 1 in outer loop, alpha = 3.6416384889111875, rho = 1.0, h = 3.6416384889111875
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 55.96 = squared loss an data 34.53 + 0.5*rho*h**2 6.630765 + alpha*h 13.261531 + L2reg 1.23 + L1reg 0.31 ; SHD = 139 ; DAG False
total norm for a microbatch 82.97875045472848 clip 1.2048967501798744
total norm for a microbatch 90.9149331030251 clip 5.734361006279214
total norm for a microbatch 89.75234298005743 clip 12.067532816689715
total norm for a microbatch 112.26224164551348 clip 95.43910848037294
total norm for a microbatch 93.57605349688781 clip 90.86039074580691
total norm for a microbatch 163.28029422196437 clip 101.32535810778914
total norm for a microbatch 104.86281355137895 clip 99.97878084163503
total norm for a microbatch 214.60425421929276 clip 104.08724864600258
total norm for a microbatch 190.9800417486131 clip 100.83543345425429
total norm for a microbatch 111.00197700134113 clip 98.57139624942721
total norm for a microbatch 136.69042783054996 clip 100.68075003306367
total norm for a microbatch 110.85870933850161 clip 101.95726866980156
cuda
Objective function 42.81 = squared loss an data 28.77 + 0.5*rho*h**2 3.077632 + alpha*h 9.034833 + L2reg 1.64 + L1reg 0.28 ; SHD = 125 ; DAG False
Proportion of microbatches that were clipped  0.8172270617927216
iteration 1 in inner loop, alpha 3.6416384889111875 rho 1.0 h 2.480980324051032
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 70.50 = squared loss an data 28.77 + 0.5*rho*h**2 30.776317 + alpha*h 9.034833 + L2reg 1.64 + L1reg 0.28 ; SHD = 125 ; DAG False
total norm for a microbatch 178.47165722512406 clip 6.535292006935008
total norm for a microbatch 163.48924515649335 clip 112.76733402198263
total norm for a microbatch 159.44539574640075 clip 127.83562222737218
total norm for a microbatch 174.2262311600268 clip 131.0034665634246
total norm for a microbatch 161.28712658867238 clip 130.16155918981158
total norm for a microbatch 107.10633689885707 clip 121.66868483138515
total norm for a microbatch 128.57544627270906 clip 127.7460365761102
total norm for a microbatch 196.89493174168555 clip 123.538194211158
total norm for a microbatch 140.24082804171456 clip 124.16619805501425
total norm for a microbatch 204.3487324054676 clip 126.86957943071641
total norm for a microbatch 125.5092035856304 clip 126.53649528226573
cuda
Objective function 45.45 = squared loss an data 31.72 + 0.5*rho*h**2 7.210191 + alpha*h 4.373058 + L2reg 1.89 + L1reg 0.26 ; SHD = 103 ; DAG False
Proportion of microbatches that were clipped  0.8223382884692647
iteration 2 in inner loop, alpha 3.6416384889111875 rho 10.0 h 1.2008489364342907
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 110.34 = squared loss an data 31.72 + 0.5*rho*h**2 72.101908 + alpha*h 4.373058 + L2reg 1.89 + L1reg 0.26 ; SHD = 103 ; DAG False
total norm for a microbatch 148.43869163465925 clip 2.948653418152897
total norm for a microbatch 155.57808611926384 clip 92.30126067140115
total norm for a microbatch 204.24589901692192 clip 161.5935138825056
total norm for a microbatch 317.05799825152593 clip 149.16061676797014
total norm for a microbatch 181.8045381650114 clip 147.17760051803302
total norm for a microbatch 208.63475836062645 clip 143.90094423123213
total norm for a microbatch 212.4653294930282 clip 153.6770707436161
total norm for a microbatch 191.32471136620345 clip 153.6687836216224
cuda
Objective function 49.79 = squared loss an data 35.32 + 0.5*rho*h**2 10.494149 + alpha*h 1.668343 + L2reg 2.08 + L1reg 0.22 ; SHD = 101 ; DAG True
Proportion of microbatches that were clipped  0.8247037818535952
iteration 3 in inner loop, alpha 3.6416384889111875 rho 100.0 h 0.4581298713952968
iteration 2 in outer loop, alpha = 49.454625628440866, rho = 100.0, h = 0.4581298713952968
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 70.78 = squared loss an data 35.32 + 0.5*rho*h**2 10.494149 + alpha*h 22.656641 + L2reg 2.08 + L1reg 0.22 ; SHD = 101 ; DAG True
total norm for a microbatch 180.06676476786623 clip 1.8892737511837898
total norm for a microbatch 142.80837009695 clip 4.232688379914981
total norm for a microbatch 312.13634958216795 clip 12.870221525249661
total norm for a microbatch 125.70309145250619 clip 22.389119785841025
total norm for a microbatch 217.36817019163541 clip 167.18606418729323
total norm for a microbatch 404.7100224485415 clip 178.8274596900568
total norm for a microbatch 193.79063844598969 clip 175.62175184859515
total norm for a microbatch 300.55210740897206 clip 171.86930289857864
total norm for a microbatch 115.07982572042394 clip 169.80642654896818
total norm for a microbatch 265.54254827315657 clip 177.64377708314908
total norm for a microbatch 354.54411566225457 clip 174.1223223130345
total norm for a microbatch 269.62067411483304 clip 177.5783546006272
total norm for a microbatch 234.43906330778094 clip 167.1572638754552
total norm for a microbatch 198.1545529906833 clip 167.82006815515993
total norm for a microbatch 182.65243243315183 clip 157.97744802275344
cuda
Objective function 56.73 = squared loss an data 35.90 + 0.5*rho*h**2 4.121841 + alpha*h 14.199320 + L2reg 2.29 + L1reg 0.22 ; SHD = 96 ; DAG True
Proportion of microbatches that were clipped  0.8252144359928791
iteration 1 in inner loop, alpha 49.454625628440866 rho 100.0 h 0.28711812832272443
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 93.83 = squared loss an data 35.90 + 0.5*rho*h**2 41.218410 + alpha*h 14.199320 + L2reg 2.29 + L1reg 0.22 ; SHD = 96 ; DAG True
total norm for a microbatch 149.14929368363983 clip 13.748719700092343
total norm for a microbatch 229.59745193745215 clip 31.423795728973765
total norm for a microbatch 165.5704784632823 clip 183.15901968527754
total norm for a microbatch 180.82839202079424 clip 184.80425095181243
total norm for a microbatch 242.2126735506463 clip 197.61980433362922
total norm for a microbatch 178.5952614965974 clip 177.05595576930327
cuda
Objective function 59.73 = squared loss an data 38.75 + 0.5*rho*h**2 10.994991 + alpha*h 7.333636 + L2reg 2.45 + L1reg 0.20 ; SHD = 92 ; DAG True
Proportion of microbatches that were clipped  0.8286009703332538
iteration 2 in inner loop, alpha 49.454625628440866 rho 1000.0 h 0.1482901939453427
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 158.69 = squared loss an data 38.75 + 0.5*rho*h**2 109.949908 + alpha*h 7.333636 + L2reg 2.45 + L1reg 0.20 ; SHD = 92 ; DAG True
total norm for a microbatch 684.2950793554195 clip 1.1023523130427815
total norm for a microbatch 300.9488223014439 clip 1.724667458972577
total norm for a microbatch 344.91856184746797 clip 5.069901930891112
total norm for a microbatch 313.6902385174294 clip 24.459772588059636
total norm for a microbatch 463.45707815401767 clip 284.121908176515
total norm for a microbatch 317.34434342832475 clip 287.9359394893322
total norm for a microbatch 325.280373546315 clip 297.23846960385816
total norm for a microbatch 213.19976556720738 clip 216.2933642840118
total norm for a microbatch 307.5857558018248 clip 224.46402598434227
total norm for a microbatch 256.18529816630667 clip 229.00122428117933
total norm for a microbatch 277.22974892648205 clip 228.6130157050007
total norm for a microbatch 366.36410629692176 clip 197.05358326698516
cuda
Objective function 55.30 = squared loss an data 39.12 + 0.5*rho*h**2 11.069572 + alpha*h 2.326951 + L2reg 2.60 + L1reg 0.19 ; SHD = 89 ; DAG True
Proportion of microbatches that were clipped  0.8283754685381609
iteration 3 in inner loop, alpha 49.454625628440866 rho 10000.0 h 0.04705225143649017
iteration 3 in outer loop, alpha = 519.9771399933426, rho = 10000.0, h = 0.04705225143649017
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 77.44 = squared loss an data 39.12 + 0.5*rho*h**2 11.069572 + alpha*h 24.466095 + L2reg 2.60 + L1reg 0.19 ; SHD = 89 ; DAG True
total norm for a microbatch 398.5787178980897 clip 25.04474568717045
total norm for a microbatch 644.5695481624206 clip 61.003513761105026
total norm for a microbatch 508.96717320848876 clip 167.70596421678044
total norm for a microbatch 369.1282858381555 clip 365.49589788336516
total norm for a microbatch 304.210461415445 clip 332.30866440639966
total norm for a microbatch 313.62622789533634 clip 281.28130066886513
total norm for a microbatch 305.5258625638477 clip 259.89530967832013
total norm for a microbatch 201.8679873070585 clip 226.14593797267992
total norm for a microbatch 294.99228833021374 clip 226.14593797267992
total norm for a microbatch 246.83661788795214 clip 240.78384466247772
total norm for a microbatch 279.10643453350025 clip 250.84739242761424
total norm for a microbatch 459.1767638675334 clip 247.23682638015296
cuda
Objective function 64.58 = squared loss an data 38.70 + 0.5*rho*h**2 5.585610 + alpha*h 17.379390 + L2reg 2.71 + L1reg 0.20 ; SHD = 90 ; DAG True
Proportion of microbatches that were clipped  0.8293789808917198
iteration 1 in inner loop, alpha 519.9771399933426 rho 10000.0 h 0.0334233735970777
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 114.85 = squared loss an data 38.70 + 0.5*rho*h**2 55.856095 + alpha*h 17.379390 + L2reg 2.71 + L1reg 0.20 ; SHD = 90 ; DAG True
total norm for a microbatch 476.78852932297616 clip 7.1683676437658574
total norm for a microbatch 638.8261196851761 clip 73.62502321720677
total norm for a microbatch 705.7237350189207 clip 169.08077693103417
total norm for a microbatch 1082.1076312092057 clip 928.6612822164898
total norm for a microbatch 605.703948334252 clip 451.96033871773733
total norm for a microbatch 375.88442252460163 clip 401.87676300375966
total norm for a microbatch 297.1782906072742 clip 230.23726918073112
total norm for a microbatch 205.08767272874803 clip 229.8063044256274
total norm for a microbatch 287.55003192369384 clip 224.97961586173406
total norm for a microbatch 379.57120505248497 clip 210.7927322309504
total norm for a microbatch 310.361784994328 clip 219.81203963368975
cuda
Objective function 50.54 = squared loss an data 40.34 + 0.5*rho*h**2 3.090723 + alpha*h 4.088174 + L2reg 2.82 + L1reg 0.20 ; SHD = 92 ; DAG True
Proportion of microbatches that were clipped  0.8283624085537422
iteration 2 in inner loop, alpha 519.9771399933426 rho 100000.0 h 0.007862217838056296
iteration 4 in outer loop, alpha = 1306.1989237989721, rho = 100000.0, h = 0.007862217838056296
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 56.72 = squared loss an data 40.34 + 0.5*rho*h**2 3.090723 + alpha*h 10.269620 + L2reg 2.82 + L1reg 0.20 ; SHD = 92 ; DAG True
total norm for a microbatch 599.6233305459679 clip 14.910992334882119
total norm for a microbatch 571.5830204571706 clip 16.170435013909188
total norm for a microbatch 606.2883666607526 clip 23.28079197829579
total norm for a microbatch 733.4505199246131 clip 28.18538658198441
total norm for a microbatch 1557.9671499348992 clip 678.2505316234354
total norm for a microbatch 1480.7820378448953 clip 1263.4750714756124
total norm for a microbatch 1729.1781810939449 clip 2135.803372776485
total norm for a microbatch 739.9155821271834 clip 688.9402806900733
total norm for a microbatch 478.8260971603628 clip 395.1312645764368
total norm for a microbatch 315.63505076783025 clip 248.41286276910895
total norm for a microbatch 243.56920749223724 clip 215.46845081281947
cuda
Objective function 52.61 = squared loss an data 43.66 + 0.5*rho*h**2 0.746620 + alpha*h 5.047474 + L2reg 2.94 + L1reg 0.21 ; SHD = 86 ; DAG True
Proportion of microbatches that were clipped  0.827663491555271
iteration 1 in inner loop, alpha 1306.1989237989721 rho 100000.0 h 0.0038642462863407445
iteration 5 in outer loop, alpha = 5170.445210139716, rho = 1000000.0, h = 0.0038642462863407445
Threshold 0.3
[[0.005 1.62  0.432 0.826 1.284 0.669 1.082 0.862 0.52  0.967 0.392 0.928
  0.974 0.834 0.776 1.239 0.873 0.97  0.909 1.047]
 [0.002 0.003 0.051 0.786 0.564 0.388 0.718 0.181 0.028 0.558 0.027 0.27
  0.424 0.259 0.578 0.439 0.254 0.312 0.113 0.254]
 [0.009 0.07  0.004 0.589 0.805 0.052 0.095 0.257 0.027 0.391 0.015 0.246
  0.179 0.214 0.441 0.452 0.168 0.154 0.012 0.037]
 [0.001 0.003 0.006 0.003 0.044 0.004 0.028 0.007 0.009 0.018 0.003 0.012
  0.011 0.008 0.015 0.11  0.019 0.007 0.003 0.005]
 [0.001 0.006 0.003 0.094 0.003 0.004 0.013 0.007 0.003 0.05  0.004 0.006
  0.018 0.013 0.08  0.109 0.01  0.013 0.007 0.012]
 [0.006 0.016 0.082 0.662 1.091 0.005 0.255 0.101 0.024 0.391 0.02  0.241
  0.318 0.345 0.524 1.097 0.873 0.559 0.137 0.132]
 [0.002 0.008 0.035 0.134 0.342 0.016 0.004 0.035 0.009 0.114 0.003 0.051
  0.021 0.025 0.202 0.15  0.015 0.01  0.005 0.017]
 [0.004 0.022 0.014 0.639 0.706 0.041 0.125 0.005 0.014 0.397 0.01  0.152
  0.179 0.082 0.536 0.373 0.092 0.12  0.011 0.021]
 [0.009 0.18  0.142 0.626 0.702 0.214 0.48  0.398 0.003 0.443 0.007 0.161
  0.446 0.257 0.354 0.705 0.256 0.436 0.144 0.148]
 [0.002 0.008 0.012 0.255 0.108 0.011 0.024 0.006 0.009 0.003 0.002 0.028
  0.032 0.009 0.048 0.091 0.006 0.011 0.004 0.01 ]
 [0.01  0.167 0.347 1.202 0.881 0.175 1.021 0.471 0.566 1.275 0.005 1.021
  0.675 0.359 0.538 0.356 0.367 0.449 0.072 0.565]
 [0.004 0.013 0.017 0.306 0.62  0.021 0.094 0.022 0.03  0.173 0.003 0.004
  0.07  0.024 0.264 0.379 0.042 0.073 0.017 0.023]
 [0.003 0.009 0.021 0.387 0.191 0.019 0.218 0.022 0.009 0.09  0.005 0.074
  0.002 0.01  0.131 0.103 0.017 0.103 0.017 0.054]
 [0.005 0.017 0.02  0.574 0.206 0.013 0.228 0.06  0.017 0.536 0.011 0.17
  0.341 0.004 0.254 0.127 0.041 0.048 0.039 0.095]
 [0.003 0.006 0.009 0.339 0.054 0.01  0.022 0.007 0.01  0.138 0.004 0.017
  0.034 0.019 0.004 0.048 0.014 0.014 0.012 0.01 ]
 [0.002 0.01  0.008 0.051 0.047 0.003 0.034 0.014 0.006 0.038 0.005 0.006
  0.044 0.018 0.106 0.004 0.008 0.017 0.003 0.008]
 [0.004 0.015 0.025 0.242 0.465 0.008 0.255 0.056 0.007 0.563 0.014 0.112
  0.178 0.135 0.386 0.739 0.003 0.075 0.021 0.034]
 [0.003 0.009 0.036 0.536 0.352 0.007 0.208 0.038 0.008 0.294 0.01  0.054
  0.053 0.111 0.247 0.269 0.064 0.003 0.009 0.067]
 [0.003 0.051 0.252 0.738 0.607 0.045 0.564 0.325 0.034 0.682 0.051 0.289
  0.334 0.135 0.346 1.087 0.242 0.436 0.002 0.241]
 [0.003 0.017 0.103 0.617 0.431 0.031 0.206 0.263 0.016 0.404 0.006 0.156
  0.097 0.053 0.347 0.483 0.158 0.054 0.024 0.004]]
[[0.    1.62  0.432 0.826 1.284 0.669 1.082 0.862 0.52  0.967 0.392 0.928
  0.974 0.834 0.776 1.239 0.873 0.97  0.909 1.047]
 [0.    0.    0.    0.786 0.564 0.388 0.718 0.    0.    0.558 0.    0.
  0.424 0.    0.578 0.439 0.    0.312 0.    0.   ]
 [0.    0.    0.    0.589 0.805 0.    0.    0.    0.    0.391 0.    0.
  0.    0.    0.441 0.452 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.662 1.091 0.    0.    0.    0.    0.391 0.    0.
  0.318 0.345 0.524 1.097 0.873 0.559 0.    0.   ]
 [0.    0.    0.    0.    0.342 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.639 0.706 0.    0.    0.    0.    0.397 0.    0.
  0.    0.    0.536 0.373 0.    0.    0.    0.   ]
 [0.    0.    0.    0.626 0.702 0.    0.48  0.398 0.    0.443 0.    0.
  0.446 0.    0.354 0.705 0.    0.436 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.347 1.202 0.881 0.    1.021 0.471 0.566 1.275 0.    1.021
  0.675 0.359 0.538 0.356 0.367 0.449 0.    0.565]
 [0.    0.    0.    0.306 0.62  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.379 0.    0.    0.    0.   ]
 [0.    0.    0.    0.387 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.574 0.    0.    0.    0.    0.    0.536 0.    0.
  0.341 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.339 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.465 0.    0.    0.    0.    0.563 0.    0.
  0.    0.    0.386 0.739 0.    0.    0.    0.   ]
 [0.    0.    0.    0.536 0.352 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.738 0.607 0.    0.564 0.325 0.    0.682 0.    0.
  0.334 0.    0.346 1.087 0.    0.436 0.    0.   ]
 [0.    0.    0.    0.617 0.431 0.    0.    0.    0.    0.404 0.    0.
  0.    0.    0.347 0.483 0.    0.    0.    0.   ]]
{'fdr': 0.58, 'tpr': 0.525, 'fpr': 0.5272727272727272, 'f1': 0.4666666666666667, 'shd': 86, 'npred': 100, 'ntrue': 80}
[1.620e+00 4.320e-01 8.259e-01 1.284e+00 6.692e-01 1.082e+00 8.621e-01
 5.201e-01 9.669e-01 3.919e-01 9.281e-01 9.740e-01 8.336e-01 7.760e-01
 1.239e+00 8.725e-01 9.696e-01 9.087e-01 1.047e+00 1.574e-03 5.114e-02
 7.865e-01 5.639e-01 3.876e-01 7.185e-01 1.815e-01 2.780e-02 5.583e-01
 2.717e-02 2.698e-01 4.238e-01 2.595e-01 5.780e-01 4.394e-01 2.540e-01
 3.123e-01 1.130e-01 2.535e-01 8.610e-03 7.037e-02 5.892e-01 8.054e-01
 5.227e-02 9.526e-02 2.572e-01 2.744e-02 3.915e-01 1.523e-02 2.461e-01
 1.795e-01 2.144e-01 4.406e-01 4.520e-01 1.683e-01 1.537e-01 1.250e-02
 3.679e-02 1.493e-03 3.378e-03 5.847e-03 4.427e-02 3.863e-03 2.807e-02
 6.672e-03 8.543e-03 1.786e-02 2.881e-03 1.234e-02 1.066e-02 8.322e-03
 1.475e-02 1.103e-01 1.942e-02 7.155e-03 2.738e-03 4.967e-03 1.228e-03
 5.512e-03 3.095e-03 9.394e-02 3.709e-03 1.258e-02 7.021e-03 3.383e-03
 5.028e-02 3.757e-03 6.424e-03 1.845e-02 1.348e-02 8.016e-02 1.091e-01
 1.040e-02 1.267e-02 6.552e-03 1.169e-02 6.126e-03 1.645e-02 8.182e-02
 6.617e-01 1.091e+00 2.549e-01 1.009e-01 2.439e-02 3.908e-01 1.965e-02
 2.410e-01 3.182e-01 3.450e-01 5.244e-01 1.097e+00 8.734e-01 5.593e-01
 1.373e-01 1.320e-01 1.960e-03 7.533e-03 3.523e-02 1.338e-01 3.420e-01
 1.616e-02 3.458e-02 9.457e-03 1.143e-01 3.204e-03 5.055e-02 2.056e-02
 2.511e-02 2.021e-01 1.501e-01 1.498e-02 1.043e-02 5.481e-03 1.687e-02
 3.877e-03 2.206e-02 1.413e-02 6.391e-01 7.058e-01 4.058e-02 1.252e-01
 1.436e-02 3.970e-01 9.604e-03 1.523e-01 1.793e-01 8.202e-02 5.357e-01
 3.735e-01 9.218e-02 1.204e-01 1.081e-02 2.082e-02 9.312e-03 1.800e-01
 1.419e-01 6.258e-01 7.020e-01 2.137e-01 4.800e-01 3.978e-01 4.432e-01
 6.872e-03 1.605e-01 4.455e-01 2.575e-01 3.537e-01 7.047e-01 2.561e-01
 4.364e-01 1.436e-01 1.478e-01 1.852e-03 8.460e-03 1.238e-02 2.551e-01
 1.083e-01 1.060e-02 2.374e-02 5.982e-03 9.236e-03 2.107e-03 2.773e-02
 3.186e-02 8.592e-03 4.776e-02 9.127e-02 6.264e-03 1.143e-02 4.041e-03
 1.030e-02 1.023e-02 1.669e-01 3.475e-01 1.202e+00 8.813e-01 1.749e-01
 1.021e+00 4.711e-01 5.663e-01 1.275e+00 1.021e+00 6.753e-01 3.591e-01
 5.381e-01 3.564e-01 3.672e-01 4.489e-01 7.172e-02 5.648e-01 4.072e-03
 1.321e-02 1.663e-02 3.062e-01 6.196e-01 2.144e-02 9.414e-02 2.225e-02
 3.036e-02 1.726e-01 2.847e-03 7.041e-02 2.424e-02 2.638e-01 3.790e-01
 4.155e-02 7.264e-02 1.652e-02 2.312e-02 3.131e-03 9.377e-03 2.150e-02
 3.873e-01 1.913e-01 1.938e-02 2.183e-01 2.225e-02 8.501e-03 9.041e-02
 4.630e-03 7.365e-02 1.036e-02 1.307e-01 1.029e-01 1.702e-02 1.032e-01
 1.696e-02 5.355e-02 4.742e-03 1.686e-02 2.042e-02 5.741e-01 2.055e-01
 1.323e-02 2.281e-01 5.960e-02 1.707e-02 5.361e-01 1.067e-02 1.705e-01
 3.407e-01 2.544e-01 1.271e-01 4.087e-02 4.816e-02 3.932e-02 9.497e-02
 2.767e-03 6.335e-03 9.361e-03 3.392e-01 5.422e-02 1.016e-02 2.174e-02
 7.193e-03 9.815e-03 1.378e-01 4.496e-03 1.651e-02 3.353e-02 1.888e-02
 4.816e-02 1.383e-02 1.406e-02 1.157e-02 9.985e-03 2.156e-03 9.859e-03
 8.072e-03 5.092e-02 4.651e-02 2.511e-03 3.439e-02 1.412e-02 6.098e-03
 3.774e-02 4.900e-03 5.841e-03 4.428e-02 1.828e-02 1.065e-01 7.591e-03
 1.650e-02 3.222e-03 8.017e-03 4.291e-03 1.462e-02 2.527e-02 2.420e-01
 4.645e-01 7.640e-03 2.555e-01 5.611e-02 6.815e-03 5.633e-01 1.381e-02
 1.118e-01 1.778e-01 1.349e-01 3.855e-01 7.389e-01 7.478e-02 2.085e-02
 3.362e-02 2.617e-03 8.559e-03 3.579e-02 5.357e-01 3.520e-01 6.948e-03
 2.082e-01 3.816e-02 7.558e-03 2.943e-01 1.000e-02 5.395e-02 5.250e-02
 1.110e-01 2.466e-01 2.687e-01 6.404e-02 9.229e-03 6.738e-02 3.152e-03
 5.053e-02 2.517e-01 7.377e-01 6.066e-01 4.470e-02 5.636e-01 3.248e-01
 3.365e-02 6.815e-01 5.059e-02 2.887e-01 3.345e-01 1.346e-01 3.465e-01
 1.087e+00 2.415e-01 4.359e-01 2.408e-01 2.939e-03 1.692e-02 1.027e-01
 6.174e-01 4.313e-01 3.092e-02 2.064e-01 2.627e-01 1.634e-02 4.039e-01
 5.959e-03 1.558e-01 9.748e-02 5.269e-02 3.474e-01 4.834e-01 1.583e-01
 5.355e-02 2.370e-02]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.724625, 0.4236428950003447)
Iterations 2500
Achieves (3.6993101023104464, 1e-05)-DP
