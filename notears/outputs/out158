samples  5000  graph  20 40 ER mlp  minibatch size  100  noise  0.6  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617292529427
iteration 1 in outer loop, alpha = 1.5634617292529427, rho = 1.0, h = 1.5634617292529427
cuda
iteration 1 in inner loop,alpha 1.5634617292529427 rho 1.0 h 1.037186820631952
iteration 2 in inner loop,alpha 1.5634617292529427 rho 10.0 h 0.44475090592949584
iteration 3 in inner loop,alpha 1.5634617292529427 rho 100.0 h 0.1396834084355092
iteration 2 in outer loop, alpha = 15.531802572803862, rho = 100.0, h = 0.1396834084355092
cuda
iteration 1 in inner loop,alpha 15.531802572803862 rho 100.0 h 0.07298852520452925
iteration 2 in inner loop,alpha 15.531802572803862 rho 1000.0 h 0.024047902613645533
iteration 3 in outer loop, alpha = 39.579705186449395, rho = 1000.0, h = 0.024047902613645533
cuda
iteration 1 in inner loop,alpha 39.579705186449395 rho 1000.0 h 0.009690280225633785
iteration 2 in inner loop,alpha 39.579705186449395 rho 10000.0 h 0.003425547527854178
iteration 4 in outer loop, alpha = 73.83518046499117, rho = 10000.0, h = 0.003425547527854178
cuda
iteration 1 in inner loop,alpha 73.83518046499117 rho 10000.0 h 0.001549462198429552
iteration 2 in inner loop,alpha 73.83518046499117 rho 100000.0 h 0.0005128523292015075
iteration 5 in outer loop, alpha = 125.12041338514192, rho = 100000.0, h = 0.0005128523292015075
cuda
iteration 1 in inner loop,alpha 125.12041338514192 rho 100000.0 h 0.0002608541649351537
iteration 6 in outer loop, alpha = 385.97457832029556, rho = 1000000.0, h = 0.0002608541649351537
Threshold 0.3
[[0.    0.011 0.023 2.286 0.037 0.032 0.041 0.003 1.993 0.014 1.83  0.002
  0.105 0.002 0.221 0.939 0.002 0.002 0.032 0.216]
 [0.028 0.005 0.204 0.058 0.391 0.121 0.032 0.002 3.051 0.053 0.017 0.009
  0.612 0.002 1.869 1.572 0.282 0.    0.227 0.303]
 [0.003 0.    0.003 0.072 0.132 1.125 0.    0.    2.29  0.077 0.061 0.
  2.247 0.001 1.324 0.856 0.001 0.    3.75  2.165]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.001 0.    0.    0.
  0.    0.    0.092 1.485 0.    0.    0.    0.001]
 [0.    0.    0.    1.667 0.001 0.18  0.001 0.    0.001 0.039 0.021 0.
  0.38  0.001 0.177 0.35  0.001 0.    0.035 0.178]
 [0.001 0.    0.    0.118 0.003 0.001 0.    0.    0.003 0.047 0.063 0.
  0.264 0.    1.925 0.456 0.    0.    0.001 0.098]
 [0.002 0.018 0.553 0.055 0.21  0.105 0.005 0.    0.034 0.002 0.017 0.
  0.039 0.002 0.049 1.634 0.009 0.001 0.056 0.026]
 [0.002 0.012 0.552 0.049 0.184 0.101 4.446 0.001 0.146 0.01  0.002 0.045
  0.013 0.003 0.131 0.24  0.065 0.007 0.062 0.047]
 [0.    0.    0.    0.061 0.975 0.117 0.001 0.    0.002 0.053 0.045 0.
  0.276 0.    0.444 0.518 0.001 0.    0.017 0.249]
 [0.    0.    0.    0.095 0.    0.    0.    0.    0.    0.001 0.826 0.
  0.    0.    0.211 0.238 0.    0.    0.    0.178]
 [0.    0.    0.    0.101 0.001 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    1.265 0.272 0.    0.    0.    2.278]
 [0.002 0.001 2.648 0.067 0.018 0.21  3.574 0.006 0.22  0.014 0.013 0.001
  0.223 0.003 0.613 0.352 0.008 0.034 0.211 0.15 ]
 [0.001 0.    0.    0.116 0.    0.001 0.    0.    0.001 3.184 0.13  0.
  0.002 0.    0.283 0.572 0.    0.    0.    0.409]
 [0.002 0.009 0.018 0.052 0.03  0.116 0.003 0.013 0.095 0.023 0.077 0.026
  0.283 0.    0.193 0.151 0.008 0.023 2.625 0.244]
 [0.    0.    0.    0.011 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.004 0.275 0.    0.    0.    0.005]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.004 0.009 0.    0.    0.    0.   ]
 [0.006 0.006 0.309 0.063 0.332 0.214 0.032 0.002 0.269 0.058 0.054 0.009
  0.416 0.002 1.877 1.492 0.003 0.    0.321 0.438]
 [0.002 4.259 0.014 0.056 0.22  0.009 0.015 0.004 0.59  0.002 0.002 0.004
  1.317 0.003 0.217 0.231 3.001 0.001 1.412 0.976]
 [0.004 0.001 0.    0.062 0.015 1.2   0.    0.    0.083 0.058 0.015 0.
  1.045 0.    0.402 1.342 0.    0.    0.003 0.482]
 [0.    0.    0.    2.082 0.    0.001 0.    0.    0.001 0.    0.    0.
  0.    0.    0.268 0.328 0.    0.    0.    0.003]]
[[0.    0.    0.    2.286 0.    0.    0.    0.    1.993 0.    1.83  0.
  0.    0.    0.    0.939 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.391 0.    0.    0.    3.051 0.    0.    0.
  0.612 0.    1.869 1.572 0.    0.    0.    0.303]
 [0.    0.    0.    0.    0.    1.125 0.    0.    2.29  0.    0.    0.
  2.247 0.    1.324 0.856 0.    0.    3.75  2.165]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.485 0.    0.    0.    0.   ]
 [0.    0.    0.    1.667 0.    0.    0.    0.    0.    0.    0.    0.
  0.38  0.    0.    0.35  0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.925 0.456 0.    0.    0.    0.   ]
 [0.    0.    0.553 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.634 0.    0.    0.    0.   ]
 [0.    0.    0.552 0.    0.    0.    4.446 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.975 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.444 0.518 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.826 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.265 0.    0.    0.    0.    2.278]
 [0.    0.    2.648 0.    0.    0.    3.574 0.    0.    0.    0.    0.
  0.    0.    0.613 0.352 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.184 0.    0.
  0.    0.    0.    0.572 0.    0.    0.    0.409]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.625 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.309 0.    0.332 0.    0.    0.    0.    0.    0.    0.
  0.416 0.    1.877 1.492 0.    0.    0.321 0.438]
 [0.    4.259 0.    0.    0.    0.    0.    0.    0.59  0.    0.    0.
  1.317 0.    0.    0.    3.001 0.    1.412 0.976]
 [0.    0.    0.    0.    0.    1.2   0.    0.    0.    0.    0.    0.
  1.045 0.    0.402 1.342 0.    0.    0.    0.482]
 [0.    0.    0.    2.082 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.328 0.    0.    0.    0.   ]]
{'fdr': 0.3442622950819672, 'tpr': 1.0, 'fpr': 0.14, 'f1': 0.7920792079207921, 'shd': 21, 'npred': 61, 'ntrue': 40}
[1.136e-02 2.336e-02 2.286e+00 3.651e-02 3.161e-02 4.084e-02 2.822e-03
 1.993e+00 1.405e-02 1.830e+00 1.718e-03 1.047e-01 2.187e-03 2.210e-01
 9.392e-01 1.605e-03 1.922e-03 3.233e-02 2.163e-01 2.838e-02 2.043e-01
 5.806e-02 3.911e-01 1.211e-01 3.198e-02 2.028e-03 3.051e+00 5.340e-02
 1.677e-02 8.526e-03 6.119e-01 1.928e-03 1.869e+00 1.572e+00 2.823e-01
 4.111e-04 2.266e-01 3.033e-01 2.949e-03 3.584e-04 7.243e-02 1.316e-01
 1.125e+00 4.965e-04 7.563e-05 2.290e+00 7.681e-02 6.130e-02 3.956e-04
 2.247e+00 1.466e-03 1.324e+00 8.559e-01 5.231e-04 3.452e-04 3.750e+00
 2.165e+00 1.365e-05 1.301e-04 2.057e-04 6.011e-04 1.345e-03 1.679e-04
 5.481e-05 5.277e-04 2.280e-05 6.374e-05 1.106e-04 6.827e-05 1.355e-04
 9.198e-02 1.485e+00 2.683e-04 1.041e-04 2.916e-04 1.117e-03 5.001e-05
 1.375e-04 1.822e-04 1.667e+00 1.804e-01 6.404e-04 1.430e-05 7.307e-04
 3.929e-02 2.053e-02 7.014e-05 3.796e-01 1.206e-03 1.772e-01 3.501e-01
 6.780e-04 3.583e-05 3.512e-02 1.778e-01 5.310e-04 2.405e-04 8.789e-05
 1.183e-01 2.760e-03 1.358e-04 1.692e-05 2.942e-03 4.729e-02 6.330e-02
 3.883e-05 2.636e-01 4.203e-06 1.925e+00 4.563e-01 2.835e-04 2.291e-05
 7.231e-04 9.787e-02 2.019e-03 1.773e-02 5.529e-01 5.496e-02 2.102e-01
 1.053e-01 2.006e-04 3.445e-02 2.432e-03 1.667e-02 2.848e-05 3.908e-02
 1.614e-03 4.929e-02 1.634e+00 9.026e-03 1.077e-03 5.555e-02 2.597e-02
 2.370e-03 1.181e-02 5.517e-01 4.861e-02 1.837e-01 1.012e-01 4.446e+00
 1.457e-01 1.016e-02 2.096e-03 4.520e-02 1.321e-02 3.098e-03 1.315e-01
 2.400e-01 6.537e-02 7.385e-03 6.201e-02 4.651e-02 5.920e-05 2.000e-04
 3.755e-04 6.119e-02 9.752e-01 1.172e-01 6.332e-04 3.023e-05 5.278e-02
 4.500e-02 9.659e-05 2.757e-01 4.498e-04 4.439e-01 5.175e-01 5.181e-04
 4.004e-05 1.731e-02 2.495e-01 4.851e-05 2.863e-05 4.892e-05 9.529e-02
 1.165e-04 2.600e-04 2.760e-05 2.153e-05 1.668e-04 8.263e-01 2.859e-05
 1.434e-04 2.870e-05 2.105e-01 2.376e-01 9.203e-05 1.408e-05 3.774e-05
 1.782e-01 7.247e-05 6.687e-05 1.626e-05 1.014e-01 5.413e-04 2.276e-04
 4.415e-04 1.674e-05 2.106e-04 1.037e-04 3.992e-05 1.185e-05 2.675e-05
 1.265e+00 2.725e-01 3.153e-04 6.066e-06 5.786e-05 2.278e+00 2.224e-03
 1.321e-03 2.648e+00 6.696e-02 1.794e-02 2.099e-01 3.574e+00 6.356e-03
 2.196e-01 1.374e-02 1.271e-02 2.229e-01 2.942e-03 6.126e-01 3.516e-01
 8.023e-03 3.370e-02 2.110e-01 1.495e-01 1.108e-03 8.936e-05 1.545e-04
 1.157e-01 4.306e-04 1.119e-03 5.741e-05 1.822e-05 5.754e-04 3.184e+00
 1.302e-01 5.383e-05 7.078e-06 2.829e-01 5.722e-01 3.218e-04 4.538e-05
 2.176e-04 4.089e-01 1.713e-03 8.845e-03 1.848e-02 5.188e-02 3.030e-02
 1.156e-01 2.721e-03 1.347e-02 9.493e-02 2.313e-02 7.669e-02 2.619e-02
 2.834e-01 1.935e-01 1.514e-01 7.719e-03 2.312e-02 2.625e+00 2.444e-01
 1.911e-05 3.799e-04 9.962e-05 1.062e-02 5.306e-04 1.253e-04 1.702e-04
 1.281e-05 2.071e-04 2.219e-04 2.091e-04 3.714e-05 3.204e-05 1.439e-05
 2.754e-01 3.666e-04 7.855e-05 4.447e-05 4.553e-03 1.586e-05 2.098e-04
 5.195e-05 1.401e-04 3.759e-05 1.676e-04 4.018e-04 4.176e-05 9.153e-05
 9.139e-05 1.007e-04 1.948e-05 3.885e-05 1.369e-05 3.882e-03 2.622e-05
 3.414e-05 4.652e-05 9.130e-05 5.912e-03 6.060e-03 3.085e-01 6.335e-02
 3.318e-01 2.135e-01 3.208e-02 2.159e-03 2.686e-01 5.760e-02 5.416e-02
 8.958e-03 4.161e-01 1.677e-03 1.877e+00 1.492e+00 4.445e-04 3.210e-01
 4.382e-01 2.002e-03 4.259e+00 1.377e-02 5.569e-02 2.201e-01 9.113e-03
 1.506e-02 3.551e-03 5.902e-01 1.985e-03 2.039e-03 3.777e-03 1.317e+00
 2.806e-03 2.173e-01 2.311e-01 3.001e+00 1.412e+00 9.756e-01 3.890e-03
 1.108e-03 4.223e-04 6.249e-02 1.483e-02 1.200e+00 1.294e-04 2.838e-05
 8.293e-02 5.843e-02 1.550e-02 1.069e-04 1.045e+00 1.256e-05 4.017e-01
 1.342e+00 1.249e-04 1.178e-04 4.817e-01 4.243e-05 1.097e-04 1.905e-04
 2.082e+00 4.052e-04 1.252e-03 2.512e-04 4.838e-05 5.625e-04 1.538e-05
 4.779e-05 8.175e-05 2.830e-05 1.436e-05 2.681e-01 3.284e-01 1.095e-04
 1.308e-04 1.199e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9991176470588236, 0.9923891402022302)
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
total norm for a microbatch 62.64864252973815 clip 9.735638618462225
total norm for a microbatch 68.1564375952546 clip 24.671505942188162
total norm for a microbatch 53.36087477556872 clip 26.799095292333973
total norm for a microbatch 61.169197247799964 clip 52.290117996177266
total norm for a microbatch 34.640332994148366 clip 48.04786188539931
cuda
Objective function 130.59 = squared loss an data 127.93 + 0.5*rho*h**2 1.893189 + alpha*h 0.000000 + L2reg 0.50 + L1reg 0.27 ; SHD = 109 ; DAG False
Proportion of microbatches that were clipped  0.92982176957352
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.945861532468136
iteration 1 in outer loop, alpha = 1.945861532468136, rho = 1.0, h = 1.945861532468136
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 134.38 = squared loss an data 127.93 + 0.5*rho*h**2 1.893189 + alpha*h 3.786377 + L2reg 0.50 + L1reg 0.27 ; SHD = 109 ; DAG False
total norm for a microbatch 66.26319225610428 clip 5.796128526196119
total norm for a microbatch 77.29487279621509 clip 7.00071832024005
total norm for a microbatch 90.73662974521841 clip 38.02246464080932
total norm for a microbatch 125.58296581783632 clip 57.77242627049642
total norm for a microbatch 107.73705266481697 clip 69.42678263216668
cuda
Objective function 55.03 = squared loss an data 47.21 + 0.5*rho*h**2 2.351405 + alpha*h 4.219789 + L2reg 0.99 + L1reg 0.25 ; SHD = 84 ; DAG False
Proportion of microbatches that were clipped  0.9498864742134285
iteration 1 in inner loop, alpha 1.945861532468136 rho 1.0 h 2.16859654502273
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 76.19 = squared loss an data 47.21 + 0.5*rho*h**2 23.514055 + alpha*h 4.219789 + L2reg 0.99 + L1reg 0.25 ; SHD = 84 ; DAG False
total norm for a microbatch 150.83179859863623 clip 3.856639643488005
total norm for a microbatch 107.87668714909208 clip 20.829888840481726
total norm for a microbatch 81.5411346714655 clip 39.41742534075676
total norm for a microbatch 81.10900502876397 clip 85.77939579947
total norm for a microbatch 194.08024108484076 clip 112.28197681063779
cuda
Objective function 37.28 = squared loss an data 29.37 + 0.5*rho*h**2 4.520387 + alpha*h 1.850183 + L2reg 1.32 + L1reg 0.23 ; SHD = 54 ; DAG False
Proportion of microbatches that were clipped  0.9621485786084498
iteration 2 in inner loop, alpha 1.945861532468136 rho 10.0 h 0.9508298456358517
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 77.97 = squared loss an data 29.37 + 0.5*rho*h**2 45.203870 + alpha*h 1.850183 + L2reg 1.32 + L1reg 0.23 ; SHD = 54 ; DAG False
total norm for a microbatch 178.70320813867357 clip 1.7191142432582052
total norm for a microbatch 193.76655799125987 clip 23.116110703380883
total norm for a microbatch 159.55436360670723 clip 32.907788644336755
total norm for a microbatch 168.5482922036053 clip 131.3249797414784
cuda
Objective function 37.45 = squared loss an data 29.63 + 0.5*rho*h**2 5.416154 + alpha*h 0.640431 + L2reg 1.56 + L1reg 0.21 ; SHD = 54 ; DAG False
Proportion of microbatches that were clipped  0.9726605217934697
iteration 3 in inner loop, alpha 1.945861532468136 rho 100.0 h 0.32912471265314736
iteration 2 in outer loop, alpha = 34.85833279778287, rho = 100.0, h = 0.32912471265314736
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 48.28 = squared loss an data 29.63 + 0.5*rho*h**2 5.416154 + alpha*h 11.472739 + L2reg 1.56 + L1reg 0.21 ; SHD = 54 ; DAG False
total norm for a microbatch 245.73708398237412 clip 1.7234391714497566
total norm for a microbatch 233.8833581170155 clip 1.7234391714497566
total norm for a microbatch 121.0407664581815 clip 2.4961687087424425
total norm for a microbatch 128.41684063840034 clip 6.133355975213887
total norm for a microbatch 156.6083076625108 clip 7.901057120076323
total norm for a microbatch 141.81204024432034 clip 13.659422633934094
total norm for a microbatch 178.95969537676237 clip 16.47420604951095
total norm for a microbatch 226.65337488823303 clip 90.77852628093751
total norm for a microbatch 173.13254695776192 clip 147.22356572614564
total norm for a microbatch 160.12368161643002 clip 150.76711102593336
cuda
Objective function 39.45 = squared loss an data 28.33 + 0.5*rho*h**2 2.063503 + alpha*h 7.081482 + L2reg 1.77 + L1reg 0.20 ; SHD = 49 ; DAG True
Proportion of microbatches that were clipped  0.9707850497796637
iteration 1 in inner loop, alpha 34.85833279778287 rho 100.0 h 0.20315033110634673
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 58.02 = squared loss an data 28.33 + 0.5*rho*h**2 20.635029 + alpha*h 7.081482 + L2reg 1.77 + L1reg 0.20 ; SHD = 49 ; DAG True
total norm for a microbatch 262.02177278808387 clip 2.08949831268089
total norm for a microbatch 187.29946433063841 clip 6.143904674117205
total norm for a microbatch 231.75994301169217 clip 13.769505993346549
cuda
Objective function 36.81 = squared loss an data 27.69 + 0.5*rho*h**2 3.898091 + alpha*h 3.077851 + L2reg 1.95 + L1reg 0.20 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.9711461820500558
iteration 2 in inner loop, alpha 34.85833279778287 rho 1000.0 h 0.08829598735162847
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 71.90 = squared loss an data 27.69 + 0.5*rho*h**2 38.980907 + alpha*h 3.077851 + L2reg 1.95 + L1reg 0.20 ; SHD = 51 ; DAG True
total norm for a microbatch 259.2317705748983 clip 5.06717997469573
total norm for a microbatch 193.59120553895033 clip 47.46975445010847
total norm for a microbatch 197.7262699324868 clip 52.02686497924699
total norm for a microbatch 208.37705697221185 clip 108.16866203959832
total norm for a microbatch 164.0695414211192 clip 150.96719307558243
cuda
Objective function 34.24 = squared loss an data 26.13 + 0.5*rho*h**2 4.756065 + alpha*h 1.075092 + L2reg 2.07 + L1reg 0.20 ; SHD = 58 ; DAG True
Proportion of microbatches that were clipped  0.9767479412239626
iteration 3 in inner loop, alpha 34.85833279778287 rho 10000.0 h 0.030841741345447815
iteration 3 in outer loop, alpha = 343.275746252261, rho = 10000.0, h = 0.030841741345447815
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 43.75 = squared loss an data 26.13 + 0.5*rho*h**2 4.756065 + alpha*h 10.587222 + L2reg 2.07 + L1reg 0.20 ; SHD = 58 ; DAG True
total norm for a microbatch 243.02032668031845 clip 1.0
total norm for a microbatch 416.3275640281903 clip 1.401524849263198
total norm for a microbatch 120.08665658630422 clip 24.48492849976962
total norm for a microbatch 168.73600692869223 clip 31.635876672410046
total norm for a microbatch 332.76418499997726 clip 60.189203515601456
cuda
Objective function 36.09 = squared loss an data 24.55 + 0.5*rho*h**2 2.084786 + alpha*h 7.009529 + L2reg 2.23 + L1reg 0.21 ; SHD = 60 ; DAG True
Proportion of microbatches that were clipped  0.9829644960993472
iteration 1 in inner loop, alpha 343.275746252261 rho 10000.0 h 0.020419528383893493
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 54.85 = squared loss an data 24.55 + 0.5*rho*h**2 20.847857 + alpha*h 7.009529 + L2reg 2.23 + L1reg 0.21 ; SHD = 60 ; DAG True
total norm for a microbatch 595.812550644956 clip 1.0
total norm for a microbatch 1367.9858712730372 clip 1.7444983489094892
total norm for a microbatch 177.2128818149867 clip 10.755457944377467
total norm for a microbatch 244.0357529892097 clip 20.31047702811882
total norm for a microbatch 321.323675522159 clip 54.65658902491213
cuda
Objective function 39.19 = squared loss an data 25.52 + 0.5*rho*h**2 7.079800 + alpha*h 4.084780 + L2reg 2.30 + L1reg 0.21 ; SHD = 64 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 343.275746252261 rho 100000.0 h 0.011899412083039351
iteration 4 in outer loop, alpha = 12242.687829291612, rho = 1000000.0, h = 0.011899412083039351
Threshold 0.3
[[0.004 0.079 0.021 0.233 0.21  0.055 0.291 0.037 0.636 0.43  0.549 0.156
  0.068 0.216 0.296 0.442 0.223 0.03  0.514 0.046]
 [0.049 0.013 0.017 0.093 0.072 0.065 0.116 0.019 1.421 0.277 0.161 0.045
  0.038 0.081 0.827 0.416 0.393 0.003 0.133 0.046]
 [0.332 0.351 0.007 0.368 0.394 0.396 0.402 0.027 1.123 0.433 0.313 0.787
  0.645 0.318 0.745 0.697 0.339 0.123 2.764 0.811]
 [0.029 0.05  0.008 0.008 0.005 0.032 0.042 0.008 0.079 0.041 0.032 0.028
  0.026 0.022 0.171 0.139 0.034 0.009 0.136 0.005]
 [0.037 0.083 0.019 1.103 0.007 0.024 0.094 0.007 0.895 0.257 0.277 0.1
  0.093 0.062 0.231 0.227 0.102 0.015 0.325 0.058]
 [0.134 0.078 0.018 0.168 0.24  0.006 0.111 0.017 0.701 0.19  0.268 0.051
  0.063 0.108 0.67  0.621 0.199 0.02  0.853 0.032]
 [0.024 0.037 0.015 0.101 0.061 0.026 0.006 0.002 0.101 0.139 0.07  0.057
  0.028 0.045 0.076 0.629 0.051 0.011 0.163 0.025]
 [0.221 0.296 0.153 0.37  0.563 0.359 3.889 0.007 0.331 0.468 0.388 0.59
  0.165 0.39  0.619 0.854 0.515 0.044 0.744 0.196]
 [0.012 0.007 0.005 0.07  0.008 0.011 0.037 0.008 0.007 0.03  0.016 0.016
  0.011 0.015 0.107 0.085 0.026 0.003 0.04  0.018]
 [0.019 0.025 0.014 0.175 0.037 0.039 0.047 0.015 0.224 0.008 0.039 0.043
  0.006 0.023 0.08  0.188 0.026 0.012 0.188 0.046]
 [0.015 0.02  0.019 0.268 0.019 0.025 0.107 0.026 0.478 0.159 0.007 0.065
  0.087 0.085 0.092 0.169 0.065 0.012 0.205 0.095]
 [0.052 0.171 0.008 0.201 0.112 0.079 0.138 0.012 0.366 0.232 0.094 0.008
  0.018 0.056 0.24  0.196 0.069 0.017 0.293 0.032]
 [0.102 0.174 0.011 0.235 0.089 0.124 0.342 0.036 0.486 1.445 0.126 0.314
  0.007 0.098 0.455 0.519 0.133 0.018 0.268 0.214]
 [0.033 0.066 0.027 0.356 0.101 0.086 0.14  0.012 0.409 0.201 0.074 0.146
  0.097 0.007 0.537 0.224 0.156 0.011 0.483 0.099]
 [0.023 0.009 0.005 0.054 0.015 0.008 0.089 0.008 0.053 0.078 0.057 0.023
  0.013 0.019 0.008 0.343 0.006 0.004 0.045 0.022]
 [0.016 0.021 0.008 0.058 0.02  0.012 0.01  0.005 0.067 0.056 0.038 0.036
  0.015 0.037 0.029 0.008 0.014 0.004 0.048 0.008]
 [0.045 0.02  0.015 0.168 0.06  0.031 0.174 0.009 0.311 0.239 0.064 0.098
  0.046 0.039 1.174 0.589 0.009 0.005 0.235 0.071]
 [0.219 3.3   0.085 0.503 0.535 0.231 0.463 0.091 0.681 0.418 0.58  0.411
  0.301 0.648 0.592 0.782 1.271 0.006 0.568 0.119]
 [0.015 0.06  0.005 0.061 0.027 0.009 0.034 0.005 0.2   0.026 0.036 0.025
  0.033 0.018 0.107 0.208 0.021 0.008 0.008 0.01 ]
 [0.114 0.098 0.005 1.498 0.161 0.142 0.212 0.025 0.375 0.254 0.081 0.283
  0.025 0.091 0.29  0.609 0.09  0.042 0.57  0.008]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.636 0.43  0.549 0.
  0.    0.    0.    0.442 0.    0.    0.514 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.421 0.    0.    0.
  0.    0.    0.827 0.416 0.393 0.    0.    0.   ]
 [0.332 0.351 0.    0.368 0.394 0.396 0.402 0.    1.123 0.433 0.313 0.787
  0.645 0.318 0.745 0.697 0.339 0.    2.764 0.811]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.103 0.    0.    0.    0.    0.895 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.325 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.701 0.    0.    0.
  0.    0.    0.67  0.621 0.    0.    0.853 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.629 0.    0.    0.    0.   ]
 [0.    0.    0.    0.37  0.563 0.359 3.889 0.    0.331 0.468 0.388 0.59
  0.    0.39  0.619 0.854 0.515 0.    0.744 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.478 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.366 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.342 0.    0.486 1.445 0.    0.314
  0.    0.    0.455 0.519 0.    0.    0.    0.   ]
 [0.    0.    0.    0.356 0.    0.    0.    0.    0.409 0.    0.    0.
  0.    0.    0.537 0.    0.    0.    0.483 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.343 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.311 0.    0.    0.
  0.    0.    1.174 0.589 0.    0.    0.    0.   ]
 [0.    3.3   0.    0.503 0.535 0.    0.463 0.    0.681 0.418 0.58  0.411
  0.301 0.648 0.592 0.782 1.271 0.    0.568 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.498 0.    0.    0.    0.    0.375 0.    0.    0.
  0.    0.    0.    0.609 0.    0.    0.57  0.   ]]
{'fdr': 0.6666666666666666, 'tpr': 0.675, 'fpr': 0.36, 'f1': 0.44628099173553726, 'shd': 64, 'npred': 81, 'ntrue': 40}
[7.947e-02 2.130e-02 2.326e-01 2.097e-01 5.533e-02 2.914e-01 3.690e-02
 6.358e-01 4.299e-01 5.495e-01 1.562e-01 6.800e-02 2.161e-01 2.957e-01
 4.419e-01 2.233e-01 3.043e-02 5.144e-01 4.577e-02 4.944e-02 1.710e-02
 9.311e-02 7.224e-02 6.535e-02 1.162e-01 1.885e-02 1.421e+00 2.770e-01
 1.609e-01 4.526e-02 3.774e-02 8.098e-02 8.267e-01 4.160e-01 3.929e-01
 3.149e-03 1.327e-01 4.608e-02 3.323e-01 3.508e-01 3.677e-01 3.943e-01
 3.960e-01 4.020e-01 2.670e-02 1.123e+00 4.332e-01 3.131e-01 7.867e-01
 6.451e-01 3.176e-01 7.455e-01 6.975e-01 3.389e-01 1.227e-01 2.764e+00
 8.112e-01 2.925e-02 5.007e-02 8.483e-03 5.071e-03 3.194e-02 4.231e-02
 8.273e-03 7.892e-02 4.057e-02 3.227e-02 2.780e-02 2.619e-02 2.245e-02
 1.713e-01 1.390e-01 3.426e-02 8.641e-03 1.358e-01 4.971e-03 3.708e-02
 8.267e-02 1.916e-02 1.103e+00 2.447e-02 9.368e-02 7.094e-03 8.954e-01
 2.569e-01 2.772e-01 9.961e-02 9.329e-02 6.217e-02 2.308e-01 2.273e-01
 1.021e-01 1.496e-02 3.252e-01 5.751e-02 1.335e-01 7.782e-02 1.769e-02
 1.682e-01 2.404e-01 1.112e-01 1.733e-02 7.005e-01 1.899e-01 2.676e-01
 5.104e-02 6.346e-02 1.085e-01 6.702e-01 6.212e-01 1.989e-01 1.992e-02
 8.530e-01 3.157e-02 2.359e-02 3.730e-02 1.485e-02 1.014e-01 6.111e-02
 2.643e-02 2.440e-03 1.013e-01 1.389e-01 7.042e-02 5.713e-02 2.825e-02
 4.513e-02 7.556e-02 6.292e-01 5.053e-02 1.073e-02 1.628e-01 2.523e-02
 2.211e-01 2.957e-01 1.528e-01 3.697e-01 5.632e-01 3.586e-01 3.889e+00
 3.306e-01 4.680e-01 3.880e-01 5.898e-01 1.648e-01 3.899e-01 6.190e-01
 8.542e-01 5.149e-01 4.378e-02 7.438e-01 1.962e-01 1.246e-02 6.895e-03
 5.016e-03 7.049e-02 8.428e-03 1.051e-02 3.720e-02 7.870e-03 2.969e-02
 1.637e-02 1.590e-02 1.120e-02 1.462e-02 1.069e-01 8.522e-02 2.581e-02
 2.683e-03 4.028e-02 1.825e-02 1.860e-02 2.520e-02 1.366e-02 1.748e-01
 3.675e-02 3.934e-02 4.675e-02 1.540e-02 2.236e-01 3.930e-02 4.318e-02
 6.377e-03 2.314e-02 7.976e-02 1.877e-01 2.645e-02 1.163e-02 1.882e-01
 4.553e-02 1.533e-02 1.994e-02 1.870e-02 2.679e-01 1.903e-02 2.464e-02
 1.067e-01 2.550e-02 4.778e-01 1.587e-01 6.467e-02 8.721e-02 8.530e-02
 9.182e-02 1.695e-01 6.493e-02 1.232e-02 2.053e-01 9.485e-02 5.174e-02
 1.706e-01 7.780e-03 2.010e-01 1.121e-01 7.893e-02 1.381e-01 1.163e-02
 3.661e-01 2.319e-01 9.412e-02 1.848e-02 5.606e-02 2.397e-01 1.956e-01
 6.937e-02 1.741e-02 2.934e-01 3.157e-02 1.021e-01 1.737e-01 1.144e-02
 2.352e-01 8.866e-02 1.243e-01 3.419e-01 3.578e-02 4.857e-01 1.445e+00
 1.259e-01 3.140e-01 9.763e-02 4.554e-01 5.192e-01 1.333e-01 1.819e-02
 2.684e-01 2.136e-01 3.297e-02 6.570e-02 2.742e-02 3.563e-01 1.009e-01
 8.635e-02 1.404e-01 1.208e-02 4.091e-01 2.015e-01 7.353e-02 1.457e-01
 9.671e-02 5.367e-01 2.239e-01 1.561e-01 1.116e-02 4.831e-01 9.932e-02
 2.314e-02 9.025e-03 5.275e-03 5.413e-02 1.531e-02 7.761e-03 8.882e-02
 8.186e-03 5.263e-02 7.844e-02 5.739e-02 2.260e-02 1.280e-02 1.920e-02
 3.430e-01 6.260e-03 3.678e-03 4.479e-02 2.196e-02 1.566e-02 2.131e-02
 7.690e-03 5.751e-02 1.983e-02 1.219e-02 9.615e-03 4.572e-03 6.686e-02
 5.643e-02 3.804e-02 3.551e-02 1.545e-02 3.689e-02 2.902e-02 1.439e-02
 4.287e-03 4.804e-02 7.725e-03 4.477e-02 2.004e-02 1.507e-02 1.679e-01
 5.965e-02 3.129e-02 1.736e-01 8.869e-03 3.107e-01 2.389e-01 6.352e-02
 9.758e-02 4.615e-02 3.903e-02 1.174e+00 5.889e-01 5.346e-03 2.349e-01
 7.119e-02 2.189e-01 3.300e+00 8.490e-02 5.031e-01 5.349e-01 2.307e-01
 4.632e-01 9.144e-02 6.811e-01 4.180e-01 5.803e-01 4.113e-01 3.006e-01
 6.484e-01 5.919e-01 7.820e-01 1.271e+00 5.683e-01 1.191e-01 1.546e-02
 5.979e-02 4.639e-03 6.110e-02 2.697e-02 8.868e-03 3.353e-02 5.390e-03
 1.995e-01 2.635e-02 3.625e-02 2.535e-02 3.280e-02 1.782e-02 1.075e-01
 2.076e-01 2.052e-02 8.110e-03 9.531e-03 1.140e-01 9.753e-02 5.063e-03
 1.498e+00 1.613e-01 1.419e-01 2.116e-01 2.537e-02 3.747e-01 2.536e-01
 8.061e-02 2.833e-01 2.465e-02 9.150e-02 2.900e-01 6.089e-01 8.953e-02
 4.159e-02 5.697e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8245588235294119, 0.5752743038213302)
Iterations 567
Achieves (13.598573656938171, 1e-05)-DP
