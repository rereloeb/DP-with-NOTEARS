samples  5000  graph  15 30 ER mlp  minibatch size  50  noise  0.75  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8739726951769526
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.3869308521822745
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13140032387342515
iteration 2 in outer loop, alpha = 14.480164056458975, rho = 100.0, h = 0.13140032387342515
cuda
iteration 1 in inner loop,alpha 14.480164056458975 rho 100.0 h 0.07402974404327978
iteration 2 in inner loop,alpha 14.480164056458975 rho 1000.0 h 0.02665464061266043
iteration 3 in outer loop, alpha = 41.134804669119404, rho = 1000.0, h = 0.02665464061266043
cuda
iteration 1 in inner loop,alpha 41.134804669119404 rho 1000.0 h 0.01654567386151129
iteration 2 in inner loop,alpha 41.134804669119404 rho 10000.0 h 0.006665540257380087
iteration 3 in inner loop,alpha 41.134804669119404 rho 100000.0 h 0.001205005546387028
iteration 4 in outer loop, alpha = 161.6353593078222, rho = 100000.0, h = 0.001205005546387028
cuda
iteration 1 in inner loop,alpha 161.6353593078222 rho 100000.0 h 0.0002378147214301407
iteration 5 in outer loop, alpha = 185.41683145083627, rho = 100000.0, h = 0.0002378147214301407
cuda
iteration 1 in inner loop,alpha 185.41683145083627 rho 100000.0 h 0.00012513507919464928
iteration 6 in outer loop, alpha = 310.5519106454856, rho = 1000000.0, h = 0.00012513507919464928
Threshold 0.3
[[0.002 0.    0.001 0.005 0.224 0.    0.    0.    0.    2.761 0.259 0.
  0.    0.    0.007]
 [2.538 0.001 0.001 0.268 0.216 0.012 0.011 0.014 0.001 2.209 0.252 1.364
  0.002 0.001 0.102]
 [0.054 0.076 0.001 0.133 0.157 0.043 0.129 0.018 0.071 0.147 0.143 0.162
  0.018 0.017 0.013]
 [0.153 0.    0.002 0.001 0.197 0.    0.001 0.    0.001 0.253 0.404 0.001
  0.    0.    1.025]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.001 0.
  0.    0.    0.001]
 [2.657 0.031 0.001 0.966 0.353 0.001 1.369 0.    0.053 1.741 1.486 1.494
  0.018 0.018 1.916]
 [0.418 0.043 0.004 0.335 0.394 0.    0.001 0.    0.049 0.195 1.937 0.093
  0.012 0.008 0.172]
 [2.685 0.021 0.021 2.08  0.128 0.578 0.051 0.    0.014 0.352 0.151 0.059
  0.028 0.024 0.118]
 [0.08  0.378 0.002 0.65  0.39  0.008 0.011 0.004 0.003 2.308 2.446 1.057
  0.    0.    0.361]
 [0.    0.    0.    0.    2.813 0.    0.    0.    0.    0.003 0.335 0.
  0.    0.    0.002]
 [0.    0.    0.001 0.001 3.142 0.    0.    0.    0.001 0.001 0.007 0.001
  0.    0.    0.002]
 [0.361 0.001 0.    0.186 0.139 0.    0.004 0.    0.001 0.429 0.294 0.001
  0.    0.    0.037]
 [0.041 0.076 0.022 0.099 3.066 0.019 0.024 0.021 1.077 0.108 2.22  0.032
  0.001 0.016 0.026]
 [0.126 1.447 0.018 1.12  1.23  0.011 0.017 0.014 3.523 0.366 0.48  0.158
  0.017 0.002 0.568]
 [0.106 0.    0.001 0.    0.095 0.    0.    0.    0.001 0.119 0.172 0.003
  0.    0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.761 0.    0.
  0.    0.    0.   ]
 [2.538 0.    0.    0.    0.    0.    0.    0.    0.    2.209 0.    1.364
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.404 0.
  0.    0.    1.025]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.657 0.    0.    0.966 0.353 0.    1.369 0.    0.    1.741 1.486 1.494
  0.    0.    1.916]
 [0.418 0.    0.    0.335 0.394 0.    0.    0.    0.    0.    1.937 0.
  0.    0.    0.   ]
 [2.685 0.    0.    2.08  0.    0.578 0.    0.    0.    0.352 0.    0.
  0.    0.    0.   ]
 [0.    0.378 0.    0.65  0.39  0.    0.    0.    0.    2.308 2.446 1.057
  0.    0.    0.361]
 [0.    0.    0.    0.    2.813 0.    0.    0.    0.    0.    0.335 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.142 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.361 0.    0.    0.    0.    0.    0.    0.    0.    0.429 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.066 0.    0.    0.    1.077 0.    2.22  0.
  0.    0.    0.   ]
 [0.    1.447 0.    1.12  1.23  0.    0.    0.    3.523 0.366 0.48  0.
  0.    0.    0.568]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.3409090909090909, 'tpr': 0.9666666666666667, 'fpr': 0.2, 'f1': 0.7837837837837838, 'shd': 16, 'npred': 44, 'ntrue': 30}
[5.382e-05 5.133e-04 4.689e-03 2.242e-01 6.799e-05 5.922e-05 3.216e-05
 4.847e-05 2.761e+00 2.585e-01 3.527e-04 3.990e-05 9.750e-05 6.706e-03
 2.538e+00 1.485e-03 2.680e-01 2.164e-01 1.195e-02 1.055e-02 1.360e-02
 1.289e-03 2.209e+00 2.517e-01 1.364e+00 1.695e-03 7.390e-04 1.015e-01
 5.399e-02 7.619e-02 1.328e-01 1.570e-01 4.269e-02 1.291e-01 1.831e-02
 7.086e-02 1.468e-01 1.433e-01 1.616e-01 1.759e-02 1.718e-02 1.334e-02
 1.532e-01 3.170e-04 2.374e-03 1.974e-01 3.944e-05 5.165e-04 4.155e-05
 8.362e-04 2.534e-01 4.036e-01 1.082e-03 2.177e-04 9.434e-05 1.025e+00
 3.776e-05 4.388e-06 4.632e-05 1.707e-04 9.520e-06 9.032e-05 5.216e-06
 1.398e-04 3.303e-04 6.596e-04 6.433e-05 1.926e-05 4.496e-05 5.314e-04
 2.657e+00 3.066e-02 8.357e-04 9.664e-01 3.526e-01 1.369e+00 1.855e-04
 5.272e-02 1.741e+00 1.486e+00 1.494e+00 1.843e-02 1.753e-02 1.916e+00
 4.183e-01 4.261e-02 4.142e-03 3.345e-01 3.938e-01 2.485e-04 8.968e-05
 4.928e-02 1.946e-01 1.937e+00 9.343e-02 1.192e-02 7.788e-03 1.718e-01
 2.685e+00 2.109e-02 2.075e-02 2.080e+00 1.281e-01 5.784e-01 5.126e-02
 1.354e-02 3.520e-01 1.507e-01 5.891e-02 2.790e-02 2.431e-02 1.180e-01
 7.956e-02 3.779e-01 1.744e-03 6.495e-01 3.897e-01 7.724e-03 1.089e-02
 3.677e-03 2.308e+00 2.446e+00 1.057e+00 1.219e-04 3.948e-04 3.612e-01
 1.963e-04 4.100e-06 2.856e-06 3.532e-04 2.813e+00 4.996e-06 9.270e-05
 1.361e-05 3.912e-04 3.346e-01 4.872e-04 3.080e-05 9.324e-05 1.627e-03
 2.603e-04 6.386e-05 5.108e-04 7.911e-04 3.142e+00 7.356e-05 3.863e-04
 1.411e-05 5.133e-04 1.354e-03 5.507e-04 1.255e-04 1.465e-04 2.170e-03
 3.614e-01 5.043e-04 3.203e-04 1.859e-01 1.390e-01 2.575e-04 3.596e-03
 8.597e-05 1.168e-03 4.286e-01 2.945e-01 1.571e-04 4.821e-04 3.687e-02
 4.055e-02 7.629e-02 2.204e-02 9.864e-02 3.066e+00 1.902e-02 2.427e-02
 2.128e-02 1.077e+00 1.084e-01 2.220e+00 3.178e-02 1.638e-02 2.551e-02
 1.259e-01 1.447e+00 1.827e-02 1.120e+00 1.230e+00 1.058e-02 1.711e-02
 1.377e-02 3.523e+00 3.657e-01 4.804e-01 1.580e-01 1.692e-02 5.682e-01
 1.060e-01 2.699e-04 1.265e-03 7.497e-05 9.516e-02 1.694e-05 2.727e-04
 3.830e-06 6.227e-04 1.194e-01 1.718e-01 2.673e-03 4.792e-04 5.940e-05]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9816666666666667, 0.9722801460162307)
cuda
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 98.78448855566148 clip 1.3306975146315496
total norm for a microbatch 39.9829345003256 clip 9.323439307706348
total norm for a microbatch 35.0497374955397 clip 32.070835442534715
total norm for a microbatch 40.01937567572785 clip 28.810936159800324
total norm for a microbatch 30.13682247310046 clip 30.86674594564765
total norm for a microbatch 38.05393685087358 clip 29.818356871254935
total norm for a microbatch 29.2212030235121 clip 28.78888419903906
total norm for a microbatch 27.23836720171865 clip 25.886413832624033
total norm for a microbatch 32.86193309460014 clip 24.941303569208166
total norm for a microbatch 24.228170125507923 clip 25.1372927700392
total norm for a microbatch 34.951889752878095 clip 23.188377574376247
total norm for a microbatch 22.008057869508797 clip 23.288242312018838
total norm for a microbatch 51.657614329175516 clip 29.10569778970795
total norm for a microbatch 32.64400298143135 clip 27.37749166730306
total norm for a microbatch 25.999713537366475 clip 28.140739117282717
total norm for a microbatch 30.64769514301706 clip 28.5644642970421
total norm for a microbatch 41.23049085673985 clip 29.494826160979706
cuda
Objective function 14.43 = squared loss an data 11.65 + 0.5*rho*h**2 1.878373 + alpha*h 0.000000 + L2reg 0.74 + L1reg 0.16 ; SHD = 62 ; DAG False
Proportion of microbatches that were clipped  0.8072736069671801
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.9382327434047824
iteration 1 in outer loop, alpha = 1.9382327434047824, rho = 1.0, h = 1.9382327434047824
cuda
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 18.19 = squared loss an data 11.65 + 0.5*rho*h**2 1.878373 + alpha*h 3.756746 + L2reg 0.74 + L1reg 0.16 ; SHD = 62 ; DAG False
total norm for a microbatch 37.94273951872695 clip 12.638281755615287
total norm for a microbatch 29.774413245940842 clip 39.17810660393228
total norm for a microbatch 29.943221712815486 clip 36.38518534875004
total norm for a microbatch 68.20679605112169 clip 40.3153016990408
total norm for a microbatch 53.08074699937892 clip 38.36779401430842
total norm for a microbatch 56.84439591447064 clip 41.375964490524034
total norm for a microbatch 47.38311635197168 clip 41.375964490524034
total norm for a microbatch 58.68579217469205 clip 41.07891962221517
total norm for a microbatch 75.68510780885563 clip 40.87938433266928
total norm for a microbatch 78.88677669579967 clip 39.67863037696256
total norm for a microbatch 38.671608474351025 clip 45.57374737323698
total norm for a microbatch 123.38921712512663 clip 43.92945523287457
total norm for a microbatch 59.559464951953544 clip 46.055431003598144
total norm for a microbatch 70.33345728560964 clip 45.5712327941282
total norm for a microbatch 55.75768738151105 clip 44.54529776535519
cuda
Objective function 14.96 = squared loss an data 10.00 + 0.5*rho*h**2 1.004793 + alpha*h 2.747636 + L2reg 1.05 + L1reg 0.16 ; SHD = 62 ; DAG False
Proportion of microbatches that were clipped  0.8104107360715738
iteration 1 in inner loop, alpha 1.9382327434047824 rho 1.0 h 1.4175983433165698
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 24.01 = squared loss an data 10.00 + 0.5*rho*h**2 10.047925 + alpha*h 2.747636 + L2reg 1.05 + L1reg 0.16 ; SHD = 62 ; DAG False
total norm for a microbatch 60.27988220099798 clip 14.019596865179597
total norm for a microbatch 62.25466481314381 clip 51.358629532726304
total norm for a microbatch 94.38162485410977 clip 50.73698971281083
total norm for a microbatch 65.42814816225508 clip 48.54380205874669
total norm for a microbatch 83.10008241320647 clip 53.281106478682666
total norm for a microbatch 67.6402728801746 clip 55.097693490354494
total norm for a microbatch 43.448743243174 clip 52.889902476380804
total norm for a microbatch 76.82231567474774 clip 55.9352579547939
total norm for a microbatch 32.08893339515803 clip 56.45812204641356
total norm for a microbatch 45.52548317789715 clip 55.444069962552646
total norm for a microbatch 70.5389639270645 clip 54.66803425561694
cuda
Objective function 15.52 = squared loss an data 11.07 + 0.5*rho*h**2 1.871701 + alpha*h 1.185876 + L2reg 1.25 + L1reg 0.14 ; SHD = 48 ; DAG False
Proportion of microbatches that were clipped  0.8145764620706298
iteration 2 in inner loop, alpha 1.9382327434047824 rho 10.0 h 0.6118334513897441
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 32.37 = squared loss an data 11.07 + 0.5*rho*h**2 18.717009 + alpha*h 1.185876 + L2reg 1.25 + L1reg 0.14 ; SHD = 48 ; DAG False
total norm for a microbatch 223.97917485282247 clip 68.13903137224267
total norm for a microbatch 98.57686776383579 clip 65.95755637981601
total norm for a microbatch 105.73574342692095 clip 63.15886521072732
total norm for a microbatch 89.41690365212507 clip 66.68925966571345
total norm for a microbatch 110.78543621300055 clip 69.010249964676
total norm for a microbatch 157.0106080089727 clip 61.81199976501483
cuda
Objective function 17.53 = squared loss an data 13.15 + 0.5*rho*h**2 2.431956 + alpha*h 0.427463 + L2reg 1.38 + L1reg 0.14 ; SHD = 45 ; DAG True
Proportion of microbatches that were clipped  0.8134479682050451
iteration 3 in inner loop, alpha 1.9382327434047824 rho 100.0 h 0.22054279043777214
iteration 2 in outer loop, alpha = 23.992511787181996, rho = 100.0, h = 0.22054279043777214
cuda
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 22.39 = squared loss an data 13.15 + 0.5*rho*h**2 2.431956 + alpha*h 5.291375 + L2reg 1.38 + L1reg 0.14 ; SHD = 45 ; DAG True
total norm for a microbatch 272.5524116452015 clip 6.665964637410296
total norm for a microbatch 93.56507976463558 clip 70.35646073242758
total norm for a microbatch 115.32130061860715 clip 69.97521267937948
total norm for a microbatch 44.3717812003805 clip 69.15031917772956
total norm for a microbatch 121.87907031101396 clip 66.28040578099942
cuda
Objective function 19.22 = squared loss an data 13.55 + 0.5*rho*h**2 0.843820 + alpha*h 3.116849 + L2reg 1.56 + L1reg 0.14 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.8143712574850299
iteration 1 in inner loop, alpha 23.992511787181996 rho 100.0 h 0.12990923645777386
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 26.82 = squared loss an data 13.55 + 0.5*rho*h**2 8.438205 + alpha*h 3.116849 + L2reg 1.56 + L1reg 0.14 ; SHD = 51 ; DAG True
total norm for a microbatch 138.502765994769 clip 1.8841220040206514
total norm for a microbatch 71.71160998526933 clip 2.683331422643094
total norm for a microbatch 94.50652927193558 clip 4.693390081710763
total norm for a microbatch 136.19490918727672 clip 6.656766355431361
total norm for a microbatch 168.85207232626098 clip 7.219865544476585
total norm for a microbatch 104.86745347762661 clip 81.58464581169007
total norm for a microbatch 90.09893479167121 clip 69.84262107035056
total norm for a microbatch 135.3385154606599 clip 73.26792301582039
total norm for a microbatch 100.19286848362543 clip 76.7921917551389
total norm for a microbatch 91.05906347126351 clip 77.91760554222819
total norm for a microbatch 102.0742028589893 clip 75.29195222207491
total norm for a microbatch 165.63277520285268 clip 74.91901013697593
cuda
Objective function 18.93 = squared loss an data 14.13 + 0.5*rho*h**2 1.566063 + alpha*h 1.342750 + L2reg 1.73 + L1reg 0.15 ; SHD = 54 ; DAG True
Proportion of microbatches that were clipped  0.8208409506398537
iteration 2 in inner loop, alpha 23.992511787181996 rho 1000.0 h 0.055965392149651905
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 33.02 = squared loss an data 14.13 + 0.5*rho*h**2 15.660626 + alpha*h 1.342750 + L2reg 1.73 + L1reg 0.15 ; SHD = 54 ; DAG True
total norm for a microbatch 184.47160543781845 clip 1.2051395848713506
total norm for a microbatch 175.45506975667016 clip 70.29306905767707
total norm for a microbatch 134.4697545566857 clip 92.19990128192924
total norm for a microbatch 111.41680343678551 clip 89.50549823854672
total norm for a microbatch 88.41785499077443 clip 91.19367126945129
total norm for a microbatch 101.32185069863448 clip 81.52099152882911
total norm for a microbatch 87.33986717634836 clip 79.06853628797235
total norm for a microbatch 117.17106496846124 clip 81.64762545613267
total norm for a microbatch 99.25952903205335 clip 86.58943496330278
total norm for a microbatch 74.55460149993516 clip 78.37247107081332
total norm for a microbatch 103.55633133055912 clip 78.37247107081332
total norm for a microbatch 55.21772215773869 clip 81.26809541540227
total norm for a microbatch 133.55079490762267 clip 79.30032706489035
total norm for a microbatch 67.62665606977887 clip 80.92797372662035
cuda
Objective function 18.15 = squared loss an data 14.07 + 0.5*rho*h**2 1.677725 + alpha*h 0.439492 + L2reg 1.81 + L1reg 0.15 ; SHD = 48 ; DAG True
Proportion of microbatches that were clipped  0.8188775510204082
iteration 3 in inner loop, alpha 23.992511787181996 rho 10000.0 h 0.018317888357923806
iteration 3 in outer loop, alpha = 207.17139536642006, rho = 10000.0, h = 0.018317888357923806
cuda
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 21.50 = squared loss an data 14.07 + 0.5*rho*h**2 1.677725 + alpha*h 3.794942 + L2reg 1.81 + L1reg 0.15 ; SHD = 48 ; DAG True
total norm for a microbatch 115.31704854273733 clip 1.0
total norm for a microbatch 115.57060355206168 clip 4.141149489093004
total norm for a microbatch 151.96511889135044 clip 13.801652501790093
total norm for a microbatch 138.92823828502873 clip 98.40878188824915
total norm for a microbatch 131.41082752700717 clip 97.71602729975673
total norm for a microbatch 102.66646582538567 clip 98.34843795225176
total norm for a microbatch 151.37673647284646 clip 91.53846159356175
total norm for a microbatch 103.1275808573067 clip 92.02419595195516
total norm for a microbatch 110.28868075331167 clip 88.01288873715805
total norm for a microbatch 108.14783117879263 clip 93.12580717507942
cuda
Objective function 18.65 = squared loss an data 13.58 + 0.5*rho*h**2 0.650864 + alpha*h 2.363686 + L2reg 1.90 + L1reg 0.16 ; SHD = 47 ; DAG True
Proportion of microbatches that were clipped  0.8205821536503897
iteration 1 in inner loop, alpha 207.17139536642006 rho 10000.0 h 0.011409327056107088
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 24.51 = squared loss an data 13.58 + 0.5*rho*h**2 6.508637 + alpha*h 2.363686 + L2reg 1.90 + L1reg 0.16 ; SHD = 47 ; DAG True
total norm for a microbatch 1611.7185310242376 clip 1.8539813233005003
total norm for a microbatch 1135.3131161565857 clip 2.4582665415381872
total norm for a microbatch 536.4325778325516 clip 3.5148638599575914
total norm for a microbatch 414.86656178659564 clip 147.66101480653734
total norm for a microbatch 662.1459140572209 clip 369.2497791144421
total norm for a microbatch 425.5076852941824 clip 308.0308272719383
total norm for a microbatch 116.53054513294694 clip 121.44092696168595
total norm for a microbatch 163.44642980895796 clip 75.99648562025536
total norm for a microbatch 112.2160290765085 clip 77.26886499802733
total norm for a microbatch 69.40693723221648 clip 73.0171493062986
cuda
Objective function 16.74 = squared loss an data 13.78 + 0.5*rho*h**2 0.345688 + alpha*h 0.544737 + L2reg 1.92 + L1reg 0.15 ; SHD = 48 ; DAG True
Proportion of microbatches that were clipped  0.8171103271327774
iteration 2 in inner loop, alpha 207.17139536642006 rho 100000.0 h 0.002629401859497449
iteration 4 in outer loop, alpha = 470.1115813161649, rho = 100000.0, h = 0.002629401859497449
cuda
noise_multiplier  0.75  noise_multiplier_b  2.5  noise_multiplier_delta  0.7585826061362605
cuda
Objective function 17.43 = squared loss an data 13.78 + 0.5*rho*h**2 0.345688 + alpha*h 1.236112 + L2reg 1.92 + L1reg 0.15 ; SHD = 48 ; DAG True
total norm for a microbatch 439.0537540836893 clip 6.194885637130592
total norm for a microbatch 421.45469166259693 clip 7.463693421837839
total norm for a microbatch 272.83207424029615 clip 20.198235809552337
total norm for a microbatch 363.95342771995826 clip 48.402653140183595
total norm for a microbatch 556.6317298864395 clip 641.5056521147005
total norm for a microbatch 333.88102881769686 clip 426.8039956411772
total norm for a microbatch 257.36130854914757 clip 214.26931476120146
total norm for a microbatch 255.40058150953345 clip 214.26931476120146
total norm for a microbatch 106.505420228258 clip 92.34934648870335
total norm for a microbatch 119.33810427656877 clip 81.46783843432189
total norm for a microbatch 84.74766328684503 clip 81.93743930380855
total norm for a microbatch 82.68552730692646 clip 85.90346873437566
total norm for a microbatch 65.8738847197211 clip 79.21073855015845
total norm for a microbatch 87.77330678708796 clip 83.2181891311501
cuda
Objective function 17.71 = squared loss an data 14.47 + 0.5*rho*h**2 0.185998 + alpha*h 0.906713 + L2reg 1.98 + L1reg 0.16 ; SHD = 49 ; DAG True
Proportion of microbatches that were clipped  0.817941317941318
iteration 1 in inner loop, alpha 470.1115813161649 rho 100000.0 h 0.0019287192113228713
iteration 5 in outer loop, alpha = 2398.830792639036, rho = 1000000.0, h = 0.0019287192113228713
Threshold 0.3
[[0.003 0.024 0.081 0.215 1.013 0.015 0.014 0.004 0.02  0.744 0.158 0.372
  0.006 0.008 0.029]
 [0.22  0.004 0.216 0.214 0.719 0.075 0.007 0.013 0.019 0.384 0.112 1.104
  0.009 0.004 0.157]
 [0.06  0.023 0.004 0.038 0.594 0.017 0.003 0.009 0.014 0.118 0.064 0.091
  0.01  0.004 0.044]
 [0.024 0.02  0.123 0.004 0.251 0.013 0.003 0.003 0.009 0.15  0.023 0.224
  0.005 0.002 0.012]
 [0.003 0.003 0.005 0.013 0.004 0.004 0.001 0.001 0.002 0.002 0.002 0.009
  0.    0.    0.006]
 [0.268 0.037 0.263 0.299 0.515 0.004 0.009 0.003 0.018 0.494 0.368 0.8
  0.01  0.006 0.348]
 [0.32  0.394 1.36  0.971 0.651 0.249 0.003 0.109 0.346 0.626 1.229 0.734
  0.193 0.059 0.706]
 [1.025 0.356 0.48  1.121 0.88  0.708 0.052 0.004 0.284 1.427 0.518 0.941
  0.292 0.03  0.586]
 [0.201 0.322 0.305 0.466 0.651 0.229 0.012 0.011 0.004 0.999 1.178 0.644
  0.015 0.001 0.287]
 [0.005 0.009 0.038 0.03  1.573 0.008 0.005 0.002 0.004 0.004 0.03  0.045
  0.004 0.002 0.017]
 [0.025 0.048 0.066 0.148 1.992 0.015 0.002 0.005 0.003 0.203 0.004 0.523
  0.002 0.001 0.052]
 [0.015 0.003 0.035 0.021 0.32  0.005 0.004 0.004 0.006 0.134 0.01  0.004
  0.002 0.001 0.044]
 [0.478 0.41  0.612 0.562 2.244 0.347 0.019 0.019 0.221 0.866 1.154 1.161
  0.007 0.01  0.192]
 [0.54  1.235 0.866 1.171 1.002 0.745 0.063 0.212 2.305 1.465 0.859 1.18
  0.314 0.003 0.852]
 [0.129 0.026 0.106 0.254 0.64  0.018 0.006 0.007 0.013 0.225 0.092 0.11
  0.023 0.003 0.003]]
[[0.    0.    0.    0.    1.013 0.    0.    0.    0.    0.744 0.    0.372
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.719 0.    0.    0.    0.    0.384 0.    1.104
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.594 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.515 0.    0.    0.    0.    0.494 0.368 0.8
  0.    0.    0.348]
 [0.32  0.394 1.36  0.971 0.651 0.    0.    0.    0.346 0.626 1.229 0.734
  0.    0.    0.706]
 [1.025 0.356 0.48  1.121 0.88  0.708 0.    0.    0.    1.427 0.518 0.941
  0.    0.    0.586]
 [0.    0.322 0.305 0.466 0.651 0.    0.    0.    0.    0.999 1.178 0.644
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.573 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.992 0.    0.    0.    0.    0.    0.    0.523
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.32  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.478 0.41  0.612 0.562 2.244 0.347 0.    0.    0.    0.866 1.154 1.161
  0.    0.    0.   ]
 [0.54  1.235 0.866 1.171 1.002 0.745 0.    0.    2.305 1.465 0.859 1.18
  0.314 0.    0.852]
 [0.    0.    0.    0.    0.64  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.6461538461538462, 'tpr': 0.7666666666666667, 'fpr': 0.56, 'f1': 0.4842105263157895, 'shd': 49, 'npred': 65, 'ntrue': 30}
[2.395e-02 8.070e-02 2.147e-01 1.013e+00 1.492e-02 1.398e-02 3.586e-03
 1.963e-02 7.441e-01 1.579e-01 3.716e-01 6.254e-03 8.139e-03 2.883e-02
 2.203e-01 2.162e-01 2.143e-01 7.191e-01 7.543e-02 7.311e-03 1.289e-02
 1.875e-02 3.837e-01 1.116e-01 1.104e+00 8.906e-03 3.948e-03 1.571e-01
 6.035e-02 2.305e-02 3.834e-02 5.938e-01 1.719e-02 3.060e-03 8.684e-03
 1.428e-02 1.184e-01 6.423e-02 9.124e-02 9.717e-03 4.231e-03 4.378e-02
 2.364e-02 1.978e-02 1.231e-01 2.506e-01 1.323e-02 3.209e-03 3.115e-03
 9.078e-03 1.502e-01 2.337e-02 2.239e-01 5.356e-03 1.834e-03 1.240e-02
 3.207e-03 2.982e-03 5.342e-03 1.338e-02 3.533e-03 1.247e-03 9.378e-04
 1.923e-03 1.865e-03 1.957e-03 8.802e-03 4.684e-04 4.451e-04 5.772e-03
 2.679e-01 3.652e-02 2.626e-01 2.990e-01 5.148e-01 9.035e-03 3.089e-03
 1.815e-02 4.940e-01 3.681e-01 7.996e-01 1.000e-02 6.344e-03 3.484e-01
 3.204e-01 3.938e-01 1.360e+00 9.714e-01 6.509e-01 2.495e-01 1.088e-01
 3.464e-01 6.257e-01 1.229e+00 7.339e-01 1.928e-01 5.884e-02 7.057e-01
 1.025e+00 3.557e-01 4.799e-01 1.121e+00 8.804e-01 7.083e-01 5.222e-02
 2.837e-01 1.427e+00 5.181e-01 9.406e-01 2.921e-01 3.000e-02 5.864e-01
 2.008e-01 3.222e-01 3.052e-01 4.660e-01 6.513e-01 2.293e-01 1.182e-02
 1.105e-02 9.989e-01 1.178e+00 6.441e-01 1.546e-02 6.759e-04 2.870e-01
 4.985e-03 9.100e-03 3.789e-02 3.016e-02 1.573e+00 8.393e-03 4.992e-03
 2.287e-03 3.684e-03 2.988e-02 4.483e-02 4.398e-03 1.554e-03 1.748e-02
 2.485e-02 4.765e-02 6.587e-02 1.478e-01 1.992e+00 1.499e-02 2.117e-03
 5.358e-03 2.921e-03 2.034e-01 5.226e-01 2.339e-03 1.418e-03 5.157e-02
 1.474e-02 2.797e-03 3.454e-02 2.078e-02 3.199e-01 5.167e-03 4.030e-03
 3.822e-03 5.692e-03 1.338e-01 9.614e-03 2.370e-03 1.032e-03 4.390e-02
 4.777e-01 4.105e-01 6.121e-01 5.622e-01 2.244e+00 3.465e-01 1.873e-02
 1.873e-02 2.212e-01 8.658e-01 1.154e+00 1.161e+00 1.037e-02 1.920e-01
 5.404e-01 1.235e+00 8.663e-01 1.171e+00 1.002e+00 7.448e-01 6.296e-02
 2.116e-01 2.305e+00 1.465e+00 8.595e-01 1.180e+00 3.140e-01 8.517e-01
 1.291e-01 2.586e-02 1.063e-01 2.543e-01 6.395e-01 1.759e-02 5.607e-03
 7.320e-03 1.254e-02 2.253e-01 9.236e-02 1.103e-01 2.301e-02 2.752e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8605555555555555, 0.5701722186233467)
Iterations 2500
Achieves (7.189593007809515, 1e-05)-DP
