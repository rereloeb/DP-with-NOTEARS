samples  5000  graph  20 80 ER mlp  minibatch size  100  noise  0.8  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7622777646747636
iteration 1 in outer loop, alpha = 1.7622777646747636, rho = 1.0, h = 1.7622777646747636
cuda
iteration 1 in inner loop,alpha 1.7622777646747636 rho 1.0 h 1.1769941079911774
iteration 2 in inner loop,alpha 1.7622777646747636 rho 10.0 h 0.546699080645368
iteration 3 in inner loop,alpha 1.7622777646747636 rho 100.0 h 0.18576399615164618
iteration 2 in outer loop, alpha = 20.338677379839382, rho = 100.0, h = 0.18576399615164618
cuda
iteration 1 in inner loop,alpha 20.338677379839382 rho 100.0 h 0.10957056978300983
iteration 2 in inner loop,alpha 20.338677379839382 rho 1000.0 h 0.04309723609533833
iteration 3 in outer loop, alpha = 63.43591347517771, rho = 1000.0, h = 0.04309723609533833
cuda
iteration 1 in inner loop,alpha 63.43591347517771 rho 1000.0 h 0.02266637719291964
iteration 2 in inner loop,alpha 63.43591347517771 rho 10000.0 h 0.007827837760750356
iteration 4 in outer loop, alpha = 141.71429108268126, rho = 10000.0, h = 0.007827837760750356
cuda
iteration 1 in inner loop,alpha 141.71429108268126 rho 10000.0 h 0.004241807695727573
iteration 2 in inner loop,alpha 141.71429108268126 rho 100000.0 h 0.001395629606655291
iteration 5 in outer loop, alpha = 281.27725174821035, rho = 100000.0, h = 0.001395629606655291
cuda
iteration 1 in inner loop,alpha 281.27725174821035 rho 100000.0 h 0.0006681448257417344
iteration 6 in outer loop, alpha = 949.4220774899447, rho = 1000000.0, h = 0.0006681448257417344
Threshold 0.3
[[0.001 3.16  0.068 0.243 0.901 1.126 0.316 0.    1.983 1.129 0.12  1.165
  0.256 0.002 0.369 0.471 0.298 1.232 1.318 0.473]
 [0.    0.001 0.02  0.488 0.489 1.679 1.78  0.    0.538 0.457 0.085 0.674
  0.31  0.001 1.216 0.263 0.358 0.864 0.479 1.925]
 [0.003 0.011 0.    0.057 0.461 0.563 1.542 0.004 0.209 0.602 0.096 0.182
  1.32  0.001 1.517 0.35  0.646 0.001 1.209 0.356]
 [0.    0.    0.    0.02  0.    0.003 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.219 0.208 0.168 0.    0.001 0.72 ]
 [0.    0.    0.001 0.548 0.004 0.321 0.226 0.    0.    0.43  0.001 0.001
  0.001 0.    0.23  0.362 1.262 0.    0.343 1.099]
 [0.    0.    0.    0.512 0.002 0.003 0.001 0.    0.    0.    0.    0.
  0.    0.    0.369 1.685 1.844 0.    0.303 0.283]
 [0.    0.    0.    0.194 0.002 0.23  0.002 0.    0.    0.01  0.    0.
  0.    0.    0.35  0.29  0.286 0.    0.173 0.226]
 [2.575 0.134 0.002 0.099 0.721 0.224 0.747 0.001 0.314 0.5   0.133 0.402
  2.015 0.715 0.223 0.126 0.187 1.145 0.496 0.468]
 [0.    0.    0.002 0.307 1.631 0.955 1.315 0.    0.001 0.812 0.469 0.29
  1.814 0.    1.241 1.172 0.85  0.    1.354 0.693]
 [0.    0.    0.    0.107 0.014 2.22  0.13  0.    0.    0.007 0.    0.001
  0.    0.    1.206 1.277 1.778 0.    0.824 0.229]
 [0.    0.    0.    0.883 0.119 0.589 2.337 0.    0.    0.255 0.001 0.
  0.    0.    1.537 0.51  1.052 0.    0.292 1.362]
 [0.    0.    0.    0.146 0.445 0.251 1.695 0.    0.001 0.344 0.126 0.001
  0.142 0.    0.17  0.811 0.294 0.    0.121 0.239]
 [0.    0.    0.    0.115 0.147 1.414 1.068 0.    0.    0.198 2.171 0.001
  0.001 0.    0.305 0.377 0.303 0.    0.485 1.129]
 [0.144 0.035 0.032 0.686 0.349 1.666 0.331 0.001 0.297 0.355 1.732 0.533
  2.088 0.001 0.438 0.424 1.071 0.35  0.74  2.037]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.232 0.002 0.    0.001 0.002]
 [0.    0.    0.    0.002 0.001 0.    0.002 0.    0.    0.    0.    0.
  0.    0.    0.001 0.007 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.005 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.163 1.192 0.004 0.    0.001 0.001]
 [0.    0.    0.03  0.043 0.473 0.295 1.42  0.    3.194 0.6   0.094 0.739
  0.088 0.    0.351 0.173 0.197 0.001 0.429 0.269]
 [0.    0.    0.    0.723 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.34  1.139 0.375 0.    0.001 0.217]
 [0.    0.    0.    0.003 0.    0.    0.001 0.    0.    0.001 0.    0.
  0.    0.    0.114 1.816 0.289 0.    0.001 0.002]]
[[0.    3.16  0.    0.    0.901 1.126 0.316 0.    1.983 1.129 0.    1.165
  0.    0.    0.369 0.471 0.    1.232 1.318 0.473]
 [0.    0.    0.    0.488 0.489 1.679 1.78  0.    0.538 0.457 0.    0.674
  0.31  0.    1.216 0.    0.358 0.864 0.479 1.925]
 [0.    0.    0.    0.    0.461 0.563 1.542 0.    0.    0.602 0.    0.
  1.32  0.    1.517 0.35  0.646 0.    1.209 0.356]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.72 ]
 [0.    0.    0.    0.548 0.    0.321 0.    0.    0.    0.43  0.    0.
  0.    0.    0.    0.362 1.262 0.    0.343 1.099]
 [0.    0.    0.    0.512 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.369 1.685 1.844 0.    0.303 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.35  0.    0.    0.    0.    0.   ]
 [2.575 0.    0.    0.    0.721 0.    0.747 0.    0.314 0.5   0.    0.402
  2.015 0.715 0.    0.    0.    1.145 0.496 0.468]
 [0.    0.    0.    0.307 1.631 0.955 1.315 0.    0.    0.812 0.469 0.
  1.814 0.    1.241 1.172 0.85  0.    1.354 0.693]
 [0.    0.    0.    0.    0.    2.22  0.    0.    0.    0.    0.    0.
  0.    0.    1.206 1.277 1.778 0.    0.824 0.   ]
 [0.    0.    0.    0.883 0.    0.589 2.337 0.    0.    0.    0.    0.
  0.    0.    1.537 0.51  1.052 0.    0.    1.362]
 [0.    0.    0.    0.    0.445 0.    1.695 0.    0.    0.344 0.    0.
  0.    0.    0.    0.811 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.414 1.068 0.    0.    0.    2.171 0.
  0.    0.    0.305 0.377 0.303 0.    0.485 1.129]
 [0.    0.    0.    0.686 0.349 1.666 0.331 0.    0.    0.355 1.732 0.533
  2.088 0.    0.438 0.424 1.071 0.35  0.74  2.037]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.163 1.192 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.473 0.    1.42  0.    3.194 0.6   0.    0.739
  0.    0.    0.351 0.    0.    0.    0.429 0.   ]
 [0.    0.    0.    0.723 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.34  1.139 0.375 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.816 0.    0.    0.    0.   ]]
{'fdr': 0.43548387096774194, 'tpr': 0.875, 'fpr': 0.4909090909090909, 'f1': 0.6862745098039216, 'shd': 59, 'npred': 124, 'ntrue': 80}
[3.160e+00 6.817e-02 2.426e-01 9.011e-01 1.126e+00 3.163e-01 2.451e-04
 1.983e+00 1.129e+00 1.199e-01 1.165e+00 2.565e-01 1.720e-03 3.689e-01
 4.710e-01 2.985e-01 1.232e+00 1.318e+00 4.734e-01 3.848e-04 1.950e-02
 4.880e-01 4.892e-01 1.679e+00 1.780e+00 1.651e-04 5.383e-01 4.574e-01
 8.496e-02 6.742e-01 3.101e-01 1.243e-03 1.216e+00 2.632e-01 3.583e-01
 8.641e-01 4.786e-01 1.925e+00 3.012e-03 1.114e-02 5.721e-02 4.609e-01
 5.628e-01 1.542e+00 3.518e-03 2.088e-01 6.016e-01 9.569e-02 1.816e-01
 1.320e+00 1.146e-03 1.517e+00 3.497e-01 6.460e-01 8.312e-04 1.209e+00
 3.563e-01 7.154e-05 3.162e-05 8.218e-05 4.798e-04 3.073e-03 1.289e-03
 7.581e-05 1.064e-04 1.709e-03 4.479e-04 2.404e-04 9.334e-05 2.514e-06
 2.192e-01 2.084e-01 1.680e-01 2.190e-05 1.282e-03 7.203e-01 4.023e-06
 3.583e-05 7.899e-04 5.476e-01 3.211e-01 2.265e-01 1.652e-05 1.329e-04
 4.301e-01 1.416e-03 1.076e-03 1.379e-03 6.984e-05 2.300e-01 3.616e-01
 1.262e+00 2.765e-05 3.429e-01 1.099e+00 7.918e-05 1.903e-04 6.185e-05
 5.120e-01 1.964e-03 1.467e-03 4.568e-05 7.263e-05 3.905e-04 7.718e-05
 2.131e-04 2.303e-04 3.153e-05 3.690e-01 1.685e+00 1.844e+00 1.213e-05
 3.033e-01 2.827e-01 5.405e-05 1.905e-04 2.073e-05 1.944e-01 1.504e-03
 2.300e-01 2.882e-05 1.555e-05 9.544e-03 1.265e-04 1.946e-04 5.499e-05
 3.963e-06 3.503e-01 2.902e-01 2.856e-01 1.849e-06 1.726e-01 2.256e-01
 2.575e+00 1.341e-01 1.790e-03 9.915e-02 7.210e-01 2.236e-01 7.472e-01
 3.139e-01 4.998e-01 1.331e-01 4.023e-01 2.015e+00 7.148e-01 2.232e-01
 1.257e-01 1.867e-01 1.145e+00 4.956e-01 4.677e-01 1.043e-05 1.566e-05
 1.501e-03 3.065e-01 1.631e+00 9.554e-01 1.315e+00 1.005e-05 8.118e-01
 4.688e-01 2.899e-01 1.814e+00 7.495e-05 1.241e+00 1.172e+00 8.500e-01
 4.937e-05 1.354e+00 6.935e-01 4.251e-05 4.399e-04 4.888e-05 1.075e-01
 1.442e-02 2.220e+00 1.302e-01 2.448e-05 1.734e-04 4.571e-04 6.324e-04
 2.456e-04 1.329e-04 1.206e+00 1.277e+00 1.778e+00 3.181e-05 8.241e-01
 2.285e-01 6.812e-06 4.479e-05 6.347e-05 8.828e-01 1.191e-01 5.885e-01
 2.337e+00 1.987e-05 6.453e-05 2.550e-01 3.044e-04 2.415e-04 6.618e-05
 1.537e+00 5.101e-01 1.052e+00 2.168e-05 2.919e-01 1.362e+00 2.695e-05
 8.945e-05 3.698e-04 1.459e-01 4.451e-01 2.513e-01 1.695e+00 1.485e-05
 8.948e-04 3.442e-01 1.264e-01 1.419e-01 3.623e-04 1.698e-01 8.107e-01
 2.942e-01 1.665e-04 1.207e-01 2.393e-01 4.894e-06 1.363e-05 1.110e-04
 1.150e-01 1.471e-01 1.414e+00 1.068e+00 2.839e-05 2.346e-04 1.983e-01
 2.171e+00 1.338e-03 8.429e-05 3.054e-01 3.769e-01 3.027e-01 1.869e-05
 4.848e-01 1.129e+00 1.440e-01 3.507e-02 3.187e-02 6.859e-01 3.486e-01
 1.666e+00 3.308e-01 6.276e-04 2.974e-01 3.551e-01 1.732e+00 5.327e-01
 2.088e+00 4.383e-01 4.245e-01 1.071e+00 3.505e-01 7.400e-01 2.037e+00
 3.940e-05 5.840e-05 2.678e-05 3.670e-03 8.969e-04 7.133e-04 3.424e-04
 2.610e-05 6.200e-05 5.067e-05 7.687e-05 1.820e-04 2.575e-05 8.649e-06
 2.320e-01 2.413e-03 1.778e-06 5.387e-04 1.665e-03 2.548e-05 2.197e-05
 2.410e-05 2.378e-03 5.501e-04 1.091e-04 2.235e-03 2.167e-05 1.954e-05
 2.333e-04 8.245e-05 4.915e-05 1.816e-05 1.198e-05 1.388e-03 1.165e-03
 5.132e-06 5.297e-04 2.246e-04 4.703e-05 8.511e-05 2.353e-05 4.955e-03
 8.604e-04 5.804e-04 8.109e-04 3.773e-05 2.227e-05 7.036e-05 4.794e-05
 6.072e-05 1.926e-05 2.702e-05 1.163e+00 1.192e+00 4.931e-06 6.828e-04
 1.480e-03 8.972e-05 1.123e-04 2.994e-02 4.292e-02 4.735e-01 2.947e-01
 1.420e+00 8.571e-05 3.194e+00 6.000e-01 9.361e-02 7.394e-01 8.795e-02
 1.207e-04 3.509e-01 1.727e-01 1.972e-01 4.294e-01 2.694e-01 6.266e-05
 1.634e-05 1.293e-04 7.226e-01 9.299e-04 8.575e-04 6.750e-04 4.296e-05
 1.412e-04 8.260e-04 2.036e-04 2.445e-04 1.451e-04 1.972e-05 3.401e-01
 1.139e+00 3.754e-01 1.686e-05 2.167e-01 3.476e-05 8.012e-05 6.030e-05
 2.951e-03 3.990e-04 3.909e-04 5.370e-04 1.730e-05 6.217e-05 7.375e-04
 2.464e-04 1.339e-04 1.966e-04 1.642e-05 1.137e-01 1.816e+00 2.890e-01
 1.372e-05 6.522e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.9407500000000001, 0.893244296384595)
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
total norm for a microbatch 66.5973018384543 clip 9.735638618462225
total norm for a microbatch 60.48717948546044 clip 24.671505942188162
total norm for a microbatch 50.9761366474025 clip 26.799095292333973
total norm for a microbatch 49.10196360149956 clip 56.41908031614354
total norm for a microbatch 70.09784810064131 clip 53.10109963145867
cuda
Objective function 130.79 = squared loss an data 127.02 + 0.5*rho*h**2 2.941248 + alpha*h 0.000000 + L2reg 0.54 + L1reg 0.28 ; SHD = 130 ; DAG False
Proportion of microbatches that were clipped  0.9338001273074474
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.4253855967525375
iteration 1 in outer loop, alpha = 2.4253855967525375, rho = 1.0, h = 2.4253855967525375
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 136.67 = squared loss an data 127.02 + 0.5*rho*h**2 2.941248 + alpha*h 5.882495 + L2reg 0.54 + L1reg 0.28 ; SHD = 130 ; DAG False
total norm for a microbatch 65.98487863846842 clip 5.796128526196119
total norm for a microbatch 145.23066271441135 clip 7.00071832024005
total norm for a microbatch 86.65828836181906 clip 38.02246464080932
total norm for a microbatch 56.20988656090629 clip 54.408022080664445
total norm for a microbatch 62.26526567420869 clip 59.63679265059251
cuda
Objective function 51.54 = squared loss an data 44.12 + 0.5*rho*h**2 1.652699 + alpha*h 4.409534 + L2reg 1.09 + L1reg 0.26 ; SHD = 102 ; DAG False
Proportion of microbatches that were clipped  0.9443723645799545
iteration 1 in inner loop, alpha 2.4253855967525375 rho 1.0 h 1.8180755830572437
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 66.42 = squared loss an data 44.12 + 0.5*rho*h**2 16.526994 + alpha*h 4.409534 + L2reg 1.09 + L1reg 0.26 ; SHD = 102 ; DAG False
total norm for a microbatch 98.15431404715957 clip 3.856639643488005
total norm for a microbatch 85.68630210096212 clip 20.829888840481726
total norm for a microbatch 142.42005404757793 clip 39.41742534075676
total norm for a microbatch 122.43816035454016 clip 83.07790943856334
total norm for a microbatch 96.13612955367927 clip 100.38505658833898
cuda
Objective function 41.14 = squared loss an data 32.72 + 0.5*rho*h**2 4.493465 + alpha*h 2.299251 + L2reg 1.40 + L1reg 0.23 ; SHD = 80 ; DAG False
Proportion of microbatches that were clipped  0.9594785613318675
iteration 2 in inner loop, alpha 2.4253855967525375 rho 10.0 h 0.9479941743596072
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 81.58 = squared loss an data 32.72 + 0.5*rho*h**2 44.934648 + alpha*h 2.299251 + L2reg 1.40 + L1reg 0.23 ; SHD = 80 ; DAG False
total norm for a microbatch 315.46040209110936 clip 1.7191142432582052
total norm for a microbatch 214.7486203081009 clip 23.116110703380883
total norm for a microbatch 163.35711406728112 clip 32.907788644336755
total norm for a microbatch 87.95561820922072 clip 118.35339392634755
cuda
Objective function 43.94 = squared loss an data 34.87 + 0.5*rho*h**2 6.430125 + alpha*h 0.869772 + L2reg 1.57 + L1reg 0.20 ; SHD = 68 ; DAG True
Proportion of microbatches that were clipped  0.9710982658959537
iteration 3 in inner loop, alpha 2.4253855967525375 rho 100.0 h 0.3586119107510939
iteration 2 in outer loop, alpha = 38.28657667186192, rho = 100.0, h = 0.3586119107510939
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 56.80 = squared loss an data 34.87 + 0.5*rho*h**2 6.430125 + alpha*h 13.730022 + L2reg 1.57 + L1reg 0.20 ; SHD = 68 ; DAG True
total norm for a microbatch 137.9723079960124 clip 1.7234391714497566
total norm for a microbatch 204.1073987397717 clip 1.7234391714497566
total norm for a microbatch 104.01344577633917 clip 2.4961687087424425
total norm for a microbatch 201.79239169554975 clip 6.133355975213887
total norm for a microbatch 134.65865221144685 clip 7.901057120076323
total norm for a microbatch 152.62698385463077 clip 13.659422633934094
total norm for a microbatch 198.41239828247316 clip 16.47420604951095
total norm for a microbatch 218.74550503999194 clip 91.14236758354487
total norm for a microbatch 173.25768931735692 clip 135.90448009513227
total norm for a microbatch 116.6387520894826 clip 137.5154583487902
cuda
Objective function 49.16 = squared loss an data 34.71 + 0.5*rho*h**2 3.055197 + alpha*h 9.464140 + L2reg 1.73 + L1reg 0.19 ; SHD = 72 ; DAG True
Proportion of microbatches that were clipped  0.9699689897176432
iteration 1 in inner loop, alpha 38.28657667186192 rho 100.0 h 0.24719211056229184
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 76.65 = squared loss an data 34.71 + 0.5*rho*h**2 30.551970 + alpha*h 9.464140 + L2reg 1.73 + L1reg 0.19 ; SHD = 72 ; DAG True
total norm for a microbatch 100.81867563138427 clip 2.08949831268089
total norm for a microbatch 161.99136604213146 clip 6.143904674117205
total norm for a microbatch 408.7142834521687 clip 13.769505993346549
cuda
Objective function 47.85 = squared loss an data 35.91 + 0.5*rho*h**2 5.796079 + alpha*h 4.122196 + L2reg 1.84 + L1reg 0.18 ; SHD = 69 ; DAG True
Proportion of microbatches that were clipped  0.972580902279611
iteration 2 in inner loop, alpha 38.28657667186192 rho 1000.0 h 0.10766688150507164
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 100.01 = squared loss an data 35.91 + 0.5*rho*h**2 57.960787 + alpha*h 4.122196 + L2reg 1.84 + L1reg 0.18 ; SHD = 69 ; DAG True
total norm for a microbatch 136.33219328434595 clip 5.06717997469573
total norm for a microbatch 258.328865506322 clip 47.46975445010847
total norm for a microbatch 194.24056569213036 clip 52.02686497924699
total norm for a microbatch 187.13286735848334 clip 108.16866203959832
total norm for a microbatch 279.76430059219143 clip 147.38711324195432
cuda
Objective function 49.90 = squared loss an data 38.00 + 0.5*rho*h**2 8.246978 + alpha*h 1.554923 + L2reg 1.93 + L1reg 0.17 ; SHD = 71 ; DAG True
Proportion of microbatches that were clipped  0.9803003390925238
iteration 3 in inner loop, alpha 38.28657667186192 rho 10000.0 h 0.04061275253608443
iteration 3 in outer loop, alpha = 444.4141020327062, rho = 10000.0, h = 0.04061275253608443
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 66.39 = squared loss an data 38.00 + 0.5*rho*h**2 8.246978 + alpha*h 18.048880 + L2reg 1.93 + L1reg 0.17 ; SHD = 71 ; DAG True
total norm for a microbatch 230.8522142192175 clip 1.0
total norm for a microbatch 284.0943333460864 clip 1.401524849263198
total norm for a microbatch 211.72681088051297 clip 24.48492849976962
total norm for a microbatch 201.422109448532 clip 31.635876672410046
total norm for a microbatch 172.36313776002817 clip 60.189203515601456
cuda
Objective function 53.35 = squared loss an data 37.67 + 0.5*rho*h**2 2.855578 + alpha*h 10.620619 + L2reg 2.04 + L1reg 0.17 ; SHD = 73 ; DAG True
Proportion of microbatches that were clipped  0.9855118611686037
iteration 1 in inner loop, alpha 444.4141020327062 rho 10000.0 h 0.02389802339723701
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 79.05 = squared loss an data 37.67 + 0.5*rho*h**2 28.555776 + alpha*h 10.620619 + L2reg 2.04 + L1reg 0.17 ; SHD = 73 ; DAG True
total norm for a microbatch 475.6583883870831 clip 1.0
total norm for a microbatch 316.82717457649125 clip 1.7444983489094892
total norm for a microbatch 154.68387864783944 clip 10.755457944377467
total norm for a microbatch 226.7613902196051 clip 20.31047702811882
total norm for a microbatch 250.1952855711741 clip 54.65658902491213
cuda
Objective function 53.94 = squared loss an data 37.55 + 0.5*rho*h**2 8.359140 + alpha*h 5.746238 + L2reg 2.12 + L1reg 0.18 ; SHD = 74 ; DAG True
Proportion of microbatches that were clipped  0.99269124573656
iteration 2 in inner loop, alpha 444.4141020327062 rho 100000.0 h 0.012929918936439577
iteration 4 in outer loop, alpha = 13374.333038472283, rho = 1000000.0, h = 0.012929918936439577
Threshold 0.3
[[0.005 2.251 0.589 0.792 0.559 0.635 0.763 0.266 0.298 0.726 0.4   0.415
  0.69  0.295 0.31  0.51  0.451 0.638 0.766 0.493]
 [0.003 0.006 0.144 0.498 0.314 0.485 0.733 0.099 0.098 0.355 0.116 0.192
  0.122 0.098 0.122 0.287 0.207 0.116 0.238 0.231]
 [0.01  0.043 0.006 0.129 0.23  0.031 0.246 0.057 0.037 0.187 0.033 0.247
  0.035 0.034 0.07  0.299 0.094 0.254 0.109 0.062]
 [0.007 0.014 0.029 0.006 0.028 0.022 0.052 0.043 0.011 0.019 0.013 0.082
  0.069 0.009 0.06  0.125 0.033 0.045 0.067 0.006]
 [0.009 0.02  0.038 0.132 0.007 0.015 0.027 0.008 0.006 0.018 0.022 0.033
  0.036 0.009 0.039 0.072 0.03  0.014 0.026 0.017]
 [0.006 0.016 0.244 0.349 0.422 0.004 0.199 0.14  0.016 0.386 0.087 0.202
  0.071 0.025 0.343 0.892 1.052 0.252 0.348 0.199]
 [0.003 0.008 0.031 0.11  0.173 0.043 0.008 0.071 0.015 0.143 0.008 0.19
  0.042 0.03  0.087 0.138 0.132 0.065 0.077 0.055]
 [0.028 0.061 0.113 0.118 0.558 0.05  0.114 0.005 0.034 0.174 0.118 0.312
  0.073 0.035 0.108 0.21  0.12  0.096 0.067 0.115]
 [0.026 0.051 0.23  0.441 0.694 0.442 0.387 0.181 0.006 0.369 0.256 0.51
  0.343 0.081 0.51  0.602 0.608 0.623 0.663 0.251]
 [0.004 0.021 0.063 0.425 0.425 0.022 0.045 0.036 0.014 0.006 0.033 0.118
  0.044 0.016 0.083 0.324 0.042 0.069 0.079 0.059]
 [0.017 0.033 0.231 0.583 0.345 0.072 0.911 0.045 0.039 0.208 0.009 0.283
  0.508 0.021 0.206 0.307 0.162 0.296 0.149 0.625]
 [0.015 0.039 0.043 0.084 0.257 0.024 0.058 0.024 0.018 0.068 0.024 0.009
  0.059 0.023 0.072 0.186 0.04  0.059 0.044 0.028]
 [0.006 0.057 0.164 0.115 0.197 0.088 0.159 0.079 0.025 0.204 0.015 0.176
  0.005 0.022 0.256 0.346 0.144 0.091 0.139 0.113]
 [0.025 0.057 0.207 0.621 0.411 0.313 0.291 0.272 0.126 0.476 0.277 0.276
  0.398 0.008 0.515 0.432 0.469 0.276 0.255 0.663]
 [0.014 0.033 0.099 0.159 0.233 0.018 0.076 0.066 0.014 0.094 0.021 0.139
  0.019 0.018 0.009 0.278 0.014 0.076 0.055 0.033]
 [0.007 0.02  0.021 0.074 0.121 0.008 0.044 0.044 0.007 0.022 0.014 0.042
  0.021 0.013 0.03  0.006 0.01  0.032 0.008 0.009]
 [0.008 0.021 0.067 0.157 0.2   0.008 0.07  0.028 0.014 0.206 0.034 0.229
  0.044 0.011 0.626 0.705 0.007 0.072 0.079 0.044]
 [0.01  0.042 0.039 0.233 0.575 0.024 0.132 0.077 0.015 0.136 0.024 0.18
  0.105 0.031 0.108 0.27  0.094 0.008 0.052 0.029]
 [0.01  0.044 0.081 0.13  0.36  0.018 0.097 0.081 0.008 0.112 0.035 0.105
  0.067 0.038 0.077 1.052 0.113 0.125 0.007 0.07 ]
 [0.013 0.028 0.11  0.663 0.432 0.046 0.122 0.073 0.035 0.247 0.01  0.331
  0.055 0.012 0.125 1.045 0.131 0.204 0.129 0.009]]
[[0.    2.251 0.589 0.792 0.559 0.635 0.763 0.    0.    0.726 0.4   0.415
  0.69  0.    0.31  0.51  0.451 0.638 0.766 0.493]
 [0.    0.    0.    0.498 0.314 0.485 0.733 0.    0.    0.355 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.349 0.422 0.    0.    0.    0.    0.386 0.    0.
  0.    0.    0.343 0.892 1.052 0.    0.348 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.558 0.    0.    0.    0.    0.    0.    0.312
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.441 0.694 0.442 0.387 0.    0.    0.369 0.    0.51
  0.343 0.    0.51  0.602 0.608 0.623 0.663 0.   ]
 [0.    0.    0.    0.425 0.425 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.324 0.    0.    0.    0.   ]
 [0.    0.    0.    0.583 0.345 0.    0.911 0.    0.    0.    0.    0.
  0.508 0.    0.    0.307 0.    0.    0.    0.625]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.346 0.    0.    0.    0.   ]
 [0.    0.    0.    0.621 0.411 0.313 0.    0.    0.    0.476 0.    0.
  0.398 0.    0.515 0.432 0.469 0.    0.    0.663]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.626 0.705 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.575 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.36  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.052 0.    0.    0.    0.   ]
 [0.    0.    0.    0.663 0.432 0.    0.    0.    0.    0.    0.    0.331
  0.    0.    0.    1.045 0.    0.    0.    0.   ]]
{'fdr': 0.4857142857142857, 'tpr': 0.45, 'fpr': 0.3090909090909091, 'f1': 0.48, 'shd': 74, 'npred': 70, 'ntrue': 80}
[2.251 0.589 0.792 0.559 0.635 0.763 0.266 0.298 0.726 0.4   0.415 0.69
 0.295 0.31  0.51  0.451 0.638 0.766 0.493 0.003 0.144 0.498 0.314 0.485
 0.733 0.099 0.098 0.355 0.116 0.192 0.122 0.098 0.122 0.287 0.207 0.116
 0.238 0.231 0.01  0.043 0.129 0.23  0.031 0.246 0.057 0.037 0.187 0.033
 0.247 0.035 0.034 0.07  0.299 0.094 0.254 0.109 0.062 0.007 0.014 0.029
 0.028 0.022 0.052 0.043 0.011 0.019 0.013 0.082 0.069 0.009 0.06  0.125
 0.033 0.045 0.067 0.006 0.009 0.02  0.038 0.132 0.015 0.027 0.008 0.006
 0.018 0.022 0.033 0.036 0.009 0.039 0.072 0.03  0.014 0.026 0.017 0.006
 0.016 0.244 0.349 0.422 0.199 0.14  0.016 0.386 0.087 0.202 0.071 0.025
 0.343 0.892 1.052 0.252 0.348 0.199 0.003 0.008 0.031 0.11  0.173 0.043
 0.071 0.015 0.143 0.008 0.19  0.042 0.03  0.087 0.138 0.132 0.065 0.077
 0.055 0.028 0.061 0.113 0.118 0.558 0.05  0.114 0.034 0.174 0.118 0.312
 0.073 0.035 0.108 0.21  0.12  0.096 0.067 0.115 0.026 0.051 0.23  0.441
 0.694 0.442 0.387 0.181 0.369 0.256 0.51  0.343 0.081 0.51  0.602 0.608
 0.623 0.663 0.251 0.004 0.021 0.063 0.425 0.425 0.022 0.045 0.036 0.014
 0.033 0.118 0.044 0.016 0.083 0.324 0.042 0.069 0.079 0.059 0.017 0.033
 0.231 0.583 0.345 0.072 0.911 0.045 0.039 0.208 0.283 0.508 0.021 0.206
 0.307 0.162 0.296 0.149 0.625 0.015 0.039 0.043 0.084 0.257 0.024 0.058
 0.024 0.018 0.068 0.024 0.059 0.023 0.072 0.186 0.04  0.059 0.044 0.028
 0.006 0.057 0.164 0.115 0.197 0.088 0.159 0.079 0.025 0.204 0.015 0.176
 0.022 0.256 0.346 0.144 0.091 0.139 0.113 0.025 0.057 0.207 0.621 0.411
 0.313 0.291 0.272 0.126 0.476 0.277 0.276 0.398 0.515 0.432 0.469 0.276
 0.255 0.663 0.014 0.033 0.099 0.159 0.233 0.018 0.076 0.066 0.014 0.094
 0.021 0.139 0.019 0.018 0.278 0.014 0.076 0.055 0.033 0.007 0.02  0.021
 0.074 0.121 0.008 0.044 0.044 0.007 0.022 0.014 0.042 0.021 0.013 0.03
 0.01  0.032 0.008 0.009 0.008 0.021 0.067 0.157 0.2   0.008 0.07  0.028
 0.014 0.206 0.034 0.229 0.044 0.011 0.626 0.705 0.072 0.079 0.044 0.01
 0.042 0.039 0.233 0.575 0.024 0.132 0.077 0.015 0.136 0.024 0.18  0.105
 0.031 0.108 0.27  0.094 0.052 0.029 0.01  0.044 0.081 0.13  0.36  0.018
 0.097 0.081 0.008 0.112 0.035 0.105 0.067 0.038 0.077 1.052 0.113 0.125
 0.07  0.013 0.028 0.11  0.663 0.432 0.046 0.122 0.073 0.035 0.247 0.01
 0.331 0.055 0.012 0.125 1.045 0.131 0.204 0.129]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.7540416666666666, 0.5427933850864495)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
