samples  5000  graph  20 60 ER mim  minibatch size  50  noise  0.8  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.09213537717594988
iteration 3 in outer loop, alpha = 15.699588471753614, rho = 100.0, h = 0.09213537717594988
cuda
iteration 1 in inner loop,alpha 15.699588471753614 rho 100.0 h 0.05367211647120129
iteration 2 in inner loop,alpha 15.699588471753614 rho 1000.0 h 0.01978300963797608
iteration 4 in outer loop, alpha = 35.48259810972969, rho = 1000.0, h = 0.01978300963797608
cuda
iteration 1 in inner loop,alpha 35.48259810972969 rho 1000.0 h 0.010932558322785724
iteration 2 in inner loop,alpha 35.48259810972969 rho 10000.0 h 0.0038357520003415857
iteration 5 in outer loop, alpha = 73.84011811314555, rho = 10000.0, h = 0.0038357520003415857
cuda
iteration 1 in inner loop,alpha 73.84011811314555 rho 10000.0 h 0.0014753128718609787
iteration 2 in inner loop,alpha 73.84011811314555 rho 100000.0 h 0.0005835795180928471
iteration 6 in outer loop, alpha = 132.19806992243025, rho = 100000.0, h = 0.0005835795180928471
cuda
iteration 1 in inner loop,alpha 132.19806992243025 rho 100000.0 h 0.00034705210253704877
iteration 7 in outer loop, alpha = 479.250172459479, rho = 1000000.0, h = 0.00034705210253704877
Threshold 0.3
[[0.    0.024 0.01  0.045 0.158 0.182 0.067 0.001 0.034 0.044 0.45  0.634
  1.036 0.001 0.098 0.001 0.113 2.565 0.515 0.014]
 [0.    0.001 0.001 0.034 0.084 0.461 0.12  0.    0.005 0.059 0.108 0.
  0.    0.    0.072 0.139 0.01  0.    0.086 0.801]
 [0.001 0.288 0.002 0.124 0.543 0.092 0.063 0.001 1.434 0.747 0.109 0.005
  0.006 0.    0.034 0.215 0.1   0.033 0.136 0.112]
 [0.    0.007 0.    0.002 0.001 0.004 0.    0.    0.    0.01  0.128 0.
  0.    0.    0.052 0.001 0.    0.    0.002 0.029]
 [0.    0.003 0.    0.16  0.002 0.008 0.003 0.    0.    0.051 0.143 0.
  0.    0.    0.038 0.12  0.004 0.    0.001 0.009]
 [0.    0.002 0.005 0.103 0.031 0.005 0.01  0.    0.027 0.022 0.051 0.
  0.003 0.    1.194 0.071 0.008 0.001 0.293 0.046]
 [0.    0.006 0.006 0.212 0.29  0.242 0.003 0.    0.015 0.157 0.397 0.001
  0.005 0.    0.09  0.064 0.001 0.    0.01  0.144]
 [0.003 0.1   0.001 1.1   0.047 0.056 0.166 0.    0.343 0.074 0.061 2.149
  1.346 0.042 0.109 0.019 0.032 2.193 0.047 0.032]
 [0.    0.025 0.    1.437 1.39  0.018 0.094 0.    0.003 0.361 1.14  0.
  0.001 0.    0.42  0.139 0.028 0.    0.006 0.016]
 [0.    0.002 0.001 0.036 0.005 0.201 0.014 0.    0.005 0.004 1.073 0.001
  0.002 0.    0.102 0.008 0.009 0.    0.006 0.665]
 [0.    0.002 0.    0.002 0.001 0.013 0.004 0.    0.001 0.001 0.002 0.
  0.    0.    0.013 0.005 0.003 0.    0.003 0.748]
 [0.    2.087 0.001 1.588 1.301 0.059 0.534 0.    1.027 0.07  0.942 0.001
  0.001 0.    0.004 0.902 0.411 0.    0.068 0.14 ]
 [0.    0.033 0.004 0.175 0.08  0.112 0.322 0.    0.005 0.555 0.046 0.557
  0.003 0.    1.6   0.158 0.197 0.    0.004 0.042]
 [0.    1.036 4.611 0.093 0.048 0.164 0.067 0.002 0.818 0.177 0.038 0.02
  0.052 0.    0.058 0.667 0.001 0.083 0.706 1.197]
 [0.    0.001 0.001 0.012 0.019 0.001 0.001 0.    0.002 0.004 0.01  0.
  0.    0.    0.002 0.012 0.001 0.    0.001 0.003]
 [0.    0.001 0.002 0.067 0.005 0.014 0.008 0.    0.004 0.284 0.047 0.
  0.    0.    0.037 0.003 0.008 0.    0.001 0.096]
 [0.    0.014 0.006 0.982 0.094 0.002 0.874 0.    0.021 0.006 0.098 0.001
  0.004 0.    0.557 0.008 0.002 0.001 0.01  0.053]
 [0.    0.048 0.004 0.291 0.048 0.048 0.063 0.    0.11  0.046 0.116 0.888
  1.901 0.    0.001 0.069 0.032 0.002 0.025 1.248]
 [0.    0.001 0.01  0.092 0.774 0.014 0.027 0.    0.415 0.19  0.153 0.001
  0.007 0.    0.694 1.03  0.01  0.007 0.004 0.066]
 [0.    0.    0.    0.003 0.001 0.017 0.001 0.    0.001 0.001 0.001 0.
  0.001 0.    0.053 0.001 0.002 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.45  0.634
  1.036 0.    0.    0.    0.    2.565 0.515 0.   ]
 [0.    0.    0.    0.    0.    0.461 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.801]
 [0.    0.    0.    0.    0.543 0.    0.    0.    1.434 0.747 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.194 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.397 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.1   0.    0.    0.    0.    0.343 0.    0.    2.149
  1.346 0.    0.    0.    0.    2.193 0.    0.   ]
 [0.    0.    0.    1.437 1.39  0.    0.    0.    0.    0.361 1.14  0.
  0.    0.    0.42  0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.073 0.
  0.    0.    0.    0.    0.    0.    0.    0.665]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.748]
 [0.    2.087 0.    1.588 1.301 0.    0.534 0.    1.027 0.    0.942 0.
  0.    0.    0.    0.902 0.411 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.322 0.    0.    0.555 0.    0.557
  0.    0.    1.6   0.    0.    0.    0.    0.   ]
 [0.    1.036 4.611 0.    0.    0.    0.    0.    0.818 0.    0.    0.
  0.    0.    0.    0.667 0.    0.    0.706 1.197]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.982 0.    0.    0.874 0.    0.    0.    0.    0.
  0.    0.    0.557 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.888
  1.901 0.    0.    0.    0.    0.    0.    1.248]
 [0.    0.    0.    0.    0.774 0.    0.    0.    0.415 0.    0.    0.
  0.    0.    0.694 1.03  0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.11320754716981132, 'tpr': 0.7833333333333333, 'fpr': 0.046153846153846156, 'f1': 0.8318584070796461, 'shd': 15, 'npred': 53, 'ntrue': 60}
[2.427e-02 9.591e-03 4.472e-02 1.576e-01 1.824e-01 6.698e-02 1.208e-03
 3.360e-02 4.389e-02 4.503e-01 6.343e-01 1.036e+00 6.594e-04 9.840e-02
 1.315e-03 1.134e-01 2.565e+00 5.146e-01 1.377e-02 7.712e-06 9.015e-04
 3.435e-02 8.359e-02 4.610e-01 1.204e-01 3.272e-06 5.308e-03 5.916e-02
 1.079e-01 9.280e-05 1.383e-04 2.898e-05 7.248e-02 1.387e-01 1.033e-02
 1.019e-04 8.567e-02 8.007e-01 1.046e-03 2.878e-01 1.242e-01 5.428e-01
 9.237e-02 6.302e-02 6.686e-04 1.434e+00 7.472e-01 1.090e-01 5.178e-03
 5.539e-03 8.409e-05 3.354e-02 2.148e-01 1.004e-01 3.330e-02 1.355e-01
 1.124e-01 1.213e-05 7.271e-03 4.711e-05 1.417e-03 4.435e-03 3.893e-04
 9.692e-06 3.994e-04 9.944e-03 1.275e-01 1.176e-04 2.790e-04 8.233e-06
 5.225e-02 1.185e-03 4.071e-04 3.928e-05 2.004e-03 2.887e-02 1.022e-04
 3.386e-03 1.861e-04 1.604e-01 8.355e-03 3.406e-03 8.188e-06 4.972e-04
 5.062e-02 1.425e-01 1.155e-04 2.722e-04 1.982e-05 3.817e-02 1.201e-01
 4.444e-03 9.373e-05 6.544e-04 8.535e-03 8.139e-05 1.778e-03 5.414e-03
 1.027e-01 3.119e-02 1.023e-02 6.223e-05 2.699e-02 2.159e-02 5.145e-02
 3.684e-04 3.336e-03 9.319e-05 1.194e+00 7.067e-02 8.216e-03 6.869e-04
 2.931e-01 4.566e-02 2.684e-05 6.045e-03 5.810e-03 2.119e-01 2.902e-01
 2.422e-01 1.939e-05 1.456e-02 1.567e-01 3.974e-01 1.185e-03 4.519e-03
 2.742e-04 8.969e-02 6.391e-02 1.400e-03 1.119e-04 9.630e-03 1.440e-01
 3.194e-03 1.001e-01 1.308e-03 1.100e+00 4.725e-02 5.633e-02 1.656e-01
 3.431e-01 7.412e-02 6.123e-02 2.149e+00 1.346e+00 4.169e-02 1.087e-01
 1.871e-02 3.222e-02 2.193e+00 4.691e-02 3.237e-02 3.585e-05 2.542e-02
 2.442e-04 1.437e+00 1.390e+00 1.814e-02 9.365e-02 5.442e-05 3.606e-01
 1.140e+00 3.253e-04 6.757e-04 1.848e-05 4.202e-01 1.393e-01 2.817e-02
 2.024e-04 5.838e-03 1.635e-02 3.887e-05 2.351e-03 7.430e-04 3.556e-02
 5.482e-03 2.007e-01 1.387e-02 5.028e-05 4.974e-03 1.073e+00 1.094e-03
 2.129e-03 9.786e-06 1.021e-01 7.530e-03 8.587e-03 3.225e-04 5.780e-03
 6.652e-01 2.339e-05 2.305e-03 2.074e-04 1.502e-03 5.483e-04 1.301e-02
 3.723e-03 4.629e-06 5.014e-04 8.243e-04 1.739e-04 1.370e-04 6.454e-06
 1.292e-02 4.627e-03 3.426e-03 1.446e-04 2.976e-03 7.483e-01 8.167e-06
 2.087e+00 1.214e-03 1.588e+00 1.301e+00 5.930e-02 5.338e-01 9.349e-06
 1.027e+00 6.994e-02 9.419e-01 1.262e-03 4.560e-04 3.839e-03 9.022e-01
 4.105e-01 1.813e-04 6.837e-02 1.403e-01 3.760e-06 3.339e-02 4.313e-03
 1.748e-01 7.958e-02 1.116e-01 3.220e-01 6.194e-06 4.558e-03 5.554e-01
 4.588e-02 5.566e-01 4.369e-04 1.600e+00 1.583e-01 1.967e-01 8.114e-05
 4.396e-03 4.178e-02 2.565e-04 1.036e+00 4.611e+00 9.252e-02 4.767e-02
 1.639e-01 6.689e-02 1.527e-03 8.182e-01 1.770e-01 3.759e-02 1.962e-02
 5.195e-02 5.839e-02 6.666e-01 1.089e-03 8.294e-02 7.060e-01 1.197e+00
 4.548e-06 8.747e-04 7.413e-04 1.243e-02 1.912e-02 8.025e-04 1.476e-03
 8.595e-06 1.506e-03 4.080e-03 1.050e-02 2.721e-04 3.368e-04 3.668e-05
 1.151e-02 8.221e-04 3.799e-05 1.288e-03 2.776e-03 5.113e-05 7.622e-04
 1.826e-03 6.719e-02 4.641e-03 1.401e-02 7.672e-03 1.074e-04 3.958e-03
 2.836e-01 4.663e-02 4.505e-04 4.585e-04 5.628e-05 3.652e-02 7.796e-03
 2.738e-04 1.436e-03 9.559e-02 4.039e-05 1.355e-02 5.936e-03 9.819e-01
 9.386e-02 2.245e-03 8.744e-01 6.505e-05 2.132e-02 5.981e-03 9.773e-02
 1.251e-03 3.670e-03 2.335e-04 5.566e-01 7.953e-03 5.430e-04 1.008e-02
 5.289e-02 5.556e-06 4.781e-02 3.638e-03 2.907e-01 4.847e-02 4.817e-02
 6.322e-02 1.883e-05 1.100e-01 4.560e-02 1.158e-01 8.882e-01 1.901e+00
 4.618e-04 8.463e-04 6.946e-02 3.232e-02 2.542e-02 1.248e+00 1.185e-04
 1.108e-03 9.713e-03 9.228e-02 7.736e-01 1.430e-02 2.734e-02 3.245e-04
 4.152e-01 1.904e-01 1.531e-01 6.772e-04 6.631e-03 1.502e-04 6.945e-01
 1.030e+00 1.024e-02 7.145e-03 6.584e-02 1.237e-05 3.093e-04 1.042e-04
 2.963e-03 1.430e-03 1.683e-02 1.223e-03 8.125e-06 7.427e-04 6.012e-04
 7.215e-04 3.579e-05 1.110e-03 1.511e-05 5.265e-02 7.034e-04 1.602e-03
 3.112e-04 1.946e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9138541666666666, 0.844416167097804)
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
total norm for a microbatch 16.92147306335002 clip 12.89077675910445
total norm for a microbatch 16.67447497942728 clip 13.529700216216881
total norm for a microbatch 11.269484298531173 clip 12.818648245629621
total norm for a microbatch 15.314526063584152 clip 13.316123881583431
total norm for a microbatch 20.461734273909016 clip 13.256486113348208
total norm for a microbatch 16.654260977193942 clip 14.450268254202859
total norm for a microbatch 21.64952662221545 clip 14.724142460358621
total norm for a microbatch 23.568459466638803 clip 15.924986230486883
total norm for a microbatch 19.0318508174643 clip 15.08198569178398
total norm for a microbatch 28.196591174342437 clip 15.726079507128397
total norm for a microbatch 12.579174963219545 clip 15.345524243451571
total norm for a microbatch 23.092219554948986 clip 15.64183469234499
cuda
Objective function 17.63 = squared loss an data 15.19 + 0.5*rho*h**2 1.689346 + alpha*h 0.000000 + L2reg 0.50 + L1reg 0.25 ; SHD = 65 ; DAG False
Proportion of microbatches that were clipped  0.7980940074301405
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.838121727619658
iteration 1 in outer loop, alpha = 1.838121727619658, rho = 1.0, h = 1.838121727619658
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 21.01 = squared loss an data 15.19 + 0.5*rho*h**2 1.689346 + alpha*h 3.378691 + L2reg 0.50 + L1reg 0.25 ; SHD = 65 ; DAG False
total norm for a microbatch 11.20355390974114 clip 1.2048967501798744
total norm for a microbatch 26.094230210879058 clip 5.734361006279214
total norm for a microbatch 18.460267139458043 clip 12.067532816689715
total norm for a microbatch 23.93872798739955 clip 18.513282299227747
total norm for a microbatch 18.75619948447911 clip 17.625102439438574
total norm for a microbatch 17.78109476217805 clip 19.97210813107446
total norm for a microbatch 20.66535645408736 clip 18.933975843675896
total norm for a microbatch 14.23904755880868 clip 18.19650379994681
total norm for a microbatch 15.78551550311404 clip 19.404205408889926
total norm for a microbatch 35.81036270054302 clip 18.37114304276188
total norm for a microbatch 17.932707145479963 clip 18.914987928965957
total norm for a microbatch 22.83010761436212 clip 20.42079594954638
cuda
Objective function 18.37 = squared loss an data 14.36 + 0.5*rho*h**2 0.735029 + alpha*h 2.228648 + L2reg 0.82 + L1reg 0.23 ; SHD = 62 ; DAG False
Proportion of microbatches that were clipped  0.8002930880078157
iteration 1 in inner loop, alpha 1.838121727619658 rho 1.0 h 1.2124594472779293
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 24.99 = squared loss an data 14.36 + 0.5*rho*h**2 7.350290 + alpha*h 2.228648 + L2reg 0.82 + L1reg 0.23 ; SHD = 62 ; DAG False
total norm for a microbatch 32.548612271919936 clip 6.535292006935008
total norm for a microbatch 26.234102847117548 clip 22.405953507236728
total norm for a microbatch 37.353268769249155 clip 22.348187850945607
total norm for a microbatch 24.402078093548493 clip 22.538473565251937
total norm for a microbatch 15.313110282419462 clip 22.573495258451615
total norm for a microbatch 25.026868589921296 clip 22.3159496205431
total norm for a microbatch 33.631335063968194 clip 23.05872456170117
total norm for a microbatch 34.25978752890718 clip 23.772997389892364
total norm for a microbatch 36.34175676617423 clip 23.51458754331691
total norm for a microbatch 28.891129256929542 clip 23.269875456277063
total norm for a microbatch 28.494463657695736 clip 23.208782586846215
cuda
Objective function 19.29 = squared loss an data 15.96 + 0.5*rho*h**2 1.228800 + alpha*h 0.911234 + L2reg 0.98 + L1reg 0.20 ; SHD = 68 ; DAG True
Proportion of microbatches that were clipped  0.8047408597830454
iteration 2 in inner loop, alpha 1.838121727619658 rho 10.0 h 0.495741878446303
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 30.35 = squared loss an data 15.96 + 0.5*rho*h**2 12.288001 + alpha*h 0.911234 + L2reg 0.98 + L1reg 0.20 ; SHD = 68 ; DAG True
total norm for a microbatch 40.80875210289971 clip 2.948653418152897
total norm for a microbatch 27.17300420063993 clip 25.25584969117044
total norm for a microbatch 38.18359079278931 clip 25.05526558451077
total norm for a microbatch 28.25525122934867 clip 23.879579756766926
total norm for a microbatch 18.904892877078204 clip 23.94213851666718
total norm for a microbatch 42.582898135081905 clip 24.364451776475953
total norm for a microbatch 25.274745432048256 clip 26.019687357063695
total norm for a microbatch 41.27341636823105 clip 26.864333926377157
cuda
Objective function 20.63 = squared loss an data 17.56 + 0.5*rho*h**2 1.531631 + alpha*h 0.321711 + L2reg 1.04 + L1reg 0.18 ; SHD = 64 ; DAG True
Proportion of microbatches that were clipped  0.8067683817562084
iteration 3 in inner loop, alpha 1.838121727619658 rho 100.0 h 0.17502175303824075
iteration 2 in outer loop, alpha = 19.340297031443733, rho = 100.0, h = 0.17502175303824075
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 23.70 = squared loss an data 17.56 + 0.5*rho*h**2 1.531631 + alpha*h 3.384973 + L2reg 1.04 + L1reg 0.18 ; SHD = 64 ; DAG True
total norm for a microbatch 19.230212883242583 clip 1.8892737511837898
total norm for a microbatch 44.68355967943117 clip 4.232688379914981
total norm for a microbatch 28.389729609800412 clip 12.870221525249661
total norm for a microbatch 41.6759959106403 clip 22.033744453236487
total norm for a microbatch 22.603093325288814 clip 28.081397572368886
total norm for a microbatch 40.314541674377914 clip 26.640202010827284
total norm for a microbatch 27.51723481626645 clip 26.798141687786348
total norm for a microbatch 29.58131332152471 clip 27.29584031314297
total norm for a microbatch 33.75974037667135 clip 28.068813521424925
total norm for a microbatch 19.112017014000234 clip 28.212927440257307
total norm for a microbatch 25.613169388405357 clip 25.939270451268044
total norm for a microbatch 38.33297336990311 clip 27.533734057012985
total norm for a microbatch 48.0508266111994 clip 27.41074443374745
total norm for a microbatch 30.86773983621521 clip 28.187888012051552
total norm for a microbatch 24.919211465658528 clip 26.53467289250504
cuda
Objective function 21.99 = squared loss an data 17.92 + 0.5*rho*h**2 0.618731 + alpha*h 2.151439 + L2reg 1.12 + L1reg 0.17 ; SHD = 72 ; DAG True
Proportion of microbatches that were clipped  0.8071694448939958
iteration 1 in inner loop, alpha 19.340297031443733 rho 100.0 h 0.1112412603942019
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 27.56 = squared loss an data 17.92 + 0.5*rho*h**2 6.187309 + alpha*h 2.151439 + L2reg 1.12 + L1reg 0.17 ; SHD = 72 ; DAG True
total norm for a microbatch 23.17223842049929 clip 13.748719700092343
total norm for a microbatch 43.33370602664338 clip 29.475677498009773
total norm for a microbatch 44.78459109561224 clip 30.034740716200826
total norm for a microbatch 37.89113132713867 clip 30.063059590478147
total norm for a microbatch 40.39033461060493 clip 29.676188927657147
total norm for a microbatch 45.522989401297544 clip 29.97806108309587
cuda
Objective function 22.32 = squared loss an data 18.80 + 0.5*rho*h**2 1.215840 + alpha*h 0.953710 + L2reg 1.19 + L1reg 0.16 ; SHD = 59 ; DAG True
Proportion of microbatches that were clipped  0.8102282669211803
iteration 2 in inner loop, alpha 19.340297031443733 rho 1000.0 h 0.04931206255650622
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 33.26 = squared loss an data 18.80 + 0.5*rho*h**2 12.158398 + alpha*h 0.953710 + L2reg 1.19 + L1reg 0.16 ; SHD = 59 ; DAG True
total norm for a microbatch 157.302991548952 clip 1.1023523130427815
total norm for a microbatch 45.933939349751725 clip 1.724667458972577
total norm for a microbatch 39.511740830497786 clip 5.069901930891112
total norm for a microbatch 43.277739138609924 clip 24.459772588059636
total norm for a microbatch 51.488858367718485 clip 41.322345222373976
total norm for a microbatch 43.21290784367915 clip 38.657392443312034
total norm for a microbatch 48.90648160251664 clip 37.7329887347359
total norm for a microbatch 43.43841011577272 clip 34.07744246792404
total norm for a microbatch 39.812984685077815 clip 35.08295688570513
total norm for a microbatch 53.35521444922026 clip 32.25675084123305
total norm for a microbatch 46.891027008413545 clip 33.78546519995097
total norm for a microbatch 39.63472751726489 clip 34.1743611187963
cuda
Objective function 22.36 = squared loss an data 19.23 + 0.5*rho*h**2 1.415162 + alpha*h 0.325373 + L2reg 1.23 + L1reg 0.15 ; SHD = 63 ; DAG True
Proportion of microbatches that were clipped  0.810670707392934
iteration 3 in inner loop, alpha 19.340297031443733 rho 10000.0 h 0.01682356924173689
iteration 3 in outer loop, alpha = 187.57598944881263, rho = 10000.0, h = 0.01682356924173689
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 25.19 = squared loss an data 19.23 + 0.5*rho*h**2 1.415162 + alpha*h 3.155698 + L2reg 1.23 + L1reg 0.15 ; SHD = 63 ; DAG True
total norm for a microbatch 51.89693625353808 clip 25.04474568717045
total norm for a microbatch 56.84892781332226 clip 55.41949710571605
total norm for a microbatch 68.12232450799141 clip 57.4096399724632
total norm for a microbatch 57.188083154365245 clip 49.464490861865286
total norm for a microbatch 33.638974974814865 clip 38.63135487023065
total norm for a microbatch 33.6825252029304 clip 38.37304417916447
total norm for a microbatch 44.65952793897513 clip 36.31674373515656
total norm for a microbatch 38.43658821625187 clip 34.507632591740894
total norm for a microbatch 43.34364577859588 clip 34.507632591740894
total norm for a microbatch 38.977929644928466 clip 33.916432142454795
total norm for a microbatch 54.3905191706369 clip 40.48148689617554
total norm for a microbatch 51.05168673246028 clip 37.42529035221266
cuda
Objective function 23.37 = squared loss an data 19.30 + 0.5*rho*h**2 0.613554 + alpha*h 2.077872 + L2reg 1.22 + L1reg 0.16 ; SHD = 63 ; DAG True
Proportion of microbatches that were clipped  0.8109872611464968
iteration 1 in inner loop, alpha 187.57598944881263 rho 10000.0 h 0.011077491884076807
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 28.89 = squared loss an data 19.30 + 0.5*rho*h**2 6.135541 + alpha*h 2.077872 + L2reg 1.22 + L1reg 0.16 ; SHD = 63 ; DAG True
total norm for a microbatch 90.50363725860551 clip 7.1683676437658574
total norm for a microbatch 137.2569175328996 clip 73.62502321720677
total norm for a microbatch 171.11151991220768 clip 116.09125591345293
total norm for a microbatch 105.19863148763837 clip 102.89860311840174
total norm for a microbatch 51.35465330934191 clip 37.247857626259815
total norm for a microbatch 65.33312109254406 clip 33.6544564244853
total norm for a microbatch 30.427189692241974 clip 30.66464561400086
total norm for a microbatch 46.231276540339614 clip 30.12142761667842
total norm for a microbatch 30.381966137389686 clip 31.94481241554833
total norm for a microbatch 35.961429747211376 clip 31.151903128652695
total norm for a microbatch 36.67805323449917 clip 33.27388369544002
cuda
Objective function 21.50 = squared loss an data 19.19 + 0.5*rho*h**2 0.400013 + alpha*h 0.530554 + L2reg 1.23 + L1reg 0.16 ; SHD = 70 ; DAG True
Proportion of microbatches that were clipped  0.8097917839054586
iteration 2 in inner loop, alpha 187.57598944881263 rho 100000.0 h 0.002828474276402204
iteration 4 in outer loop, alpha = 470.423417089033, rho = 100000.0, h = 0.002828474276402204
cuda
noise_multiplier  0.8  noise_multiplier_b  2.5  noise_multiplier_delta  0.8104408984731079
cuda
Objective function 22.30 = squared loss an data 19.19 + 0.5*rho*h**2 0.400013 + alpha*h 1.330581 + L2reg 1.23 + L1reg 0.16 ; SHD = 70 ; DAG True
total norm for a microbatch 119.53236798410553 clip 14.910992334882119
total norm for a microbatch 114.32951829697572 clip 16.170435013909188
total norm for a microbatch 121.3208102164496 clip 23.28079197829579
total norm for a microbatch 116.20650735714578 clip 28.18538658198441
total norm for a microbatch 273.3511375449083 clip 292.8078920232048
total norm for a microbatch 289.65680236403716 clip 243.1360581897746
total norm for a microbatch 114.47271059147172 clip 115.19175124237255
total norm for a microbatch 48.56142916235165 clip 47.23591866804495
total norm for a microbatch 51.41739190170817 clip 38.83096615431207
total norm for a microbatch 64.7641564323188 clip 33.88905602698982
total norm for a microbatch 41.71623765172361 clip 31.087770143971156
cuda
Objective function 21.52 = squared loss an data 19.45 + 0.5*rho*h**2 0.079328 + alpha*h 0.592538 + L2reg 1.24 + L1reg 0.16 ; SHD = 77 ; DAG True
Proportion of microbatches that were clipped  0.8088529576562875
iteration 1 in inner loop, alpha 470.423417089033 rho 100000.0 h 0.0012595842527076684
iteration 5 in outer loop, alpha = 1730.0076697967015, rho = 1000000.0, h = 0.0012595842527076684
Threshold 0.3
[[0.002 0.046 0.201 0.427 0.609 0.509 0.32  0.018 0.079 0.287 0.062 0.2
  0.228 0.014 0.357 0.566 0.032 0.677 0.474 0.421]
 [0.029 0.002 0.32  0.638 0.611 0.573 0.204 0.015 0.221 0.279 0.078 0.208
  0.241 0.006 0.547 0.276 0.161 0.561 0.176 0.582]
 [0.011 0.007 0.002 0.333 0.204 0.072 0.02  0.005 0.009 0.046 0.01  0.011
  0.015 0.001 0.099 0.155 0.008 0.066 0.009 0.015]
 [0.005 0.002 0.008 0.002 0.024 0.014 0.041 0.001 0.004 0.009 0.006 0.003
  0.004 0.002 0.017 0.025 0.005 0.007 0.004 0.017]
 [0.003 0.003 0.019 0.092 0.002 0.01  0.016 0.001 0.004 0.01  0.004 0.003
  0.013 0.002 0.086 0.049 0.004 0.022 0.017 0.044]
 [0.004 0.004 0.038 0.163 0.158 0.002 0.065 0.005 0.012 0.019 0.006 0.013
  0.018 0.002 0.262 0.107 0.011 0.037 0.005 0.072]
 [0.006 0.009 0.121 0.068 0.184 0.07  0.002 0.004 0.007 0.011 0.006 0.005
  0.008 0.003 0.062 0.069 0.003 0.013 0.004 0.033]
 [0.165 0.182 0.507 1.199 0.846 0.473 0.515 0.002 0.122 0.694 0.19  0.781
  0.501 0.045 0.698 0.509 0.244 1.256 0.181 0.905]
 [0.04  0.011 0.347 0.704 0.507 0.293 0.365 0.022 0.002 0.427 0.016 0.063
  0.188 0.002 0.303 0.459 0.044 0.418 0.191 0.396]
 [0.009 0.007 0.122 0.204 0.223 0.199 0.139 0.003 0.009 0.003 0.004 0.024
  0.118 0.002 0.092 0.218 0.009 0.073 0.009 0.092]
 [0.039 0.037 0.287 0.333 0.591 0.317 0.298 0.015 0.198 0.427 0.003 0.181
  0.26  0.007 0.298 0.138 0.015 0.245 0.041 0.445]
 [0.018 0.01  0.177 0.642 0.876 0.157 0.404 0.002 0.065 0.1   0.021 0.001
  0.288 0.005 0.329 0.367 0.04  0.633 0.093 0.5  ]
 [0.013 0.011 0.171 0.424 0.137 0.141 0.372 0.004 0.011 0.033 0.009 0.006
  0.002 0.005 0.465 0.083 0.012 0.244 0.039 0.089]
 [0.201 0.393 2.02  0.587 0.568 0.615 0.632 0.061 0.73  0.849 0.319 0.452
  0.41  0.002 0.359 0.48  0.236 0.477 0.681 0.767]
 [0.007 0.005 0.022 0.167 0.029 0.013 0.04  0.003 0.009 0.034 0.006 0.006
  0.006 0.005 0.001 0.021 0.009 0.038 0.017 0.015]
 [0.004 0.008 0.012 0.099 0.06  0.02  0.04  0.003 0.006 0.006 0.016 0.004
  0.035 0.003 0.135 0.002 0.009 0.062 0.008 0.06 ]
 [0.063 0.015 0.224 0.426 0.439 0.226 0.666 0.01  0.045 0.213 0.174 0.061
  0.211 0.012 0.337 0.338 0.002 0.452 0.059 0.102]
 [0.002 0.003 0.052 0.268 0.111 0.062 0.114 0.001 0.006 0.04  0.01  0.004
  0.011 0.002 0.076 0.04  0.004 0.003 0.007 0.051]
 [0.004 0.013 0.334 0.372 0.208 0.536 0.435 0.009 0.013 0.269 0.088 0.042
  0.07  0.002 0.115 0.374 0.029 0.257 0.001 0.185]
 [0.003 0.003 0.15  0.149 0.042 0.032 0.071 0.001 0.006 0.023 0.006 0.003
  0.026 0.003 0.138 0.047 0.024 0.069 0.015 0.002]]
[[0.    0.    0.    0.427 0.609 0.509 0.32  0.    0.    0.    0.    0.
  0.    0.    0.357 0.566 0.    0.677 0.474 0.421]
 [0.    0.    0.32  0.638 0.611 0.573 0.    0.    0.    0.    0.    0.
  0.    0.    0.547 0.    0.    0.561 0.    0.582]
 [0.    0.    0.    0.333 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.507 1.199 0.846 0.473 0.515 0.    0.    0.694 0.    0.781
  0.501 0.    0.698 0.509 0.    1.256 0.    0.905]
 [0.    0.    0.347 0.704 0.507 0.    0.365 0.    0.    0.427 0.    0.
  0.    0.    0.303 0.459 0.    0.418 0.    0.396]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.333 0.591 0.317 0.    0.    0.    0.427 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.445]
 [0.    0.    0.    0.642 0.876 0.    0.404 0.    0.    0.    0.    0.
  0.    0.    0.329 0.367 0.    0.633 0.    0.5  ]
 [0.    0.    0.    0.424 0.    0.    0.372 0.    0.    0.    0.    0.
  0.    0.    0.465 0.    0.    0.    0.    0.   ]
 [0.    0.393 2.02  0.587 0.568 0.615 0.632 0.    0.73  0.849 0.319 0.452
  0.41  0.    0.359 0.48  0.    0.477 0.681 0.767]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.426 0.439 0.    0.666 0.    0.    0.    0.    0.
  0.    0.    0.337 0.338 0.    0.452 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.334 0.372 0.    0.536 0.435 0.    0.    0.    0.    0.
  0.    0.    0.    0.374 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.625, 'tpr': 0.5, 'fpr': 0.38461538461538464, 'f1': 0.42857142857142855, 'shd': 77, 'npred': 80, 'ntrue': 60}
[4.632e-02 2.010e-01 4.275e-01 6.093e-01 5.086e-01 3.205e-01 1.760e-02
 7.861e-02 2.868e-01 6.231e-02 2.005e-01 2.284e-01 1.423e-02 3.568e-01
 5.656e-01 3.223e-02 6.773e-01 4.742e-01 4.206e-01 2.879e-02 3.196e-01
 6.384e-01 6.113e-01 5.731e-01 2.041e-01 1.506e-02 2.209e-01 2.790e-01
 7.807e-02 2.081e-01 2.407e-01 5.782e-03 5.472e-01 2.761e-01 1.613e-01
 5.605e-01 1.761e-01 5.821e-01 1.069e-02 7.230e-03 3.333e-01 2.043e-01
 7.225e-02 2.020e-02 5.078e-03 8.651e-03 4.646e-02 1.010e-02 1.059e-02
 1.535e-02 8.673e-04 9.892e-02 1.547e-01 7.539e-03 6.633e-02 8.699e-03
 1.521e-02 5.022e-03 2.458e-03 7.518e-03 2.384e-02 1.403e-02 4.054e-02
 1.141e-03 3.982e-03 9.075e-03 6.320e-03 3.260e-03 3.902e-03 1.660e-03
 1.669e-02 2.480e-02 4.662e-03 7.316e-03 4.396e-03 1.715e-02 2.851e-03
 2.986e-03 1.916e-02 9.225e-02 1.007e-02 1.565e-02 1.163e-03 4.136e-03
 1.009e-02 4.314e-03 2.567e-03 1.348e-02 2.338e-03 8.617e-02 4.945e-02
 3.720e-03 2.238e-02 1.700e-02 4.376e-02 3.782e-03 3.741e-03 3.757e-02
 1.631e-01 1.580e-01 6.501e-02 5.200e-03 1.187e-02 1.902e-02 6.240e-03
 1.250e-02 1.815e-02 1.750e-03 2.618e-01 1.067e-01 1.110e-02 3.734e-02
 4.664e-03 7.210e-02 5.990e-03 8.974e-03 1.207e-01 6.838e-02 1.841e-01
 6.982e-02 3.672e-03 6.712e-03 1.133e-02 6.407e-03 5.066e-03 7.517e-03
 2.776e-03 6.237e-02 6.877e-02 3.059e-03 1.349e-02 4.251e-03 3.263e-02
 1.655e-01 1.824e-01 5.065e-01 1.199e+00 8.459e-01 4.728e-01 5.154e-01
 1.216e-01 6.938e-01 1.897e-01 7.815e-01 5.010e-01 4.527e-02 6.977e-01
 5.090e-01 2.435e-01 1.256e+00 1.809e-01 9.046e-01 4.008e-02 1.050e-02
 3.465e-01 7.044e-01 5.071e-01 2.928e-01 3.654e-01 2.216e-02 4.272e-01
 1.581e-02 6.287e-02 1.877e-01 2.105e-03 3.032e-01 4.588e-01 4.387e-02
 4.176e-01 1.909e-01 3.960e-01 8.694e-03 6.821e-03 1.221e-01 2.040e-01
 2.228e-01 1.988e-01 1.385e-01 3.280e-03 8.522e-03 3.604e-03 2.361e-02
 1.185e-01 1.631e-03 9.188e-02 2.181e-01 8.509e-03 7.347e-02 8.788e-03
 9.247e-02 3.934e-02 3.712e-02 2.866e-01 3.332e-01 5.907e-01 3.175e-01
 2.982e-01 1.454e-02 1.977e-01 4.272e-01 1.809e-01 2.600e-01 6.506e-03
 2.980e-01 1.379e-01 1.478e-02 2.447e-01 4.143e-02 4.449e-01 1.784e-02
 9.712e-03 1.771e-01 6.416e-01 8.757e-01 1.567e-01 4.041e-01 2.333e-03
 6.508e-02 9.952e-02 2.105e-02 2.884e-01 5.060e-03 3.294e-01 3.666e-01
 4.014e-02 6.329e-01 9.283e-02 5.004e-01 1.260e-02 1.074e-02 1.714e-01
 4.239e-01 1.372e-01 1.407e-01 3.723e-01 4.118e-03 1.115e-02 3.281e-02
 8.568e-03 6.110e-03 4.595e-03 4.649e-01 8.273e-02 1.154e-02 2.441e-01
 3.858e-02 8.945e-02 2.006e-01 3.930e-01 2.020e+00 5.869e-01 5.684e-01
 6.149e-01 6.317e-01 6.149e-02 7.301e-01 8.492e-01 3.185e-01 4.516e-01
 4.096e-01 3.594e-01 4.798e-01 2.356e-01 4.775e-01 6.805e-01 7.672e-01
 7.491e-03 4.716e-03 2.155e-02 1.670e-01 2.868e-02 1.310e-02 4.039e-02
 3.418e-03 8.853e-03 3.438e-02 6.362e-03 6.378e-03 6.030e-03 5.118e-03
 2.052e-02 9.156e-03 3.785e-02 1.699e-02 1.545e-02 3.997e-03 7.580e-03
 1.178e-02 9.928e-02 5.976e-02 2.007e-02 4.040e-02 3.155e-03 5.545e-03
 6.491e-03 1.631e-02 4.303e-03 3.467e-02 2.749e-03 1.352e-01 9.473e-03
 6.246e-02 8.031e-03 5.968e-02 6.289e-02 1.549e-02 2.241e-01 4.259e-01
 4.390e-01 2.263e-01 6.664e-01 9.871e-03 4.501e-02 2.127e-01 1.740e-01
 6.089e-02 2.112e-01 1.221e-02 3.367e-01 3.377e-01 4.516e-01 5.871e-02
 1.020e-01 2.223e-03 3.405e-03 5.232e-02 2.682e-01 1.109e-01 6.167e-02
 1.139e-01 1.037e-03 5.633e-03 3.985e-02 1.016e-02 3.523e-03 1.088e-02
 1.921e-03 7.628e-02 4.007e-02 4.181e-03 7.195e-03 5.072e-02 4.104e-03
 1.327e-02 3.344e-01 3.715e-01 2.082e-01 5.363e-01 4.348e-01 9.496e-03
 1.313e-02 2.693e-01 8.805e-02 4.199e-02 7.041e-02 2.399e-03 1.147e-01
 3.745e-01 2.887e-02 2.570e-01 1.849e-01 3.477e-03 3.173e-03 1.501e-01
 1.491e-01 4.199e-02 3.163e-02 7.056e-02 1.183e-03 5.725e-03 2.287e-02
 6.047e-03 3.373e-03 2.597e-02 2.949e-03 1.380e-01 4.712e-02 2.360e-02
 6.939e-02 1.459e-02]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7671354166666666, 0.42267675232453744)
Iterations 2500
Achieves (6.086687062773559, 1e-05)-DP
