samples  5000  graph  15 30 ER mlp  minibatch size  50  noise  0.5  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.1318689309952319
iteration 2 in outer loop, alpha = 14.527024768639649, rho = 100.0, h = 0.1318689309952319
cuda
iteration 1 in inner loop,alpha 14.527024768639649 rho 100.0 h 0.07489788782598694
iteration 2 in inner loop,alpha 14.527024768639649 rho 1000.0 h 0.026262814890728237
iteration 3 in outer loop, alpha = 40.789839659367885, rho = 1000.0, h = 0.026262814890728237
cuda
iteration 1 in inner loop,alpha 40.789839659367885 rho 1000.0 h 0.016448538201942142
iteration 2 in inner loop,alpha 40.789839659367885 rho 10000.0 h 0.006675573394245404
iteration 3 in inner loop,alpha 40.789839659367885 rho 100000.0 h 0.0012889427930087294
iteration 4 in outer loop, alpha = 169.68411896024082, rho = 100000.0, h = 0.0012889427930087294
cuda
iteration 1 in inner loop,alpha 169.68411896024082 rho 100000.0 h 0.00027330133441338944
iteration 5 in outer loop, alpha = 197.01425240157977, rho = 100000.0, h = 0.00027330133441338944
cuda
iteration 1 in inner loop,alpha 197.01425240157977 rho 100000.0 h 0.00013396645665686435
iteration 6 in outer loop, alpha = 330.9807090584441, rho = 1000000.0, h = 0.00013396645665686435
Threshold 0.3
[[0.002 0.    0.002 0.007 0.205 0.    0.    0.    0.    2.704 0.227 0.
  0.    0.    0.011]
 [2.533 0.001 0.002 0.262 0.212 0.011 0.01  0.003 0.001 2.192 0.262 1.393
  0.    0.001 0.1  ]
 [0.051 0.071 0.    0.146 0.155 0.037 0.121 0.007 0.069 0.133 0.151 0.145
  0.003 0.004 0.007]
 [0.141 0.001 0.001 0.001 0.194 0.    0.    0.    0.001 0.224 0.406 0.002
  0.    0.    1.021]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.001 0.
  0.    0.    0.001]
 [2.657 0.03  0.    0.968 0.36  0.001 1.34  0.    0.055 1.751 1.32  1.504
  0.013 0.006 1.92 ]
 [0.41  0.044 0.002 0.336 0.403 0.    0.001 0.    0.054 0.158 1.701 0.088
  0.004 0.003 0.165]
 [2.694 0.004 0.004 2.087 0.133 0.594 0.047 0.    0.005 0.32  0.157 0.056
  0.023 0.025 0.12 ]
 [0.086 0.381 0.002 0.643 0.396 0.002 0.003 0.002 0.003 2.311 2.204 1.087
  0.    0.    0.367]
 [0.    0.    0.    0.    2.78  0.    0.    0.    0.    0.002 0.318 0.
  0.    0.    0.004]
 [0.    0.    0.    0.001 3.107 0.    0.    0.    0.001 0.002 0.008 0.
  0.    0.    0.003]
 [0.36  0.    0.004 0.182 0.144 0.    0.004 0.    0.001 0.447 0.447 0.001
  0.001 0.    0.033]
 [0.04  0.077 0.003 0.103 3.053 0.017 0.007 0.004 1.216 0.116 1.991 0.043
  0.001 0.016 0.016]
 [0.126 1.427 0.005 1.124 1.229 0.006 0.007 0.003 3.474 0.373 0.484 0.156
  0.012 0.001 0.564]
 [0.065 0.    0.    0.    0.098 0.    0.001 0.    0.001 0.113 0.155 0.003
  0.    0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.704 0.    0.
  0.    0.    0.   ]
 [2.533 0.    0.    0.    0.    0.    0.    0.    0.    2.192 0.    1.393
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.406 0.
  0.    0.    1.021]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.657 0.    0.    0.968 0.36  0.    1.34  0.    0.    1.751 1.32  1.504
  0.    0.    1.92 ]
 [0.41  0.    0.    0.336 0.403 0.    0.    0.    0.    0.    1.701 0.
  0.    0.    0.   ]
 [2.694 0.    0.    2.087 0.    0.594 0.    0.    0.    0.32  0.    0.
  0.    0.    0.   ]
 [0.    0.381 0.    0.643 0.396 0.    0.    0.    0.    2.311 2.204 1.087
  0.    0.    0.367]
 [0.    0.    0.    0.    2.78  0.    0.    0.    0.    0.    0.318 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.107 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.36  0.    0.    0.    0.    0.    0.    0.    0.    0.447 0.447 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.053 0.    0.    0.    1.216 0.    1.991 0.
  0.    0.    0.   ]
 [0.    1.427 0.    1.124 1.229 0.    0.    0.    3.474 0.373 0.484 0.
  0.    0.    0.564]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[5.101e-05 2.136e-03 6.584e-03 2.050e-01 6.137e-05 8.106e-05 3.460e-05
 1.807e-04 2.704e+00 2.270e-01 3.585e-04 1.779e-04 9.891e-05 1.050e-02
 2.533e+00 2.481e-03 2.623e-01 2.123e-01 1.101e-02 9.507e-03 3.063e-03
 1.258e-03 2.192e+00 2.619e-01 1.393e+00 3.706e-04 6.184e-04 9.976e-02
 5.090e-02 7.087e-02 1.455e-01 1.552e-01 3.726e-02 1.213e-01 6.653e-03
 6.908e-02 1.327e-01 1.511e-01 1.452e-01 2.799e-03 3.832e-03 6.888e-03
 1.410e-01 5.170e-04 1.174e-03 1.938e-01 3.184e-04 2.637e-04 1.738e-04
 6.247e-04 2.242e-01 4.060e-01 2.025e-03 2.040e-04 1.244e-04 1.021e+00
 5.595e-05 4.374e-06 4.525e-04 2.109e-04 9.289e-06 1.001e-04 4.244e-06
 1.528e-04 3.083e-04 7.485e-04 1.946e-04 1.898e-05 6.070e-05 6.992e-04
 2.657e+00 3.004e-02 2.600e-04 9.680e-01 3.604e-01 1.340e+00 2.319e-04
 5.450e-02 1.751e+00 1.320e+00 1.504e+00 1.301e-02 6.098e-03 1.920e+00
 4.105e-01 4.412e-02 1.808e-03 3.356e-01 4.026e-01 2.371e-04 8.314e-05
 5.364e-02 1.585e-01 1.701e+00 8.802e-02 4.210e-03 2.853e-03 1.647e-01
 2.694e+00 3.770e-03 4.072e-03 2.087e+00 1.332e-01 5.939e-01 4.712e-02
 5.410e-03 3.204e-01 1.567e-01 5.574e-02 2.313e-02 2.458e-02 1.197e-01
 8.632e-02 3.808e-01 2.437e-03 6.433e-01 3.957e-01 2.229e-03 3.049e-03
 1.965e-03 2.311e+00 2.204e+00 1.087e+00 6.332e-05 4.097e-04 3.672e-01
 1.912e-04 3.904e-06 1.892e-04 4.448e-04 2.780e+00 6.137e-06 3.685e-04
 9.566e-06 3.551e-04 3.184e-01 2.811e-04 2.352e-05 9.438e-05 3.596e-03
 2.872e-04 5.089e-05 3.458e-04 7.312e-04 3.107e+00 8.889e-05 4.763e-04
 1.909e-05 5.806e-04 1.700e-03 4.190e-04 1.114e-04 2.676e-04 2.956e-03
 3.601e-01 3.615e-04 3.688e-03 1.816e-01 1.435e-01 2.312e-04 3.584e-03
 2.386e-04 1.155e-03 4.467e-01 4.469e-01 9.325e-04 4.501e-04 3.280e-02
 4.044e-02 7.703e-02 3.309e-03 1.032e-01 3.053e+00 1.687e-02 6.840e-03
 4.320e-03 1.216e+00 1.156e-01 1.991e+00 4.283e-02 1.647e-02 1.602e-02
 1.258e-01 1.427e+00 4.668e-03 1.124e+00 1.229e+00 6.128e-03 6.769e-03
 3.457e-03 3.474e+00 3.734e-01 4.841e-01 1.557e-01 1.181e-02 5.640e-01
 6.495e-02 4.034e-04 4.977e-04 4.109e-04 9.778e-02 2.017e-04 8.768e-04
 1.953e-04 5.311e-04 1.125e-01 1.546e-01 2.800e-03 2.019e-04 5.695e-05]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.98, 0.9717590714446698)
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 88.41755086630708 clip 1.3306975146315496
total norm for a microbatch 39.874437558357755 clip 9.323439307706348
total norm for a microbatch 36.52994846291962 clip 31.81529229448199
total norm for a microbatch 37.18192627856116 clip 28.12770525756048
total norm for a microbatch 29.765497620858977 clip 29.656443525576062
total norm for a microbatch 35.9956348587184 clip 28.42088340912776
total norm for a microbatch 27.6700361820578 clip 27.660055903415927
total norm for a microbatch 27.267483309324245 clip 26.094335721926686
total norm for a microbatch 36.52994790807809 clip 25.343574008093846
total norm for a microbatch 23.87478866150635 clip 24.738296567223784
total norm for a microbatch 30.047148942603414 clip 24.134713215451963
total norm for a microbatch 22.264923546638226 clip 25.02683405446407
total norm for a microbatch 54.03557064062659 clip 29.57513443288304
total norm for a microbatch 37.17459699240425 clip 30.13602210220348
total norm for a microbatch 30.381403531813472 clip 28.594612245600363
total norm for a microbatch 30.632941715921117 clip 29.258304215192567
total norm for a microbatch 43.58313071748439 clip 30.21126486455853
cuda
Objective function 13.00 = squared loss an data 10.67 + 0.5*rho*h**2 1.379487 + alpha*h 0.000000 + L2reg 0.80 + L1reg 0.16 ; SHD = 51 ; DAG False
Proportion of microbatches that were clipped  0.8073542456253527
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.661015700309143
iteration 1 in outer loop, alpha = 1.661015700309143, rho = 1.0, h = 1.661015700309143
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 15.76 = squared loss an data 10.67 + 0.5*rho*h**2 1.379487 + alpha*h 2.758973 + L2reg 0.80 + L1reg 0.16 ; SHD = 51 ; DAG False
total norm for a microbatch 41.78954226731761 clip 12.638281755615287
total norm for a microbatch 30.935653366514558 clip 40.12975526769444
total norm for a microbatch 31.803087126347172 clip 38.17426875645039
total norm for a microbatch 77.89956900757383 clip 40.31530169904079
total norm for a microbatch 56.87543405172968 clip 36.86337129251924
total norm for a microbatch 55.804780836520166 clip 40.39476285954934
total norm for a microbatch 47.84637530196137 clip 40.39476285954934
total norm for a microbatch 48.258984288049895 clip 40.75159929226386
total norm for a microbatch 75.62921512323724 clip 41.872357573282784
total norm for a microbatch 74.0150370235719 clip 38.429014687975915
total norm for a microbatch 44.545609496116626 clip 43.43787842516695
total norm for a microbatch 124.4749444973055 clip 43.5794215924176
total norm for a microbatch 86.68803035881797 clip 44.24957172386569
total norm for a microbatch 72.0983316323071 clip 46.67817252536523
total norm for a microbatch 54.926118488581274 clip 43.488937645227324
cuda
Objective function 13.03 = squared loss an data 8.84 + 0.5*rho*h**2 0.794378 + alpha*h 2.093641 + L2reg 1.14 + L1reg 0.16 ; SHD = 58 ; DAG False
Proportion of microbatches that were clipped  0.8106547376982514
iteration 1 in inner loop, alpha 1.661015700309143 rho 1.0 h 1.2604583390875312
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 20.18 = squared loss an data 8.84 + 0.5*rho*h**2 7.943776 + alpha*h 2.093641 + L2reg 1.14 + L1reg 0.16 ; SHD = 58 ; DAG False
total norm for a microbatch 60.96626944166097 clip 14.019596865179597
total norm for a microbatch 66.99307918931785 clip 53.88396700341717
total norm for a microbatch 97.31249891582713 clip 51.55531065834106
total norm for a microbatch 69.79488557277938 clip 50.93073261791668
total norm for a microbatch 81.59767601018605 clip 50.379371379137844
total norm for a microbatch 73.99092385379345 clip 55.097693490354565
total norm for a microbatch 39.93207152790343 clip 52.88990247638094
total norm for a microbatch 73.29837457408297 clip 53.31378424560995
total norm for a microbatch 35.46525122093086 clip 50.88148783312185
total norm for a microbatch 48.368985472841224 clip 56.790824047093054
total norm for a microbatch 69.53219998589842 clip 52.946351005896176
cuda
Objective function 14.09 = squared loss an data 10.21 + 0.5*rho*h**2 1.467641 + alpha*h 0.899909 + L2reg 1.37 + L1reg 0.15 ; SHD = 48 ; DAG True
Proportion of microbatches that were clipped  0.8145764620706298
iteration 2 in inner loop, alpha 1.661015700309143 rho 10.0 h 0.5417824563420375
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 27.30 = squared loss an data 10.21 + 0.5*rho*h**2 14.676412 + alpha*h 0.899909 + L2reg 1.37 + L1reg 0.15 ; SHD = 48 ; DAG True
total norm for a microbatch 200.85199684589546 clip 68.13903137224267
total norm for a microbatch 95.2061384440328 clip 64.91063319746371
total norm for a microbatch 98.86298078567239 clip 61.16977667324231
total norm for a microbatch 77.6869065851968 clip 64.07433639170455
total norm for a microbatch 92.83749790921217 clip 65.77600089599694
total norm for a microbatch 148.126867306307 clip 58.44566484546959
cuda
Objective function 15.46 = squared loss an data 11.82 + 0.5*rho*h**2 1.697876 + alpha*h 0.306085 + L2reg 1.50 + L1reg 0.14 ; SHD = 44 ; DAG True
Proportion of microbatches that were clipped  0.8132046394679212
iteration 3 in inner loop, alpha 1.661015700309143 rho 100.0 h 0.18427565334475027
iteration 2 in outer loop, alpha = 20.08858103478417, rho = 100.0, h = 0.18427565334475027
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 18.86 = squared loss an data 11.82 + 0.5*rho*h**2 1.697876 + alpha*h 3.701836 + L2reg 1.50 + L1reg 0.14 ; SHD = 44 ; DAG True
total norm for a microbatch 287.5835042448067 clip 6.665964637410296
total norm for a microbatch 78.25684786561982 clip 70.35646073242758
total norm for a microbatch 105.01233806390192 clip 70.5372595708
total norm for a microbatch 46.52980064765292 clip 66.97253927650848
total norm for a microbatch 136.35180345610664 clip 68.98536045470769
cuda
Objective function 17.12 = squared loss an data 12.68 + 0.5*rho*h**2 0.543604 + alpha*h 2.094622 + L2reg 1.66 + L1reg 0.15 ; SHD = 53 ; DAG True
Proportion of microbatches that were clipped  0.8143712574850299
iteration 1 in inner loop, alpha 20.08858103478417 rho 100.0 h 0.10426926357309085
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 22.02 = squared loss an data 12.68 + 0.5*rho*h**2 5.436040 + alpha*h 2.094622 + L2reg 1.66 + L1reg 0.15 ; SHD = 53 ; DAG True
total norm for a microbatch 147.7948302030265 clip 1.8841220040206514
total norm for a microbatch 83.52481897780055 clip 2.683331422643094
total norm for a microbatch 74.7349729404762 clip 4.693390081710763
total norm for a microbatch 168.27821644080976 clip 6.656766355431361
total norm for a microbatch 173.36523031119873 clip 7.219865544476585
total norm for a microbatch 114.12398876185101 clip 77.76108242987311
total norm for a microbatch 105.93105845970717 clip 66.03893157517967
total norm for a microbatch 122.28873552696955 clip 70.39504666222568
total norm for a microbatch 100.95252373300723 clip 69.7629429245917
total norm for a microbatch 82.32367322123957 clip 76.06984482943103
total norm for a microbatch 95.40051723553076 clip 71.76330355788475
total norm for a microbatch 140.9212510942412 clip 72.55955444051925
cuda
Objective function 17.11 = squared loss an data 13.14 + 0.5*rho*h**2 1.070465 + alpha*h 0.929502 + L2reg 1.81 + L1reg 0.16 ; SHD = 50 ; DAG True
Proportion of microbatches that were clipped  0.8203640410142278
iteration 2 in inner loop, alpha 20.08858103478417 rho 1000.0 h 0.04627018126468663
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 26.74 = squared loss an data 13.14 + 0.5*rho*h**2 10.704648 + alpha*h 0.929502 + L2reg 1.81 + L1reg 0.16 ; SHD = 50 ; DAG True
total norm for a microbatch 205.5263840784426 clip 1.2051395848713506
total norm for a microbatch 190.13709768588842 clip 70.29306905767707
total norm for a microbatch 123.37228342751654 clip 79.83476489229076
total norm for a microbatch 116.05910443478095 clip 78.12421470474757
total norm for a microbatch 77.1470445896148 clip 80.88153074814264
total norm for a microbatch 86.71596374446473 clip 74.05888735818304
total norm for a microbatch 76.83988974192633 clip 71.8309200208456
total norm for a microbatch 103.87632833086167 clip 73.58290553543829
total norm for a microbatch 77.47577064990413 clip 76.7979394727444
total norm for a microbatch 65.19709748311895 clip 71.19856981821955
total norm for a microbatch 85.76966071601575 clip 71.19856981821955
total norm for a microbatch 47.695034929554026 clip 74.42214230724367
total norm for a microbatch 112.50311431185852 clip 70.89800203026546
total norm for a microbatch 52.43796215019951 clip 70.07454092513788
cuda
Objective function 16.62 = squared loss an data 13.14 + 0.5*rho*h**2 1.149632 + alpha*h 0.304610 + L2reg 1.86 + L1reg 0.16 ; SHD = 45 ; DAG True
Proportion of microbatches that were clipped  0.8183195153061225
iteration 3 in inner loop, alpha 20.08858103478417 rho 10000.0 h 0.015163321454165768
iteration 3 in outer loop, alpha = 171.72179557644185, rho = 10000.0, h = 0.015163321454165768
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 18.92 = squared loss an data 13.14 + 0.5*rho*h**2 1.149632 + alpha*h 2.603873 + L2reg 1.86 + L1reg 0.16 ; SHD = 45 ; DAG True
total norm for a microbatch 105.69682660510503 clip 1.0
total norm for a microbatch 114.58237373325717 clip 4.141149489093004
total norm for a microbatch 132.5056320652788 clip 13.801652501790093
total norm for a microbatch 119.10772869469258 clip 81.21744361227182
total norm for a microbatch 110.98090680864811 clip 81.94641775383239
total norm for a microbatch 67.04714655227811 clip 81.16764134742131
total norm for a microbatch 143.42023611512576 clip 81.83943592814083
total norm for a microbatch 91.82469484737875 clip 81.61814006225329
total norm for a microbatch 113.77939023423484 clip 78.06042971549871
total norm for a microbatch 111.08119312250342 clip 83.92733326895062
cuda
Objective function 16.71 = squared loss an data 12.72 + 0.5*rho*h**2 0.380399 + alpha*h 1.497822 + L2reg 1.94 + L1reg 0.17 ; SHD = 49 ; DAG True
Proportion of microbatches that were clipped  0.8198663909654843
iteration 1 in inner loop, alpha 171.72179557644185 rho 10000.0 h 0.008722374427044599
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 20.13 = squared loss an data 12.72 + 0.5*rho*h**2 3.803991 + alpha*h 1.497822 + L2reg 1.94 + L1reg 0.17 ; SHD = 49 ; DAG True
total norm for a microbatch 2289.848562362981 clip 1.8539813233005003
total norm for a microbatch 1162.2002134829095 clip 2.4582665415381872
total norm for a microbatch 524.9345264833843 clip 3.5148638599575914
total norm for a microbatch 340.6630013431808 clip 147.66101480653734
total norm for a microbatch 420.5185557134609 clip 369.2497791144421
total norm for a microbatch 161.49671316346186 clip 130.86939831908742
total norm for a microbatch 81.03899887660234 clip 81.40428775156802
total norm for a microbatch 157.9462278200124 clip 72.43481814381504
total norm for a microbatch 101.05425695627011 clip 74.8354043403783
total norm for a microbatch 63.07103560158638 clip 67.4033240911978
cuda
Objective function 15.43 = squared loss an data 12.61 + 0.5*rho*h**2 0.281394 + alpha*h 0.407378 + L2reg 1.96 + L1reg 0.17 ; SHD = 50 ; DAG True
Proportion of microbatches that were clipped  0.8168697883258499
iteration 2 in inner loop, alpha 171.72179557644185 rho 100000.0 h 0.002372313643359547
iteration 4 in outer loop, alpha = 408.9531599123966, rho = 100000.0, h = 0.002372313643359547
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 15.99 = squared loss an data 12.61 + 0.5*rho*h**2 0.281394 + alpha*h 0.970165 + L2reg 1.96 + L1reg 0.17 ; SHD = 50 ; DAG True
total norm for a microbatch 433.3347833842407 clip 6.194885637130592
total norm for a microbatch 415.108873198534 clip 7.463693421837839
total norm for a microbatch 243.7642275378657 clip 20.198235809552337
total norm for a microbatch 284.92421573962685 clip 48.402653140183595
total norm for a microbatch 338.46407204167906 clip 403.3553740343729
total norm for a microbatch 170.80580987353633 clip 146.1049035614264
total norm for a microbatch 102.49826003999146 clip 127.38751042502754
total norm for a microbatch 101.8634945119212 clip 127.38751042502754
total norm for a microbatch 95.03199901612874 clip 78.69492202543712
total norm for a microbatch 110.54359448297839 clip 71.67975293804793
total norm for a microbatch 80.7013743769042 clip 76.85772771571797
total norm for a microbatch 87.79037704375939 clip 78.0402102006633
total norm for a microbatch 62.57085974376604 clip 71.96010565904507
total norm for a microbatch 85.30128886755226 clip 74.99833725057465
cuda
Objective function 15.94 = squared loss an data 13.01 + 0.5*rho*h**2 0.122232 + alpha*h 0.639412 + L2reg 2.00 + L1reg 0.17 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.8171396504729838
iteration 1 in inner loop, alpha 408.9531599123966 rho 100000.0 h 0.001563534132163369
iteration 5 in outer loop, alpha = 1972.4872920757655, rho = 1000000.0, h = 0.001563534132163369
Threshold 0.3
[[0.003 0.012 0.242 0.258 0.809 0.008 0.014 0.002 0.017 0.975 0.138 0.401
  0.008 0.006 0.116]
 [0.311 0.004 0.521 0.531 0.592 0.117 0.015 0.013 0.012 1.142 0.256 1.183
  0.01  0.002 0.322]
 [0.013 0.008 0.004 0.048 0.343 0.011 0.002 0.008 0.01  0.032 0.014 0.064
  0.006 0.002 0.05 ]
 [0.02  0.008 0.087 0.003 0.2   0.015 0.002 0.002 0.008 0.114 0.022 0.194
  0.006 0.001 0.05 ]
 [0.003 0.002 0.012 0.019 0.003 0.002 0.001 0.001 0.001 0.001 0.001 0.017
  0.001 0.    0.008]
 [0.546 0.027 0.41  0.222 0.451 0.003 0.008 0.005 0.012 0.707 0.498 1.065
  0.013 0.005 0.427]
 [0.281 0.24  1.317 1.1   0.685 0.215 0.003 0.072 0.195 0.588 1.338 0.678
  0.182 0.031 0.906]
 [1.093 0.286 0.604 1.253 0.82  0.636 0.074 0.003 0.224 1.227 0.471 0.913
  0.29  0.022 0.797]
 [0.265 0.377 0.415 0.644 0.611 0.239 0.021 0.016 0.004 1.172 1.444 0.701
  0.017 0.001 0.436]
 [0.003 0.004 0.1   0.037 1.833 0.004 0.006 0.002 0.004 0.004 0.024 0.071
  0.005 0.001 0.062]
 [0.022 0.011 0.221 0.146 2.136 0.01  0.002 0.007 0.002 0.218 0.003 0.463
  0.002 0.001 0.07 ]
 [0.011 0.003 0.044 0.024 0.17  0.003 0.004 0.003 0.003 0.064 0.012 0.003
  0.002 0.001 0.07 ]
 [0.443 0.335 0.616 0.59  2.386 0.26  0.019 0.023 0.182 0.792 1.331 1.185
  0.002 0.009 0.389]
 [0.528 1.386 0.999 1.111 1.233 0.657 0.161 0.274 2.539 1.354 1.002 1.288
  0.294 0.003 1.128]
 [0.035 0.013 0.099 0.073 0.453 0.01  0.004 0.005 0.006 0.074 0.066 0.068
  0.01  0.002 0.002]]
[[0.    0.    0.    0.    0.809 0.    0.    0.    0.    0.975 0.    0.401
  0.    0.    0.   ]
 [0.311 0.    0.521 0.531 0.592 0.    0.    0.    0.    1.142 0.    1.183
  0.    0.    0.322]
 [0.    0.    0.    0.    0.343 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.546 0.    0.41  0.    0.451 0.    0.    0.    0.    0.707 0.498 1.065
  0.    0.    0.427]
 [0.    0.    1.317 1.1   0.685 0.    0.    0.    0.    0.588 1.338 0.678
  0.    0.    0.906]
 [1.093 0.    0.604 1.253 0.82  0.636 0.    0.    0.    1.227 0.471 0.913
  0.    0.    0.797]
 [0.    0.377 0.415 0.644 0.611 0.    0.    0.    0.    1.172 1.444 0.701
  0.    0.    0.436]
 [0.    0.    0.    0.    1.833 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.136 0.    0.    0.    0.    0.    0.    0.463
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.443 0.335 0.616 0.59  2.386 0.    0.    0.    0.    0.792 1.331 1.185
  0.    0.    0.389]
 [0.528 1.386 0.999 1.111 1.233 0.657 0.    0.    2.539 1.354 1.002 1.288
  0.    0.    1.128]
 [0.    0.    0.    0.    0.453 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.6212121212121212, 'tpr': 0.8333333333333334, 'fpr': 0.5466666666666666, 'f1': 0.5208333333333334, 'shd': 46, 'npred': 66, 'ntrue': 30}
[1.184e-02 2.424e-01 2.580e-01 8.095e-01 7.862e-03 1.399e-02 2.234e-03
 1.657e-02 9.749e-01 1.379e-01 4.006e-01 8.359e-03 5.865e-03 1.161e-01
 3.113e-01 5.215e-01 5.313e-01 5.919e-01 1.165e-01 1.522e-02 1.327e-02
 1.158e-02 1.142e+00 2.560e-01 1.183e+00 1.012e-02 2.070e-03 3.222e-01
 1.320e-02 8.467e-03 4.848e-02 3.432e-01 1.137e-02 2.001e-03 7.848e-03
 1.012e-02 3.244e-02 1.369e-02 6.445e-02 5.787e-03 1.698e-03 5.000e-02
 1.968e-02 7.882e-03 8.719e-02 2.003e-01 1.498e-02 2.496e-03 1.737e-03
 7.934e-03 1.145e-01 2.185e-02 1.936e-01 6.164e-03 1.100e-03 5.000e-02
 2.567e-03 1.893e-03 1.248e-02 1.888e-02 2.402e-03 9.309e-04 8.563e-04
 8.994e-04 1.121e-03 1.057e-03 1.656e-02 7.727e-04 3.936e-04 8.355e-03
 5.465e-01 2.679e-02 4.096e-01 2.220e-01 4.510e-01 8.167e-03 5.383e-03
 1.193e-02 7.071e-01 4.980e-01 1.065e+00 1.269e-02 5.432e-03 4.272e-01
 2.806e-01 2.403e-01 1.317e+00 1.100e+00 6.852e-01 2.154e-01 7.180e-02
 1.952e-01 5.881e-01 1.338e+00 6.784e-01 1.820e-01 3.073e-02 9.060e-01
 1.093e+00 2.856e-01 6.035e-01 1.253e+00 8.204e-01 6.359e-01 7.449e-02
 2.240e-01 1.227e+00 4.715e-01 9.135e-01 2.904e-01 2.187e-02 7.973e-01
 2.647e-01 3.774e-01 4.151e-01 6.442e-01 6.114e-01 2.389e-01 2.076e-02
 1.602e-02 1.172e+00 1.444e+00 7.013e-01 1.708e-02 8.453e-04 4.361e-01
 3.480e-03 3.869e-03 1.005e-01 3.697e-02 1.833e+00 3.908e-03 5.813e-03
 1.559e-03 3.732e-03 2.437e-02 7.145e-02 4.556e-03 6.355e-04 6.176e-02
 2.222e-02 1.103e-02 2.213e-01 1.463e-01 2.136e+00 9.584e-03 2.216e-03
 6.798e-03 2.436e-03 2.177e-01 4.628e-01 1.843e-03 5.534e-04 6.984e-02
 1.144e-02 3.120e-03 4.372e-02 2.359e-02 1.703e-01 2.794e-03 4.345e-03
 2.723e-03 2.687e-03 6.423e-02 1.209e-02 1.788e-03 7.468e-04 7.031e-02
 4.430e-01 3.346e-01 6.157e-01 5.897e-01 2.386e+00 2.604e-01 1.876e-02
 2.306e-02 1.822e-01 7.923e-01 1.331e+00 1.185e+00 8.853e-03 3.886e-01
 5.281e-01 1.386e+00 9.985e-01 1.111e+00 1.233e+00 6.571e-01 1.608e-01
 2.736e-01 2.539e+00 1.354e+00 1.002e+00 1.288e+00 2.943e-01 1.128e+00
 3.500e-02 1.299e-02 9.858e-02 7.300e-02 4.528e-01 1.005e-02 3.528e-03
 4.667e-03 6.342e-03 7.370e-02 6.626e-02 6.816e-02 1.046e-02 2.127e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8851851851851852, 0.6617833303730043)
Iterations 2500
Achieves (24.160548736236485, 1e-05)-DP
