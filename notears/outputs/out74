samples  5000  graph  15 30 ER mlp  minibatch size  75  noise  0.5  minibatches per NN training  56 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740600044
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.026287269043208994
iteration 3 in outer loop, alpha = 40.98488863421454, rho = 1000.0, h = 0.026287269043208994
cuda
iteration 1 in inner loop,alpha 40.98488863421454 rho 1000.0 h 0.016396895735928396
iteration 2 in inner loop,alpha 40.98488863421454 rho 10000.0 h 0.006531290021200675
iteration 4 in outer loop, alpha = 106.2977888462213, rho = 10000.0, h = 0.006531290021200675
cuda
iteration 1 in inner loop,alpha 106.2977888462213 rho 10000.0 h 0.002132149557262153
iteration 2 in inner loop,alpha 106.2977888462213 rho 100000.0 h 0.0005741385438504665
iteration 5 in outer loop, alpha = 163.71164323126794, rho = 100000.0, h = 0.0005741385438504665
cuda
iteration 1 in inner loop,alpha 163.71164323126794 rho 100000.0 h 0.00019832704703759418
iteration 6 in outer loop, alpha = 362.0386902688621, rho = 1000000.0, h = 0.00019832704703759418
Threshold 0.3
[[0.002 0.    0.004 0.004 0.219 0.    0.001 0.    0.    2.77  0.171 0.
  0.    0.    0.016]
 [2.548 0.001 0.    0.265 0.2   0.009 0.011 0.007 0.001 2.215 0.193 1.364
  0.002 0.001 0.099]
 [0.032 0.071 0.001 0.15  0.148 0.032 0.153 0.007 0.073 0.124 0.14  0.154
  0.008 0.006 0.008]
 [0.159 0.    0.001 0.001 0.188 0.    0.    0.    0.    0.223 0.353 0.001
  0.    0.    1.024]
 [0.    0.    0.    0.001 0.005 0.    0.    0.    0.    0.    0.001 0.
  0.    0.    0.001]
 [2.678 0.031 0.001 0.967 0.361 0.001 1.442 0.    0.05  1.764 1.242 1.49
  0.012 0.004 1.924]
 [0.418 0.04  0.003 0.344 0.414 0.    0.001 0.    0.039 0.136 1.79  0.097
  0.006 0.003 0.166]
 [2.699 0.006 0.004 2.094 0.166 0.626 0.048 0.    0.004 0.301 0.182 0.061
  0.017 0.011 0.117]
 [0.09  0.381 0.002 0.647 0.424 0.006 0.007 0.003 0.003 2.351 2.313 1.053
  0.    0.001 0.37 ]
 [0.    0.    0.    0.    2.851 0.    0.    0.    0.    0.003 0.275 0.
  0.    0.    0.004]
 [0.001 0.    0.    0.001 3.141 0.    0.    0.    0.001 0.002 0.01  0.
  0.    0.    0.012]
 [0.341 0.    0.001 0.186 0.135 0.    0.003 0.    0.002 0.407 0.396 0.002
  0.    0.001 0.032]
 [0.039 0.076 0.005 0.099 3.077 0.014 0.004 0.006 1.316 0.106 2.015 0.033
  0.001 0.016 0.017]
 [0.128 1.437 0.007 1.132 1.246 0.002 0.006 0.005 3.518 0.376 0.461 0.154
  0.01  0.002 0.563]
 [0.061 0.    0.003 0.    0.103 0.    0.    0.    0.    0.107 0.128 0.008
  0.001 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.77  0.    0.
  0.    0.    0.   ]
 [2.548 0.    0.    0.    0.    0.    0.    0.    0.    2.215 0.    1.364
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.353 0.
  0.    0.    1.024]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.678 0.    0.    0.967 0.361 0.    1.442 0.    0.    1.764 1.242 1.49
  0.    0.    1.924]
 [0.418 0.    0.    0.344 0.414 0.    0.    0.    0.    0.    1.79  0.
  0.    0.    0.   ]
 [2.699 0.    0.    2.094 0.    0.626 0.    0.    0.    0.301 0.    0.
  0.    0.    0.   ]
 [0.    0.381 0.    0.647 0.424 0.    0.    0.    0.    2.351 2.313 1.053
  0.    0.    0.37 ]
 [0.    0.    0.    0.    2.851 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.141 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.341 0.    0.    0.    0.    0.    0.    0.    0.    0.407 0.396 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.077 0.    0.    0.    1.316 0.    2.015 0.
  0.    0.    0.   ]
 [0.    1.437 0.    1.132 1.246 0.    0.    0.    3.518 0.376 0.461 0.
  0.    0.    0.563]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.3409090909090909, 'tpr': 0.9666666666666667, 'fpr': 0.2, 'f1': 0.7837837837837838, 'shd': 16, 'npred': 44, 'ntrue': 30}
[5.697e-05 4.108e-03 4.496e-03 2.187e-01 6.416e-05 1.122e-03 2.512e-05
 2.582e-04 2.770e+00 1.706e-01 3.458e-04 3.753e-04 1.217e-04 1.576e-02
 2.548e+00 2.272e-04 2.649e-01 2.000e-01 9.001e-03 1.142e-02 6.523e-03
 1.383e-03 2.215e+00 1.930e-01 1.364e+00 1.769e-03 8.959e-04 9.886e-02
 3.186e-02 7.052e-02 1.497e-01 1.476e-01 3.240e-02 1.527e-01 6.697e-03
 7.256e-02 1.238e-01 1.403e-01 1.541e-01 7.968e-03 6.110e-03 8.312e-03
 1.587e-01 3.990e-04 6.650e-04 1.881e-01 1.755e-05 3.251e-04 2.742e-05
 1.319e-04 2.233e-01 3.529e-01 1.244e-03 1.687e-04 1.198e-04 1.024e+00
 6.232e-05 3.909e-06 8.238e-05 6.065e-04 9.446e-06 1.263e-04 3.668e-06
 1.518e-04 3.329e-04 9.465e-04 6.399e-05 1.586e-05 6.310e-05 1.047e-03
 2.678e+00 3.122e-02 8.669e-04 9.666e-01 3.608e-01 1.442e+00 1.012e-04
 4.988e-02 1.764e+00 1.242e+00 1.490e+00 1.180e-02 3.544e-03 1.924e+00
 4.179e-01 3.983e-02 3.237e-03 3.440e-01 4.138e-01 2.271e-04 5.974e-05
 3.943e-02 1.356e-01 1.790e+00 9.746e-02 5.771e-03 2.960e-03 1.665e-01
 2.699e+00 5.865e-03 4.169e-03 2.094e+00 1.660e-01 6.258e-01 4.835e-02
 4.079e-03 3.013e-01 1.824e-01 6.129e-02 1.711e-02 1.085e-02 1.169e-01
 9.043e-02 3.810e-01 2.033e-03 6.469e-01 4.236e-01 5.753e-03 6.777e-03
 2.741e-03 2.351e+00 2.313e+00 1.053e+00 4.809e-05 5.628e-04 3.703e-01
 2.127e-04 1.477e-05 1.587e-04 4.984e-04 2.851e+00 6.637e-06 1.051e-04
 3.135e-05 4.137e-04 2.749e-01 9.405e-05 1.242e-05 1.027e-04 3.690e-03
 6.476e-04 1.436e-04 2.092e-04 1.297e-03 3.141e+00 8.737e-05 4.464e-04
 1.531e-05 6.118e-04 2.485e-03 3.885e-04 1.122e-04 1.839e-04 1.247e-02
 3.412e-01 4.242e-04 1.194e-03 1.859e-01 1.345e-01 2.638e-04 3.136e-03
 1.647e-04 1.671e-03 4.073e-01 3.958e-01 7.657e-05 5.294e-04 3.243e-02
 3.937e-02 7.571e-02 4.502e-03 9.873e-02 3.077e+00 1.412e-02 4.008e-03
 6.018e-03 1.316e+00 1.057e-01 2.015e+00 3.251e-02 1.650e-02 1.667e-02
 1.284e-01 1.437e+00 6.716e-03 1.132e+00 1.246e+00 2.274e-03 5.543e-03
 5.001e-03 3.518e+00 3.757e-01 4.613e-01 1.539e-01 9.839e-03 5.626e-01
 6.064e-02 2.370e-04 3.194e-03 9.050e-05 1.035e-01 1.044e-05 3.670e-04
 1.737e-05 2.437e-04 1.074e-01 1.275e-01 7.933e-03 7.330e-04 4.967e-05]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9783333333333334, 0.9713026819923372)
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 39.29498155563945 clip 11.646765979092327
total norm for a microbatch 40.49947986685788 clip 12.832099025143812
total norm for a microbatch 35.221256738780546 clip 21.86182732016133
total norm for a microbatch 24.020925744912866 clip 31.267054388002112
total norm for a microbatch 39.96408225381135 clip 27.82555619841206
cuda
Objective function 44.55 = squared loss an data 42.50 + 0.5*rho*h**2 1.597135 + alpha*h 0.000000 + L2reg 0.30 + L1reg 0.15 ; SHD = 61 ; DAG False
Proportion of microbatches that were clipped  0.9196003805899143
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.7872519573793149
iteration 1 in outer loop, alpha = 1.7872519573793149, rho = 1.0, h = 1.7872519573793149
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 47.74 = squared loss an data 42.50 + 0.5*rho*h**2 1.597135 + alpha*h 3.194270 + L2reg 0.30 + L1reg 0.15 ; SHD = 61 ; DAG False
total norm for a microbatch 33.3407604507667 clip 1.0
total norm for a microbatch 19.566694254462274 clip 2.4616591315130796
total norm for a microbatch 59.15199126868078 clip 7.389439204055648
total norm for a microbatch 52.11225427436889 clip 30.540785851151238
cuda
Objective function 19.20 = squared loss an data 15.96 + 0.5*rho*h**2 0.579450 + alpha*h 1.924017 + L2reg 0.59 + L1reg 0.14 ; SHD = 43 ; DAG False
Proportion of microbatches that were clipped  0.9266617610988472
iteration 1 in inner loop, alpha 1.7872519573793149 rho 1.0 h 1.0765222711465299
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 24.41 = squared loss an data 15.96 + 0.5*rho*h**2 5.794501 + alpha*h 1.924017 + L2reg 0.59 + L1reg 0.14 ; SHD = 43 ; DAG False
total norm for a microbatch 44.74451488279594 clip 1.1877296861305904
total norm for a microbatch 38.978132554403174 clip 1.4119691359870834
total norm for a microbatch 55.90403257119524 clip 1.5393880579826311
total norm for a microbatch 46.04285217488558 clip 5.9844036202079804
cuda
Objective function 16.44 = squared loss an data 13.56 + 0.5*rho*h**2 1.102596 + alpha*h 0.839284 + L2reg 0.80 + L1reg 0.14 ; SHD = 31 ; DAG False
Proportion of microbatches that were clipped  0.9369260512324794
iteration 2 in inner loop, alpha 1.7872519573793149 rho 10.0 h 0.46959475123747296
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 26.36 = squared loss an data 13.56 + 0.5*rho*h**2 11.025962 + alpha*h 0.839284 + L2reg 0.80 + L1reg 0.14 ; SHD = 31 ; DAG False
total norm for a microbatch 71.4321399587852 clip 2.0694086360808486
total norm for a microbatch 108.32718531946395 clip 2.0694086360808486
total norm for a microbatch 46.9943356156734 clip 4.630790436325045
total norm for a microbatch 36.10930443396285 clip 25.671469897255648
total norm for a microbatch 50.06797335698679 clip 33.282187174240704
total norm for a microbatch 80.45497033917832 clip 47.67845834185293
total norm for a microbatch 78.5328968354618 clip 47.56505937246342
cuda
Objective function 16.80 = squared loss an data 14.31 + 0.5*rho*h**2 1.172999 + alpha*h 0.273747 + L2reg 0.92 + L1reg 0.12 ; SHD = 26 ; DAG True
Proportion of microbatches that were clipped  0.9491841491841492
iteration 3 in inner loop, alpha 1.7872519573793149 rho 100.0 h 0.1531664841759195
iteration 2 in outer loop, alpha = 17.103900374971264, rho = 100.0, h = 0.1531664841759195
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 19.14 = squared loss an data 14.31 + 0.5*rho*h**2 1.172999 + alpha*h 2.619744 + L2reg 0.92 + L1reg 0.12 ; SHD = 26 ; DAG True
total norm for a microbatch 133.49402504452138 clip 7.532203776268782
total norm for a microbatch 67.50109858563461 clip 24.242403000842597
total norm for a microbatch 53.93157292324887 clip 28.857824329943572
cuda
Objective function 18.31 = squared loss an data 14.58 + 0.5*rho*h**2 0.642451 + alpha*h 1.938787 + L2reg 1.02 + L1reg 0.12 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.9535392848455364
iteration 1 in inner loop, alpha 17.103900374971264 rho 100.0 h 0.11335349972337383
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 24.09 = squared loss an data 14.58 + 0.5*rho*h**2 6.424508 + alpha*h 1.938787 + L2reg 1.02 + L1reg 0.12 ; SHD = 37 ; DAG True
total norm for a microbatch 69.14110445646392 clip 1.8937634398313465
total norm for a microbatch 109.06383419065651 clip 8.309015809750663
total norm for a microbatch 68.70931249995292 clip 12.849725257324994
total norm for a microbatch 46.94569002235248 clip 38.61960038134015
total norm for a microbatch 55.35028525583962 clip 59.49797192776898
cuda
Objective function 18.13 = squared loss an data 14.98 + 0.5*rho*h**2 1.115825 + alpha*h 0.807994 + L2reg 1.10 + L1reg 0.12 ; SHD = 33 ; DAG True
Proportion of microbatches that were clipped  0.9585082736478143
iteration 2 in inner loop, alpha 17.103900374971264 rho 1000.0 h 0.04724034736695515
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 28.17 = squared loss an data 14.98 + 0.5*rho*h**2 11.158252 + alpha*h 0.807994 + L2reg 1.10 + L1reg 0.12 ; SHD = 33 ; DAG True
total norm for a microbatch 162.33912102309637 clip 1.0
total norm for a microbatch 123.59608771775873 clip 2.1821566733038065
total norm for a microbatch 74.69105975780664 clip 3.69642809845825
total norm for a microbatch 74.2196390978264 clip 63.4502285922214
cuda
Objective function 18.10 = squared loss an data 15.30 + 0.5*rho*h**2 1.248625 + alpha*h 0.270288 + L2reg 1.16 + L1reg 0.12 ; SHD = 35 ; DAG True
Proportion of microbatches that were clipped  0.9650299401197605
iteration 3 in inner loop, alpha 17.103900374971264 rho 10000.0 h 0.015802687234078405
iteration 3 in outer loop, alpha = 175.1307727157553, rho = 10000.0, h = 0.015802687234078405
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 20.59 = squared loss an data 15.30 + 0.5*rho*h**2 1.248625 + alpha*h 2.767537 + L2reg 1.16 + L1reg 0.12 ; SHD = 35 ; DAG True
total norm for a microbatch 135.82113300191548 clip 1.2011102063933885
total norm for a microbatch 54.4152267102771 clip 13.481710760883978
total norm for a microbatch 202.53007752396576 clip 16.037198389478334
total norm for a microbatch 89.15916410893493 clip 22.694251948313326
total norm for a microbatch 78.26251872439259 clip 22.694251948313326
total norm for a microbatch 106.06512049341526 clip 46.44791381823781
total norm for a microbatch 61.916613963159975 clip 63.47732337781322
cuda
Objective function 18.85 = squared loss an data 15.30 + 0.5*rho*h**2 0.486553 + alpha*h 1.727598 + L2reg 1.21 + L1reg 0.12 ; SHD = 32 ; DAG True
Proportion of microbatches that were clipped  0.968370986920333
iteration 1 in inner loop, alpha 175.1307727157553 rho 10000.0 h 0.00986461702731134
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 23.22 = squared loss an data 15.30 + 0.5*rho*h**2 4.865533 + alpha*h 1.727598 + L2reg 1.21 + L1reg 0.12 ; SHD = 32 ; DAG True
total norm for a microbatch 126.78500073718925 clip 20.21282318015155
total norm for a microbatch 92.23985485763978 clip 23.911562793767857
cuda
Objective function 18.52 = squared loss an data 14.69 + 0.5*rho*h**2 1.494059 + alpha*h 0.957329 + L2reg 1.26 + L1reg 0.12 ; SHD = 31 ; DAG True
Proportion of microbatches that were clipped  0.9846005774783445
iteration 2 in inner loop, alpha 175.1307727157553 rho 100000.0 h 0.005466368932644627
iteration 4 in outer loop, alpha = 5641.499705360383, rho = 1000000.0, h = 0.005466368932644627
Threshold 0.3
[[0.005 0.02  0.087 0.294 0.355 0.022 0.032 0.004 0.043 0.845 0.118 0.064
  0.032 0.017 0.187]
 [0.252 0.005 0.228 0.412 0.329 0.131 0.073 0.014 0.106 0.525 0.082 0.749
  0.071 0.005 0.385]
 [0.061 0.035 0.006 0.281 0.136 0.017 0.023 0.028 0.038 0.07  0.086 0.115
  0.018 0.018 0.283]
 [0.016 0.018 0.028 0.006 0.145 0.02  0.008 0.006 0.019 0.053 0.068 0.023
  0.013 0.009 0.367]
 [0.004 0.006 0.049 0.046 0.008 0.01  0.005 0.005 0.004 0.004 0.003 0.029
  0.004 0.003 0.028]
 [0.378 0.045 0.29  0.258 0.351 0.006 0.014 0.02  0.05  0.367 0.302 0.473
  0.046 0.019 0.273]
 [0.253 0.112 0.328 0.794 0.547 0.353 0.008 0.117 0.154 0.489 0.968 0.345
  0.128 0.039 0.403]
 [1.268 0.35  0.195 1.014 0.339 0.392 0.023 0.005 0.135 0.505 0.279 0.343
  0.123 0.056 0.542]
 [0.131 0.05  0.179 0.31  0.405 0.114 0.053 0.046 0.007 1.081 0.757 0.302
  0.024 0.004 0.235]
 [0.009 0.014 0.061 0.179 1.833 0.017 0.011 0.005 0.005 0.008 0.146 0.06
  0.017 0.004 0.107]
 [0.064 0.068 0.048 0.121 2.059 0.021 0.007 0.017 0.006 0.068 0.008 0.199
  0.004 0.006 0.097]
 [0.092 0.008 0.062 0.29  0.187 0.015 0.019 0.016 0.024 0.065 0.038 0.005
  0.012 0.005 0.149]
 [0.204 0.053 0.255 0.453 1.014 0.183 0.084 0.041 0.278 0.369 1.303 0.378
  0.006 0.05  0.504]
 [0.268 0.976 0.251 0.517 0.509 0.249 0.143 0.198 2.497 0.503 0.743 0.936
  0.17  0.004 0.42 ]
 [0.032 0.019 0.03  0.016 0.108 0.019 0.015 0.013 0.019 0.06  0.084 0.028
  0.012 0.009 0.006]]
[[0.    0.    0.    0.    0.355 0.    0.    0.    0.    0.845 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.412 0.329 0.    0.    0.    0.    0.525 0.    0.749
  0.    0.    0.385]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.367]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.378 0.    0.    0.    0.351 0.    0.    0.    0.    0.367 0.302 0.473
  0.    0.    0.   ]
 [0.    0.    0.328 0.794 0.547 0.353 0.    0.    0.    0.489 0.968 0.345
  0.    0.    0.403]
 [1.268 0.35  0.    1.014 0.339 0.392 0.    0.    0.    0.505 0.    0.343
  0.    0.    0.542]
 [0.    0.    0.    0.31  0.405 0.    0.    0.    0.    1.081 0.757 0.302
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.833 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.059 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.453 1.014 0.    0.    0.    0.    0.369 1.303 0.378
  0.    0.    0.504]
 [0.    0.976 0.    0.517 0.509 0.    0.    0.    2.497 0.503 0.743 0.936
  0.    0.    0.42 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.52, 'tpr': 0.8, 'fpr': 0.3466666666666667, 'f1': 0.6, 'shd': 31, 'npred': 50, 'ntrue': 30}
[0.02  0.087 0.294 0.355 0.022 0.032 0.004 0.043 0.845 0.118 0.064 0.032
 0.017 0.187 0.252 0.228 0.412 0.329 0.131 0.073 0.014 0.106 0.525 0.082
 0.749 0.071 0.005 0.385 0.061 0.035 0.281 0.136 0.017 0.023 0.028 0.038
 0.07  0.086 0.115 0.018 0.018 0.283 0.016 0.018 0.028 0.145 0.02  0.008
 0.006 0.019 0.053 0.068 0.023 0.013 0.009 0.367 0.004 0.006 0.049 0.046
 0.01  0.005 0.005 0.004 0.004 0.003 0.029 0.004 0.003 0.028 0.378 0.045
 0.29  0.258 0.351 0.014 0.02  0.05  0.367 0.302 0.473 0.046 0.019 0.273
 0.253 0.112 0.328 0.794 0.547 0.353 0.117 0.154 0.489 0.968 0.345 0.128
 0.039 0.403 1.268 0.35  0.195 1.014 0.339 0.392 0.023 0.135 0.505 0.279
 0.343 0.123 0.056 0.542 0.131 0.05  0.179 0.31  0.405 0.114 0.053 0.046
 1.081 0.757 0.302 0.024 0.004 0.235 0.009 0.014 0.061 0.179 1.833 0.017
 0.011 0.005 0.005 0.146 0.06  0.017 0.004 0.107 0.064 0.068 0.048 0.121
 2.059 0.021 0.007 0.017 0.006 0.068 0.199 0.004 0.006 0.097 0.092 0.008
 0.062 0.29  0.187 0.015 0.019 0.016 0.024 0.065 0.038 0.012 0.005 0.149
 0.204 0.053 0.255 0.453 1.014 0.183 0.084 0.041 0.278 0.369 1.303 0.378
 0.05  0.504 0.268 0.976 0.251 0.517 0.509 0.249 0.143 0.198 2.497 0.503
 0.743 0.936 0.17  0.42  0.032 0.019 0.03  0.016 0.108 0.019 0.015 0.013
 0.019 0.06  0.084 0.028 0.012 0.009]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8990740740740741, 0.7204874220906975)
Iterations 504
Achieves (17.551612243819033, 1e-05)-DP
