samples  5000  graph  20 60 ER mim  minibatch size  100  noise  0.6  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.09213537717594988
iteration 3 in outer loop, alpha = 15.699588471753614, rho = 100.0, h = 0.09213537717594988
cuda
iteration 1 in inner loop,alpha 15.699588471753614 rho 100.0 h 0.05367211647120129
iteration 2 in inner loop,alpha 15.699588471753614 rho 1000.0 h 0.01978300963797608
iteration 4 in outer loop, alpha = 35.48259810972969, rho = 1000.0, h = 0.01978300963797608
cuda
iteration 1 in inner loop,alpha 35.48259810972969 rho 1000.0 h 0.010570787404574844
iteration 2 in inner loop,alpha 35.48259810972969 rho 10000.0 h 0.0034988383385510247
iteration 5 in outer loop, alpha = 70.47098149523994, rho = 10000.0, h = 0.0034988383385510247
cuda
iteration 1 in inner loop,alpha 70.47098149523994 rho 10000.0 h 0.0015034461635607954
iteration 2 in inner loop,alpha 70.47098149523994 rho 100000.0 h 0.0005740038083814625
iteration 6 in outer loop, alpha = 127.87136233338619, rho = 100000.0, h = 0.0005740038083814625
cuda
iteration 1 in inner loop,alpha 127.87136233338619 rho 100000.0 h 0.0003590948377123482
iteration 7 in outer loop, alpha = 486.96620004573435, rho = 1000000.0, h = 0.0003590948377123482
Threshold 0.3
[[0.    0.025 0.009 0.054 0.159 0.182 0.043 0.    0.033 0.042 0.468 0.609
  1.063 0.    0.099 0.    0.072 2.57  0.513 0.017]
 [0.    0.001 0.    0.038 0.081 0.46  0.157 0.    0.007 0.047 0.045 0.
  0.    0.    0.071 0.142 0.02  0.    0.087 0.8  ]
 [0.    0.288 0.002 0.124 0.543 0.096 0.055 0.    1.454 0.77  0.147 0.006
  0.002 0.    0.033 0.215 0.051 0.042 0.135 0.114]
 [0.    0.006 0.    0.002 0.001 0.004 0.    0.    0.    0.001 0.002 0.
  0.    0.    0.05  0.    0.    0.    0.002 0.024]
 [0.    0.003 0.    0.162 0.002 0.009 0.004 0.    0.001 0.015 0.011 0.
  0.    0.    0.036 0.109 0.004 0.    0.001 0.002]
 [0.    0.002 0.006 0.108 0.03  0.005 0.009 0.    0.021 0.027 0.002 0.
  0.003 0.    1.191 0.078 0.007 0.    0.296 0.043]
 [0.    0.003 0.002 0.211 0.287 0.238 0.003 0.    0.002 0.006 0.003 0.001
  0.003 0.    0.093 0.019 0.001 0.    0.004 0.015]
 [0.    0.1   0.001 1.077 0.046 0.057 0.18  0.    0.348 0.063 0.077 2.119
  1.358 0.042 0.108 0.015 0.035 2.222 0.047 0.039]
 [0.    0.026 0.    1.426 1.388 0.018 0.078 0.    0.003 0.377 1.213 0.
  0.001 0.    0.422 0.142 0.005 0.    0.006 0.016]
 [0.    0.003 0.001 0.048 0.013 0.184 0.111 0.    0.005 0.004 1.051 0.001
  0.002 0.    0.096 0.008 0.038 0.    0.007 0.667]
 [0.    0.007 0.    0.042 0.01  0.019 0.307 0.    0.    0.001 0.002 0.
  0.    0.    0.016 0.005 0.31  0.    0.003 0.738]
 [0.    2.09  0.001 1.58  1.302 0.058 0.539 0.    1.033 0.031 1.072 0.001
  0.001 0.    0.    0.911 0.362 0.    0.067 0.127]
 [0.    0.033 0.003 0.176 0.081 0.115 0.413 0.    0.    0.537 0.056 0.586
  0.003 0.    1.595 0.151 0.224 0.    0.005 0.063]
 [0.    1.037 4.614 0.092 0.045 0.165 0.074 0.    0.83  0.181 0.051 0.018
  0.052 0.    0.057 0.667 0.    0.092 0.704 1.196]
 [0.    0.001 0.    0.013 0.021 0.001 0.001 0.    0.002 0.005 0.002 0.
  0.    0.    0.002 0.012 0.001 0.    0.001 0.002]
 [0.    0.001 0.002 0.069 0.005 0.014 0.005 0.    0.004 0.289 0.038 0.
  0.    0.    0.037 0.003 0.001 0.    0.001 0.091]
 [0.    0.01  0.001 0.977 0.096 0.001 0.896 0.    0.001 0.003 0.003 0.001
  0.003 0.    0.552 0.001 0.002 0.    0.003 0.021]
 [0.    0.051 0.003 0.294 0.051 0.048 0.059 0.    0.11  0.058 0.107 0.922
  1.981 0.    0.002 0.072 0.047 0.002 0.025 1.25 ]
 [0.    0.001 0.01  0.093 0.771 0.016 0.01  0.    0.409 0.21  0.118 0.001
  0.006 0.    0.692 1.037 0.001 0.007 0.004 0.064]
 [0.    0.    0.    0.01  0.007 0.021 0.064 0.    0.001 0.001 0.001 0.
  0.    0.    0.055 0.001 0.002 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.468 0.609
  1.063 0.    0.    0.    0.    2.57  0.513 0.   ]
 [0.    0.    0.    0.    0.    0.46  0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.8  ]
 [0.    0.    0.    0.    0.543 0.    0.    0.    1.454 0.77  0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.191 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.077 0.    0.    0.    0.    0.348 0.    0.    2.119
  1.358 0.    0.    0.    0.    2.222 0.    0.   ]
 [0.    0.    0.    1.426 1.388 0.    0.    0.    0.    0.377 1.213 0.
  0.    0.    0.422 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.051 0.
  0.    0.    0.    0.    0.    0.    0.    0.667]
 [0.    0.    0.    0.    0.    0.    0.307 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.31  0.    0.    0.738]
 [0.    2.09  0.    1.58  1.302 0.    0.539 0.    1.033 0.    1.072 0.
  0.    0.    0.    0.911 0.362 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.413 0.    0.    0.537 0.    0.586
  0.    0.    1.595 0.    0.    0.    0.    0.   ]
 [0.    1.037 4.614 0.    0.    0.    0.    0.    0.83  0.    0.    0.
  0.    0.    0.    0.667 0.    0.    0.704 1.196]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.977 0.    0.    0.896 0.    0.    0.    0.    0.
  0.    0.    0.552 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.922
  1.981 0.    0.    0.    0.    0.    0.    1.25 ]
 [0.    0.    0.    0.    0.771 0.    0.    0.    0.409 0.    0.    0.
  0.    0.    0.692 1.037 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.1111111111111111, 'tpr': 0.8, 'fpr': 0.046153846153846156, 'f1': 0.8421052631578948, 'shd': 15, 'npred': 54, 'ntrue': 60}
[2.486e-02 9.052e-03 5.371e-02 1.587e-01 1.817e-01 4.324e-02 7.783e-05
 3.297e-02 4.157e-02 4.679e-01 6.090e-01 1.063e+00 6.620e-05 9.920e-02
 1.199e-04 7.198e-02 2.570e+00 5.132e-01 1.733e-02 1.126e-06 1.915e-04
 3.752e-02 8.124e-02 4.595e-01 1.570e-01 1.967e-06 6.746e-03 4.734e-02
 4.465e-02 9.145e-05 1.241e-04 1.117e-05 7.147e-02 1.416e-01 1.994e-02
 6.033e-05 8.681e-02 8.001e-01 1.329e-04 2.876e-01 1.239e-01 5.433e-01
 9.557e-02 5.487e-02 9.814e-05 1.454e+00 7.700e-01 1.467e-01 5.973e-03
 1.684e-03 8.824e-06 3.279e-02 2.152e-01 5.116e-02 4.198e-02 1.350e-01
 1.140e-01 1.943e-06 6.149e-03 3.541e-05 1.429e-03 4.294e-03 7.652e-05
 6.439e-07 4.076e-04 6.969e-04 2.417e-03 1.168e-04 2.391e-04 2.773e-06
 5.044e-02 4.442e-04 4.113e-04 2.866e-05 2.138e-03 2.433e-02 2.326e-05
 3.362e-03 1.562e-04 1.615e-01 8.756e-03 4.352e-03 3.272e-06 5.028e-04
 1.479e-02 1.130e-02 1.424e-04 2.360e-04 2.487e-06 3.561e-02 1.090e-01
 4.084e-03 8.145e-05 6.774e-04 2.316e-03 3.363e-05 1.803e-03 5.570e-03
 1.079e-01 2.973e-02 8.892e-03 1.957e-05 2.099e-02 2.682e-02 1.550e-03
 3.541e-04 3.029e-03 8.747e-05 1.191e+00 7.812e-02 7.076e-03 4.046e-04
 2.963e-01 4.264e-02 1.778e-05 3.222e-03 1.544e-03 2.112e-01 2.870e-01
 2.379e-01 1.243e-05 1.981e-03 6.335e-03 3.498e-03 9.065e-04 2.884e-03
 5.064e-05 9.267e-02 1.922e-02 1.254e-03 7.032e-05 3.652e-03 1.453e-02
 9.723e-05 9.976e-02 5.420e-04 1.077e+00 4.617e-02 5.668e-02 1.796e-01
 3.484e-01 6.290e-02 7.720e-02 2.119e+00 1.358e+00 4.159e-02 1.083e-01
 1.520e-02 3.480e-02 2.222e+00 4.680e-02 3.931e-02 5.116e-06 2.562e-02
 2.259e-04 1.426e+00 1.388e+00 1.762e-02 7.764e-02 3.284e-05 3.771e-01
 1.213e+00 2.637e-04 5.863e-04 7.462e-06 4.218e-01 1.418e-01 5.032e-03
 1.301e-04 6.096e-03 1.628e-02 3.014e-05 2.860e-03 7.135e-04 4.782e-02
 1.289e-02 1.844e-01 1.106e-01 7.831e-06 4.781e-03 1.051e+00 1.023e-03
 2.152e-03 8.150e-06 9.641e-02 7.607e-03 3.762e-02 1.767e-04 6.659e-03
 6.674e-01 1.076e-05 7.326e-03 1.884e-04 4.233e-02 1.039e-02 1.911e-02
 3.068e-01 2.812e-06 4.663e-04 9.583e-04 1.422e-04 1.094e-04 4.526e-06
 1.565e-02 4.977e-03 3.098e-01 1.040e-04 3.115e-03 7.384e-01 4.834e-06
 2.090e+00 1.239e-03 1.580e+00 1.302e+00 5.773e-02 5.391e-01 8.956e-06
 1.033e+00 3.073e-02 1.072e+00 1.120e-03 1.385e-04 2.312e-04 9.109e-01
 3.624e-01 1.557e-04 6.708e-02 1.272e-01 3.493e-06 3.342e-02 2.902e-03
 1.760e-01 8.144e-02 1.149e-01 4.132e-01 5.591e-06 2.045e-04 5.373e-01
 5.595e-02 5.863e-01 2.602e-05 1.595e+00 1.514e-01 2.239e-01 8.181e-05
 5.186e-03 6.313e-02 6.420e-05 1.037e+00 4.614e+00 9.236e-02 4.476e-02
 1.649e-01 7.367e-02 1.374e-04 8.297e-01 1.807e-01 5.050e-02 1.814e-02
 5.180e-02 5.694e-02 6.668e-01 1.426e-04 9.162e-02 7.044e-01 1.196e+00
 1.247e-06 8.462e-04 2.263e-04 1.309e-02 2.126e-02 8.230e-04 1.180e-03
 2.075e-06 1.621e-03 4.910e-03 1.863e-03 2.690e-04 4.387e-04 1.392e-05
 1.168e-02 8.276e-04 3.563e-05 1.273e-03 2.022e-03 2.094e-05 7.319e-04
 1.957e-03 6.869e-02 5.457e-03 1.403e-02 4.923e-03 1.338e-05 4.236e-03
 2.889e-01 3.825e-02 4.394e-04 4.116e-04 1.148e-05 3.687e-02 5.019e-04
 7.436e-05 1.471e-03 9.093e-02 1.698e-05 9.857e-03 1.438e-03 9.775e-01
 9.645e-02 6.159e-04 8.955e-01 1.274e-05 8.237e-04 2.900e-03 2.626e-03
 1.021e-03 2.838e-03 1.546e-05 5.519e-01 5.584e-04 2.566e-05 3.136e-03
 2.148e-02 2.832e-06 5.119e-02 2.742e-03 2.936e-01 5.090e-02 4.832e-02
 5.921e-02 1.817e-05 1.095e-01 5.775e-02 1.070e-01 9.224e-01 1.981e+00
 2.816e-04 1.511e-03 7.191e-02 4.708e-02 2.543e-02 1.250e+00 1.105e-04
 9.766e-04 9.999e-03 9.346e-02 7.712e-01 1.574e-02 9.841e-03 5.082e-05
 4.089e-01 2.098e-01 1.183e-01 5.084e-04 5.927e-03 1.513e-04 6.918e-01
 1.037e+00 6.998e-04 6.563e-03 6.361e-02 1.143e-06 3.312e-04 8.955e-05
 1.025e-02 7.466e-03 2.128e-02 6.445e-02 3.856e-06 6.839e-04 6.269e-04
 7.442e-04 1.349e-05 4.989e-04 1.461e-05 5.470e-02 7.610e-04 2.434e-03
 3.206e-04 2.016e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9345833333333334, 0.8643355792196534)
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
total norm for a microbatch 18.56925300873508 clip 9.658064220834715
total norm for a microbatch 13.101563021593043 clip 12.057043019503748
total norm for a microbatch 19.310507357179212 clip 11.850477109767569
total norm for a microbatch 13.69599137269465 clip 12.538543808903832
total norm for a microbatch 14.743890071493087 clip 11.895945606449352
cuda
Objective function 18.53 = squared loss an data 17.12 + 0.5*rho*h**2 0.922499 + alpha*h 0.000000 + L2reg 0.27 + L1reg 0.22 ; SHD = 42 ; DAG False
Proportion of microbatches that were clipped  0.879057924888606
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.3583070322639834
iteration 1 in outer loop, alpha = 1.3583070322639834, rho = 1.0, h = 1.3583070322639834
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 20.37 = squared loss an data 17.12 + 0.5*rho*h**2 0.922499 + alpha*h 1.844998 + L2reg 0.27 + L1reg 0.22 ; SHD = 42 ; DAG False
total norm for a microbatch 15.845932188211274 clip 5.796128526196119
total norm for a microbatch 15.23423125993222 clip 7.00071832024005
total norm for a microbatch 14.443188721619515 clip 14.793341188199381
total norm for a microbatch 25.21201644808155 clip 15.24896463113323
total norm for a microbatch 13.473789992371104 clip 15.367773853702284
cuda
Objective function 17.87 = squared loss an data 15.34 + 0.5*rho*h**2 0.489524 + alpha*h 1.344002 + L2reg 0.49 + L1reg 0.21 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.8840415180019462
iteration 1 in inner loop, alpha 1.3583070322639834 rho 1.0 h 0.98946828750082
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 22.28 = squared loss an data 15.34 + 0.5*rho*h**2 4.895237 + alpha*h 1.344002 + L2reg 0.49 + L1reg 0.21 ; SHD = 40 ; DAG True
total norm for a microbatch 23.349256698358136 clip 3.856639643488005
total norm for a microbatch 14.837617706037483 clip 16.91818229016676
total norm for a microbatch 33.58159598557168 clip 18.360616922374124
total norm for a microbatch 17.763997600310745 clip 19.06356335668951
total norm for a microbatch 23.301274932792317 clip 19.472697522379598
cuda
Objective function 18.29 = squared loss an data 16.15 + 0.5*rho*h**2 0.785101 + alpha*h 0.538240 + L2reg 0.63 + L1reg 0.18 ; SHD = 42 ; DAG True
Proportion of microbatches that were clipped  0.8928851892571069
iteration 2 in inner loop, alpha 1.3583070322639834 rho 10.0 h 0.39625762565364653
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 25.36 = squared loss an data 16.15 + 0.5*rho*h**2 7.851005 + alpha*h 0.538240 + L2reg 0.63 + L1reg 0.18 ; SHD = 42 ; DAG True
total norm for a microbatch 28.54561715067854 clip 1.7191142432582052
total norm for a microbatch 21.123069559121916 clip 19.385586140665833
total norm for a microbatch 31.42527818056026 clip 21.02494030599906
total norm for a microbatch 27.67183476208712 clip 20.607910616200023
cuda
Objective function 19.28 = squared loss an data 17.48 + 0.5*rho*h**2 0.760369 + alpha*h 0.167504 + L2reg 0.71 + L1reg 0.16 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.9000156225589752
iteration 3 in inner loop, alpha 1.3583070322639834 rho 100.0 h 0.12331817821459268
iteration 2 in outer loop, alpha = 13.690124853723251, rho = 100.0, h = 0.12331817821459268
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 20.80 = squared loss an data 17.48 + 0.5*rho*h**2 0.760369 + alpha*h 1.688241 + L2reg 0.71 + L1reg 0.16 ; SHD = 39 ; DAG True
total norm for a microbatch 34.64641957663797 clip 1.7234391714497566
total norm for a microbatch 32.77907905813605 clip 1.7234391714497566
total norm for a microbatch 19.478665380498377 clip 2.4961687087424425
total norm for a microbatch 27.45268913219342 clip 6.133355975213887
total norm for a microbatch 40.081958871091295 clip 7.901057120076323
total norm for a microbatch 49.79802406725534 clip 13.550583191110325
total norm for a microbatch 31.912820562082636 clip 16.147994913512754
total norm for a microbatch 20.589618540088644 clip 22.29634498321409
total norm for a microbatch 30.631909295641822 clip 22.196892169854436
total norm for a microbatch 30.58969660563543 clip 22.46000866578889
cuda
Objective function 20.23 = squared loss an data 17.86 + 0.5*rho*h**2 0.323112 + alpha*h 1.100523 + L2reg 0.79 + L1reg 0.16 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.8966867961481965
iteration 1 in inner loop, alpha 13.690124853723251 rho 100.0 h 0.08038812023022857
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 23.14 = squared loss an data 17.86 + 0.5*rho*h**2 3.231125 + alpha*h 1.100523 + L2reg 0.79 + L1reg 0.16 ; SHD = 43 ; DAG True
total norm for a microbatch 24.017794219055965 clip 2.08949831268089
total norm for a microbatch 21.241302893380233 clip 6.143904674117205
total norm for a microbatch 30.47043375125454 clip 13.714537978693137
cuda
Objective function 20.39 = squared loss an data 18.40 + 0.5*rho*h**2 0.568980 + alpha*h 0.461818 + L2reg 0.81 + L1reg 0.14 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.8986131037780966
iteration 2 in inner loop, alpha 13.690124853723251 rho 1000.0 h 0.03373365864240796
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 25.51 = squared loss an data 18.40 + 0.5*rho*h**2 5.689799 + alpha*h 0.461818 + L2reg 0.81 + L1reg 0.14 ; SHD = 46 ; DAG True
total norm for a microbatch 33.20618959654368 clip 5.06717997469573
total norm for a microbatch 31.042787066896825 clip 25.33261661770053
total norm for a microbatch 54.09493290523182 clip 26.25247366324778
total norm for a microbatch 25.56802423792228 clip 25.62815865668592
total norm for a microbatch 23.916656448446307 clip 25.766174061258717
cuda
Objective function 20.63 = squared loss an data 18.87 + 0.5*rho*h**2 0.653406 + alpha*h 0.156500 + L2reg 0.81 + L1reg 0.14 ; SHD = 47 ; DAG True
Proportion of microbatches that were clipped  0.9023090586145648
iteration 3 in inner loop, alpha 13.690124853723251 rho 10000.0 h 0.01143158644930864
iteration 3 in outer loop, alpha = 128.00598934680966, rho = 10000.0, h = 0.01143158644930864
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 21.94 = squared loss an data 18.87 + 0.5*rho*h**2 0.653406 + alpha*h 1.463312 + L2reg 0.81 + L1reg 0.14 ; SHD = 47 ; DAG True
total norm for a microbatch 50.63906011486032 clip 1.0
total norm for a microbatch 52.51884652788473 clip 1.401524849263198
total norm for a microbatch 21.77279094648911 clip 23.904285798769248
total norm for a microbatch 33.919691688343306 clip 26.0050776794913
total norm for a microbatch 33.660920688689366 clip 26.509230487774662
cuda
Objective function 21.01 = squared loss an data 18.90 + 0.5*rho*h**2 0.230825 + alpha*h 0.869735 + L2reg 0.86 + L1reg 0.14 ; SHD = 41 ; DAG True
Proportion of microbatches that were clipped  0.9081356471899379
iteration 1 in inner loop, alpha 128.00598934680966 rho 10000.0 h 0.0067944855081307765
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 23.08 = squared loss an data 18.90 + 0.5*rho*h**2 2.308252 + alpha*h 0.869735 + L2reg 0.86 + L1reg 0.14 ; SHD = 41 ; DAG True
total norm for a microbatch 63.18009595816969 clip 1.0
total norm for a microbatch 91.8271041411847 clip 1.7444983489094892
total norm for a microbatch 33.739917570330796 clip 10.755457944377467
total norm for a microbatch 40.65955202610315 clip 20.31047702811882
total norm for a microbatch 35.89145248244289 clip 35.06031187985113
cuda
Objective function 21.05 = squared loss an data 19.05 + 0.5*rho*h**2 0.558069 + alpha*h 0.427651 + L2reg 0.87 + L1reg 0.14 ; SHD = 47 ; DAG True
Proportion of microbatches that were clipped  0.9101835309403931
iteration 2 in inner loop, alpha 128.00598934680966 rho 100000.0 h 0.003340866096557704
iteration 4 in outer loop, alpha = 3468.872085904514, rho = 1000000.0, h = 0.003340866096557704
Threshold 0.3
[[0.003 0.105 0.372 0.334 0.343 0.252 0.328 0.038 0.337 0.395 0.407 0.18
  0.692 0.078 0.348 0.441 0.364 0.617 0.599 0.377]
 [0.024 0.004 0.121 0.138 0.176 0.648 0.17  0.005 0.112 0.264 0.218 0.236
  0.075 0.024 0.21  0.218 0.192 0.077 0.112 0.47 ]
 [0.009 0.035 0.002 0.087 0.251 0.068 0.092 0.011 0.031 0.285 0.091 0.049
  0.024 0.002 0.059 0.176 0.094 0.083 0.064 0.083]
 [0.01  0.013 0.035 0.004 0.021 0.042 0.029 0.003 0.007 0.025 0.031 0.007
  0.035 0.004 0.1   0.042 0.009 0.034 0.024 0.082]
 [0.012 0.012 0.023 0.115 0.004 0.065 0.022 0.003 0.005 0.03  0.077 0.007
  0.035 0.006 0.07  0.064 0.044 0.03  0.01  0.042]
 [0.012 0.006 0.032 0.075 0.072 0.003 0.011 0.01  0.036 0.01  0.195 0.011
  0.02  0.011 0.461 0.072 0.116 0.019 0.014 0.14 ]
 [0.008 0.019 0.051 0.082 0.232 0.404 0.004 0.009 0.046 0.184 0.263 0.013
  0.017 0.018 0.205 0.159 0.627 0.042 0.065 0.225]
 [0.125 0.534 0.257 0.535 0.576 0.331 0.291 0.003 0.174 0.368 0.368 0.843
  0.579 0.085 0.436 0.225 0.46  1.225 0.333 0.471]
 [0.01  0.038 0.207 0.656 0.634 0.079 0.066 0.018 0.004 0.261 0.407 0.014
  0.073 0.004 0.374 0.252 0.26  0.062 0.311 0.261]
 [0.008 0.015 0.01  0.16  0.102 0.289 0.022 0.01  0.013 0.004 0.32  0.031
  0.013 0.005 0.269 0.309 0.068 0.034 0.013 0.223]
 [0.009 0.018 0.05  0.152 0.076 0.022 0.015 0.008 0.01  0.012 0.003 0.009
  0.029 0.008 0.065 0.109 0.101 0.032 0.024 0.399]
 [0.02  0.019 0.095 0.731 0.505 0.301 0.367 0.003 0.297 0.145 0.398 0.003
  0.261 0.023 0.22  0.384 0.211 0.381 0.154 0.213]
 [0.004 0.053 0.152 0.115 0.104 0.177 0.277 0.008 0.062 0.385 0.14  0.017
  0.005 0.023 0.669 0.145 0.152 0.013 0.074 0.184]
 [0.051 0.144 1.465 0.697 0.415 0.242 0.219 0.051 0.681 0.298 0.273 0.153
  0.205 0.003 0.555 0.532 0.326 0.4   0.639 0.747]
 [0.008 0.01  0.053 0.043 0.073 0.008 0.015 0.006 0.011 0.014 0.065 0.008
  0.005 0.006 0.003 0.044 0.033 0.026 0.012 0.141]
 [0.01  0.016 0.036 0.08  0.084 0.064 0.025 0.006 0.014 0.005 0.036 0.008
  0.016 0.005 0.073 0.003 0.077 0.022 0.004 0.123]
 [0.009 0.02  0.041 0.358 0.08  0.04  0.006 0.008 0.015 0.033 0.037 0.014
  0.02  0.006 0.194 0.055 0.004 0.034 0.018 0.12 ]
 [0.006 0.029 0.055 0.12  0.125 0.122 0.088 0.003 0.084 0.111 0.129 0.009
  0.218 0.009 0.179 0.135 0.109 0.003 0.075 0.492]
 [0.007 0.052 0.049 0.145 0.47  0.339 0.051 0.009 0.013 0.365 0.152 0.024
  0.056 0.007 0.319 0.556 0.153 0.044 0.004 0.168]
 [0.009 0.007 0.032 0.048 0.102 0.035 0.015 0.005 0.02  0.013 0.012 0.02
  0.014 0.004 0.039 0.043 0.031 0.007 0.023 0.004]]
[[0.    0.    0.372 0.334 0.343 0.    0.328 0.    0.337 0.395 0.407 0.
  0.692 0.    0.348 0.441 0.364 0.617 0.599 0.377]
 [0.    0.    0.    0.    0.    0.648 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.47 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.461 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.404 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.627 0.    0.    0.   ]
 [0.    0.534 0.    0.535 0.576 0.331 0.    0.    0.    0.368 0.368 0.843
  0.579 0.    0.436 0.    0.46  1.225 0.333 0.471]
 [0.    0.    0.    0.656 0.634 0.    0.    0.    0.    0.    0.407 0.
  0.    0.    0.374 0.    0.    0.    0.311 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.32  0.
  0.    0.    0.    0.309 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.399]
 [0.    0.    0.    0.731 0.505 0.301 0.367 0.    0.    0.    0.398 0.
  0.    0.    0.    0.384 0.    0.381 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.385 0.    0.
  0.    0.    0.669 0.    0.    0.    0.    0.   ]
 [0.    0.    1.465 0.697 0.415 0.    0.    0.    0.681 0.    0.    0.
  0.    0.    0.555 0.532 0.326 0.4   0.639 0.747]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.358 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.492]
 [0.    0.    0.    0.    0.47  0.339 0.    0.    0.    0.365 0.    0.
  0.    0.    0.319 0.556 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.4090909090909091, 'tpr': 0.65, 'fpr': 0.2076923076923077, 'f1': 0.6190476190476191, 'shd': 47, 'npred': 66, 'ntrue': 60}
[0.105 0.372 0.334 0.343 0.252 0.328 0.038 0.337 0.395 0.407 0.18  0.692
 0.078 0.348 0.441 0.364 0.617 0.599 0.377 0.024 0.121 0.138 0.176 0.648
 0.17  0.005 0.112 0.264 0.218 0.236 0.075 0.024 0.21  0.218 0.192 0.077
 0.112 0.47  0.009 0.035 0.087 0.251 0.068 0.092 0.011 0.031 0.285 0.091
 0.049 0.024 0.002 0.059 0.176 0.094 0.083 0.064 0.083 0.01  0.013 0.035
 0.021 0.042 0.029 0.003 0.007 0.025 0.031 0.007 0.035 0.004 0.1   0.042
 0.009 0.034 0.024 0.082 0.012 0.012 0.023 0.115 0.065 0.022 0.003 0.005
 0.03  0.077 0.007 0.035 0.006 0.07  0.064 0.044 0.03  0.01  0.042 0.012
 0.006 0.032 0.075 0.072 0.011 0.01  0.036 0.01  0.195 0.011 0.02  0.011
 0.461 0.072 0.116 0.019 0.014 0.14  0.008 0.019 0.051 0.082 0.232 0.404
 0.009 0.046 0.184 0.263 0.013 0.017 0.018 0.205 0.159 0.627 0.042 0.065
 0.225 0.125 0.534 0.257 0.535 0.576 0.331 0.291 0.174 0.368 0.368 0.843
 0.579 0.085 0.436 0.225 0.46  1.225 0.333 0.471 0.01  0.038 0.207 0.656
 0.634 0.079 0.066 0.018 0.261 0.407 0.014 0.073 0.004 0.374 0.252 0.26
 0.062 0.311 0.261 0.008 0.015 0.01  0.16  0.102 0.289 0.022 0.01  0.013
 0.32  0.031 0.013 0.005 0.269 0.309 0.068 0.034 0.013 0.223 0.009 0.018
 0.05  0.152 0.076 0.022 0.015 0.008 0.01  0.012 0.009 0.029 0.008 0.065
 0.109 0.101 0.032 0.024 0.399 0.02  0.019 0.095 0.731 0.505 0.301 0.367
 0.003 0.297 0.145 0.398 0.261 0.023 0.22  0.384 0.211 0.381 0.154 0.213
 0.004 0.053 0.152 0.115 0.104 0.177 0.277 0.008 0.062 0.385 0.14  0.017
 0.023 0.669 0.145 0.152 0.013 0.074 0.184 0.051 0.144 1.465 0.697 0.415
 0.242 0.219 0.051 0.681 0.298 0.273 0.153 0.205 0.555 0.532 0.326 0.4
 0.639 0.747 0.008 0.01  0.053 0.043 0.073 0.008 0.015 0.006 0.011 0.014
 0.065 0.008 0.005 0.006 0.044 0.033 0.026 0.012 0.141 0.01  0.016 0.036
 0.08  0.084 0.064 0.025 0.006 0.014 0.005 0.036 0.008 0.016 0.005 0.073
 0.077 0.022 0.004 0.123 0.009 0.02  0.041 0.358 0.08  0.04  0.006 0.008
 0.015 0.033 0.037 0.014 0.02  0.006 0.194 0.055 0.034 0.018 0.12  0.006
 0.029 0.055 0.12  0.125 0.122 0.088 0.003 0.084 0.111 0.129 0.009 0.218
 0.009 0.179 0.135 0.109 0.075 0.492 0.007 0.052 0.049 0.145 0.47  0.339
 0.051 0.009 0.013 0.365 0.152 0.024 0.056 0.007 0.319 0.556 0.153 0.044
 0.168 0.009 0.007 0.032 0.048 0.102 0.035 0.015 0.005 0.02  0.013 0.012
 0.02  0.014 0.004 0.039 0.043 0.031 0.007 0.023]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8815624999999999, 0.6838384511364782)
Iterations 567
Achieves (13.598573656938171, 1e-05)-DP
