samples  5000  graph  20 80 ER mlp  minibatch size  100  noise  1.0  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7622777646747636
iteration 1 in outer loop, alpha = 1.7622777646747636, rho = 1.0, h = 1.7622777646747636
cuda
iteration 1 in inner loop,alpha 1.7622777646747636 rho 1.0 h 1.1769941079911774
iteration 2 in inner loop,alpha 1.7622777646747636 rho 10.0 h 0.546699080645368
iteration 3 in inner loop,alpha 1.7622777646747636 rho 100.0 h 0.18576399615164618
iteration 2 in outer loop, alpha = 20.338677379839382, rho = 100.0, h = 0.18576399615164618
cuda
iteration 1 in inner loop,alpha 20.338677379839382 rho 100.0 h 0.1095608426621908
iteration 2 in inner loop,alpha 20.338677379839382 rho 1000.0 h 0.04370983218316127
iteration 3 in outer loop, alpha = 64.04850956300065, rho = 1000.0, h = 0.04370983218316127
cuda
iteration 1 in inner loop,alpha 64.04850956300065 rho 1000.0 h 0.023592627798716848
iteration 2 in inner loop,alpha 64.04850956300065 rho 10000.0 h 0.008006307647868027
iteration 4 in outer loop, alpha = 144.11158604168094, rho = 10000.0, h = 0.008006307647868027
cuda
iteration 1 in inner loop,alpha 144.11158604168094 rho 10000.0 h 0.0041674095898649455
iteration 2 in inner loop,alpha 144.11158604168094 rho 100000.0 h 0.0013706579506482797
iteration 5 in outer loop, alpha = 281.1773811065089, rho = 100000.0, h = 0.0013706579506482797
cuda
iteration 1 in inner loop,alpha 281.1773811065089 rho 100000.0 h 0.0006270167601378773
iteration 6 in outer loop, alpha = 908.1941412443862, rho = 1000000.0, h = 0.0006270167601378773
Threshold 0.3
[[0.001 3.15  0.082 0.259 0.913 1.118 0.317 0.    1.95  1.129 0.124 1.124
  0.274 0.001 0.367 0.432 0.297 1.212 1.306 0.477]
 [0.    0.001 0.025 0.52  0.491 1.659 1.79  0.    0.571 0.477 0.093 0.677
  0.318 0.001 1.184 0.301 0.346 0.864 0.484 1.895]
 [0.002 0.009 0.001 0.072 0.447 0.552 1.519 0.    0.187 0.495 0.095 0.19
  1.304 0.003 1.497 0.317 0.608 0.004 1.197 0.328]
 [0.    0.    0.    0.02  0.    0.002 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.21  0.251 0.182 0.    0.001 0.733]
 [0.    0.    0.    0.664 0.004 0.33  0.221 0.    0.    0.436 0.002 0.001
  0.003 0.    0.215 0.348 1.225 0.    0.35  1.063]
 [0.    0.    0.    0.62  0.002 0.003 0.001 0.    0.    0.    0.    0.
  0.    0.    0.354 1.667 1.84  0.    0.299 0.304]
 [0.    0.    0.    0.172 0.001 0.228 0.002 0.    0.    0.009 0.    0.
  0.    0.    0.329 0.362 0.307 0.    0.167 0.199]
 [2.503 0.131 0.005 0.119 0.717 0.223 0.722 0.001 0.301 0.463 0.135 0.383
  1.957 0.689 0.221 0.187 0.175 1.132 0.497 0.45 ]
 [0.    0.    0.002 0.373 1.602 0.951 1.297 0.    0.001 0.829 0.444 0.297
  1.773 0.    1.217 1.25  0.809 0.    1.341 0.69 ]
 [0.    0.    0.    0.125 0.014 2.215 0.119 0.    0.    0.007 0.001 0.001
  0.    0.    1.18  1.447 1.744 0.    0.825 0.228]
 [0.    0.    0.    1.053 0.114 0.595 2.322 0.    0.    0.26  0.001 0.
  0.    0.    1.512 0.482 1.031 0.    0.274 1.303]
 [0.    0.    0.    0.172 0.442 0.262 1.668 0.    0.    0.261 0.129 0.001
  0.116 0.    0.162 0.831 0.283 0.    0.122 0.22 ]
 [0.    0.    0.    0.092 0.139 1.392 1.08  0.    0.    0.179 2.147 0.002
  0.001 0.    0.304 0.324 0.297 0.    0.487 1.084]
 [0.159 0.038 0.033 0.842 0.34  1.633 0.324 0.001 0.287 0.364 1.712 0.533
  2.062 0.001 0.437 0.453 1.02  0.355 0.735 1.958]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.256 0.003 0.    0.001 0.002]
 [0.    0.    0.    0.002 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.001 0.006 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.003 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.113 1.212 0.004 0.    0.001 0.002]
 [0.    0.    0.032 0.061 0.46  0.295 1.387 0.    3.188 0.549 0.096 0.728
  0.083 0.    0.343 0.19  0.191 0.001 0.422 0.245]
 [0.    0.    0.    0.814 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.334 1.065 0.397 0.    0.001 0.271]
 [0.    0.    0.    0.002 0.    0.    0.001 0.    0.    0.001 0.    0.
  0.    0.    0.115 1.757 0.281 0.    0.001 0.002]]
[[0.    3.15  0.    0.    0.913 1.118 0.317 0.    1.95  1.129 0.    1.124
  0.    0.    0.367 0.432 0.    1.212 1.306 0.477]
 [0.    0.    0.    0.52  0.491 1.659 1.79  0.    0.571 0.477 0.    0.677
  0.318 0.    1.184 0.301 0.346 0.864 0.484 1.895]
 [0.    0.    0.    0.    0.447 0.552 1.519 0.    0.    0.495 0.    0.
  1.304 0.    1.497 0.317 0.608 0.    1.197 0.328]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.733]
 [0.    0.    0.    0.664 0.    0.33  0.    0.    0.    0.436 0.    0.
  0.    0.    0.    0.348 1.225 0.    0.35  1.063]
 [0.    0.    0.    0.62  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.354 1.667 1.84  0.    0.    0.304]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.329 0.362 0.307 0.    0.    0.   ]
 [2.503 0.    0.    0.    0.717 0.    0.722 0.    0.301 0.463 0.    0.383
  1.957 0.689 0.    0.    0.    1.132 0.497 0.45 ]
 [0.    0.    0.    0.373 1.602 0.951 1.297 0.    0.    0.829 0.444 0.
  1.773 0.    1.217 1.25  0.809 0.    1.341 0.69 ]
 [0.    0.    0.    0.    0.    2.215 0.    0.    0.    0.    0.    0.
  0.    0.    1.18  1.447 1.744 0.    0.825 0.   ]
 [0.    0.    0.    1.053 0.    0.595 2.322 0.    0.    0.    0.    0.
  0.    0.    1.512 0.482 1.031 0.    0.    1.303]
 [0.    0.    0.    0.    0.442 0.    1.668 0.    0.    0.    0.    0.
  0.    0.    0.    0.831 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.392 1.08  0.    0.    0.    2.147 0.
  0.    0.    0.304 0.324 0.    0.    0.487 1.084]
 [0.    0.    0.    0.842 0.34  1.633 0.324 0.    0.    0.364 1.712 0.533
  2.062 0.    0.437 0.453 1.02  0.355 0.735 1.958]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.113 1.212 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.46  0.    1.387 0.    3.188 0.549 0.    0.728
  0.    0.    0.343 0.    0.    0.    0.422 0.   ]
 [0.    0.    0.    0.814 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.334 1.065 0.397 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.757 0.    0.    0.    0.   ]]
{'fdr': 0.44, 'tpr': 0.875, 'fpr': 0.5, 'f1': 0.6829268292682927, 'shd': 60, 'npred': 125, 'ntrue': 80}
[3.150e+00 8.175e-02 2.593e-01 9.129e-01 1.118e+00 3.169e-01 2.648e-04
 1.950e+00 1.129e+00 1.239e-01 1.124e+00 2.740e-01 1.329e-03 3.671e-01
 4.319e-01 2.968e-01 1.212e+00 1.306e+00 4.767e-01 4.500e-04 2.533e-02
 5.196e-01 4.908e-01 1.659e+00 1.790e+00 1.809e-04 5.708e-01 4.772e-01
 9.311e-02 6.774e-01 3.179e-01 1.051e-03 1.184e+00 3.006e-01 3.461e-01
 8.637e-01 4.835e-01 1.895e+00 2.069e-03 8.740e-03 7.212e-02 4.468e-01
 5.524e-01 1.519e+00 2.668e-04 1.874e-01 4.955e-01 9.518e-02 1.897e-01
 1.304e+00 2.793e-03 1.497e+00 3.175e-01 6.077e-01 3.760e-03 1.197e+00
 3.284e-01 5.283e-05 4.236e-05 1.355e-04 4.639e-04 1.570e-03 1.212e-03
 3.362e-06 5.565e-05 1.402e-03 3.190e-04 1.970e-04 1.003e-04 5.027e-06
 2.101e-01 2.506e-01 1.822e-01 1.570e-05 1.050e-03 7.327e-01 1.864e-05
 1.258e-05 1.716e-05 6.636e-01 3.296e-01 2.214e-01 1.878e-05 1.092e-04
 4.355e-01 1.709e-03 1.183e-03 2.650e-03 5.865e-05 2.147e-01 3.475e-01
 1.225e+00 1.050e-05 3.498e-01 1.063e+00 8.564e-05 1.962e-04 5.828e-05
 6.195e-01 1.651e-03 1.442e-03 5.131e-05 1.099e-04 4.697e-04 1.083e-04
 3.174e-04 2.623e-04 3.772e-05 3.541e-01 1.667e+00 1.840e+00 1.227e-05
 2.986e-01 3.037e-01 5.929e-05 7.318e-05 3.915e-05 1.725e-01 7.949e-04
 2.277e-01 3.336e-05 7.230e-06 8.820e-03 1.364e-04 2.170e-04 5.343e-05
 3.925e-06 3.292e-01 3.616e-01 3.071e-01 1.858e-06 1.670e-01 1.988e-01
 2.503e+00 1.311e-01 4.590e-03 1.195e-01 7.165e-01 2.226e-01 7.219e-01
 3.013e-01 4.629e-01 1.354e-01 3.828e-01 1.957e+00 6.894e-01 2.211e-01
 1.870e-01 1.754e-01 1.132e+00 4.967e-01 4.499e-01 1.088e-05 1.725e-05
 1.869e-03 3.726e-01 1.602e+00 9.510e-01 1.297e+00 1.107e-05 8.285e-01
 4.440e-01 2.970e-01 1.773e+00 5.134e-05 1.217e+00 1.250e+00 8.093e-01
 5.601e-05 1.341e+00 6.904e-01 4.222e-05 5.467e-05 8.299e-05 1.246e-01
 1.408e-02 2.215e+00 1.192e-01 5.786e-05 1.651e-04 6.515e-04 1.116e-03
 3.383e-04 1.461e-04 1.180e+00 1.447e+00 1.744e+00 3.339e-05 8.249e-01
 2.283e-01 7.464e-06 1.074e-05 7.013e-05 1.053e+00 1.143e-01 5.953e-01
 2.322e+00 2.306e-05 7.498e-05 2.596e-01 4.672e-04 2.217e-04 4.327e-05
 1.512e+00 4.824e-01 1.031e+00 1.436e-05 2.736e-01 1.303e+00 2.999e-05
 2.583e-05 3.268e-04 1.724e-01 4.421e-01 2.617e-01 1.668e+00 1.702e-05
 2.325e-04 2.609e-01 1.285e-01 1.157e-01 3.813e-05 1.619e-01 8.312e-01
 2.825e-01 1.761e-04 1.220e-01 2.201e-01 5.538e-06 2.968e-05 1.893e-04
 9.247e-02 1.388e-01 1.392e+00 1.080e+00 3.244e-05 2.644e-04 1.788e-01
 2.147e+00 1.765e-03 7.633e-05 3.036e-01 3.240e-01 2.974e-01 2.429e-05
 4.874e-01 1.084e+00 1.593e-01 3.773e-02 3.327e-02 8.422e-01 3.396e-01
 1.633e+00 3.238e-01 8.227e-04 2.874e-01 3.636e-01 1.712e+00 5.332e-01
 2.062e+00 4.368e-01 4.527e-01 1.020e+00 3.550e-01 7.349e-01 1.958e+00
 4.524e-05 1.083e-04 3.976e-05 3.701e-03 1.125e-03 7.273e-04 3.992e-04
 3.093e-05 3.065e-05 4.708e-05 8.984e-05 1.785e-04 4.179e-05 1.029e-05
 2.558e-01 2.685e-03 2.096e-06 6.412e-04 2.038e-03 3.045e-05 3.376e-05
 3.331e-05 2.186e-03 5.101e-04 1.074e-04 3.282e-04 2.698e-05 2.206e-05
 2.714e-04 9.154e-05 6.382e-05 2.018e-05 1.339e-05 1.184e-03 1.149e-03
 5.501e-06 5.756e-04 2.431e-04 5.178e-05 8.164e-05 2.881e-05 2.964e-03
 8.720e-04 5.812e-04 8.399e-04 4.319e-05 3.459e-05 6.182e-05 7.347e-05
 9.411e-05 2.743e-05 3.907e-05 1.113e+00 1.212e+00 5.403e-06 6.112e-04
 1.567e-03 9.259e-05 2.108e-05 3.226e-02 6.072e-02 4.602e-01 2.951e-01
 1.387e+00 8.785e-05 3.188e+00 5.495e-01 9.552e-02 7.277e-01 8.335e-02
 1.590e-04 3.432e-01 1.905e-01 1.912e-01 4.220e-01 2.455e-01 6.772e-05
 1.335e-05 1.553e-04 8.140e-01 1.003e-03 7.774e-04 9.843e-04 4.810e-05
 1.584e-04 5.423e-04 1.776e-04 2.941e-04 1.425e-04 2.177e-05 3.343e-01
 1.065e+00 3.974e-01 1.798e-05 2.709e-01 3.877e-05 7.696e-05 6.979e-05
 2.400e-03 2.551e-04 2.862e-04 6.152e-04 2.002e-05 8.326e-05 1.022e-03
 3.391e-04 1.496e-04 2.264e-04 6.784e-05 1.151e-01 1.757e+00 2.807e-01
 1.556e-05 5.262e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.93625, 0.8889366372674152)
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
total norm for a microbatch 64.6332517773605 clip 9.735638618462225
total norm for a microbatch 61.464755030041836 clip 24.671505942188162
total norm for a microbatch 50.72753305725794 clip 26.799095292333973
total norm for a microbatch 48.97343456679499 clip 55.9695282793698
total norm for a microbatch 72.71344548268704 clip 52.677985547349486
cuda
Objective function 135.04 = squared loss an data 130.85 + 0.5*rho*h**2 3.374741 + alpha*h 0.000000 + L2reg 0.53 + L1reg 0.29 ; SHD = 136 ; DAG False
Proportion of microbatches that were clipped  0.9334818586887333
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.597976497090656
iteration 1 in outer loop, alpha = 2.597976497090656, rho = 1.0, h = 2.597976497090656
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 141.79 = squared loss an data 130.85 + 0.5*rho*h**2 3.374741 + alpha*h 6.749482 + L2reg 0.53 + L1reg 0.29 ; SHD = 136 ; DAG False
total norm for a microbatch 63.077391597308484 clip 5.796128526196119
total norm for a microbatch 140.86222969097395 clip 7.00071832024005
total norm for a microbatch 86.24630002413747 clip 38.02246464080932
total norm for a microbatch 58.150272849619 clip 54.1908246767461
total norm for a microbatch 62.92111711953885 clip 57.069664737984034
cuda
Objective function 55.14 = squared loss an data 47.16 + 0.5*rho*h**2 1.775520 + alpha*h 4.895680 + L2reg 1.05 + L1reg 0.27 ; SHD = 107 ; DAG False
Proportion of microbatches that were clipped  0.9427505676289328
iteration 1 in inner loop, alpha 2.597976497090656 rho 1.0 h 1.8844203211525112
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 71.12 = squared loss an data 47.16 + 0.5*rho*h**2 17.755200 + alpha*h 4.895680 + L2reg 1.05 + L1reg 0.27 ; SHD = 107 ; DAG False
total norm for a microbatch 109.00284233947835 clip 3.856639643488005
total norm for a microbatch 80.57736874795975 clip 20.829888840481726
total norm for a microbatch 117.189695592984 clip 39.41742534075676
total norm for a microbatch 112.11013798724181 clip 81.75924037156902
total norm for a microbatch 93.94439358112406 clip 98.0044962204344
cuda
Objective function 44.57 = squared loss an data 35.09 + 0.5*rho*h**2 5.251902 + alpha*h 2.662616 + L2reg 1.33 + L1reg 0.24 ; SHD = 90 ; DAG False
Proportion of microbatches that were clipped  0.9583791424532747
iteration 2 in inner loop, alpha 2.597976497090656 rho 10.0 h 1.024880641168199
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 91.84 = squared loss an data 35.09 + 0.5*rho*h**2 52.519016 + alpha*h 2.662616 + L2reg 1.33 + L1reg 0.24 ; SHD = 90 ; DAG False
total norm for a microbatch 318.5948780974032 clip 1.7191142432582052
total norm for a microbatch 187.56639194144208 clip 23.116110703380883
total norm for a microbatch 170.97741692837982 clip 32.907788644336755
total norm for a microbatch 77.42870118010556 clip 116.47480838418387
cuda
Objective function 47.83 = squared loss an data 37.57 + 0.5*rho*h**2 7.566021 + alpha*h 1.010611 + L2reg 1.49 + L1reg 0.20 ; SHD = 74 ; DAG True
Proportion of microbatches that were clipped  0.9689111076394313
iteration 3 in inner loop, alpha 2.597976497090656 rho 100.0 h 0.38899924299512634
iteration 2 in outer loop, alpha = 41.49790079660329, rho = 100.0, h = 0.38899924299512634
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 62.97 = squared loss an data 37.57 + 0.5*rho*h**2 7.566021 + alpha*h 16.142652 + L2reg 1.49 + L1reg 0.20 ; SHD = 74 ; DAG True
total norm for a microbatch 127.36301979848452 clip 1.7234391714497566
total norm for a microbatch 203.47574404128136 clip 1.7234391714497566
total norm for a microbatch 111.69134715613609 clip 2.4961687087424425
total norm for a microbatch 200.32141066417853 clip 6.133355975213887
total norm for a microbatch 138.92342672453753 clip 7.901057120076323
total norm for a microbatch 152.19947602963487 clip 13.659422633934094
total norm for a microbatch 193.42883550638976 clip 16.47420604951095
total norm for a microbatch 196.89385003915092 clip 91.14236758354487
total norm for a microbatch 167.57295807354464 clip 133.21339108569302
total norm for a microbatch 117.73491203154582 clip 134.79246983265455
cuda
Objective function 53.67 = squared loss an data 36.97 + 0.5*rho*h**2 3.653062 + alpha*h 11.216818 + L2reg 1.64 + L1reg 0.19 ; SHD = 76 ; DAG True
Proportion of microbatches that were clipped  0.9691529296556226
iteration 1 in inner loop, alpha 41.49790079660329 rho 100.0 h 0.2702984343227435
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 86.54 = squared loss an data 36.97 + 0.5*rho*h**2 36.530622 + alpha*h 11.216818 + L2reg 1.64 + L1reg 0.19 ; SHD = 76 ; DAG True
total norm for a microbatch 108.46960321950868 clip 2.08949831268089
total norm for a microbatch 162.05975355722626 clip 6.143904674117205
total norm for a microbatch 407.59911136442946 clip 13.769505993346549
cuda
Objective function 52.14 = squared loss an data 37.83 + 0.5*rho*h**2 7.360903 + alpha*h 5.035084 + L2reg 1.73 + L1reg 0.17 ; SHD = 69 ; DAG True
Proportion of microbatches that were clipped  0.9728997289972899
iteration 2 in inner loop, alpha 41.49790079660329 rho 1000.0 h 0.12133345299447384
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 118.39 = squared loss an data 37.83 + 0.5*rho*h**2 73.609034 + alpha*h 5.035084 + L2reg 1.73 + L1reg 0.17 ; SHD = 69 ; DAG True
total norm for a microbatch 143.1807811066158 clip 5.06717997469573
total norm for a microbatch 258.93902973553276 clip 47.46975445010847
total norm for a microbatch 193.78082463911744 clip 52.02686497924699
total norm for a microbatch 190.45167984610157 clip 108.16866203959832
total norm for a microbatch 284.25149444082217 clip 149.16641304814848
cuda
Objective function 53.95 = squared loss an data 40.16 + 0.5*rho*h**2 9.954789 + alpha*h 1.851643 + L2reg 1.82 + L1reg 0.16 ; SHD = 75 ; DAG True
Proportion of microbatches that were clipped  0.9811077022444695
iteration 3 in inner loop, alpha 41.49790079660329 rho 10000.0 h 0.04462014958401994
iteration 3 in outer loop, alpha = 487.6993966368027, rho = 10000.0, h = 0.04462014958401994
cuda
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 73.86 = squared loss an data 40.16 + 0.5*rho*h**2 9.954789 + alpha*h 21.761220 + L2reg 1.82 + L1reg 0.16 ; SHD = 75 ; DAG True
total norm for a microbatch 244.72912753151795 clip 1.0
total norm for a microbatch 285.89239119850964 clip 1.401524849263198
total norm for a microbatch 216.9200235452371 clip 24.48492849976962
total norm for a microbatch 223.28761759617873 clip 31.635876672410046
total norm for a microbatch 183.69971258031904 clip 60.189203515601456
cuda
Objective function 60.42 = squared loss an data 40.42 + 0.5*rho*h**2 4.045674 + alpha*h 13.872752 + L2reg 1.92 + L1reg 0.16 ; SHD = 69 ; DAG True
Proportion of microbatches that were clipped  0.9871039643368891
iteration 1 in inner loop, alpha 487.6993966368027 rho 10000.0 h 0.028445293356547552
noise_multiplier  1.0  noise_multiplier_b  5.0  noise_multiplier_delta  1.005037815259212
cuda
Objective function 96.83 = squared loss an data 40.42 + 0.5*rho*h**2 40.456736 + alpha*h 13.872752 + L2reg 1.92 + L1reg 0.16 ; SHD = 69 ; DAG True
total norm for a microbatch 576.0035896696246 clip 1.0
total norm for a microbatch 313.33951810018004 clip 1.7444983489094892
total norm for a microbatch 169.57383789742636 clip 10.755457944377467
total norm for a microbatch 240.89506016614172 clip 20.31047702811882
total norm for a microbatch 265.07210016649947 clip 54.65658902491213
cuda
Objective function 61.56 = squared loss an data 40.47 + 0.5*rho*h**2 11.523324 + alpha*h 7.403824 + L2reg 2.00 + L1reg 0.16 ; SHD = 73 ; DAG True
Proportion of microbatches that were clipped  0.9959395809647555
iteration 2 in inner loop, alpha 487.6993966368027 rho 100000.0 h 0.015181122184017681
iteration 4 in outer loop, alpha = 15668.821580654483, rho = 1000000.0, h = 0.015181122184017681
Threshold 0.3
[[0.008 2.14  0.595 0.722 0.543 0.689 0.754 0.243 0.307 0.665 0.465 0.389
  0.716 0.304 0.32  0.485 0.461 0.581 0.715 0.444]
 [0.004 0.011 0.155 0.394 0.275 0.452 0.666 0.094 0.107 0.316 0.137 0.168
  0.145 0.103 0.127 0.285 0.202 0.107 0.22  0.186]
 [0.011 0.043 0.01  0.111 0.204 0.034 0.215 0.056 0.036 0.171 0.048 0.234
  0.045 0.047 0.071 0.289 0.098 0.238 0.092 0.049]
 [0.005 0.022 0.056 0.01  0.03  0.033 0.071 0.048 0.02  0.025 0.018 0.107
  0.087 0.012 0.085 0.149 0.043 0.05  0.082 0.019]
 [0.01  0.025 0.045 0.129 0.007 0.016 0.031 0.01  0.013 0.038 0.025 0.043
  0.051 0.017 0.046 0.073 0.024 0.012 0.023 0.014]
 [0.007 0.019 0.255 0.201 0.398 0.005 0.193 0.141 0.014 0.236 0.16  0.19
  0.089 0.037 0.377 0.735 1.047 0.245 0.389 0.271]
 [0.007 0.013 0.046 0.103 0.161 0.044 0.01  0.074 0.021 0.13  0.012 0.167
  0.04  0.034 0.082 0.131 0.154 0.061 0.084 0.025]
 [0.036 0.067 0.119 0.117 0.504 0.053 0.111 0.007 0.04  0.181 0.151 0.222
  0.08  0.038 0.109 0.224 0.134 0.09  0.066 0.08 ]
 [0.024 0.056 0.233 0.363 0.585 0.452 0.308 0.182 0.008 0.317 0.22  0.481
  0.381 0.1   0.458 0.624 0.612 0.579 0.583 0.265]
 [0.006 0.021 0.074 0.334 0.272 0.053 0.052 0.035 0.021 0.007 0.036 0.14
  0.058 0.02  0.084 0.333 0.118 0.066 0.076 0.059]
 [0.018 0.03  0.205 0.446 0.24  0.04  0.662 0.039 0.051 0.152 0.008 0.25
  0.375 0.023 0.099 0.23  0.137 0.181 0.133 0.058]
 [0.016 0.043 0.047 0.08  0.221 0.033 0.067 0.032 0.022 0.056 0.029 0.01
  0.064 0.035 0.069 0.183 0.055 0.06  0.052 0.027]
 [0.006 0.051 0.157 0.115 0.154 0.066 0.131 0.08  0.02  0.171 0.029 0.157
  0.008 0.026 0.246 0.266 0.118 0.084 0.126 0.02 ]
 [0.026 0.054 0.197 0.523 0.345 0.244 0.266 0.248 0.116 0.451 0.237 0.245
  0.363 0.009 0.484 0.372 0.442 0.259 0.244 0.407]
 [0.012 0.033 0.106 0.134 0.219 0.015 0.086 0.072 0.016 0.091 0.078 0.159
  0.027 0.018 0.01  0.291 0.014 0.076 0.062 0.029]
 [0.007 0.026 0.034 0.068 0.131 0.008 0.048 0.04  0.009 0.021 0.027 0.044
  0.018 0.013 0.03  0.006 0.014 0.032 0.007 0.009]
 [0.009 0.031 0.068 0.149 0.186 0.007 0.061 0.027 0.016 0.065 0.034 0.206
  0.051 0.015 0.576 0.671 0.01  0.069 0.082 0.029]
 [0.009 0.049 0.04  0.192 0.532 0.026 0.149 0.084 0.017 0.149 0.034 0.188
  0.139 0.033 0.121 0.305 0.115 0.003 0.058 0.025]
 [0.011 0.053 0.088 0.11  0.276 0.018 0.087 0.087 0.012 0.109 0.057 0.104
  0.077 0.043 0.074 0.994 0.113 0.121 0.008 0.067]
 [0.016 0.036 0.161 0.506 0.473 0.04  0.208 0.122 0.034 0.25  0.171 0.37
  0.217 0.021 0.241 1.081 0.176 0.265 0.135 0.011]]
[[0.    2.14  0.595 0.722 0.543 0.689 0.754 0.    0.307 0.665 0.465 0.389
  0.716 0.304 0.32  0.485 0.461 0.581 0.715 0.444]
 [0.    0.    0.    0.394 0.    0.452 0.666 0.    0.    0.316 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.398 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.377 0.735 1.047 0.    0.389 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.504 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.363 0.585 0.452 0.308 0.    0.    0.317 0.    0.481
  0.381 0.    0.458 0.624 0.612 0.579 0.583 0.   ]
 [0.    0.    0.    0.334 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.333 0.    0.    0.    0.   ]
 [0.    0.    0.    0.446 0.    0.    0.662 0.    0.    0.    0.    0.
  0.375 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.523 0.345 0.    0.    0.    0.    0.451 0.    0.
  0.363 0.    0.484 0.372 0.442 0.    0.    0.407]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.576 0.671 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.532 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.305 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.994 0.    0.    0.    0.   ]
 [0.    0.    0.    0.506 0.473 0.    0.    0.    0.    0.    0.    0.37
  0.    0.    0.    1.081 0.    0.    0.    0.   ]]
{'fdr': 0.46774193548387094, 'tpr': 0.4125, 'fpr': 0.2636363636363636, 'f1': 0.4647887323943662, 'shd': 73, 'npred': 62, 'ntrue': 80}
[2.14  0.595 0.722 0.543 0.689 0.754 0.243 0.307 0.665 0.465 0.389 0.716
 0.304 0.32  0.485 0.461 0.581 0.715 0.444 0.004 0.155 0.394 0.275 0.452
 0.666 0.094 0.107 0.316 0.137 0.168 0.145 0.103 0.127 0.285 0.202 0.107
 0.22  0.186 0.011 0.043 0.111 0.204 0.034 0.215 0.056 0.036 0.171 0.048
 0.234 0.045 0.047 0.071 0.289 0.098 0.238 0.092 0.049 0.005 0.022 0.056
 0.03  0.033 0.071 0.048 0.02  0.025 0.018 0.107 0.087 0.012 0.085 0.149
 0.043 0.05  0.082 0.019 0.01  0.025 0.045 0.129 0.016 0.031 0.01  0.013
 0.038 0.025 0.043 0.051 0.017 0.046 0.073 0.024 0.012 0.023 0.014 0.007
 0.019 0.255 0.201 0.398 0.193 0.141 0.014 0.236 0.16  0.19  0.089 0.037
 0.377 0.735 1.047 0.245 0.389 0.271 0.007 0.013 0.046 0.103 0.161 0.044
 0.074 0.021 0.13  0.012 0.167 0.04  0.034 0.082 0.131 0.154 0.061 0.084
 0.025 0.036 0.067 0.119 0.117 0.504 0.053 0.111 0.04  0.181 0.151 0.222
 0.08  0.038 0.109 0.224 0.134 0.09  0.066 0.08  0.024 0.056 0.233 0.363
 0.585 0.452 0.308 0.182 0.317 0.22  0.481 0.381 0.1   0.458 0.624 0.612
 0.579 0.583 0.265 0.006 0.021 0.074 0.334 0.272 0.053 0.052 0.035 0.021
 0.036 0.14  0.058 0.02  0.084 0.333 0.118 0.066 0.076 0.059 0.018 0.03
 0.205 0.446 0.24  0.04  0.662 0.039 0.051 0.152 0.25  0.375 0.023 0.099
 0.23  0.137 0.181 0.133 0.058 0.016 0.043 0.047 0.08  0.221 0.033 0.067
 0.032 0.022 0.056 0.029 0.064 0.035 0.069 0.183 0.055 0.06  0.052 0.027
 0.006 0.051 0.157 0.115 0.154 0.066 0.131 0.08  0.02  0.171 0.029 0.157
 0.026 0.246 0.266 0.118 0.084 0.126 0.02  0.026 0.054 0.197 0.523 0.345
 0.244 0.266 0.248 0.116 0.451 0.237 0.245 0.363 0.484 0.372 0.442 0.259
 0.244 0.407 0.012 0.033 0.106 0.134 0.219 0.015 0.086 0.072 0.016 0.091
 0.078 0.159 0.027 0.018 0.291 0.014 0.076 0.062 0.029 0.007 0.026 0.034
 0.068 0.131 0.008 0.048 0.04  0.009 0.021 0.027 0.044 0.018 0.013 0.03
 0.014 0.032 0.007 0.009 0.009 0.031 0.068 0.149 0.186 0.007 0.061 0.027
 0.016 0.065 0.034 0.206 0.051 0.015 0.576 0.671 0.069 0.082 0.029 0.009
 0.049 0.04  0.192 0.532 0.026 0.149 0.084 0.017 0.149 0.034 0.188 0.139
 0.033 0.121 0.305 0.115 0.058 0.025 0.011 0.053 0.088 0.11  0.276 0.018
 0.087 0.087 0.012 0.109 0.057 0.104 0.077 0.043 0.074 0.994 0.113 0.121
 0.067 0.016 0.036 0.161 0.506 0.473 0.04  0.208 0.122 0.034 0.25  0.171
 0.37  0.217 0.021 0.241 1.081 0.176 0.265 0.135]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.73, 0.49798870329537237)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
