samples  5000  graph  12 24 ER mim  minibatch size  50  noise  0.5  minibatches per NN training  250 adaclip
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3547868753004586
iteration 1 in outer loop, alpha = 1.3547868753004586, rho = 1.0, h = 1.3547868753004586
cuda
iteration 1 in inner loop,alpha 1.3547868753004586 rho 1.0 h 0.835132432012383
iteration 2 in inner loop,alpha 1.3547868753004586 rho 10.0 h 0.3386757599307266
iteration 2 in outer loop, alpha = 4.741544474607725, rho = 10.0, h = 0.3386757599307266
cuda
iteration 1 in inner loop,alpha 4.741544474607725 rho 10.0 h 0.20318510575572546
iteration 2 in inner loop,alpha 4.741544474607725 rho 100.0 h 0.07300549426944869
iteration 3 in outer loop, alpha = 12.042093901552594, rho = 100.0, h = 0.07300549426944869
cuda
iteration 1 in inner loop,alpha 12.042093901552594 rho 100.0 h 0.04226042266080654
iteration 2 in inner loop,alpha 12.042093901552594 rho 1000.0 h 0.011904890679122815
iteration 4 in outer loop, alpha = 23.94698458067541, rho = 1000.0, h = 0.011904890679122815
cuda
iteration 1 in inner loop,alpha 23.94698458067541 rho 1000.0 h 0.006328819807894348
iteration 2 in inner loop,alpha 23.94698458067541 rho 10000.0 h 0.0019465761461248121
iteration 5 in outer loop, alpha = 43.412746041923526, rho = 10000.0, h = 0.0019465761461248121
cuda
iteration 1 in inner loop,alpha 43.412746041923526 rho 10000.0 h 0.0008584486838003613
iteration 2 in inner loop,alpha 43.412746041923526 rho 100000.0 h 0.00029853351281516893
iteration 6 in outer loop, alpha = 73.26609732344042, rho = 100000.0, h = 0.00029853351281516893
cuda
iteration 1 in inner loop,alpha 73.26609732344042 rho 100000.0 h 0.00017049331371410403
iteration 7 in outer loop, alpha = 243.75941103754445, rho = 1000000.0, h = 0.00017049331371410403
Threshold 0.3
[[0.005 0.    0.506 0.004 0.77  0.    0.063 0.    0.    0.138 0.    0.004]
 [2.093 0.002 0.375 0.154 0.209 0.001 0.084 0.001 0.    0.764 0.278 1.303]
 [0.006 0.001 0.005 0.006 0.005 0.    0.048 0.    0.    0.564 0.002 0.005]
 [0.045 0.001 0.08  0.002 0.154 0.001 0.814 0.001 0.    0.91  0.    0.331]
 [0.003 0.001 0.436 0.005 0.004 0.001 0.817 0.    0.    0.625 0.002 0.002]
 [0.046 0.041 0.584 0.039 0.083 0.    0.058 0.007 0.004 0.016 0.01  0.086]
 [0.002 0.    0.002 0.001 0.001 0.    0.003 0.    0.    0.014 0.    0.001]
 [3.165 0.03  0.645 0.028 0.044 0.005 0.866 0.    0.005 0.013 0.025 0.082]
 [0.163 2.371 0.004 2.092 0.183 0.004 0.815 0.009 0.    0.98  3.454 0.041]
 [0.002 0.001 0.003 0.001 0.002 0.001 0.01  0.    0.    0.005 0.    0.002]
 [0.168 0.004 0.013 2.469 0.036 0.003 0.075 0.001 0.    0.116 0.003 0.909]
 [0.22  0.001 0.005 0.001 0.03  0.    1.069 0.001 0.    0.096 0.001 0.004]]
[[0.    0.    0.506 0.    0.77  0.    0.    0.    0.    0.    0.    0.   ]
 [2.093 0.    0.375 0.    0.    0.    0.    0.    0.    0.764 0.    1.303]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.564 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.814 0.    0.    0.91  0.    0.331]
 [0.    0.    0.436 0.    0.    0.    0.817 0.    0.    0.625 0.    0.   ]
 [0.    0.    0.584 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [3.165 0.    0.645 0.    0.    0.    0.866 0.    0.    0.    0.    0.   ]
 [0.    2.371 0.    2.092 0.    0.    0.815 0.    0.    0.98  3.454 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    2.469 0.    0.    0.    0.    0.    0.    0.    0.909]
 [0.    0.    0.    0.    0.    0.    1.069 0.    0.    0.    0.    0.   ]]
{'fdr': 0.12, 'tpr': 0.9166666666666666, 'fpr': 0.07142857142857142, 'f1': 0.8979591836734694, 'shd': 3, 'npred': 25, 'ntrue': 24}
[3.932e-04 5.062e-01 4.358e-03 7.703e-01 1.536e-04 6.348e-02 9.145e-07
 3.975e-06 1.378e-01 1.885e-04 3.517e-03 2.093e+00 3.754e-01 1.535e-01
 2.094e-01 6.908e-04 8.398e-02 7.452e-04 3.164e-05 7.643e-01 2.782e-01
 1.303e+00 5.573e-03 1.377e-03 6.070e-03 4.595e-03 1.065e-04 4.781e-02
 1.485e-05 2.004e-05 5.640e-01 1.684e-03 5.022e-03 4.463e-02 9.078e-04
 7.972e-02 1.542e-01 7.137e-04 8.138e-01 7.962e-04 9.991e-06 9.100e-01
 2.838e-04 3.310e-01 2.721e-03 6.437e-04 4.363e-01 5.160e-03 5.236e-04
 8.172e-01 4.028e-05 2.193e-05 6.250e-01 1.656e-03 2.454e-03 4.611e-02
 4.121e-02 5.844e-01 3.877e-02 8.316e-02 5.820e-02 6.857e-03 4.257e-03
 1.595e-02 1.019e-02 8.596e-02 2.031e-03 3.855e-04 2.289e-03 6.857e-04
 1.079e-03 4.409e-04 9.512e-06 4.785e-06 1.447e-02 2.639e-04 1.114e-03
 3.165e+00 3.034e-02 6.446e-01 2.771e-02 4.432e-02 4.656e-03 8.663e-01
 4.900e-03 1.321e-02 2.518e-02 8.230e-02 1.634e-01 2.371e+00 4.062e-03
 2.092e+00 1.828e-01 4.155e-03 8.149e-01 9.095e-03 9.801e-01 3.454e+00
 4.139e-02 1.693e-03 7.793e-04 3.414e-03 8.397e-04 2.177e-03 7.157e-04
 1.007e-02 1.666e-04 3.295e-06 2.965e-04 1.960e-03 1.678e-01 4.383e-03
 1.346e-02 2.469e+00 3.601e-02 2.547e-03 7.516e-02 5.474e-04 6.836e-06
 1.155e-01 9.090e-01 2.198e-01 6.574e-04 4.521e-03 5.996e-04 3.044e-02
 4.869e-04 1.069e+00 1.493e-03 5.901e-06 9.558e-02 1.232e-03]
[[0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9564043209876544, 0.9326354592199662)
cuda
1692
cuda
Objective function 178.80 = squared loss an data 15.69 + 0.5*rho*h**2 162.682537 + alpha*h 0.000000 + L2reg 0.22 + L1reg 0.21 ; SHD = 78 ; DAG False
||w||^2 19001588005.95301
exp ma of ||w||^2 1237339387814.2175
||w|| 137846.2477035665
exp ma of ||w|| 714743.2523337982
||w||^2 2.395604216589484
exp ma of ||w||^2 16.787054697500555
||w|| 1.5477739552626812
exp ma of ||w|| 2.8234041074562892
||w||^2 0.8154186845928948
exp ma of ||w||^2 4.1874150575572875
||w|| 0.9030053624386152
exp ma of ||w|| 1.686276492075604
||w||^2 0.07161276627049287
exp ma of ||w||^2 0.31340477496751123
||w|| 0.2676056170383814
exp ma of ||w|| 0.51547091160301
||w||^2 0.0335058028207187
exp ma of ||w||^2 0.041641915486011226
||w|| 0.18304590358901426
exp ma of ||w|| 0.1969633796927739
||w||^2 0.08038682881016013
exp ma of ||w||^2 0.04159879988366009
||w|| 0.28352571102134655
exp ma of ||w|| 0.19702929074392592
||w||^2 0.07311626301322954
exp ma of ||w||^2 0.03693184973365004
||w|| 0.2704001904829757
exp ma of ||w|| 0.1842037727922945
||w||^2 0.031106465393088624
exp ma of ||w||^2 0.04007836389549058
||w|| 0.17637025087323718
exp ma of ||w|| 0.19090689820227763
||w||^2 0.016223943044721526
exp ma of ||w||^2 0.03830832873349416
||w|| 0.12737324304861491
exp ma of ||w|| 0.18687385505918727
||w||^2 0.03135081104843676
exp ma of ||w||^2 0.03655372791708998
||w|| 0.17706160241124205
exp ma of ||w|| 0.18285159562838804
||w||^2 0.04261447201091006
exp ma of ||w||^2 0.041897941271506894
||w|| 0.20643272998948123
exp ma of ||w|| 0.19519229817591127
cuda
Objective function 9.29 = squared loss an data 7.66 + 0.5*rho*h**2 1.171428 + alpha*h 0.000000 + L2reg 0.34 + L1reg 0.11 ; SHD = 53 ; DAG False
Proportion of microbatches that were clipped  0.24291008701256847
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.5306392564866815
iteration 1 in outer loop, alpha = 1.5306392564866815, rho = 1.0, h = 1.5306392564866815
cuda
1692
cuda
Objective function 11.63 = squared loss an data 7.66 + 0.5*rho*h**2 1.171428 + alpha*h 2.342857 + L2reg 0.34 + L1reg 0.11 ; SHD = 53 ; DAG False
||w||^2 3702725.8296031454
exp ma of ||w||^2 3260121.5084009636
||w|| 1924.246821383147
exp ma of ||w|| 1627.1163739055864
||w||^2 505.7697724857329
exp ma of ||w||^2 4110.855069400872
||w|| 22.48932574546718
exp ma of ||w|| 57.58138995573305
||w||^2 80.61016238536412
exp ma of ||w||^2 37.826082724687105
||w|| 8.978316233312576
exp ma of ||w|| 5.525804330903684
||w||^2 0.13817174103492677
exp ma of ||w||^2 0.08385842030587695
||w|| 0.3717145962091437
exp ma of ||w|| 0.2755767995794687
||w||^2 0.02839547299615382
exp ma of ||w||^2 0.07017557040090444
||w|| 0.16850956351540947
exp ma of ||w|| 0.2500165777952528
||w||^2 0.11758287946393538
exp ma of ||w||^2 0.07957028292139012
||w|| 0.3429036008325596
exp ma of ||w|| 0.2651992432735482
||w||^2 0.07426555858374774
exp ma of ||w||^2 0.07746462246365539
||w|| 0.27251707943493697
exp ma of ||w|| 0.2651474197385391
||w||^2 0.045443094050834715
exp ma of ||w||^2 0.06972690987967672
||w|| 0.21317385874171982
exp ma of ||w|| 0.24853942048256994
||w||^2 0.08742712799431038
exp ma of ||w||^2 0.06718503210113923
||w|| 0.2956807873269929
exp ma of ||w|| 0.2433948049915217
||w||^2 0.010670025249032402
exp ma of ||w||^2 0.06853167861599378
||w|| 0.10329581428611907
exp ma of ||w|| 0.24556903280605422
cuda
Objective function 9.51 = squared loss an data 6.64 + 0.5*rho*h**2 0.523657 + alpha*h 1.566431 + L2reg 0.68 + L1reg 0.10 ; SHD = 36 ; DAG False
Proportion of microbatches that were clipped  0.40813338755591705
iteration 1 in inner loop, alpha 1.5306392564866815 rho 1.0 h 1.02338334177497
1692
cuda
Objective function 14.22 = squared loss an data 6.64 + 0.5*rho*h**2 5.236567 + alpha*h 1.566431 + L2reg 0.68 + L1reg 0.10 ; SHD = 36 ; DAG False
||w||^2 1157945134.080313
exp ma of ||w||^2 2813860110.0511203
||w|| 34028.59289010218
exp ma of ||w|| 46628.24908300277
||w||^2 819760410.1430169
exp ma of ||w||^2 2443596436.2608924
||w|| 28631.458400560336
exp ma of ||w|| 43714.91261795717
||w||^2 37103129.26899133
exp ma of ||w||^2 36867270.91897461
||w|| 6091.2338051491115
exp ma of ||w|| 5344.039235448162
||w||^2 357624.3361173897
exp ma of ||w||^2 712921.1691447018
||w|| 598.0170032009038
exp ma of ||w|| 746.0878316454963
||w||^2 3904.4820363159342
exp ma of ||w||^2 27019.048357874086
||w|| 62.485854689809074
exp ma of ||w|| 144.41764841321654
||w||^2 9.684864598625012
exp ma of ||w||^2 10.669591524856548
||w|| 3.112051509635567
exp ma of ||w|| 2.899873350917699
||w||^2 1.2359585106646833
exp ma of ||w||^2 4.87003184003026
||w|| 1.1117367092368065
exp ma of ||w|| 1.9663843408299368
||w||^2 0.037547648446021376
exp ma of ||w||^2 0.0979473573234305
||w|| 0.19377215601324504
exp ma of ||w|| 0.29766827131044793
||w||^2 0.048714725220122795
exp ma of ||w||^2 0.09526933880241671
||w|| 0.22071412555639205
exp ma of ||w|| 0.2908194767080269
cuda
Objective function 10.11 = squared loss an data 7.69 + 0.5*rho*h**2 0.864659 + alpha*h 0.636517 + L2reg 0.83 + L1reg 0.09 ; SHD = 30 ; DAG False
Proportion of microbatches that were clipped  0.3863398955403777
iteration 2 in inner loop, alpha 1.5306392564866815 rho 10.0 h 0.41585064401542127
1692
cuda
Objective function 17.89 = squared loss an data 7.69 + 0.5*rho*h**2 8.646588 + alpha*h 0.636517 + L2reg 0.83 + L1reg 0.09 ; SHD = 30 ; DAG False
||w||^2 5213059215.546052
exp ma of ||w||^2 4961201788.188084
||w|| 72201.51809723984
exp ma of ||w|| 55692.36576563182
||w||^2 50387481.90949091
exp ma of ||w||^2 106971958.34026921
||w|| 7098.414041847018
exp ma of ||w|| 9026.3984582432
||w||^2 28755.59593566258
exp ma of ||w||^2 77887.21160287084
||w|| 169.5747502892532
exp ma of ||w|| 247.63695206003334
||w||^2 7.179365461139093
exp ma of ||w||^2 35.64091198871437
||w|| 2.6794337948788907
exp ma of ||w|| 5.095974394110557
||w||^2 12.551565928404823
exp ma of ||w||^2 24.849776115628018
||w|| 3.542818924021495
exp ma of ||w|| 4.279308674344471
||w||^2 0.12899110399240749
exp ma of ||w||^2 0.3866166257226243
||w|| 0.35915331544120194
exp ma of ||w|| 0.57285074173034
||w||^2 0.09519166308728454
exp ma of ||w||^2 0.1335789765217799
||w|| 0.30853146207037707
exp ma of ||w|| 0.34196117850641333
||w||^2 0.09160869155608853
exp ma of ||w||^2 0.12577197280676394
||w|| 0.30266927752265926
exp ma of ||w|| 0.3364349603974298
||w||^2 0.06518927160894154
exp ma of ||w||^2 0.11596936067913832
||w|| 0.2553218980207956
exp ma of ||w|| 0.32180533460068744
||w||^2 0.07687920532832077
exp ma of ||w||^2 0.11674615724661964
||w|| 0.2772709961902268
exp ma of ||w|| 0.32138573926025504
||w||^2 0.07595382434130973
exp ma of ||w||^2 0.12625974200032988
||w|| 0.2755972139578151
exp ma of ||w|| 0.33539586287253503
||w||^2 0.04349013212161226
exp ma of ||w||^2 0.12433025545921132
||w|| 0.20854287837663568
exp ma of ||w|| 0.3283160695576199
||w||^2 0.028187474172488958
exp ma of ||w||^2 0.1354735888547675
||w|| 0.1678912569864463
exp ma of ||w|| 0.34273269574961407
||w||^2 0.3928196419067315
exp ma of ||w||^2 0.12667095294190128
||w|| 0.6267532544045795
exp ma of ||w|| 0.33314354475486274
cuda
Objective function 10.63 = squared loss an data 8.57 + 0.5*rho*h**2 0.913717 + alpha*h 0.206916 + L2reg 0.86 + L1reg 0.07 ; SHD = 25 ; DAG True
Proportion of microbatches that were clipped  0.3761148046051565
iteration 3 in inner loop, alpha 1.5306392564866815 rho 100.0 h 0.13518260500948642
iteration 2 in outer loop, alpha = 15.048899757435324, rho = 100.0, h = 0.13518260500948642
cuda
1692
cuda
Objective function 12.45 = squared loss an data 8.57 + 0.5*rho*h**2 0.913717 + alpha*h 2.034349 + L2reg 0.86 + L1reg 0.07 ; SHD = 25 ; DAG True
||w||^2 8205071.917143846
exp ma of ||w||^2 195479718.2099217
||w|| 2864.4496709043165
exp ma of ||w|| 12079.313583202358
||w||^2 4887900.609283969
exp ma of ||w||^2 13700271.972387936
||w|| 2210.8596991405784
exp ma of ||w|| 3201.074132844442
||w||^2 2091.93669905629
exp ma of ||w||^2 2679.420277439671
||w|| 45.737694509630565
exp ma of ||w|| 44.867314262746916
||w||^2 2130.7293356157197
exp ma of ||w||^2 1799.9734168437692
||w|| 46.159823825657305
exp ma of ||w|| 36.99950707426866
||w||^2 0.47558679587640945
exp ma of ||w||^2 1.9498140501848846
||w|| 0.6896280126824964
exp ma of ||w|| 1.2152937588152006
||w||^2 0.9670121307885905
exp ma of ||w||^2 1.8901956186522813
||w|| 0.9833677495162176
exp ma of ||w|| 1.1962240847733268
||w||^2 0.4448915358688433
exp ma of ||w||^2 1.462811121441659
||w|| 0.667001900948448
exp ma of ||w|| 1.056296023134118
||w||^2 0.1892774747288135
exp ma of ||w||^2 0.13200485967314063
||w|| 0.4350603115992236
exp ma of ||w|| 0.34714102697508065
||w||^2 0.25195607093119604
exp ma of ||w||^2 0.12157830791755402
||w|| 0.5019522596135971
exp ma of ||w|| 0.33006393400615724
||w||^2 0.037312125013918944
exp ma of ||w||^2 0.13288996390649988
||w|| 0.19316346707884216
exp ma of ||w|| 0.34388015973150377
||w||^2 0.169350698024507
exp ma of ||w||^2 0.14203383817229567
||w|| 0.4115224149721458
exp ma of ||w|| 0.3563330767423732
cuda
Objective function 11.68 = squared loss an data 8.97 + 0.5*rho*h**2 0.421886 + alpha*h 1.382348 + L2reg 0.84 + L1reg 0.07 ; SHD = 19 ; DAG True
Proportion of microbatches that were clipped  0.37045123726346435
iteration 1 in inner loop, alpha 15.048899757435324 rho 100.0 h 0.0918570505894678
1692
cuda
Objective function 15.48 = squared loss an data 8.97 + 0.5*rho*h**2 4.218859 + alpha*h 1.382348 + L2reg 0.84 + L1reg 0.07 ; SHD = 19 ; DAG True
||w||^2 1338432172.154244
exp ma of ||w||^2 3519447856.4797935
||w|| 36584.58927136184
exp ma of ||w|| 48388.097775853974
||w||^2 790030153.1635511
exp ma of ||w||^2 1595026731.873321
||w|| 28107.475040699603
exp ma of ||w|| 32804.408338893336
||w||^2 627047.2278778694
exp ma of ||w||^2 3809432.336384636
||w|| 791.8631370873817
exp ma of ||w|| 1666.3431557305566
||w||^2 19347.010234329744
exp ma of ||w||^2 187113.2288931754
||w|| 139.09353052651207
exp ma of ||w|| 368.84259426926104
||w||^2 39061.856565040405
exp ma of ||w||^2 127665.4235095957
||w|| 197.6407259778217
exp ma of ||w|| 303.5070838997691
||w||^2 12761.376180233237
exp ma of ||w||^2 89433.11989346049
||w|| 112.96626124747706
exp ma of ||w|| 254.1486514873696
||w||^2 0.4105210156510403
exp ma of ||w||^2 0.3802924745222817
||w|| 0.6407191394449212
exp ma of ||w|| 0.5722917485094566
||w||^2 0.13082368167209146
exp ma of ||w||^2 0.2273800366041078
||w|| 0.3616955649051996
exp ma of ||w|| 0.4521588630298826
||w||^2 0.1606791519186359
exp ma of ||w||^2 0.20907433693083752
||w|| 0.400848040931518
exp ma of ||w|| 0.4329846638265311
||w||^2 0.056737531233543526
exp ma of ||w||^2 0.12273526493606426
||w|| 0.23819641314164142
exp ma of ||w|| 0.3358297519087342
||w||^2 0.14038918056373192
exp ma of ||w||^2 0.11776921518150872
||w|| 0.37468544215612637
exp ma of ||w|| 0.33084648320093146
||w||^2 0.13811515337675306
exp ma of ||w||^2 0.12400041855877196
||w|| 0.3716384713357231
exp ma of ||w|| 0.33802507514819635
||w||^2 0.07810675319691654
exp ma of ||w||^2 0.1277676158102362
||w|| 0.27947585440770467
exp ma of ||w|| 0.33808492253939953
cuda
Objective function 11.39 = squared loss an data 9.42 + 0.5*rho*h**2 0.544585 + alpha*h 0.496652 + L2reg 0.86 + L1reg 0.07 ; SHD = 25 ; DAG True
Proportion of microbatches that were clipped  0.3239403079853945
iteration 2 in inner loop, alpha 15.048899757435324 rho 1000.0 h 0.03300257700103515
iteration 3 in outer loop, alpha = 48.05147675847047, rho = 1000.0, h = 0.03300257700103515
cuda
1692
cuda
Objective function 12.48 = squared loss an data 9.42 + 0.5*rho*h**2 0.544585 + alpha*h 1.585823 + L2reg 0.86 + L1reg 0.07 ; SHD = 25 ; DAG True
||w||^2 808.5409396280843
exp ma of ||w||^2 2972.4552953153743
||w|| 28.43485430995004
exp ma of ||w|| 44.00246004581845
||w||^2 9.959801124879345
exp ma of ||w||^2 33.05699904344338
||w|| 3.155915259457919
exp ma of ||w|| 4.706838172171859
||w||^2 0.15877728897698262
exp ma of ||w||^2 0.879137389553617
||w|| 0.3984686800452234
exp ma of ||w|| 0.8101317864903318
v before min max tensor([[  5.383, -29.176,  21.207,  ...,   0.784,   6.055,  45.863],
        [ 40.518, -21.134,   7.179,  ..., -17.142, -28.425, -19.279],
        [-15.038, -25.261, -22.357,  ..., -15.001, -12.976,   1.297],
        ...,
        [ 42.508, -13.907,   3.835,  ...,  20.473, 107.212, -28.849],
        [-29.000, -20.721, -22.244,  ..., -29.752, 125.031, -30.444],
        [ 25.045,  17.685,  -6.647,  ..., -19.643, -14.837,  -2.329]],
       device='cuda:0')
v tensor([[5.383e+00, 1.000e-12, 1.000e+01,  ..., 7.841e-01, 6.055e+00,
         1.000e+01],
        [1.000e+01, 1.000e-12, 7.179e+00,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.297e+00],
        ...,
        [1.000e+01, 1.000e-12, 3.835e+00,  ..., 1.000e+01, 1.000e+01,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12]], device='cuda:0')
v before min max tensor([ 2.431e+01, -5.707e+00, -1.164e+01, -2.713e+01, -2.963e+01, -1.171e+01,
         4.422e+01, -1.295e+01,  7.815e+00, -1.502e+01, -2.648e+01, -2.324e+01,
        -1.962e+01, -8.901e-01,  7.287e+00, -1.615e+01, -1.625e+01, -1.460e+01,
        -2.426e+01, -2.227e+01, -1.089e+01,  1.912e-01, -8.288e+00, -1.286e+01,
        -2.485e+01,  1.870e+01, -1.426e+01, -1.944e+01, -2.318e+01, -9.526e+00,
         4.005e+01, -2.280e+01,  2.200e+01, -1.902e+01,  8.464e+00, -1.526e+01,
        -5.798e+00,  1.911e+01,  1.484e+01, -2.766e+01,  1.673e+01,  1.230e+01,
        -6.439e+00, -2.336e+01,  2.243e+01,  1.102e+01, -1.835e+01, -1.955e+01,
        -1.519e+01, -3.009e+01, -2.089e+01, -2.352e+01, -2.613e+01,  2.554e+02,
        -2.373e+01, -1.004e+00,  4.423e+01,  1.821e+01, -2.327e+01,  2.117e+01,
        -1.665e+01,  2.011e+02, -2.302e+01, -5.608e+00, -8.938e+00, -2.201e+01,
         3.117e+01,  8.532e+00, -1.127e+01,  3.370e+00,  6.002e+01,  4.945e+01,
        -2.924e+01, -2.623e+01,  4.535e+00, -2.235e+01, -1.355e+01, -2.380e+01,
         1.868e+01, -2.748e+01, -1.046e+01, -2.451e+01, -1.932e+01, -3.209e+01,
        -1.836e+01,  6.046e+00, -2.109e+01,  2.136e+01, -2.424e+01, -6.895e-01,
         3.247e+01, -4.340e+00, -1.514e+01, -2.556e+01, -2.489e+01, -2.349e+01,
         8.660e+01,  3.914e+00, -7.856e+00,  3.731e+01, -1.425e+01,  1.031e+01,
         2.940e+01,  1.052e+01, -2.035e+01, -2.500e+01,  5.892e+01, -2.117e+00,
        -1.234e+01, -2.136e+01,  1.608e+01,  1.730e+00,  1.827e+01,  9.168e-01,
         4.476e+01, -2.646e+01, -2.034e+01, -2.541e+01, -1.829e+01, -2.174e+01],
       device='cuda:0')
v tensor([1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 7.815e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 7.287e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.912e-01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 8.464e+00, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 8.532e+00, 1.000e-12, 3.370e+00, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 4.535e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 6.046e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 3.914e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.730e+00, 1.000e+01, 9.168e-01,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[-2.463e+01],
         [ 3.676e+00],
         [-8.502e+00],
         [-1.937e+01],
         [ 2.871e+01],
         [-2.849e+01],
         [-1.840e+01],
         [ 3.142e+01],
         [-1.268e+01],
         [ 9.359e+00]],

        [[-4.632e-01],
         [-1.203e+01],
         [-2.450e+01],
         [ 5.467e+01],
         [ 7.295e-01],
         [ 3.046e-02],
         [-2.370e+01],
         [-8.200e+00],
         [-3.252e+01],
         [ 1.655e+00]],

        [[ 2.893e+00],
         [ 2.328e+00],
         [-8.537e+00],
         [-2.652e+01],
         [-1.851e+01],
         [-2.566e+01],
         [ 4.147e+00],
         [ 1.590e+02],
         [-2.182e+01],
         [ 1.273e+00]],

        [[-2.018e+01],
         [-4.627e-01],
         [ 8.068e+01],
         [ 1.982e+01],
         [-2.937e+01],
         [-4.468e-01],
         [-2.792e+01],
         [-2.905e+01],
         [-1.402e+01],
         [-2.869e+01]],

        [[-1.270e+01],
         [-1.040e+01],
         [ 2.272e+01],
         [-1.860e+01],
         [-1.065e+01],
         [-1.404e+01],
         [-2.488e+01],
         [ 4.309e+01],
         [-2.149e+01],
         [ 4.276e+01]],

        [[ 2.496e+01],
         [-3.062e+00],
         [-1.842e+01],
         [-2.244e+01],
         [ 8.935e-01],
         [-1.770e+01],
         [-2.878e+01],
         [-2.262e+01],
         [-9.857e+00],
         [-4.003e+00]],

        [[-4.914e+00],
         [ 1.069e+01],
         [-2.818e+01],
         [-2.805e+01],
         [ 4.181e+01],
         [-1.964e+01],
         [-1.046e+01],
         [ 1.780e+01],
         [-5.339e+00],
         [-2.392e+01]],

        [[ 2.362e+01],
         [-1.554e+01],
         [-5.761e+00],
         [-1.538e+01],
         [ 5.241e+00],
         [ 1.191e-01],
         [-2.535e+01],
         [-1.480e+01],
         [-1.698e+01],
         [ 2.331e+00]],

        [[ 4.341e+00],
         [-3.147e+01],
         [ 5.502e+00],
         [-3.398e+00],
         [-3.271e+00],
         [-2.005e+01],
         [-1.640e+01],
         [-2.837e-01],
         [-8.182e+00],
         [-2.244e+01]],

        [[ 8.263e+00],
         [ 2.768e+01],
         [ 1.634e+01],
         [-2.025e+01],
         [-2.003e+01],
         [-2.600e+01],
         [ 1.519e+01],
         [-2.417e+01],
         [-2.266e+01],
         [-2.316e+01]],

        [[-2.498e+01],
         [-7.952e+00],
         [-1.920e+01],
         [-1.949e+01],
         [ 4.327e+01],
         [-2.581e+01],
         [-1.992e+01],
         [-3.007e+01],
         [-1.952e+01],
         [ 1.225e+01]],

        [[-2.630e+01],
         [ 9.437e+00],
         [ 6.007e+01],
         [ 8.332e+01],
         [ 1.638e+01],
         [ 3.368e+00],
         [-2.487e+01],
         [ 6.434e+00],
         [ 3.957e+01],
         [ 1.042e+02]]], device='cuda:0')
v tensor([[[1.000e-12],
         [3.676e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [9.359e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [7.295e-01],
         [3.046e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.655e+00]],

        [[2.893e+00],
         [2.328e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.147e+00],
         [1.000e+01],
         [1.000e-12],
         [1.273e+00]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.935e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.241e+00],
         [1.191e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.331e+00]],

        [[4.341e+00],
         [1.000e-12],
         [5.502e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[8.263e+00],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [9.437e+00],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [3.368e+00],
         [1.000e-12],
         [6.434e+00],
         [1.000e+01],
         [1.000e+01]]], device='cuda:0')
v before min max tensor([[  0.789],
        [-10.811],
        [  1.830],
        [-21.140],
        [ -3.095],
        [-19.492],
        [ 18.800],
        [ 13.056],
        [ 24.337],
        [ 61.537],
        [ 35.408],
        [203.177]], device='cuda:0')
v tensor([[7.890e-01],
        [1.000e-12],
        [1.830e+00],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01],
        [1.000e+01]], device='cuda:0')
a after update for 1 param tensor([[ 0.012, -0.000,  0.019,  ..., -0.037, -0.004,  0.014],
        [ 0.024, -0.017,  0.054,  ...,  0.041, -0.071, -0.015],
        [ 0.011, -0.026, -0.019,  ..., -0.058,  0.009, -0.035],
        ...,
        [ 0.038,  0.037, -0.021,  ..., -0.006,  0.035,  0.031],
        [ 0.009, -0.018,  0.019,  ...,  0.018,  0.058, -0.007],
        [ 0.005, -0.038, -0.014,  ..., -0.003,  0.017,  0.016]],
       device='cuda:0')
s after update for 1 param tensor([[2.218, 1.967, 1.951,  ..., 1.542, 1.421, 2.140],
        [2.470, 1.488, 2.137,  ..., 1.647, 1.923, 1.729],
        [2.089, 1.642, 1.448,  ..., 1.630, 1.383, 1.299],
        ...,
        [1.973, 1.591, 1.770,  ..., 1.804, 1.656, 2.208],
        [1.860, 1.408, 1.544,  ..., 1.908, 2.091, 2.110],
        [2.013, 2.013, 1.781,  ..., 1.254, 1.202, 1.891]], device='cuda:0')
b after update for 1 param tensor([[81.229, 76.491, 76.190,  ..., 67.719, 65.026, 79.794],
        [85.716, 66.524, 79.734,  ..., 70.001, 75.637, 71.713],
        [78.835, 69.894, 65.635,  ..., 69.626, 64.137, 62.165],
        ...,
        [76.618, 68.806, 72.568,  ..., 73.249, 70.195, 81.053],
        [74.385, 64.715, 67.774,  ..., 75.342, 78.877, 79.221],
        [77.376, 77.389, 72.787,  ..., 61.073, 59.793, 75.004]],
       device='cuda:0')
a after update for 1 param tensor([-0.038,  0.046,  0.016, -0.036, -0.115,  0.026, -0.016, -0.005, -0.004,
         0.027,  0.009,  0.018,  0.034,  0.036,  0.024,  0.044,  0.004,  0.003,
         0.075,  0.016, -0.048, -0.007,  0.011,  0.013, -0.003,  0.014,  0.011,
        -0.012, -0.062,  0.052, -0.016,  0.055,  0.007,  0.011,  0.069,  0.027,
         0.021, -0.057, -0.015, -0.063,  0.024, -0.014, -0.011, -0.109,  0.016,
        -0.007, -0.021,  0.015, -0.018,  0.043,  0.013,  0.019,  0.025, -0.026,
        -0.025,  0.059,  0.013,  0.009,  0.008, -0.033, -0.028, -0.013, -0.049,
         0.012,  0.009,  0.062,  0.044, -0.021, -0.013, -0.018, -0.072, -0.014,
        -0.018, -0.006, -0.003,  0.067, -0.048, -0.007,  0.013, -0.048, -0.031,
        -0.009,  0.023,  0.004,  0.024, -0.020,  0.030, -0.019, -0.010, -0.058,
        -0.054, -0.006,  0.032, -0.014,  0.007, -0.015, -0.006, -0.010,  0.020,
         0.020, -0.004,  0.014,  0.019, -0.029,  0.011, -0.005,  0.016,  0.024,
         0.015, -0.055, -0.004,  0.005,  0.022, -0.006,  0.039, -0.028,  0.049,
        -0.020,  0.022, -0.027], device='cuda:0')
s after update for 1 param tensor([1.572, 1.205, 1.001, 2.070, 1.986, 1.751, 1.880, 1.799, 1.651, 1.139,
        1.830, 1.861, 1.457, 1.398, 1.680, 1.715, 1.824, 1.669, 1.648, 1.822,
        1.325, 1.925, 0.892, 1.243, 1.710, 1.799, 1.197, 1.436, 1.829, 1.804,
        2.060, 1.766, 2.031, 1.343, 1.477, 1.497, 1.760, 2.316, 2.143, 2.181,
        2.050, 1.799, 0.543, 1.511, 2.155, 2.017, 1.392, 1.801, 1.003, 1.979,
        1.353, 1.530, 1.668, 2.358, 1.863, 1.909, 1.629, 1.712, 1.547, 1.766,
        1.299, 2.010, 1.472, 1.608, 1.247, 1.556, 1.854, 1.458, 1.691, 1.453,
        2.260, 2.188, 1.976, 1.749, 1.001, 1.517, 1.831, 1.657, 1.733, 2.320,
        1.296, 1.743, 1.300, 2.143, 1.601, 1.869, 1.527, 1.905, 1.567, 1.274,
        1.755, 0.855, 1.057, 1.670, 1.606, 1.500, 1.902, 1.276, 1.852, 1.916,
        1.649, 1.490, 2.295, 1.923, 1.658, 1.875, 2.121, 1.207, 0.971, 1.632,
        1.773, 1.285, 1.665, 1.705, 1.950, 1.952, 1.501, 1.763, 1.346, 1.456],
       device='cuda:0')
b after update for 1 param tensor([68.377, 59.876, 54.581, 78.465, 76.857, 72.169, 74.788, 73.147, 70.082,
        58.206, 73.785, 74.410, 65.826, 64.489, 70.688, 71.433, 73.669, 70.471,
        70.014, 73.620, 62.786, 75.675, 51.507, 60.819, 71.314, 73.157, 59.683,
        65.350, 73.767, 73.250, 78.279, 72.477, 77.733, 63.206, 66.284, 66.728,
        72.360, 82.998, 79.843, 80.557, 78.099, 73.155, 40.209, 67.038, 80.064,
        77.458, 64.352, 73.205, 54.616, 76.731, 63.435, 67.463, 70.438, 83.761,
        74.439, 75.360, 69.621, 71.372, 67.833, 72.489, 62.153, 77.325, 66.168,
        69.169, 60.899, 68.043, 74.263, 65.867, 70.917, 65.738, 81.989, 80.675,
        76.675, 72.139, 54.559, 67.178, 73.802, 70.215, 71.809, 83.082, 62.093,
        72.010, 62.198, 79.837, 69.008, 74.566, 67.400, 75.276, 68.268, 61.568,
        72.264, 50.439, 56.084, 70.489, 69.119, 66.801, 75.228, 61.612, 74.217,
        75.501, 70.042, 66.589, 82.628, 75.628, 70.235, 74.681, 79.437, 59.914,
        53.740, 69.678, 72.629, 61.839, 70.377, 71.229, 76.163, 76.207, 66.816,
        72.424, 63.280, 65.821], device='cuda:0')
a after update for 1 param tensor([[[ 0.014],
         [-0.004],
         [ 0.016],
         [-0.003],
         [ 0.056],
         [ 0.046],
         [-0.016],
         [ 0.006],
         [-0.005],
         [ 0.077]],

        [[ 0.000],
         [-0.021],
         [ 0.091],
         [-0.006],
         [-0.010],
         [-0.024],
         [-0.020],
         [-0.009],
         [ 0.008],
         [-0.025]],

        [[ 0.012],
         [ 0.018],
         [ 0.017],
         [ 0.048],
         [-0.032],
         [-0.022],
         [ 0.056],
         [-0.020],
         [-0.019],
         [ 0.007]],

        [[ 0.040],
         [ 0.018],
         [ 0.006],
         [-0.053],
         [ 0.013],
         [-0.011],
         [ 0.027],
         [ 0.057],
         [-0.011],
         [ 0.000]],

        [[-0.003],
         [-0.032],
         [-0.016],
         [ 0.002],
         [ 0.001],
         [-0.024],
         [-0.071],
         [ 0.030],
         [ 0.036],
         [-0.038]],

        [[-0.057],
         [-0.057],
         [-0.040],
         [ 0.019],
         [ 0.030],
         [-0.027],
         [-0.043],
         [-0.006],
         [ 0.014],
         [-0.032]],

        [[-0.006],
         [ 0.037],
         [ 0.077],
         [ 0.004],
         [-0.038],
         [ 0.034],
         [ 0.032],
         [ 0.020],
         [-0.002],
         [-0.052]],

        [[ 0.029],
         [ 0.005],
         [ 0.015],
         [-0.027],
         [-0.053],
         [-0.053],
         [ 0.076],
         [-0.002],
         [ 0.035],
         [-0.020]],

        [[ 0.023],
         [-0.026],
         [-0.050],
         [-0.003],
         [ 0.027],
         [-0.061],
         [ 0.001],
         [ 0.057],
         [ 0.027],
         [ 0.026]],

        [[-0.023],
         [ 0.042],
         [-0.033],
         [ 0.020],
         [ 0.003],
         [ 0.012],
         [ 0.023],
         [-0.037],
         [-0.020],
         [ 0.046]],

        [[ 0.028],
         [-0.064],
         [ 0.046],
         [-0.065],
         [-0.025],
         [ 0.028],
         [ 0.036],
         [-0.045],
         [ 0.034],
         [ 0.045]],

        [[-0.024],
         [-0.038],
         [ 0.052],
         [ 0.019],
         [-0.011],
         [-0.024],
         [-0.001],
         [ 0.046],
         [ 0.017],
         [-0.010]]], device='cuda:0')
s after update for 1 param tensor([[[1.647],
         [1.330],
         [1.412],
         [1.433],
         [1.765],
         [1.890],
         [1.774],
         [1.520],
         [2.004],
         [2.056]],

        [[1.234],
         [0.878],
         [1.940],
         [2.044],
         [1.934],
         [1.386],
         [1.575],
         [1.692],
         [2.107],
         [1.476]],

        [[1.280],
         [1.712],
         [1.656],
         [1.705],
         [1.605],
         [1.642],
         [2.030],
         [2.124],
         [1.393],
         [1.818]],

        [[1.481],
         [1.609],
         [2.070],
         [1.723],
         [1.893],
         [1.403],
         [1.799],
         [1.855],
         [1.196],
         [1.929]],

        [[1.856],
         [1.824],
         [2.209],
         [1.718],
         [1.302],
         [1.308],
         [2.015],
         [1.996],
         [1.456],
         [1.907]],

        [[1.654],
         [1.826],
         [1.418],
         [1.433],
         [1.835],
         [1.418],
         [1.865],
         [1.632],
         [0.893],
         [1.910]],

        [[1.871],
         [1.577],
         [1.844],
         [1.801],
         [1.990],
         [1.372],
         [1.690],
         [2.304],
         [1.538],
         [1.747]],

        [[1.720],
         [1.204],
         [1.975],
         [1.184],
         [2.092],
         [1.481],
         [1.885],
         [1.145],
         [1.887],
         [1.741]],

        [[1.596],
         [2.013],
         [2.017],
         [1.467],
         [1.592],
         [1.618],
         [1.144],
         [0.580],
         [0.686],
         [1.705]],

        [[1.821],
         [1.714],
         [1.910],
         [1.305],
         [1.826],
         [1.689],
         [1.605],
         [1.544],
         [2.043],
         [1.496]],

        [[1.634],
         [1.671],
         [1.303],
         [1.515],
         [1.655],
         [1.654],
         [1.316],
         [1.929],
         [1.624],
         [1.822]],

        [[1.964],
         [1.656],
         [1.986],
         [1.524],
         [1.918],
         [2.322],
         [1.620],
         [1.586],
         [1.808],
         [1.985]]], device='cuda:0')
b after update for 1 param tensor([[[69.988],
         [62.906],
         [64.810],
         [65.294],
         [72.464],
         [74.992],
         [72.651],
         [67.245],
         [77.215],
         [78.215]],

        [[60.600],
         [51.094],
         [75.967],
         [77.977],
         [75.848],
         [64.209],
         [68.442],
         [70.954],
         [79.165],
         [66.259]],

        [[61.701],
         [71.373],
         [70.197],
         [71.225],
         [69.105],
         [69.891],
         [77.703],
         [79.496],
         [64.377],
         [73.539]],

        [[66.367],
         [69.192],
         [78.466],
         [71.589],
         [75.043],
         [64.613],
         [73.149],
         [74.291],
         [59.653],
         [75.753]],

        [[74.297],
         [73.662],
         [81.074],
         [71.484],
         [62.230],
         [62.390],
         [77.431],
         [77.050],
         [65.819],
         [75.311]],

        [[70.151],
         [73.707],
         [64.948],
         [65.291],
         [73.882],
         [64.951],
         [74.482],
         [69.684],
         [51.533],
         [75.374]],

        [[74.615],
         [68.499],
         [74.073],
         [73.188],
         [76.946],
         [63.886],
         [70.903],
         [82.798],
         [67.646],
         [72.101]],

        [[71.537],
         [59.860],
         [76.654],
         [59.338],
         [78.887],
         [66.376],
         [74.892],
         [58.359],
         [74.928],
         [71.958]],

        [[68.899],
         [77.376],
         [77.466],
         [66.051],
         [68.810],
         [69.387],
         [58.332],
         [41.532],
         [45.186],
         [71.229]],

        [[73.598],
         [71.398],
         [75.372],
         [62.314],
         [73.708],
         [70.888],
         [69.102],
         [67.767],
         [77.960],
         [66.711]],

        [[69.716],
         [70.513],
         [62.261],
         [67.137],
         [70.161],
         [70.140],
         [62.562],
         [75.747],
         [69.499],
         [73.617]],

        [[76.438],
         [70.184],
         [76.872],
         [67.334],
         [75.531],
         [83.113],
         [69.430],
         [68.688],
         [73.329],
         [76.839]]], device='cuda:0')
a after update for 1 param tensor([[ 0.038],
        [-0.008],
        [-0.029],
        [ 0.048],
        [ 0.045],
        [ 0.002],
        [-0.013],
        [-0.035],
        [-0.017],
        [ 0.004],
        [ 0.021],
        [ 0.018]], device='cuda:0')
s after update for 1 param tensor([[1.662],
        [0.727],
        [1.708],
        [2.193],
        [1.884],
        [1.520],
        [2.167],
        [1.792],
        [1.516],
        [2.039],
        [1.949],
        [1.957]], device='cuda:0')
b after update for 1 param tensor([[70.317],
        [46.491],
        [71.291],
        [80.763],
        [74.869],
        [67.236],
        [80.299],
        [73.007],
        [67.153],
        [77.881],
        [76.143],
        [76.308]], device='cuda:0')
||w||^2 0.09934147202266917
exp ma of ||w||^2 0.13605279129268802
||w|| 0.3151848220055483
exp ma of ||w|| 0.35439107814975496
||w||^2 0.0404166314262214
exp ma of ||w||^2 0.12662699568341798
||w|| 0.20103888038442067
exp ma of ||w|| 0.33893970945662255
||w||^2 0.08481914407244223
exp ma of ||w||^2 0.1448867938106164
||w|| 0.2912372642235918
exp ma of ||w|| 0.36296097902037083
||w||^2 0.10916767694385017
exp ma of ||w||^2 0.13860756382420847
||w|| 0.33040532220872315
exp ma of ||w|| 0.3563733481629079
cuda
Objective function 11.82 = squared loss an data 9.53 + 0.5*rho*h**2 0.278278 + alpha*h 1.133604 + L2reg 0.81 + L1reg 0.07 ; SHD = 17 ; DAG True
Proportion of microbatches that were clipped  0.3001835741080693
iteration 1 in inner loop, alpha 48.05147675847047 rho 1000.0 h 0.023591459929201974
1692
cuda
Objective function 14.33 = squared loss an data 9.53 + 0.5*rho*h**2 2.782785 + alpha*h 1.133604 + L2reg 0.81 + L1reg 0.07 ; SHD = 17 ; DAG True
||w||^2 1353599.4072588193
exp ma of ||w||^2 14059113.148583353
||w|| 1163.4429110441213
exp ma of ||w|| 2845.915241739829
||w||^2 6.156867905666483
exp ma of ||w||^2 32.15062423464778
||w|| 2.48130367058659
exp ma of ||w|| 4.20548573496236
||w||^2 1.538030392982698
exp ma of ||w||^2 8.470790248689463
||w|| 1.2401735334148596
exp ma of ||w|| 2.204224627405017
||w||^2 0.8340710669801111
exp ma of ||w||^2 2.5618006119284615
||w|| 0.9132749131450568
exp ma of ||w|| 1.2335432502875616
||w||^2 0.15464008278970726
exp ma of ||w||^2 0.25625445286401394
||w|| 0.3932430327287532
exp ma of ||w|| 0.48467992777008617
||w||^2 0.14547748865589066
exp ma of ||w||^2 0.17252447016033454
||w|| 0.38141511330293487
exp ma of ||w|| 0.4048738749974009
||w||^2 0.1873099060480977
exp ma of ||w||^2 0.1699425387196729
||w|| 0.4327931446408292
exp ma of ||w|| 0.40030919737113924
||w||^2 0.19375747852860786
exp ma of ||w||^2 0.1485603989933948
||w|| 0.4401789164971533
exp ma of ||w|| 0.37292409006226723
||w||^2 0.30770353596802097
exp ma of ||w||^2 0.15547913718541062
||w|| 0.5547103171638518
exp ma of ||w|| 0.38172811598675416
||w||^2 0.2522323453022213
exp ma of ||w||^2 0.1602289136719607
||w|| 0.5022273840624596
exp ma of ||w|| 0.387934847193403
||w||^2 0.09299524771551328
exp ma of ||w||^2 0.13672437114479222
||w|| 0.30495122186263374
exp ma of ||w|| 0.3591429858297691
||w||^2 0.21782818171897103
exp ma of ||w||^2 0.13980258939687196
||w|| 0.4667206677649609
exp ma of ||w|| 0.3604417824543583
cuda
Objective function 11.48 = squared loss an data 9.80 + 0.5*rho*h**2 0.404174 + alpha*h 0.432022 + L2reg 0.77 + L1reg 0.06 ; SHD = 15 ; DAG True
Proportion of microbatches that were clipped  0.25356261444152534
iteration 2 in inner loop, alpha 48.05147675847047 rho 10000.0 h 0.008990822413803556
1692
cuda
Objective function 15.11 = squared loss an data 9.80 + 0.5*rho*h**2 4.041744 + alpha*h 0.432022 + L2reg 0.77 + L1reg 0.06 ; SHD = 15 ; DAG True
||w||^2 20635.246459153543
exp ma of ||w||^2 12746929.357039414
||w|| 143.64973532573438
exp ma of ||w|| 449.40653004021294
||w||^2 538.1055589706929
exp ma of ||w||^2 107868.57533213354
||w|| 23.197102383071314
exp ma of ||w|| 41.84404816497006
||w||^2 1.0366970917429223
exp ma of ||w||^2 485.75976778958335
||w|| 1.0181832309279713
exp ma of ||w|| 2.858619494128308
||w||^2 0.17423253594175622
exp ma of ||w||^2 0.2304407119374407
||w|| 0.4174117103553232
exp ma of ||w|| 0.47206091396180294
||w||^2 0.1710167546073419
exp ma of ||w||^2 0.21731560849612896
||w|| 0.4135417205160102
exp ma of ||w|| 0.45861546682187543
||w||^2 0.23410922909983684
exp ma of ||w||^2 0.18984434393903876
||w|| 0.4838483534123443
exp ma of ||w|| 0.42675217658457176
||w||^2 0.22741387610605335
exp ma of ||w||^2 0.17933544624689762
||w|| 0.4768793097902795
exp ma of ||w|| 0.41513954919201695
||w||^2 0.22885278042734386
exp ma of ||w||^2 0.1842390785292721
||w|| 0.47838559805594466
exp ma of ||w|| 0.42059332123251914
||w||^2 0.17927353410682537
exp ma of ||w||^2 0.19315010091295842
||w|| 0.4234070548618969
exp ma of ||w|| 0.43173817531435127
||w||^2 0.2701264707138603
exp ma of ||w||^2 0.17839589148054003
||w|| 0.5197369245241868
exp ma of ||w|| 0.41569237152092464
||w||^2 0.1287114233764376
exp ma of ||w||^2 0.20526756480519753
||w|| 0.3587637431185565
exp ma of ||w|| 0.4437651045574957
||w||^2 0.11894185593633212
exp ma of ||w||^2 0.16107873227804523
||w|| 0.3448794803062834
exp ma of ||w|| 0.3937374062191306
cuda
Objective function 11.47 = squared loss an data 9.83 + 0.5*rho*h**2 0.652492 + alpha*h 0.173584 + L2reg 0.76 + L1reg 0.06 ; SHD = 15 ; DAG True
Proportion of microbatches that were clipped  0.19504743527898377
iteration 3 in inner loop, alpha 48.05147675847047 rho 100000.0 h 0.003612456611188364
iteration 4 in outer loop, alpha = 409.29713787730685, rho = 100000.0, h = 0.003612456611188364
cuda
1692
cuda
Objective function 12.77 = squared loss an data 9.83 + 0.5*rho*h**2 0.652492 + alpha*h 1.478568 + L2reg 0.76 + L1reg 0.06 ; SHD = 15 ; DAG True
||w||^2 0.323320890562041
exp ma of ||w||^2 0.22153369658756597
||w|| 0.5686131290799052
exp ma of ||w|| 0.4663480884342962
||w||^2 0.49112374954812427
exp ma of ||w||^2 0.21170913437279576
||w|| 0.7008022185667824
exp ma of ||w|| 0.45458811983105685
||w||^2 0.16033798476131342
exp ma of ||w||^2 0.22304087929151958
||w|| 0.40042225807429016
exp ma of ||w|| 0.4671873402244757
cuda
Objective function 11.93 = squared loss an data 9.78 + 0.5*rho*h**2 0.303827 + alpha*h 1.008943 + L2reg 0.77 + L1reg 0.06 ; SHD = 15 ; DAG True
Proportion of microbatches that were clipped  0.17210159442352377
iteration 1 in inner loop, alpha 409.29713787730685 rho 100000.0 h 0.0024650625634627943
iteration 5 in outer loop, alpha = 2874.359701340101, rho = 1000000.0, h = 0.0024650625634627943
Threshold 0.3
[[0.005 0.01  0.604 0.117 0.547 0.028 0.286 0.034 0.006 0.162 0.06  0.084]
 [0.672 0.003 0.39  0.305 0.239 0.287 0.345 0.084 0.004 0.699 0.298 0.57 ]
 [0.01  0.008 0.007 0.039 0.01  0.01  0.063 0.005 0.008 0.027 0.024 0.052]
 [0.046 0.014 0.107 0.003 0.069 0.046 0.127 0.04  0.005 0.44  0.01  0.207]
 [0.007 0.014 0.444 0.075 0.004 0.026 0.184 0.016 0.012 0.303 0.041 0.094]
 [0.188 0.024 0.42  0.109 0.177 0.007 0.183 0.078 0.016 0.14  0.046 0.142]
 [0.028 0.017 0.066 0.063 0.033 0.03  0.004 0.01  0.006 0.089 0.033 0.023]
 [0.188 0.083 0.83  0.165 0.275 0.087 0.53  0.004 0.006 0.247 0.059 0.168]
 [0.495 1.416 0.17  0.868 0.511 0.348 0.632 0.799 0.005 0.743 1.808 0.675]
 [0.029 0.007 0.227 0.013 0.016 0.049 0.059 0.021 0.004 0.003 0.01  0.051]
 [0.089 0.018 0.286 0.539 0.139 0.166 0.167 0.12  0.003 0.587 0.003 0.288]
 [0.056 0.01  0.148 0.028 0.066 0.048 0.335 0.041 0.004 0.138 0.017 0.005]]
[[0.    0.    0.604 0.    0.547 0.    0.    0.    0.    0.    0.    0.   ]
 [0.672 0.    0.39  0.305 0.    0.    0.345 0.    0.    0.699 0.    0.57 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.44  0.    0.   ]
 [0.    0.    0.444 0.    0.    0.    0.    0.    0.    0.303 0.    0.   ]
 [0.    0.    0.42  0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.83  0.    0.    0.    0.53  0.    0.    0.    0.    0.   ]
 [0.495 1.416 0.    0.868 0.511 0.348 0.632 0.799 0.    0.743 1.808 0.675]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.539 0.    0.    0.    0.    0.    0.587 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.335 0.    0.    0.    0.    0.   ]]
{'fdr': 0.37037037037037035, 'tpr': 0.7083333333333334, 'fpr': 0.23809523809523808, 'f1': 0.6666666666666667, 'shd': 15, 'npred': 27, 'ntrue': 24}
[0.01  0.604 0.117 0.547 0.028 0.286 0.034 0.006 0.162 0.06  0.084 0.672
 0.39  0.305 0.239 0.287 0.345 0.084 0.004 0.699 0.298 0.57  0.01  0.008
 0.039 0.01  0.01  0.063 0.005 0.008 0.027 0.024 0.052 0.046 0.014 0.107
 0.069 0.046 0.127 0.04  0.005 0.44  0.01  0.207 0.007 0.014 0.444 0.075
 0.026 0.184 0.016 0.012 0.303 0.041 0.094 0.188 0.024 0.42  0.109 0.177
 0.183 0.078 0.016 0.14  0.046 0.142 0.028 0.017 0.066 0.063 0.033 0.03
 0.01  0.006 0.089 0.033 0.023 0.188 0.083 0.83  0.165 0.275 0.087 0.53
 0.006 0.247 0.059 0.168 0.495 1.416 0.17  0.868 0.511 0.348 0.632 0.799
 0.743 1.808 0.675 0.029 0.007 0.227 0.013 0.016 0.049 0.059 0.021 0.004
 0.01  0.051 0.089 0.018 0.286 0.539 0.139 0.166 0.167 0.12  0.003 0.587
 0.288 0.056 0.01  0.148 0.028 0.066 0.048 0.335 0.041 0.004 0.138 0.017]
[[0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8553240740740741, 0.6976481252614041)
Iterations 2500
Achieves (24.160548736236485, 1e-05)-DP
