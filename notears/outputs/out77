samples  5000  graph  15 30 ER mlp  minibatch size  100  noise  0.5  minibatches per NN training  32 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006583550827196305
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0011537970975332712
iteration 4 in outer loop, alpha = 156.00431709949223, rho = 100000.0, h = 0.0011537970975332712
cuda
iteration 1 in inner loop,alpha 156.00431709949223 rho 100000.0 h 0.00033575511004180214
iteration 5 in outer loop, alpha = 491.7594271412944, rho = 1000000.0, h = 0.00033575511004180214
Threshold 0.3
[[0.002 0.    0.    0.005 0.194 0.    0.    0.    0.    2.734 0.179 0.001
  0.001 0.    0.006]
 [2.52  0.001 0.002 0.262 0.215 0.01  0.01  0.001 0.002 2.189 0.199 1.342
  0.001 0.001 0.098]
 [0.027 0.068 0.001 0.147 0.166 0.035 0.137 0.001 0.067 0.125 0.135 0.162
  0.001 0.001 0.006]
 [0.153 0.    0.001 0.001 0.204 0.    0.    0.    0.    0.214 0.335 0.001
  0.    0.    1.022]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    0.001]
 [2.646 0.03  0.001 0.968 0.376 0.001 1.377 0.    0.055 1.746 0.956 1.472
  0.012 0.002 1.923]
 [0.414 0.044 0.    0.336 0.408 0.    0.001 0.    0.059 0.139 1.162 0.096
  0.001 0.    0.163]
 [2.69  0.002 0.    2.092 0.137 0.612 0.046 0.    0.003 0.313 0.139 0.061
  0.014 0.022 0.118]
 [0.103 0.383 0.002 0.638 0.415 0.002 0.001 0.001 0.003 2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.001 0.    0.    0.002 0.402 0.001
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.057 0.    0.001 0.    0.001 0.001 0.015 0.
  0.    0.    0.005]
 [0.37  0.    0.    0.18  0.153 0.    0.004 0.    0.002 0.427 0.58  0.001
  0.    0.001 0.032]
 [0.041 0.078 0.    0.112 3.047 0.017 0.008 0.    1.212 0.107 1.585 0.031
  0.001 0.016 0.021]
 [0.126 1.398 0.    1.127 1.231 0.003 0.006 0.    3.368 0.386 0.434 0.158
  0.011 0.002 0.562]
 [0.089 0.    0.    0.    0.111 0.    0.    0.    0.    0.106 0.081 0.002
  0.004 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.734 0.    0.
  0.    0.    0.   ]
 [2.52  0.    0.    0.    0.    0.    0.    0.    0.    2.189 0.    1.342
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.335 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.968 0.376 0.    1.377 0.    0.    1.746 0.956 1.472
  0.    0.    1.923]
 [0.414 0.    0.    0.336 0.408 0.    0.    0.    0.    0.    1.162 0.
  0.    0.    0.   ]
 [2.69  0.    0.    2.092 0.    0.612 0.    0.    0.    0.313 0.    0.
  0.    0.    0.   ]
 [0.    0.383 0.    0.638 0.415 0.    0.    0.    0.    2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.    0.    0.    0.    0.402 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.057 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.427 0.58  0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.047 0.    0.    0.    1.212 0.    1.585 0.
  0.    0.    0.   ]
 [0.    1.398 0.    1.127 1.231 0.    0.    0.    3.368 0.386 0.434 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[7.166e-05 3.031e-04 4.737e-03 1.942e-01 6.655e-05 2.207e-04 4.993e-05
 2.937e-04 2.734e+00 1.793e-01 5.105e-04 1.050e-03 1.242e-04 5.740e-03
 2.520e+00 1.954e-03 2.620e-01 2.147e-01 9.724e-03 9.801e-03 9.328e-04
 1.620e-03 2.189e+00 1.994e-01 1.342e+00 5.488e-04 9.886e-04 9.781e-02
 2.679e-02 6.849e-02 1.472e-01 1.660e-01 3.508e-02 1.368e-01 5.716e-04
 6.687e-02 1.254e-01 1.352e-01 1.619e-01 8.477e-04 1.272e-03 6.092e-03
 1.532e-01 1.509e-04 9.882e-04 2.036e-01 4.044e-05 3.897e-04 2.489e-04
 6.858e-05 2.135e-01 3.353e-01 1.155e-03 4.106e-04 1.293e-04 1.022e+00
 6.064e-05 4.791e-06 5.172e-05 2.471e-04 1.136e-05 1.928e-04 4.191e-06
 2.163e-04 3.383e-04 1.833e-03 1.007e-04 2.631e-05 7.614e-05 1.086e-03
 2.646e+00 3.009e-02 1.478e-03 9.684e-01 3.765e-01 1.377e+00 1.229e-04
 5.513e-02 1.746e+00 9.560e-01 1.472e+00 1.180e-02 1.838e-03 1.923e+00
 4.141e-01 4.437e-02 3.959e-04 3.360e-01 4.079e-01 4.254e-04 5.935e-05
 5.872e-02 1.392e-01 1.162e+00 9.588e-02 6.731e-04 4.063e-04 1.632e-01
 2.690e+00 2.126e-03 4.852e-04 2.092e+00 1.369e-01 6.121e-01 4.646e-02
 3.330e-03 3.125e-01 1.385e-01 6.062e-02 1.448e-02 2.181e-02 1.178e-01
 1.034e-01 3.829e-01 2.017e-03 6.383e-01 4.147e-01 2.074e-03 1.394e-03
 1.328e-03 2.308e+00 1.722e+00 1.030e+00 5.492e-05 3.972e-04 3.674e-01
 1.834e-04 4.607e-06 8.949e-05 4.394e-04 2.722e+00 6.113e-06 8.992e-04
 1.147e-05 4.538e-04 4.020e-01 1.020e-03 1.265e-04 1.194e-04 2.179e-03
 6.634e-04 5.288e-05 3.430e-04 1.176e-03 3.057e+00 1.252e-04 7.885e-04
 1.415e-04 9.900e-04 8.589e-04 1.728e-04 2.153e-04 3.103e-04 5.194e-03
 3.696e-01 4.210e-04 8.175e-05 1.805e-01 1.529e-01 2.371e-04 3.593e-03
 6.136e-05 1.729e-03 4.267e-01 5.799e-01 2.072e-04 6.430e-04 3.169e-02
 4.077e-02 7.764e-02 2.020e-04 1.125e-01 3.047e+00 1.651e-02 8.033e-03
 3.454e-04 1.212e+00 1.069e-01 1.585e+00 3.144e-02 1.628e-02 2.119e-02
 1.264e-01 1.398e+00 2.136e-04 1.127e+00 1.231e+00 3.196e-03 6.293e-03
 3.254e-04 3.368e+00 3.864e-01 4.339e-01 1.576e-01 1.130e-02 5.624e-01
 8.923e-02 3.157e-04 3.306e-04 7.760e-05 1.109e-01 7.500e-05 1.536e-04
 2.248e-04 2.530e-04 1.060e-01 8.126e-02 2.152e-03 4.216e-03 1.418e-04]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9805555555555556, 0.9709424613206593)
cuda
2565
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
||w||^2 4781.528405973686
exp ma of ||w||^2 1107630522.4299123
||w|| 69.148596558236
exp ma of ||w|| 1184.2652085957307
||w||^2 2.4920474658866385
exp ma of ||w||^2 9546490.731349612
||w|| 1.5786220148872365
exp ma of ||w|| 13.667948934505555
cuda
Objective function 105.97 = squared loss an data 103.89 + 0.5*rho*h**2 1.772890 + alpha*h 0.000000 + L2reg 0.15 + L1reg 0.15 ; SHD = 54 ; DAG False
Proportion of microbatches that were clipped  0.6982511923688395
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.8830241050921863
iteration 1 in outer loop, alpha = 1.8830241050921863, rho = 1.0, h = 1.8830241050921863
cuda
2565
cuda
Objective function 109.51 = squared loss an data 103.89 + 0.5*rho*h**2 1.772890 + alpha*h 3.545780 + L2reg 0.15 + L1reg 0.15 ; SHD = 54 ; DAG False
||w||^2 1649512983.6401894
exp ma of ||w||^2 1750057657.3534763
||w|| 40614.19682377321
exp ma of ||w|| 31064.033928049976
||w||^2 13751639.620432753
exp ma of ||w||^2 360351580.2710537
||w|| 3708.3203233314075
exp ma of ||w|| 13609.414365022367
||w||^2 16017762.629317148
exp ma of ||w||^2 81101309.0708658
||w|| 4002.2197127740437
exp ma of ||w|| 6125.034325117087
||w||^2 0.35633273573882995
exp ma of ||w||^2 31.61545347439891
||w|| 0.596936123667206
exp ma of ||w|| 1.0931593518228144
cuda
Objective function 51.66 = squared loss an data 48.25 + 0.5*rho*h**2 0.733777 + alpha*h 2.281145 + L2reg 0.25 + L1reg 0.14 ; SHD = 46 ; DAG False
Proportion of microbatches that were clipped  0.6956251939187093
iteration 1 in inner loop, alpha 1.8830241050921863 rho 1.0 h 1.2114264410434856
2565
cuda
Objective function 58.26 = squared loss an data 48.25 + 0.5*rho*h**2 7.337770 + alpha*h 2.281145 + L2reg 0.25 + L1reg 0.14 ; SHD = 46 ; DAG False
||w||^2 313310.53192845226
exp ma of ||w||^2 2846251.292725518
||w|| 559.7414866958248
exp ma of ||w|| 1089.3142184835242
||w||^2 12845.453181863139
exp ma of ||w||^2 429873.26484199875
||w|| 113.33778355810183
exp ma of ||w|| 366.91981002195604
||w||^2 8843.923624713487
exp ma of ||w||^2 303116.2457032934
||w|| 94.04213749545194
exp ma of ||w|| 302.36595984153576
||w||^2 663.3765422962013
exp ma of ||w||^2 29466.05835443235
||w|| 25.7560971868061
exp ma of ||w|| 71.52703929256543
cuda
Objective function 31.51 = squared loss an data 28.36 + 0.5*rho*h**2 1.590901 + alpha*h 1.062166 + L2reg 0.37 + L1reg 0.12 ; SHD = 40 ; DAG False
Proportion of microbatches that were clipped  0.7628205128205128
iteration 2 in inner loop, alpha 1.8830241050921863 rho 10.0 h 0.5640745701978638
2565
cuda
Objective function 45.83 = squared loss an data 28.36 + 0.5*rho*h**2 15.909006 + alpha*h 1.062166 + L2reg 0.37 + L1reg 0.12 ; SHD = 40 ; DAG False
||w||^2 36680595618.91118
exp ma of ||w||^2 88274629178.70137
||w|| 191521.78888813456
exp ma of ||w|| 236634.98888668563
||w||^2 5436711.666649895
exp ma of ||w||^2 61310512.51725626
||w|| 2331.6757207317432
exp ma of ||w|| 5794.182401004948
||w||^2 4271351.797945616
exp ma of ||w||^2 40358497.02400858
||w|| 2066.724896532099
exp ma of ||w|| 4706.186826052232
cuda
Objective function 27.95 = squared loss an data 24.90 + 0.5*rho*h**2 2.088601 + alpha*h 0.384856 + L2reg 0.47 + L1reg 0.11 ; SHD = 35 ; DAG True
Proportion of microbatches that were clipped  0.7792374239025953
iteration 3 in inner loop, alpha 1.8830241050921863 rho 100.0 h 0.20438202140387673
iteration 2 in outer loop, alpha = 22.32122624547986, rho = 100.0, h = 0.20438202140387673
cuda
2565
cuda
Objective function 32.13 = squared loss an data 24.90 + 0.5*rho*h**2 2.088601 + alpha*h 4.562057 + L2reg 0.47 + L1reg 0.11 ; SHD = 35 ; DAG True
||w||^2 16915676047.531643
exp ma of ||w||^2 50440471225.53676
||w|| 130060.2785155085
exp ma of ||w|| 198402.22435367244
||w||^2 13313344.490815751
exp ma of ||w||^2 175581051.05186805
||w|| 3648.745605110851
exp ma of ||w|| 10185.562074251648
||w||^2 1.367865985216783
exp ma of ||w||^2 39.163655822514386
||w|| 1.1695580298628978
exp ma of ||w|| 1.3025801009755669
cuda
Objective function 24.98 = squared loss an data 20.50 + 0.5*rho*h**2 0.859966 + alpha*h 2.927343 + L2reg 0.58 + L1reg 0.11 ; SHD = 33 ; DAG True
Proportion of microbatches that were clipped  0.7761755485893417
iteration 1 in inner loop, alpha 22.32122624547986 rho 100.0 h 0.13114614784924683
2565
cuda
Objective function 32.72 = squared loss an data 20.50 + 0.5*rho*h**2 8.599656 + alpha*h 2.927343 + L2reg 0.58 + L1reg 0.11 ; SHD = 33 ; DAG True
||w||^2 7253077098.634733
exp ma of ||w||^2 57961926033.281235
||w|| 85164.99925811503
exp ma of ||w|| 207790.08368709468
||w||^2 2.156175719411515
exp ma of ||w||^2 616.2758358305666
||w|| 1.4683922226065877
exp ma of ||w|| 4.340977538593429
cuda
Objective function 22.27 = squared loss an data 19.04 + 0.5*rho*h**2 1.302896 + alpha*h 1.139431 + L2reg 0.68 + L1reg 0.11 ; SHD = 26 ; DAG True
Proportion of microbatches that were clipped  0.7619620824556124
iteration 2 in inner loop, alpha 22.32122624547986 rho 1000.0 h 0.05104695071643839
iteration 3 in outer loop, alpha = 73.36817696191825, rho = 1000.0, h = 0.05104695071643839
cuda
2565
cuda
Objective function 24.88 = squared loss an data 19.04 + 0.5*rho*h**2 1.302896 + alpha*h 3.745222 + L2reg 0.68 + L1reg 0.11 ; SHD = 26 ; DAG True
||w||^2 2.997688945244576
exp ma of ||w||^2 227.29311525518037
||w|| 1.731383534992919
exp ma of ||w|| 2.359195495367066
cuda
Objective function 21.33 = squared loss an data 17.27 + 0.5*rho*h**2 0.613603 + alpha*h 2.570198 + L2reg 0.77 + L1reg 0.11 ; SHD = 25 ; DAG True
Proportion of microbatches that were clipped  0.7578818487909397
iteration 1 in inner loop, alpha 73.36817696191825 rho 1000.0 h 0.03503150979233638
2565
cuda
Objective function 26.86 = squared loss an data 17.27 + 0.5*rho*h**2 6.136033 + alpha*h 2.570198 + L2reg 0.77 + L1reg 0.11 ; SHD = 25 ; DAG True
||w||^2 79481155.33088645
exp ma of ||w||^2 177395793.64109874
||w|| 8915.220430863526
exp ma of ||w|| 9768.29781218518
||w||^2 1.6054841240073054
exp ma of ||w||^2 736.2851965847929
||w|| 1.2670770000309
exp ma of ||w|| 2.906860233833564
cuda
Objective function 20.11 = squared loss an data 17.23 + 0.5*rho*h**2 0.935603 + alpha*h 1.003618 + L2reg 0.84 + L1reg 0.10 ; SHD = 24 ; DAG True
Proportion of microbatches that were clipped  0.7438530967942732
iteration 2 in inner loop, alpha 73.36817696191825 rho 10000.0 h 0.01367920002313383
2565
cuda
Objective function 28.53 = squared loss an data 17.23 + 0.5*rho*h**2 9.356026 + alpha*h 1.003618 + L2reg 0.84 + L1reg 0.10 ; SHD = 24 ; DAG True
||w||^2 118273006.7961819
exp ma of ||w||^2 22758182715.281906
||w|| 10875.33938763209
exp ma of ||w|| 31355.782952313264
||w||^2 60927.2270153306
exp ma of ||w||^2 74456607.11829878
||w|| 246.83441213763246
exp ma of ||w|| 1217.1447366737252
||w||^2 7.730374762324507
exp ma of ||w||^2 8340.88490407856
||w|| 2.7803551503943713
exp ma of ||w|| 3.9644796610470086
||w||^2 0.9000854512359298
exp ma of ||w||^2 242.01305150845883
||w|| 0.9487283337372874
exp ma of ||w|| 1.4187808981000485
||w||^2 3.8422094872897405
exp ma of ||w||^2 5.515713274139137
||w|| 1.960155475284994
exp ma of ||w|| 1.0686436538241582
cuda
Objective function 19.19 = squared loss an data 16.67 + 0.5*rho*h**2 1.187966 + alpha*h 0.357622 + L2reg 0.88 + L1reg 0.10 ; SHD = 24 ; DAG True
Proportion of microbatches that were clipped  0.7575372897492859
iteration 3 in inner loop, alpha 73.36817696191825 rho 100000.0 h 0.004874352736926824
iteration 4 in outer loop, alpha = 560.8034506546006, rho = 100000.0, h = 0.004874352736926824
cuda
2565
cuda
Objective function 21.57 = squared loss an data 16.67 + 0.5*rho*h**2 1.187966 + alpha*h 2.733554 + L2reg 0.88 + L1reg 0.10 ; SHD = 24 ; DAG True
||w||^2 59218684548.13158
exp ma of ||w||^2 4247716032671.6943
||w|| 243348.89469264407
exp ma of ||w|| 1119030.1216811785
||w||^2 1.4735144277547876
exp ma of ||w||^2 264.4614801222611
||w|| 1.213884025660931
exp ma of ||w|| 1.4531741223783272
cuda
Objective function 19.95 = squared loss an data 16.65 + 0.5*rho*h**2 0.505137 + alpha*h 1.782504 + L2reg 0.93 + L1reg 0.09 ; SHD = 22 ; DAG True
Proportion of microbatches that were clipped  0.7600256657042027
iteration 1 in inner loop, alpha 560.8034506546006 rho 100000.0 h 0.003178481988509674
iteration 5 in outer loop, alpha = 3739.2854391642745, rho = 1000000.0, h = 0.003178481988509674
Threshold 0.3
[[0.004 0.015 0.061 0.304 0.258 0.018 0.024 0.007 0.041 0.752 0.077 0.036
  0.048 0.018 0.083]
 [0.185 0.005 0.14  0.446 0.274 0.047 0.063 0.102 0.319 0.378 0.133 0.91
  0.082 0.005 0.181]
 [0.05  0.028 0.005 0.078 0.2   0.099 0.051 0.075 0.07  0.083 0.131 0.1
  0.061 0.024 0.131]
 [0.017 0.008 0.078 0.004 0.131 0.016 0.025 0.016 0.035 0.062 0.146 0.03
  0.041 0.016 0.098]
 [0.007 0.011 0.018 0.018 0.004 0.004 0.007 0.015 0.006 0.004 0.004 0.014
  0.005 0.005 0.015]
 [0.265 0.122 0.059 0.315 0.311 0.004 0.016 0.099 0.163 0.299 0.086 0.478
  0.117 0.04  0.156]
 [0.159 0.082 0.091 0.187 0.367 0.322 0.005 0.082 0.212 0.137 1.02  0.263
  0.088 0.032 0.114]
 [0.719 0.057 0.065 0.279 0.282 0.041 0.058 0.007 0.064 0.194 0.088 0.085
  0.043 0.029 0.047]
 [0.059 0.014 0.063 0.123 0.173 0.026 0.016 0.067 0.005 0.668 0.833 0.012
  0.036 0.005 0.08 ]
 [0.009 0.015 0.065 0.095 1.661 0.017 0.014 0.016 0.009 0.005 0.027 0.016
  0.024 0.007 0.057]
 [0.055 0.019 0.02  0.028 1.69  0.027 0.005 0.045 0.007 0.228 0.003 0.019
  0.007 0.006 0.073]
 [0.123 0.007 0.042 0.11  0.183 0.012 0.018 0.048 0.568 0.166 0.258 0.006
  0.034 0.005 0.043]
 [0.082 0.055 0.083 0.08  0.428 0.05  0.057 0.073 0.085 0.128 1.022 0.12
  0.006 0.03  0.046]
 [0.228 0.942 0.165 0.162 0.348 0.098 0.117 0.113 1.406 0.575 0.436 1.275
  0.096 0.006 0.061]
 [0.038 0.024 0.039 0.055 0.266 0.023 0.025 0.08  0.043 0.068 0.051 0.101
  0.072 0.029 0.006]]
[[0.    0.    0.    0.304 0.    0.    0.    0.    0.    0.752 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.446 0.    0.    0.    0.    0.319 0.378 0.    0.91
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.315 0.311 0.    0.    0.    0.    0.    0.    0.478
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.367 0.322 0.    0.    0.    0.    1.02  0.
  0.    0.    0.   ]
 [0.719 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.668 0.833 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.661 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.69  0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.568 0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.428 0.    0.    0.    0.    0.    1.022 0.
  0.    0.    0.   ]
 [0.    0.942 0.    0.    0.348 0.    0.    0.    1.406 0.575 0.436 1.275
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.38461538461538464, 'tpr': 0.5333333333333333, 'fpr': 0.13333333333333333, 'f1': 0.5714285714285715, 'shd': 22, 'npred': 26, 'ntrue': 30}
[0.015 0.061 0.304 0.258 0.018 0.024 0.007 0.041 0.752 0.077 0.036 0.048
 0.018 0.083 0.185 0.14  0.446 0.274 0.047 0.063 0.102 0.319 0.378 0.133
 0.91  0.082 0.005 0.181 0.05  0.028 0.078 0.2   0.099 0.051 0.075 0.07
 0.083 0.131 0.1   0.061 0.024 0.131 0.017 0.008 0.078 0.131 0.016 0.025
 0.016 0.035 0.062 0.146 0.03  0.041 0.016 0.098 0.007 0.011 0.018 0.018
 0.004 0.007 0.015 0.006 0.004 0.004 0.014 0.005 0.005 0.015 0.265 0.122
 0.059 0.315 0.311 0.016 0.099 0.163 0.299 0.086 0.478 0.117 0.04  0.156
 0.159 0.082 0.091 0.187 0.367 0.322 0.082 0.212 0.137 1.02  0.263 0.088
 0.032 0.114 0.719 0.057 0.065 0.279 0.282 0.041 0.058 0.064 0.194 0.088
 0.085 0.043 0.029 0.047 0.059 0.014 0.063 0.123 0.173 0.026 0.016 0.067
 0.668 0.833 0.012 0.036 0.005 0.08  0.009 0.015 0.065 0.095 1.661 0.017
 0.014 0.016 0.009 0.027 0.016 0.024 0.007 0.057 0.055 0.019 0.02  0.028
 1.69  0.027 0.005 0.045 0.007 0.228 0.019 0.007 0.006 0.073 0.123 0.007
 0.042 0.11  0.183 0.012 0.018 0.048 0.568 0.166 0.258 0.034 0.005 0.043
 0.082 0.055 0.083 0.08  0.428 0.05  0.057 0.073 0.085 0.128 1.022 0.12
 0.03  0.046 0.228 0.942 0.165 0.162 0.348 0.098 0.117 0.113 1.406 0.575
 0.436 1.275 0.096 0.061 0.038 0.024 0.039 0.055 0.266 0.023 0.025 0.08
 0.043 0.068 0.051 0.101 0.072 0.029]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.837037037037037, 0.6252676747353775)
Iterations 320
Achieves (18.273120775215382, 1e-05)-DP
