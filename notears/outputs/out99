samples  5000  graph  10 20 ER mim  minibatch size  100  noise  0.6  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.2478778610411254
iteration 1 in outer loop, alpha = 1.2478778610411254, rho = 1.0, h = 1.2478778610411254
cuda
iteration 1 in inner loop,alpha 1.2478778610411254 rho 1.0 h 0.8084888745241194
iteration 2 in inner loop,alpha 1.2478778610411254 rho 10.0 h 0.3305783307043271
iteration 3 in inner loop,alpha 1.2478778610411254 rho 100.0 h 0.08917603829053178
iteration 2 in outer loop, alpha = 10.165481690094303, rho = 100.0, h = 0.08917603829053178
cuda
iteration 1 in inner loop,alpha 10.165481690094303 rho 100.0 h 0.04548874290316185
iteration 2 in inner loop,alpha 10.165481690094303 rho 1000.0 h 0.015079571234085876
iteration 3 in outer loop, alpha = 25.24505292418018, rho = 1000.0, h = 0.015079571234085876
cuda
iteration 1 in inner loop,alpha 25.24505292418018 rho 1000.0 h 0.006795731970465013
iteration 2 in inner loop,alpha 25.24505292418018 rho 10000.0 h 0.0010007113732211081
iteration 4 in outer loop, alpha = 35.25216665639126, rho = 10000.0, h = 0.0010007113732211081
cuda
iteration 1 in inner loop,alpha 35.25216665639126 rho 10000.0 h 0.0006304363443092598
iteration 2 in inner loop,alpha 35.25216665639126 rho 100000.0 h 0.0002552132767164039
iteration 5 in outer loop, alpha = 290.46544337279516, rho = 1000000.0, h = 0.0002552132767164039
Threshold 0.3
[[0.006 0.    0.001 0.    0.497 0.    0.018 0.001 0.    1.76 ]
 [2.11  0.006 0.581 0.    0.104 0.002 1.127 0.003 0.    1.048]
 [0.291 0.006 0.007 0.    0.086 0.003 1.222 0.007 0.    0.245]
 [1.502 2.89  1.108 0.    0.012 2.624 0.222 2.695 0.017 0.146]
 [0.    0.    0.001 0.    0.005 0.001 0.001 0.001 0.    0.002]
 [0.424 0.303 0.728 0.    0.041 0.005 0.958 1.04  0.006 0.138]
 [0.053 0.001 0.002 0.    0.984 0.001 0.005 0.001 0.    0.087]
 [0.299 0.333 0.023 0.    0.012 0.001 0.205 0.002 0.007 0.044]
 [0.2   1.847 0.008 0.003 1.482 0.076 0.528 0.041 0.    0.067]
 [0.001 0.    0.001 0.    0.888 0.001 0.014 0.001 0.    0.005]]
[[0.    0.    0.    0.    0.497 0.    0.    0.    0.    1.76 ]
 [2.11  0.    0.581 0.    0.    0.    1.127 0.    0.    1.048]
 [0.    0.    0.    0.    0.    0.    1.222 0.    0.    0.   ]
 [1.502 2.89  1.108 0.    0.    2.624 0.    2.695 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.424 0.303 0.728 0.    0.    0.    0.958 1.04  0.    0.   ]
 [0.    0.    0.    0.    0.984 0.    0.    0.    0.    0.   ]
 [0.    0.333 0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    1.847 0.    0.    1.482 0.    0.528 0.    0.    0.   ]
 [0.    0.    0.    0.    0.888 0.    0.    0.    0.    0.   ]]
{'fdr': 0.17391304347826086, 'tpr': 0.95, 'fpr': 0.16, 'f1': 0.8837209302325583, 'shd': 4, 'npred': 23, 'ntrue': 20}
[3.793e-04 1.282e-03 2.153e-04 4.967e-01 2.331e-04 1.848e-02 5.735e-04
 7.305e-05 1.760e+00 2.110e+00 5.813e-01 5.979e-05 1.036e-01 2.483e-03
 1.127e+00 2.934e-03 3.824e-05 1.048e+00 2.906e-01 5.615e-03 2.661e-05
 8.569e-02 3.464e-03 1.222e+00 6.616e-03 3.535e-04 2.446e-01 1.502e+00
 2.890e+00 1.108e+00 1.184e-02 2.624e+00 2.219e-01 2.695e+00 1.684e-02
 1.464e-01 2.843e-04 3.796e-04 7.651e-04 7.134e-05 7.212e-04 1.497e-03
 7.347e-04 3.787e-05 2.200e-03 4.236e-01 3.027e-01 7.281e-01 7.481e-05
 4.108e-02 9.575e-01 1.040e+00 6.014e-03 1.385e-01 5.298e-02 9.968e-04
 1.659e-03 6.068e-05 9.839e-01 8.308e-04 1.200e-03 3.611e-04 8.660e-02
 2.993e-01 3.329e-01 2.256e-02 1.895e-04 1.207e-02 1.274e-03 2.046e-01
 7.096e-03 4.363e-02 1.996e-01 1.847e+00 7.515e-03 2.832e-03 1.482e+00
 7.563e-02 5.277e-01 4.148e-02 6.676e-02 9.921e-04 2.851e-04 5.999e-04
 1.246e-04 8.882e-01 7.037e-04 1.407e-02 7.412e-04 7.471e-05]
[[0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9664285714285714, 0.9457863715661705)
cuda
1210
cuda
Objective function 188.58 = squared loss an data 11.52 + 0.5*rho*h**2 176.710652 + alpha*h 0.000000 + L2reg 0.19 + L1reg 0.16 ; SHD = 55 ; DAG False
||w||^2 0.22158522133657932
exp ma of ||w||^2 491.366171647096
||w|| 0.47072839444480014
exp ma of ||w|| 0.3746003230658317
||w||^2 0.43919532222119656
exp ma of ||w||^2 0.24130387277750623
||w|| 0.6627181318035569
exp ma of ||w|| 0.47323507266212134
||w||^2 0.5867651777960202
exp ma of ||w||^2 0.6208248198394913
||w|| 0.7660059907050468
exp ma of ||w|| 0.742649573529196
cuda
Objective function 8.48 = squared loss an data 7.98 + 0.5*rho*h**2 0.342956 + alpha*h 0.000000 + L2reg 0.09 + L1reg 0.06 ; SHD = 18 ; DAG False
Proportion of microbatches that were clipped  0.7511520737327189
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 0.8281975371006602
iteration 1 in outer loop, alpha = 0.8281975371006602, rho = 1.0, h = 0.8281975371006602
cuda
1210
cuda
Objective function 9.17 = squared loss an data 7.98 + 0.5*rho*h**2 0.342956 + alpha*h 0.685911 + L2reg 0.09 + L1reg 0.06 ; SHD = 18 ; DAG False
||w||^2 3.125251421749387
exp ma of ||w||^2 1745.0994378331138
||w|| 1.7678380643456535
exp ma of ||w|| 11.929254929939024
||w||^2 0.08169484397389004
exp ma of ||w||^2 7.175124576831207
||w|| 0.2858230990908363
exp ma of ||w|| 0.4500175600582694
||w||^2 0.04391522474045734
exp ma of ||w||^2 0.10178461556400371
||w|| 0.20955959710893066
exp ma of ||w|| 0.30215882935893756
||w||^2 0.3038234229773351
exp ma of ||w||^2 0.203491994402846
||w|| 0.5512017987791178
exp ma of ||w|| 0.4245050912738681
||w||^2 0.10069052887403071
exp ma of ||w||^2 0.20301616322991015
||w|| 0.31731770967601336
exp ma of ||w|| 0.425226007880572
cuda
Objective function 8.22 = squared loss an data 7.39 + 0.5*rho*h**2 0.149510 + alpha*h 0.452880 + L2reg 0.16 + L1reg 0.07 ; SHD = 18 ; DAG False
Proportion of microbatches that were clipped  0.7328578375749716
iteration 1 in inner loop, alpha 0.8281975371006602 rho 1.0 h 0.5468266552364867
1210
cuda
Objective function 9.56 = squared loss an data 7.39 + 0.5*rho*h**2 1.495097 + alpha*h 0.452880 + L2reg 0.16 + L1reg 0.07 ; SHD = 18 ; DAG False
||w||^2 117563199.21236733
exp ma of ||w||^2 491248023.19864786
||w|| 10842.656464739963
exp ma of ||w|| 19852.20711857034
||w||^2 32521317.751690455
exp ma of ||w||^2 89709582.02141985
||w|| 5702.7465095066655
exp ma of ||w|| 8173.096718533336
||w||^2 47.56965701020751
exp ma of ||w||^2 23359.778719126196
||w|| 6.897075975383156
exp ma of ||w|| 72.1554148588683
||w||^2 0.0803232197427452
exp ma of ||w||^2 6.615627567112353
||w|| 0.2834135136911174
exp ma of ||w|| 0.4602705255846996
||w||^2 0.05876802111995957
exp ma of ||w||^2 0.10119206386205455
||w|| 0.2424211647525017
exp ma of ||w|| 0.29814571733260553
||w||^2 0.1432850962953705
exp ma of ||w||^2 0.22631508040813245
||w|| 0.37853017884360357
exp ma of ||w|| 0.44919801374377366
||w||^2 0.490280415241941
exp ma of ||w||^2 0.2978587785493953
||w|| 0.7002002679533484
exp ma of ||w|| 0.508769215470812
||w||^2 0.7770466115495183
exp ma of ||w||^2 0.31842645787010165
||w|| 0.8815024739327271
exp ma of ||w|| 0.5348134734701229
||w||^2 0.3674914925483402
exp ma of ||w||^2 0.32640530649417626
||w|| 0.6062107657806319
exp ma of ||w|| 0.5364763645876751
||w||^2 0.0831552095252611
exp ma of ||w||^2 0.40424015287769144
||w|| 0.28836645006876427
exp ma of ||w|| 0.5926017528724395
cuda
Objective function 8.14 = squared loss an data 7.44 + 0.5*rho*h**2 0.235300 + alpha*h 0.179664 + L2reg 0.22 + L1reg 0.06 ; SHD = 11 ; DAG True
Proportion of microbatches that were clipped  0.7385538364382697
iteration 2 in inner loop, alpha 0.8281975371006602 rho 10.0 h 0.2169333900233532
1210
cuda
Objective function 10.26 = squared loss an data 7.44 + 0.5*rho*h**2 2.353005 + alpha*h 0.179664 + L2reg 0.22 + L1reg 0.06 ; SHD = 11 ; DAG True
||w||^2 555690328.0913182
exp ma of ||w||^2 3006440989.240571
||w|| 23573.084823402267
exp ma of ||w|| 48972.68915911154
||w||^2 56510387.38822587
exp ma of ||w||^2 295801275.3973049
||w|| 7517.339116218309
exp ma of ||w|| 15210.515736012872
||w||^2 84534.47413416352
exp ma of ||w||^2 753587.5335937851
||w|| 290.7481283416344
exp ma of ||w|| 612.8157838132031
||w||^2 0.1000243965466657
exp ma of ||w||^2 3.947802922668375
||w|| 0.316266337991677
exp ma of ||w|| 0.4760344538582439
||w||^2 0.0588070952236031
exp ma of ||w||^2 0.19402124446579494
||w|| 0.24250174272281652
exp ma of ||w|| 0.4082171121925966
||w||^2 0.1636473847827096
exp ma of ||w||^2 0.27171228655048185
||w|| 0.40453353975005535
exp ma of ||w|| 0.49109334595350174
||w||^2 0.4325249267602603
exp ma of ||w||^2 0.41383219700284013
||w|| 0.6576662730901291
exp ma of ||w|| 0.6043571481484773
cuda
Objective function 8.49 = squared loss an data 7.96 + 0.5*rho*h**2 0.182202 + alpha*h 0.049995 + L2reg 0.25 + L1reg 0.05 ; SHD = 12 ; DAG True
Proportion of microbatches that were clipped  0.7435458786936237
iteration 3 in inner loop, alpha 0.8281975371006602 rho 100.0 h 0.06036582619215736
iteration 2 in outer loop, alpha = 6.864780156316396, rho = 100.0, h = 0.06036582619215736
cuda
1210
cuda
Objective function 8.86 = squared loss an data 7.96 + 0.5*rho*h**2 0.182202 + alpha*h 0.414398 + L2reg 0.25 + L1reg 0.05 ; SHD = 12 ; DAG True
||w||^2 3669951003.231557
exp ma of ||w||^2 3297915239.959052
||w|| 60580.1205283677
exp ma of ||w|| 52877.502582814275
||w||^2 172763174.39211762
exp ma of ||w||^2 709365933.9767003
||w|| 13143.940596035787
exp ma of ||w|| 23570.791384254993
||w||^2 58587453.34134333
exp ma of ||w||^2 550682199.1313969
||w|| 7654.244139125909
exp ma of ||w|| 20640.523416971602
||w||^2 44299384.9208831
exp ma of ||w||^2 119563115.00933498
||w|| 6655.778310677354
exp ma of ||w|| 9425.569316201574
||w||^2 0.15001558488693822
exp ma of ||w||^2 15.590338531655414
||w|| 0.387318454100677
exp ma of ||w|| 0.6875999273219587
v before min max tensor([[ 2.021e+01,  1.798e+00, -2.774e+00,  9.851e-01,  5.460e+00,  3.176e+00,
         -5.101e-01,  5.798e+00, -6.531e-01, -2.392e+00],
        [-4.722e+00,  2.471e+01, -3.643e-02, -8.092e-01, -3.704e+00,  1.042e+00,
         -3.047e+00,  3.034e+00,  5.288e+00, -1.328e+00],
        [-4.203e+00,  4.201e-01, -3.379e+00, -9.719e-01,  2.833e+00, -2.085e+00,
         -3.869e+00, -2.017e+00, -4.111e-01, -3.755e+00],
        [ 1.023e+00, -7.181e-01, -2.174e-01,  4.948e-01, -3.099e+00, -3.419e+00,
          3.323e-01,  3.645e+00, -1.437e+00,  7.060e+00],
        [ 2.302e+01,  6.500e+01,  3.174e-01, -2.293e+00,  4.727e+00,  8.169e+00,
          2.339e-01, -1.960e+00,  1.308e+00, -3.035e+00],
        [ 1.548e+01,  2.193e+01, -1.648e+00, -3.411e+00, -4.081e+00, -2.210e+00,
         -5.196e-01, -3.883e+00, -3.043e-02,  3.418e+00],
        [ 5.564e+01, -2.858e+00,  7.608e+00, -6.259e-01, -6.211e-01, -4.261e+00,
         -1.616e+00,  4.840e+00, -1.016e+00,  7.108e+00],
        [-4.429e+00,  2.986e-01,  4.897e+00, -9.321e-01,  8.987e+00,  1.863e+01,
          9.680e+00, -2.846e+00, -2.938e+00,  7.739e+00],
        [ 3.409e+01, -2.667e+00, -1.070e+00, -4.565e+00,  1.908e+01, -2.362e+00,
         -3.015e+00, -3.582e+00, -3.038e+00,  4.449e+00],
        [ 4.297e+01,  4.324e+01, -2.161e+00, -2.562e+00, -1.646e+00, -1.605e+00,
          4.166e+00, -2.979e+00,  1.172e+01, -3.367e+00],
        [-3.764e+00, -2.227e+00, -4.448e+00, -2.247e+00,  7.119e+00, -2.914e+00,
          7.396e-01, -4.752e+00, -9.341e-01,  2.105e-01],
        [-3.004e+00, -4.738e+00, -2.911e+00,  1.179e+01,  2.602e+01, -1.882e+00,
          2.779e-02,  3.314e-01,  4.019e-01, -1.042e+00],
        [ 2.319e+00,  3.918e+01, -2.981e+00, -2.740e-01, -2.027e+00, -3.186e+00,
         -1.988e+00, -1.789e+00, -1.553e+00, -2.123e+00],
        [ 9.530e+00,  1.528e+01,  4.664e+00, -3.044e+00,  1.225e+01, -2.001e+00,
         -1.857e-01,  5.302e+00, -2.782e+00, -6.720e-01],
        [-9.102e-01,  6.676e+01,  6.040e+00,  4.276e+00, -1.449e+00,  2.598e+00,
         -4.026e+00, -1.932e+00, -2.619e+00,  9.785e-01],
        [ 3.328e+00,  2.587e+01, -2.393e+00,  5.480e+00,  9.432e+00,  2.478e+00,
         -8.401e-02, -3.049e+00, -3.712e+00,  1.495e-01],
        [ 1.360e+01,  4.649e+01,  1.257e+01, -2.012e-01,  2.143e-01,  3.759e+00,
          1.163e+01,  1.028e+01,  7.650e-01,  4.737e+00],
        [-3.036e+00, -3.740e+00,  4.344e+00, -1.038e+00,  1.041e+01, -3.965e+00,
         -2.948e+00,  6.627e+00, -1.301e+00, -2.724e+00],
        [ 4.035e+00,  2.983e+00, -4.272e+00, -1.630e-01,  1.056e+01, -1.856e+00,
         -3.473e+00, -2.966e+00,  1.921e+01,  3.522e+00],
        [ 1.329e+01, -8.806e-01, -3.758e-01,  2.991e+01,  3.916e+00,  6.129e+00,
         -4.349e+00, -1.252e-01,  4.236e+00, -3.404e+00],
        [ 5.524e+00, -2.829e+00,  1.375e+00, -4.960e-01, -1.254e+00, -3.740e+00,
         -4.244e-01,  1.897e+01,  1.461e-01, -3.930e+00],
        [-3.268e+00,  1.038e+01,  3.983e+00, -1.457e+00,  1.432e+01, -1.913e+00,
          4.687e+00, -3.874e+00, -2.008e+00,  3.650e+01],
        [-1.582e+00,  1.094e+00,  1.443e+01,  5.680e+00,  5.182e+00,  3.313e+00,
          4.267e+00,  7.103e+00, -9.756e-01, -5.321e-01],
        [-2.544e+00,  5.969e+00,  9.847e+01,  5.287e+00,  1.079e+01,  2.499e+00,
          2.114e+00, -3.639e-01,  8.224e-01,  1.625e+01],
        [ 1.998e+00, -3.542e+00,  2.813e+01,  2.808e+00,  2.923e+00, -1.972e+00,
          5.148e-01, -9.521e-01, -2.085e+00, -2.609e+00],
        [ 2.182e+00,  5.240e+00,  2.737e+00,  1.799e+01, -3.150e+00,  1.314e+01,
         -2.093e+00, -2.330e+00,  3.262e-01, -2.769e+00],
        [ 2.101e+01, -1.372e+00,  8.985e+00, -4.372e+00,  2.301e+01, -4.215e+00,
         -3.045e+00, -2.201e+00, -3.537e+00, -3.556e+00],
        [ 8.758e+00, -4.169e+00,  1.925e+01, -2.355e+00, -2.967e+00, -2.133e+00,
          1.248e+01, -2.710e+00, -3.075e+00, -1.309e+00],
        [-2.142e+00, -8.653e-01, -3.157e+00, -2.215e+00, -1.400e+00, -1.553e+00,
          8.190e+00,  6.217e-01,  1.658e+00, -4.105e+00],
        [-1.804e+00, -3.058e+00,  3.166e+01,  2.326e+01, -2.547e+00, -3.005e+00,
          2.205e+00, -5.223e-01, -1.061e+00, -1.524e+00],
        [ 1.173e+01,  2.412e+00,  2.454e+00,  3.518e+01,  3.039e+00,  1.514e+00,
         -1.538e+00,  3.501e+01, -9.476e-01, -2.559e+00],
        [ 7.289e+00, -3.252e+00,  1.391e+01,  1.366e+01,  1.684e+01,  1.440e+01,
          4.361e+01,  7.266e+00,  5.200e+00, -2.359e+00],
        [-3.994e+00,  3.499e+00,  1.252e+00,  6.970e+00, -2.207e+00,  6.226e+01,
         -1.662e+00,  5.339e-03,  7.455e+00,  7.993e-01],
        [-3.504e+00, -1.440e+00, -2.294e+00, -2.108e+00, -1.024e+00, -4.225e+00,
          7.450e+00, -1.990e+00,  1.783e-02, -2.356e+00],
        [-1.526e+00, -1.618e+00,  9.567e+00,  1.227e+00, -1.350e-01,  2.740e+00,
         -5.348e-01,  8.217e+00, -1.457e+00, -2.518e+00],
        [ 1.982e+01,  8.468e+00,  2.502e+00,  9.832e+00,  6.897e+01,  6.926e-01,
          3.324e-01,  1.945e+00, -1.999e+00, -3.641e+00],
        [-3.942e+00,  5.192e+00,  4.694e+01, -1.363e+00, -2.746e+00,  1.585e+01,
         -2.123e+00, -1.163e+00, -5.745e-01,  6.887e+00],
        [ 2.804e+00, -4.759e+00,  2.512e+01,  6.792e+00, -3.794e+00,  1.521e+01,
          8.964e-01, -2.102e+00,  1.485e+01,  1.398e+00],
        [-3.493e+00,  6.215e+00,  1.035e+01,  2.998e+01, -4.573e+00,  8.258e+00,
         -3.520e+00, -1.778e+00,  5.436e+00, -4.789e-01],
        [-1.536e+00,  2.722e+01,  4.023e+01, -3.356e+00, -1.219e+00,  4.290e+00,
          1.507e-01, -4.405e+00,  2.373e+00, -3.842e+00],
        [-3.104e+00, -2.037e+00, -8.470e-01, -9.946e-01,  2.170e+01,  7.429e+00,
         -4.628e-01, -7.257e-01,  1.938e+00, -1.840e+00],
        [ 1.241e+01, -2.868e+00, -2.750e-01,  1.515e+01,  9.813e+01,  8.069e-01,
          4.465e+00, -3.460e+00, -1.736e+00, -1.339e+00],
        [ 1.171e+00,  1.837e+00, -2.298e+00,  1.047e+00,  4.395e+01, -2.255e+00,
         -1.541e+00, -4.006e+00, -2.391e-01,  4.964e+00],
        [ 9.713e+00, -8.430e-01, -2.294e+00, -3.727e+00,  1.521e+01,  7.263e+00,
          4.531e-01, -3.171e+00,  1.558e+00,  5.220e+00],
        [-1.547e+00, -2.737e+00,  7.710e-01,  1.130e+01, -1.988e+00, -2.598e+00,
          1.188e+01, -1.343e+00, -2.829e+00, -2.623e-01],
        [-2.574e+00, -6.586e-01, -3.938e+00, -2.278e+00, -3.121e+00,  6.840e-01,
          4.038e+00,  1.113e+01,  9.380e-01,  2.143e+00],
        [ 1.241e+01, -3.362e+00, -1.401e+00, -3.340e+00,  1.627e+01, -4.578e+00,
         -1.249e+00, -5.116e-01,  3.931e+00,  8.379e+00],
        [-4.238e+00, -9.137e-01, -1.431e-02, -1.234e-01, -4.717e-01,  3.653e-01,
         -1.324e-01, -2.878e+00,  5.854e-01,  4.446e+00],
        [-2.904e+00, -2.705e+00, -1.936e+00, -4.008e+00,  3.881e+01, -1.463e-01,
         -3.126e+00, -1.978e-01, -2.003e+00,  1.276e+01],
        [-9.514e-01, -1.738e-01, -4.295e+00, -1.089e+00, -5.873e-02, -3.622e+00,
         -1.758e+00,  9.091e+00, -2.266e+00, -4.133e+00],
        [-3.627e+00, -1.912e-01, -2.512e+00,  1.918e+01,  1.810e+00,  2.150e+01,
         -3.567e+00, -4.031e+00, -2.497e-01, -1.848e+00],
        [-1.531e+00, -1.150e+00,  3.544e+01,  1.949e+01,  1.270e+01, -1.011e+00,
          4.624e+00, -2.837e+00, -1.245e-01, -2.999e+00],
        [ 9.909e-01,  1.384e+01, -3.465e+00, -2.667e+00,  4.300e+00,  7.446e+00,
         -1.660e+00, -2.857e+00, -2.639e+00, -4.698e-01],
        [-4.035e+00, -2.714e+00,  4.191e+00, -3.218e+00,  2.416e+01,  2.883e+01,
         -4.275e+00, -1.106e+00, -2.240e+00,  7.424e+00],
        [ 1.760e+00,  6.643e+00,  2.885e+00,  2.587e+00,  3.737e+00, -1.586e+00,
         -2.062e+00,  3.366e-01, -2.920e-01, -7.196e-01],
        [-3.712e+00, -3.139e+00,  4.139e+00,  1.284e+00,  1.131e-01,  3.403e+01,
         -2.914e+00, -2.066e+00,  2.194e+00,  2.977e+01],
        [-2.190e+00,  1.526e+00,  5.154e+00, -1.734e+00,  3.958e-01,  4.524e+01,
         -4.392e+00,  2.472e+01, -1.991e+00, -3.389e+00],
        [-1.422e+00, -1.479e+00,  4.053e+01,  5.781e+00, -2.369e+00,  2.638e+01,
          2.047e+00,  2.910e+00, -5.384e-01, -2.358e+00],
        [-4.099e+00,  1.468e+01,  2.512e+00,  2.053e-01, -5.939e-01,  1.984e+01,
         -3.367e+00,  3.026e+00,  2.741e+01,  3.874e+00],
        [-8.815e-01, -3.449e-01,  1.393e+00,  1.104e+00,  2.337e+00,  6.560e+00,
          4.710e+00,  5.323e+00, -4.002e+00,  4.652e+00],
        [ 2.935e+00, -4.240e-01, -5.501e-01,  2.173e+00, -4.154e-01, -2.837e-01,
         -9.086e-01, -1.363e+00,  1.818e+01, -4.374e-01],
        [-6.646e-01,  1.801e-01,  8.986e+00,  1.222e+01, -2.785e+00,  1.893e-01,
          4.196e+00, -7.478e-01,  2.561e+00,  7.392e+00],
        [ 4.285e-01, -9.954e-01, -3.874e+00, -1.412e+00,  1.125e+01, -3.220e+00,
          1.260e+02, -5.616e-01,  8.215e+00,  9.006e+00],
        [-4.452e+00,  1.519e+00,  3.823e-03,  9.447e-01, -2.534e+00, -2.262e+00,
         -1.966e+00, -2.406e+00,  5.815e-01,  6.517e+00],
        [-6.183e-01,  4.503e-01,  2.082e+00, -2.844e+00, -8.917e-01, -8.628e-01,
         -1.791e+00, -3.476e+00, -1.640e+00,  9.410e+00],
        [-2.756e+00, -4.002e+00, -1.392e+00,  2.985e-01, -5.495e+00,  3.304e+00,
          1.930e+01,  3.764e+01,  8.192e-01, -7.025e-01],
        [-3.888e+00, -8.853e-01,  1.129e+01,  6.604e+00,  6.476e+00,  1.140e+00,
         -3.388e+00, -3.522e+00, -4.024e+00, -2.962e+00],
        [ 2.240e+00, -2.861e+00, -7.360e-01,  1.593e+00,  6.458e+00,  3.789e+00,
          2.243e+01, -3.036e+00, -1.397e+00,  8.553e+00],
        [-7.100e-01,  3.923e+00, -3.606e+00,  1.060e+01, -2.756e+00,  1.888e+01,
         -2.832e+00,  5.051e+00, -3.604e+00,  1.820e+00],
        [-8.148e-01, -6.258e-01, -3.391e+00, -2.701e+00, -3.224e+00,  1.612e+01,
          1.024e+02, -4.302e+00, -2.155e+00,  2.653e+00],
        [-1.792e+00,  1.037e+01,  6.349e+00,  1.972e-01,  1.311e-02,  1.948e+01,
          1.823e-01,  7.637e+00, -3.117e+00, -3.407e+00],
        [ 1.155e-01, -3.097e+00,  5.082e+00, -3.569e+00, -2.807e+00,  2.135e+01,
          7.406e+00,  7.485e+00,  1.334e+01, -4.272e+00],
        [-8.516e-01, -1.347e+00,  4.248e+00, -2.815e+00, -3.972e+00, -2.562e+00,
          1.622e+01,  8.651e+00,  3.420e+00, -9.521e-01],
        [-4.559e+00, -2.154e+00, -2.617e+00,  2.491e+00,  6.017e-01,  8.918e+00,
          2.522e+00,  5.741e+01,  4.051e+00, -3.902e+00],
        [-1.127e+00, -6.623e-02, -3.204e+00, -3.262e+00,  1.292e+00,  4.263e-01,
         -6.453e-01, -4.426e+00, -4.396e+00,  4.175e+00],
        [-5.105e+00, -1.751e+00,  1.474e+01,  3.713e-01,  1.473e+00,  2.302e+01,
          8.578e+00,  5.934e+00, -4.749e+00, -6.312e-04],
        [-2.052e+00, -3.829e+00,  9.510e+00, -1.082e+00,  3.500e+00, -3.716e+00,
          3.071e+00,  1.447e+00,  2.622e+00,  1.358e+00],
        [-2.797e+00, -1.028e+00, -3.482e+00, -2.449e+00, -3.773e+00, -1.635e+00,
          2.493e+00,  8.070e-01,  9.365e-02, -6.679e-01],
        [-1.546e+00, -4.396e+00, -3.767e+00, -3.152e+00, -3.437e+00,  8.530e+00,
         -1.401e+00,  1.404e+00,  7.207e+00, -8.801e-01],
        [-4.000e+00, -2.116e+00, -4.592e-01,  2.744e+01, -3.720e-02,  1.629e+00,
          1.890e+00,  4.240e+01,  8.020e+00, -3.862e+00],
        [ 4.704e+00, -3.438e+00, -2.605e+00, -1.953e+00, -2.268e-01,  3.035e+00,
         -3.237e+00, -1.494e+00, -4.299e+00,  7.568e-01],
        [ 1.459e+01,  2.688e+02, -4.871e+00, -2.424e+00, -4.311e+00,  1.773e+00,
         -3.159e+00, -3.205e-01,  7.884e+00, -2.809e+00],
        [ 4.137e+00,  4.742e+01, -3.564e+00,  1.811e+00,  6.066e+00,  4.774e+00,
          1.919e+01,  4.084e+00, -2.305e+00,  2.533e+00],
        [ 2.235e+01, -3.497e+00, -2.182e+00,  1.437e+01,  1.758e+01, -2.956e+00,
         -1.370e+00,  2.797e+00,  1.085e+01, -3.883e+00],
        [-3.408e+00,  1.056e+01, -4.041e+00,  2.956e+00, -4.249e+00, -4.241e+00,
         -2.588e+00, -3.085e+00, -4.057e+00, -3.831e+00],
        [-2.618e+00, -3.685e+00, -3.759e+00, -1.171e+00,  8.103e+00, -5.370e-02,
         -3.427e+00,  6.631e-01,  5.126e+01,  1.590e+01],
        [ 5.273e+00, -1.277e-01,  3.059e+01, -1.467e+00,  3.271e+00,  1.126e+00,
          3.120e-01, -2.787e+00, -4.846e+00,  2.254e+01],
        [ 1.182e+01, -4.881e-01, -1.378e+00,  9.754e+00, -1.414e+00,  1.914e+00,
         -3.529e+00,  1.122e+01,  1.752e+01, -4.311e-01],
        [-3.632e+00,  1.948e+01, -1.376e+00, -1.785e+00,  3.372e+00,  4.267e+01,
          4.437e+00,  3.043e+00,  4.077e+00,  3.799e+00],
        [-8.022e-01,  1.194e+00,  8.986e+00, -1.374e+00,  1.485e+01,  3.754e-01,
          8.020e+00, -2.764e+00, -2.063e+00, -2.509e+00],
        [-3.225e+00,  1.363e+01, -3.745e+00, -3.862e+00, -1.243e-03,  2.716e+00,
          5.978e-01,  2.521e+00, -1.335e+00,  1.093e+00],
        [-4.432e+00, -3.219e+00, -2.584e+00,  3.562e-01,  2.907e+01,  3.676e+00,
         -2.212e+00, -3.945e+00, -2.845e+00,  2.016e+01],
        [-7.437e-01, -3.531e+00,  7.077e+00, -1.902e+00, -7.946e-01, -2.421e+00,
         -1.488e+00, -3.198e+00,  2.146e+01,  7.505e-02],
        [-2.452e+00,  8.783e+00, -2.315e+00,  5.096e+00, -1.114e+00, -5.883e-01,
          3.256e+00, -2.235e+00,  2.048e+01,  3.042e+01],
        [ 2.933e+00,  4.512e+00, -1.898e+00,  8.794e+00, -3.049e+00, -1.966e+00,
         -2.305e+00,  3.487e+00, -2.002e+00, -4.783e+00],
        [ 2.168e+00,  7.463e+01, -1.443e+00, -2.794e+00, -3.134e+00,  9.785e-01,
         -2.023e+00, -3.464e+00,  6.920e-01,  1.308e+00],
        [ 2.500e+01, -2.629e+00,  4.041e+00,  1.918e+00, -3.698e+00, -1.872e+00,
          1.770e+01, -7.912e-01, -1.012e-02, -4.210e+00],
        [-3.093e+00, -1.660e-01, -2.737e+00, -2.515e+00,  6.861e+00,  3.187e+01,
          1.823e+01, -2.652e+00,  4.174e+00, -2.906e-01],
        [ 1.649e+00,  7.241e+00, -2.811e+00, -7.925e-01, -3.135e+00, -2.624e+00,
          2.094e+01,  3.192e+00,  1.491e+00, -2.431e+00],
        [-2.542e+00, -5.232e-01,  7.696e-01,  2.096e+00,  1.387e+00,  6.147e+00,
         -3.294e+00,  1.006e+00, -3.795e+00,  3.645e+01]], device='cuda:0')
v tensor([[1.000e+01, 1.798e+00, 1.000e-12, 9.851e-01, 5.460e+00, 3.176e+00,
         1.000e-12, 5.798e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.042e+00,
         1.000e-12, 3.034e+00, 5.288e+00, 1.000e-12],
        [1.000e-12, 4.201e-01, 1.000e-12, 1.000e-12, 2.833e+00, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.023e+00, 1.000e-12, 1.000e-12, 4.948e-01, 1.000e-12, 1.000e-12,
         3.323e-01, 3.645e+00, 1.000e-12, 7.060e+00],
        [1.000e+01, 1.000e+01, 3.174e-01, 1.000e-12, 4.727e+00, 8.169e+00,
         2.339e-01, 1.000e-12, 1.308e+00, 1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 3.418e+00],
        [1.000e+01, 1.000e-12, 7.608e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 4.840e+00, 1.000e-12, 7.108e+00],
        [1.000e-12, 2.986e-01, 4.897e+00, 1.000e-12, 8.987e+00, 1.000e+01,
         9.680e+00, 1.000e-12, 1.000e-12, 7.739e+00],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 4.449e+00],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         4.166e+00, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 7.119e+00, 1.000e-12,
         7.396e-01, 1.000e-12, 1.000e-12, 2.105e-01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         2.779e-02, 3.314e-01, 4.019e-01, 1.000e-12],
        [2.319e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [9.530e+00, 1.000e+01, 4.664e+00, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 5.302e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 6.040e+00, 4.276e+00, 1.000e-12, 2.598e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 9.785e-01],
        [3.328e+00, 1.000e+01, 1.000e-12, 5.480e+00, 9.432e+00, 2.478e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.495e-01],
        [1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 2.143e-01, 3.759e+00,
         1.000e+01, 1.000e+01, 7.650e-01, 4.737e+00],
        [1.000e-12, 1.000e-12, 4.344e+00, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 6.627e+00, 1.000e-12, 1.000e-12],
        [4.035e+00, 2.983e+00, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 3.522e+00],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 3.916e+00, 6.129e+00,
         1.000e-12, 1.000e-12, 4.236e+00, 1.000e-12],
        [5.524e+00, 1.000e-12, 1.375e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e+01, 1.461e-01, 1.000e-12],
        [1.000e-12, 1.000e+01, 3.983e+00, 1.000e-12, 1.000e+01, 1.000e-12,
         4.687e+00, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.094e+00, 1.000e+01, 5.680e+00, 5.182e+00, 3.313e+00,
         4.267e+00, 7.103e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 5.969e+00, 1.000e+01, 5.287e+00, 1.000e+01, 2.499e+00,
         2.114e+00, 1.000e-12, 8.224e-01, 1.000e+01],
        [1.998e+00, 1.000e-12, 1.000e+01, 2.808e+00, 2.923e+00, 1.000e-12,
         5.148e-01, 1.000e-12, 1.000e-12, 1.000e-12],
        [2.182e+00, 5.240e+00, 2.737e+00, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 3.262e-01, 1.000e-12],
        [1.000e+01, 1.000e-12, 8.985e+00, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [8.758e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         8.190e+00, 6.217e-01, 1.658e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
         2.205e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e+01, 2.412e+00, 2.454e+00, 1.000e+01, 3.039e+00, 1.514e+00,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [7.289e+00, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
         1.000e+01, 7.266e+00, 5.200e+00, 1.000e-12],
        [1.000e-12, 3.499e+00, 1.252e+00, 6.970e+00, 1.000e-12, 1.000e+01,
         1.000e-12, 5.339e-03, 7.455e+00, 7.993e-01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         7.450e+00, 1.000e-12, 1.783e-02, 1.000e-12],
        [1.000e-12, 1.000e-12, 9.567e+00, 1.227e+00, 1.000e-12, 2.740e+00,
         1.000e-12, 8.217e+00, 1.000e-12, 1.000e-12],
        [1.000e+01, 8.468e+00, 2.502e+00, 9.832e+00, 1.000e+01, 6.926e-01,
         3.324e-01, 1.945e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 5.192e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 6.887e+00],
        [2.804e+00, 1.000e-12, 1.000e+01, 6.792e+00, 1.000e-12, 1.000e+01,
         8.964e-01, 1.000e-12, 1.000e+01, 1.398e+00],
        [1.000e-12, 6.215e+00, 1.000e+01, 1.000e+01, 1.000e-12, 8.258e+00,
         1.000e-12, 1.000e-12, 5.436e+00, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 4.290e+00,
         1.507e-01, 1.000e-12, 2.373e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 7.429e+00,
         1.000e-12, 1.000e-12, 1.938e+00, 1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 8.069e-01,
         4.465e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.171e+00, 1.837e+00, 1.000e-12, 1.047e+00, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 4.964e+00],
        [9.713e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 7.263e+00,
         4.531e-01, 1.000e-12, 1.558e+00, 5.220e+00],
        [1.000e-12, 1.000e-12, 7.710e-01, 1.000e+01, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 6.840e-01,
         4.038e+00, 1.000e+01, 9.380e-01, 2.143e+00],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 3.931e+00, 8.379e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.653e-01,
         1.000e-12, 1.000e-12, 5.854e-01, 4.446e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 9.091e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.810e+00, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
         4.624e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [9.909e-01, 1.000e+01, 1.000e-12, 1.000e-12, 4.300e+00, 7.446e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 4.191e+00, 1.000e-12, 1.000e+01, 1.000e+01,
         1.000e-12, 1.000e-12, 1.000e-12, 7.424e+00],
        [1.760e+00, 6.643e+00, 2.885e+00, 2.587e+00, 3.737e+00, 1.000e-12,
         1.000e-12, 3.366e-01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 4.139e+00, 1.284e+00, 1.131e-01, 1.000e+01,
         1.000e-12, 1.000e-12, 2.194e+00, 1.000e+01],
        [1.000e-12, 1.526e+00, 5.154e+00, 1.000e-12, 3.958e-01, 1.000e+01,
         1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 5.781e+00, 1.000e-12, 1.000e+01,
         2.047e+00, 2.910e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 2.512e+00, 2.053e-01, 1.000e-12, 1.000e+01,
         1.000e-12, 3.026e+00, 1.000e+01, 3.874e+00],
        [1.000e-12, 1.000e-12, 1.393e+00, 1.104e+00, 2.337e+00, 6.560e+00,
         4.710e+00, 5.323e+00, 1.000e-12, 4.652e+00],
        [2.935e+00, 1.000e-12, 1.000e-12, 2.173e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.801e-01, 8.986e+00, 1.000e+01, 1.000e-12, 1.893e-01,
         4.196e+00, 1.000e-12, 2.561e+00, 7.392e+00],
        [4.285e-01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
         1.000e+01, 1.000e-12, 8.215e+00, 9.006e+00],
        [1.000e-12, 1.519e+00, 3.823e-03, 9.447e-01, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 5.815e-01, 6.517e+00],
        [1.000e-12, 4.503e-01, 2.082e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 9.410e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 2.985e-01, 1.000e-12, 3.304e+00,
         1.000e+01, 1.000e+01, 8.192e-01, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e+01, 6.604e+00, 6.476e+00, 1.140e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [2.240e+00, 1.000e-12, 1.000e-12, 1.593e+00, 6.458e+00, 3.789e+00,
         1.000e+01, 1.000e-12, 1.000e-12, 8.553e+00],
        [1.000e-12, 3.923e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
         1.000e-12, 5.051e+00, 1.000e-12, 1.820e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
         1.000e+01, 1.000e-12, 1.000e-12, 2.653e+00],
        [1.000e-12, 1.000e+01, 6.349e+00, 1.972e-01, 1.311e-02, 1.000e+01,
         1.823e-01, 7.637e+00, 1.000e-12, 1.000e-12],
        [1.155e-01, 1.000e-12, 5.082e+00, 1.000e-12, 1.000e-12, 1.000e+01,
         7.406e+00, 7.485e+00, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e-12, 4.248e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 8.651e+00, 3.420e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 2.491e+00, 6.017e-01, 8.918e+00,
         2.522e+00, 1.000e+01, 4.051e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.292e+00, 4.263e-01,
         1.000e-12, 1.000e-12, 1.000e-12, 4.175e+00],
        [1.000e-12, 1.000e-12, 1.000e+01, 3.713e-01, 1.473e+00, 1.000e+01,
         8.578e+00, 5.934e+00, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 9.510e+00, 1.000e-12, 3.500e+00, 1.000e-12,
         3.071e+00, 1.447e+00, 2.622e+00, 1.358e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         2.493e+00, 8.070e-01, 9.365e-02, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 8.530e+00,
         1.000e-12, 1.404e+00, 7.207e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.629e+00,
         1.890e+00, 1.000e+01, 8.020e+00, 1.000e-12],
        [4.704e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.035e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 7.568e-01],
        [1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.773e+00,
         1.000e-12, 1.000e-12, 7.884e+00, 1.000e-12],
        [4.137e+00, 1.000e+01, 1.000e-12, 1.811e+00, 6.066e+00, 4.774e+00,
         1.000e+01, 4.084e+00, 1.000e-12, 2.533e+00],
        [1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
         1.000e-12, 2.797e+00, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 2.956e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 8.103e+00, 1.000e-12,
         1.000e-12, 6.631e-01, 1.000e+01, 1.000e+01],
        [5.273e+00, 1.000e-12, 1.000e+01, 1.000e-12, 3.271e+00, 1.126e+00,
         3.120e-01, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12, 9.754e+00, 1.000e-12, 1.914e+00,
         1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 3.372e+00, 1.000e+01,
         4.437e+00, 3.043e+00, 4.077e+00, 3.799e+00],
        [1.000e-12, 1.194e+00, 8.986e+00, 1.000e-12, 1.000e+01, 3.754e-01,
         8.020e+00, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 2.716e+00,
         5.978e-01, 2.521e+00, 1.000e-12, 1.093e+00],
        [1.000e-12, 1.000e-12, 1.000e-12, 3.562e-01, 1.000e+01, 3.676e+00,
         1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01],
        [1.000e-12, 1.000e-12, 7.077e+00, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e-12, 1.000e-12, 1.000e+01, 7.505e-02],
        [1.000e-12, 8.783e+00, 1.000e-12, 5.096e+00, 1.000e-12, 1.000e-12,
         3.256e+00, 1.000e-12, 1.000e+01, 1.000e+01],
        [2.933e+00, 4.512e+00, 1.000e-12, 8.794e+00, 1.000e-12, 1.000e-12,
         1.000e-12, 3.487e+00, 1.000e-12, 1.000e-12],
        [2.168e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 9.785e-01,
         1.000e-12, 1.000e-12, 6.920e-01, 1.308e+00],
        [1.000e+01, 1.000e-12, 4.041e+00, 1.918e+00, 1.000e-12, 1.000e-12,
         1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 6.861e+00, 1.000e+01,
         1.000e+01, 1.000e-12, 4.174e+00, 1.000e-12],
        [1.649e+00, 7.241e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
         1.000e+01, 3.192e+00, 1.491e+00, 1.000e-12],
        [1.000e-12, 1.000e-12, 7.696e-01, 2.096e+00, 1.387e+00, 6.147e+00,
         1.000e-12, 1.006e+00, 1.000e-12, 1.000e+01]], device='cuda:0')
v before min max tensor([-2.837e+00, -2.287e+00,  1.210e+01, -6.334e-02,  2.815e+00, -2.132e+00,
         2.008e-01, -1.854e+00,  6.580e+00, -2.892e+00, -2.681e+00,  1.945e+01,
        -2.281e+00,  5.518e-01, -3.480e+00, -2.740e+00,  1.556e+00,  6.644e-03,
        -8.509e-01, -1.595e+00,  6.804e+00,  2.054e+00,  1.491e-01, -4.802e+00,
        -3.834e+00,  1.686e+00, -1.200e+00, -2.284e-01, -1.976e+00,  5.724e+00,
        -4.123e+00, -2.925e+00, -1.733e+00, -3.467e+00,  1.863e+00,  1.476e+00,
         2.454e-01,  5.506e+00,  1.865e+01, -1.697e+00, -1.280e+00, -1.423e+00,
        -3.587e-01, -1.077e-01, -3.970e+00,  6.481e+01, -1.629e+00, -7.139e-01,
        -2.039e+00, -1.799e+00,  3.315e+00, -1.238e+00, -1.877e+00, -1.392e-01,
        -3.632e+00, -3.076e+00,  7.512e-01, -2.078e+00,  6.056e+00, -2.149e+00,
        -9.368e-01,  6.963e-01, -2.980e+00,  1.686e+01, -5.216e+00,  1.207e+01,
         1.006e+00,  3.386e+00, -1.983e+00,  1.177e+01, -2.647e+00, -2.305e+00,
        -6.681e-01, -4.243e+00,  8.126e-01, -2.096e+00, -2.163e+00,  2.658e+00,
         2.645e+01, -2.189e+00, -1.161e+00, -2.426e+00, -3.974e-01, -2.476e-01,
         1.025e+00, -2.404e+00,  1.421e+00, -2.303e+00, -9.512e-01,  1.628e+00,
         2.263e+01,  2.902e+00, -1.388e+00,  1.387e+00, -3.424e+00,  6.916e+00,
         1.140e+01,  7.922e-02,  1.568e+00,  5.647e+00], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 2.815e+00, 1.000e-12,
        2.008e-01, 1.000e-12, 6.580e+00, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 5.518e-01, 1.000e-12, 1.000e-12, 1.556e+00, 6.644e-03,
        1.000e-12, 1.000e-12, 6.804e+00, 2.054e+00, 1.491e-01, 1.000e-12,
        1.000e-12, 1.686e+00, 1.000e-12, 1.000e-12, 1.000e-12, 5.724e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.863e+00, 1.476e+00,
        2.454e-01, 5.506e+00, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 3.315e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 7.512e-01, 1.000e-12, 6.056e+00, 1.000e-12,
        1.000e-12, 6.963e-01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.006e+00, 3.386e+00, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 8.126e-01, 1.000e-12, 1.000e-12, 2.658e+00,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.025e+00, 1.000e-12, 1.421e+00, 1.000e-12, 1.000e-12, 1.628e+00,
        1.000e+01, 2.902e+00, 1.000e-12, 1.387e+00, 1.000e-12, 6.916e+00,
        1.000e+01, 7.922e-02, 1.568e+00, 5.647e+00], device='cuda:0')
v before min max tensor([[[ 2.952e+00],
         [-3.658e+00],
         [ 3.888e+01],
         [-1.064e+00],
         [ 5.122e+00],
         [ 1.669e+01],
         [ 1.406e+01],
         [ 3.171e+00],
         [ 4.317e-01],
         [ 1.203e+01]],

        [[ 1.843e+01],
         [-6.503e-01],
         [-2.651e+00],
         [ 5.053e+00],
         [-1.933e+00],
         [ 2.953e+01],
         [ 3.575e+01],
         [ 2.325e+01],
         [ 1.041e-01],
         [-4.675e+00]],

        [[-3.216e+00],
         [-7.080e-01],
         [ 4.382e+01],
         [ 2.701e+00],
         [-2.877e+00],
         [ 4.854e+01],
         [-2.068e+00],
         [ 3.194e+00],
         [-3.230e+00],
         [-4.342e+00]],

        [[ 5.983e+00],
         [ 1.292e+00],
         [-3.991e+00],
         [-1.038e+00],
         [-6.268e-01],
         [-2.736e+00],
         [ 4.307e+00],
         [-6.939e-01],
         [ 8.084e+00],
         [ 2.694e+01]],

        [[-1.898e+00],
         [ 6.422e+00],
         [-3.619e+00],
         [-4.434e+00],
         [-2.685e+00],
         [ 1.131e+01],
         [-2.311e+00],
         [ 1.830e-01],
         [-2.579e+00],
         [-2.529e+00]],

        [[-3.535e+00],
         [ 6.107e+00],
         [ 1.138e+00],
         [ 6.387e+00],
         [-3.118e+00],
         [ 8.158e+00],
         [-2.203e+00],
         [-3.533e+00],
         [ 5.655e+00],
         [ 1.168e+01]],

        [[-3.029e+00],
         [-1.411e+00],
         [ 3.903e+00],
         [-2.916e+00],
         [-3.212e+00],
         [-3.219e+00],
         [-3.226e+00],
         [-1.859e+00],
         [-2.231e+00],
         [ 1.973e+01]],

        [[ 1.415e+01],
         [ 2.109e+01],
         [-4.094e+00],
         [ 4.465e-01],
         [-3.227e+00],
         [ 1.444e+01],
         [-1.981e+00],
         [ 4.854e+00],
         [ 1.707e+00],
         [ 8.032e+00]],

        [[ 7.649e-01],
         [ 4.405e-02],
         [ 7.472e+00],
         [ 1.716e-02],
         [-4.087e+00],
         [ 3.741e-02],
         [-2.021e+00],
         [ 6.548e-01],
         [-5.956e-02],
         [-2.765e+00]],

        [[ 2.119e+01],
         [-2.863e+00],
         [ 1.323e+01],
         [-2.316e+00],
         [-4.457e+00],
         [ 1.083e+01],
         [ 5.745e+00],
         [ 2.530e+01],
         [ 7.458e+00],
         [ 2.302e+01]]], device='cuda:0')
v tensor([[[2.952e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [5.122e+00],
         [1.000e+01],
         [1.000e+01],
         [3.171e+00],
         [4.317e-01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [5.053e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.041e-01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [2.701e+00],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [3.194e+00],
         [1.000e-12],
         [1.000e-12]],

        [[5.983e+00],
         [1.292e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.307e+00],
         [1.000e-12],
         [8.084e+00],
         [1.000e+01]],

        [[1.000e-12],
         [6.422e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.830e-01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [6.107e+00],
         [1.138e+00],
         [6.387e+00],
         [1.000e-12],
         [8.158e+00],
         [1.000e-12],
         [1.000e-12],
         [5.655e+00],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [3.903e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [4.465e-01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [4.854e+00],
         [1.707e+00],
         [8.032e+00]],

        [[7.649e-01],
         [4.405e-02],
         [7.472e+00],
         [1.716e-02],
         [1.000e-12],
         [3.741e-02],
         [1.000e-12],
         [6.548e-01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [5.745e+00],
         [1.000e+01],
         [7.458e+00],
         [1.000e+01]]], device='cuda:0')
v before min max tensor([[ 1.323],
        [19.040],
        [ 1.530],
        [-2.738],
        [-4.138],
        [ 0.686],
        [-1.768],
        [-0.366],
        [14.101],
        [80.962]], device='cuda:0')
v tensor([[1.323e+00],
        [1.000e+01],
        [1.530e+00],
        [1.000e-12],
        [1.000e-12],
        [6.861e-01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e+01]], device='cuda:0')
a after update for 1 param tensor([[-6.242e-03, -1.491e-02, -2.661e-02, -4.109e-03, -7.858e-03,  4.634e-03,
         -3.166e-02,  1.589e-02,  2.237e-03, -1.635e-02],
        [-8.120e-04,  8.583e-03, -5.088e-03,  1.708e-03, -1.252e-02, -6.741e-03,
          4.161e-03, -1.772e-02, -7.995e-03, -1.109e-03],
        [ 1.438e-03,  6.709e-04, -3.155e-02,  1.228e-03,  1.823e-02,  1.319e-02,
         -1.565e-02,  1.410e-02,  3.678e-03,  1.444e-02],
        [ 4.032e-03,  1.198e-04,  2.365e-03,  4.224e-04,  1.320e-02,  1.071e-02,
          6.809e-03, -1.596e-03,  1.305e-02, -7.459e-04],
        [ 1.409e-02, -2.914e-02,  3.190e-03,  3.240e-03, -7.822e-03, -4.383e-03,
         -1.167e-02,  1.626e-02,  1.656e-03, -4.505e-03],
        [ 8.845e-03,  1.543e-02,  1.116e-02,  3.658e-02, -1.496e-02, -9.935e-03,
          9.540e-04, -2.101e-03,  1.055e-02, -4.837e-03],
        [-3.640e-02, -2.416e-02, -5.621e-05,  2.373e-03, -1.093e-02, -2.293e-02,
         -7.211e-03, -4.313e-03,  5.971e-03,  8.058e-03],
        [ 2.060e-02, -1.083e-02,  5.365e-03, -4.720e-03,  1.492e-02,  1.076e-02,
         -1.180e-04,  5.471e-03, -5.891e-03, -8.526e-03],
        [-7.467e-03, -5.986e-03,  1.417e-02, -1.926e-02,  1.144e-02,  4.973e-03,
          4.881e-03,  2.347e-03, -2.899e-03, -2.231e-03],
        [-8.113e-04,  1.228e-02,  6.046e-03, -8.970e-03,  3.238e-03,  1.427e-02,
          6.787e-04, -1.441e-02,  1.011e-03,  2.044e-02],
        [ 4.444e-03, -1.476e-02,  2.690e-02,  2.612e-03,  1.068e-02, -1.378e-02,
          5.864e-03,  1.677e-02, -4.609e-03,  1.191e-02],
        [-3.198e-03, -1.126e-03,  6.366e-03,  2.282e-02, -2.242e-02, -8.388e-03,
         -2.446e-02,  1.324e-02,  2.623e-02,  2.152e-03],
        [ 6.940e-03, -1.492e-02,  7.615e-03, -4.262e-04,  8.693e-03,  3.304e-03,
         -1.691e-02,  5.583e-03,  2.891e-03,  9.221e-05],
        [-8.395e-03, -4.684e-03,  9.489e-03, -1.625e-02, -2.648e-04,  2.979e-03,
          1.168e-02,  1.286e-02, -5.805e-03,  9.966e-03],
        [-4.267e-03, -1.423e-02,  5.203e-03,  1.163e-02,  3.828e-03, -9.815e-03,
         -1.720e-02, -1.302e-03,  1.614e-02, -3.765e-03],
        [ 2.696e-03, -1.692e-03, -9.502e-03,  6.834e-03,  1.385e-03,  2.126e-02,
          3.313e-04,  6.728e-03,  4.047e-03, -1.242e-02],
        [ 1.550e-02, -4.464e-03, -4.010e-03, -1.127e-02,  1.977e-03,  5.164e-04,
         -6.273e-04, -3.196e-04, -2.830e-03, -4.694e-03],
        [-2.379e-03,  4.566e-03, -1.026e-02,  1.204e-02,  2.049e-02, -7.365e-03,
          3.764e-03,  2.917e-03,  2.426e-03, -9.347e-03],
        [ 1.101e-02,  4.279e-03, -4.088e-03, -3.241e-04, -6.314e-03,  8.450e-03,
         -1.034e-02, -4.124e-02,  2.512e-02,  1.105e-02],
        [-4.748e-03,  1.131e-02, -8.823e-03, -6.788e-03, -4.317e-04,  6.619e-03,
         -3.521e-02, -2.500e-03, -1.774e-03,  6.211e-03],
        [-5.250e-03,  5.861e-04,  9.245e-03, -1.106e-02, -2.973e-03, -1.458e-02,
          8.420e-04,  3.967e-02,  1.550e-02,  1.233e-02],
        [-2.346e-02,  7.407e-03,  1.089e-02,  9.004e-03, -7.734e-04, -8.410e-03,
          5.341e-03,  6.326e-03, -9.108e-03, -1.802e-02],
        [-1.383e-02, -5.552e-03, -8.376e-04,  1.217e-03, -1.908e-02,  1.014e-03,
         -2.959e-02, -4.499e-04,  1.388e-02,  1.569e-03],
        [ 7.988e-03,  1.286e-03, -2.739e-02,  4.607e-04,  1.469e-02, -7.533e-03,
          1.165e-03, -2.249e-02, -5.562e-03,  1.165e-02],
        [ 2.914e-03, -3.267e-02, -2.801e-02,  1.931e-03,  6.240e-03,  1.864e-02,
          1.563e-03, -3.001e-03,  7.532e-03, -2.000e-02],
        [-1.898e-03,  3.630e-02,  1.083e-03,  2.675e-02, -2.722e-03, -8.357e-03,
         -7.517e-03, -1.282e-02,  1.432e-03,  9.089e-03],
        [ 7.996e-03,  5.601e-03,  5.562e-03,  1.469e-03, -6.919e-03,  1.006e-02,
         -2.109e-03,  7.016e-03,  1.164e-02, -4.224e-03],
        [-1.940e-02,  4.164e-02,  8.608e-03,  8.913e-03,  5.142e-03,  4.449e-03,
          1.307e-02, -9.580e-03,  1.521e-02,  1.790e-02],
        [ 3.715e-03, -2.758e-03, -2.487e-03, -2.342e-02,  3.974e-03,  8.591e-03,
         -1.287e-02, -1.701e-03,  1.366e-03,  2.370e-03],
        [-1.490e-03,  1.258e-02, -2.376e-02,  1.220e-02,  2.857e-02, -2.466e-03,
         -1.196e-02,  5.624e-04, -6.078e-03, -2.267e-02],
        [ 3.555e-03,  1.981e-03, -1.314e-02, -1.951e-02,  1.394e-03, -2.789e-03,
         -1.219e-02, -1.624e-02, -5.112e-03, -1.835e-03],
        [ 6.414e-03, -3.311e-03, -5.485e-03, -2.945e-03,  9.919e-04, -2.317e-02,
         -1.251e-02, -1.415e-02, -5.233e-03, -2.332e-03],
        [-1.597e-02,  1.355e-02, -3.199e-03,  5.358e-03,  7.258e-04, -2.726e-03,
         -1.627e-02, -7.090e-03,  7.133e-03, -2.034e-03],
        [-8.727e-03,  1.099e-02, -2.564e-03,  1.519e-02, -4.406e-04, -6.142e-04,
         -2.371e-03, -8.156e-03,  4.722e-03, -1.778e-02],
        [-3.363e-03,  7.364e-03, -2.100e-02,  6.560e-03, -1.064e-02, -1.551e-03,
         -1.615e-02, -7.446e-03, -2.903e-02, -8.791e-03],
        [ 4.643e-02, -6.990e-03,  3.152e-03, -6.180e-04,  1.097e-02,  4.024e-03,
          1.953e-03, -1.296e-04, -1.175e-03, -2.157e-02],
        [-5.212e-03, -6.980e-03,  3.125e-02,  4.011e-03, -6.629e-03,  1.625e-02,
          1.302e-02, -2.248e-03,  3.495e-02, -8.100e-03],
        [ 1.805e-02, -2.821e-02,  3.054e-02,  1.440e-03,  4.985e-03,  1.361e-02,
          2.589e-04,  2.396e-03, -9.558e-03, -8.871e-03],
        [-1.156e-02, -3.844e-03,  9.176e-03, -2.478e-02,  1.762e-02,  2.048e-02,
          8.430e-04,  6.875e-03, -1.079e-02,  9.532e-03],
        [ 5.427e-03, -1.632e-02, -1.403e-03,  1.079e-02, -6.102e-03, -1.175e-02,
         -9.230e-05, -1.458e-02,  4.046e-03,  1.008e-03],
        [-4.070e-03,  3.124e-03, -9.077e-03, -7.904e-04,  2.129e-02, -1.319e-02,
          6.018e-03, -7.045e-03,  1.693e-02,  2.058e-02],
        [-4.568e-03, -1.120e-03,  3.893e-03,  1.908e-02,  2.045e-03, -1.049e-02,
          1.264e-02,  1.572e-02,  1.853e-03, -3.343e-04],
        [-1.188e-03, -1.294e-02,  5.047e-03, -2.095e-04,  2.207e-04,  4.076e-02,
          4.908e-03, -2.682e-03,  4.585e-03,  3.166e-03],
        [ 1.034e-02,  7.256e-03,  3.042e-02, -4.641e-03, -1.736e-02,  2.582e-03,
          5.246e-02,  2.707e-03, -2.634e-03, -3.376e-03],
        [ 1.255e-02,  8.309e-03, -1.930e-02,  7.582e-03, -4.394e-04,  2.240e-03,
         -7.862e-04,  1.552e-02,  1.564e-02, -5.158e-03],
        [-1.183e-02, -1.163e-02,  4.077e-04, -2.622e-02, -3.251e-03,  6.570e-03,
         -1.653e-02,  1.256e-02,  2.967e-02,  6.422e-03],
        [-2.334e-02,  1.326e-03, -4.958e-04,  4.924e-03,  6.729e-03,  6.586e-03,
         -3.487e-03, -2.712e-03,  7.750e-03,  1.061e-02],
        [ 2.614e-02,  1.274e-04,  1.338e-03,  8.419e-03, -2.583e-02, -9.196e-04,
         -6.106e-03, -4.637e-03, -4.792e-03, -1.551e-03],
        [-2.025e-02,  2.334e-02,  3.490e-02,  6.403e-03, -6.969e-03, -6.582e-03,
         -2.492e-02,  1.344e-02, -1.674e-02, -1.410e-04],
        [-7.423e-03,  4.122e-03,  1.518e-02,  8.841e-04, -2.427e-02, -3.346e-02,
          4.609e-03,  1.633e-03,  5.589e-03, -5.524e-03],
        [-6.096e-03, -8.990e-04,  4.366e-03, -1.096e-02,  1.782e-03, -9.833e-03,
         -5.316e-03,  1.609e-02, -2.307e-03, -9.448e-04],
        [-1.467e-03, -1.401e-02,  5.252e-02, -2.439e-02,  1.352e-03,  5.479e-03,
          2.162e-02, -1.456e-02,  1.095e-03,  5.532e-03],
        [-1.763e-02, -6.640e-03,  1.850e-02,  4.087e-02, -1.061e-02, -2.752e-03,
          2.432e-03,  1.337e-02, -1.680e-02,  8.321e-03],
        [-1.181e-02,  6.885e-04,  3.114e-03,  1.088e-02, -9.390e-03, -1.556e-04,
         -9.682e-03,  1.139e-02, -6.138e-03, -3.621e-02],
        [-5.526e-03,  1.061e-02, -7.028e-03,  5.091e-05, -3.752e-03, -1.774e-02,
         -1.187e-02,  1.526e-02, -2.683e-04, -4.256e-03],
        [-6.133e-04, -8.556e-03, -1.034e-02, -3.324e-02,  5.515e-03, -2.144e-03,
         -3.716e-02, -1.920e-02,  5.489e-03,  5.541e-02],
        [-1.950e-03,  4.143e-03, -1.465e-02, -1.368e-03,  8.205e-03,  7.708e-03,
         -2.885e-03, -2.869e-03, -1.638e-02,  2.808e-02],
        [-3.713e-03,  1.866e-03,  1.798e-02,  3.892e-03, -1.008e-02, -5.106e-03,
          2.252e-02,  9.414e-04, -2.765e-03,  1.726e-02],
        [-1.407e-02, -2.823e-02, -1.377e-02, -1.152e-04, -2.254e-02,  9.214e-04,
          2.030e-03, -9.365e-03,  6.315e-04,  6.008e-03],
        [-2.362e-03, -3.880e-03, -6.661e-04, -5.028e-03, -1.118e-03,  2.596e-03,
         -1.544e-02,  9.829e-03,  2.647e-02, -1.363e-02],
        [-3.980e-03, -3.711e-03, -5.957e-03, -3.584e-03, -2.398e-03,  1.913e-02,
         -1.088e-03, -1.080e-02, -1.193e-02, -3.374e-02],
        [-1.792e-03,  1.284e-02,  4.838e-04,  1.576e-02,  1.033e-02, -1.766e-03,
         -2.269e-03, -9.090e-03, -4.994e-03,  6.592e-03],
        [ 3.068e-03, -6.500e-03, -2.331e-02,  5.764e-03,  1.581e-02, -1.108e-02,
         -2.931e-03, -3.080e-02, -1.907e-02, -2.961e-03],
        [ 2.827e-02,  2.083e-02,  7.388e-04, -2.044e-03,  8.738e-03,  2.294e-03,
          5.594e-03,  1.171e-02, -1.662e-02,  2.166e-03],
        [ 1.767e-03, -7.957e-03, -1.275e-03, -2.342e-02, -3.152e-03, -1.123e-02,
         -6.904e-03, -2.208e-02, -5.711e-03,  5.642e-03],
        [ 1.668e-02,  1.190e-03,  1.141e-03,  1.900e-03,  2.119e-02,  2.582e-04,
          2.419e-03,  6.818e-03,  2.247e-02,  1.957e-02],
        [-4.897e-03,  1.142e-02, -1.992e-03, -2.043e-03,  4.198e-04, -6.031e-03,
         -4.858e-04,  1.599e-02,  1.127e-02, -2.845e-02],
        [ 1.749e-02,  5.661e-03,  2.738e-02,  1.210e-02, -1.146e-02,  2.719e-03,
          1.771e-02,  1.499e-02,  8.044e-03, -8.602e-03],
        [-2.031e-04,  6.392e-03, -2.313e-02, -1.213e-02,  3.625e-03,  7.371e-03,
          2.009e-04, -2.826e-02, -6.587e-03,  1.013e-02],
        [ 3.936e-03, -1.020e-02,  8.434e-03,  7.889e-04, -6.478e-03,  7.942e-03,
         -3.478e-03, -8.981e-03, -1.777e-03,  4.693e-03],
        [ 7.495e-03,  2.416e-03, -1.351e-02, -1.601e-02, -4.081e-03,  5.856e-03,
          1.457e-03,  2.707e-03, -4.603e-03, -5.209e-03],
        [-8.755e-04, -8.667e-03, -4.735e-03,  2.864e-03,  8.992e-03,  1.032e-03,
          5.392e-03, -3.935e-03,  1.617e-02, -2.616e-02],
        [-3.338e-03, -1.949e-03, -1.313e-03, -2.076e-02, -5.205e-03,  7.563e-03,
          1.203e-02,  3.815e-03, -3.000e-02, -2.490e-04],
        [-8.604e-03,  3.660e-04, -3.164e-02, -5.554e-03,  6.910e-04,  1.596e-02,
          4.340e-04,  1.049e-02,  1.113e-02, -2.591e-03],
        [ 2.071e-02, -2.571e-02, -4.858e-03, -1.097e-02,  6.197e-04, -3.421e-03,
         -8.268e-03, -2.833e-03, -3.268e-03, -1.753e-03],
        [ 3.701e-03,  5.955e-03,  2.205e-02, -5.428e-03,  7.008e-04,  1.651e-02,
          1.519e-02,  4.302e-02,  9.870e-03,  3.019e-03],
        [-4.782e-03,  3.078e-02, -4.835e-03,  4.243e-03, -1.194e-03, -2.217e-04,
          1.744e-04,  4.507e-05,  9.599e-03,  1.158e-02],
        [-1.805e-02,  3.497e-02, -1.258e-02,  1.143e-02,  9.662e-03, -2.217e-02,
         -4.120e-03,  1.020e-02, -5.829e-05,  1.000e-03],
        [-6.271e-03,  2.226e-02,  9.192e-03,  2.564e-03,  1.071e-02,  7.982e-03,
         -2.287e-02, -4.781e-03, -9.377e-03,  1.652e-03],
        [-2.958e-02, -5.813e-03,  2.370e-03,  1.511e-03, -5.203e-04, -7.182e-03,
          6.501e-03, -7.546e-03, -6.621e-04, -4.387e-03],
        [-2.692e-02,  7.057e-03,  7.230e-03, -1.003e-03,  1.237e-03,  2.030e-04,
         -2.135e-03,  1.717e-03, -1.405e-03, -6.815e-03],
        [ 8.823e-03, -1.183e-02, -5.606e-04,  4.708e-03,  3.114e-03,  2.272e-02,
          7.244e-03, -4.734e-03,  1.067e-02,  1.445e-02],
        [-7.151e-03,  1.213e-02, -1.332e-02,  1.453e-03, -8.811e-03,  2.791e-03,
          2.889e-03,  6.733e-03,  1.800e-02, -5.875e-03],
        [ 3.207e-04,  1.780e-02,  5.312e-04, -1.426e-02,  3.342e-03,  2.247e-02,
          5.217e-03, -1.678e-02, -8.389e-03,  9.122e-03],
        [ 6.775e-03, -8.208e-03,  5.014e-03, -3.881e-04, -2.593e-04,  3.653e-02,
         -4.371e-03,  4.124e-03, -1.032e-02,  5.945e-04],
        [-7.427e-03, -7.686e-03, -1.534e-03,  4.437e-03,  6.384e-04,  1.692e-03,
         -2.565e-03,  5.296e-03, -3.802e-03,  9.117e-03],
        [ 2.202e-03,  5.132e-03, -9.698e-03, -2.653e-03, -7.696e-03, -7.753e-03,
         -8.256e-04, -4.964e-04,  2.096e-02,  7.000e-03],
        [ 2.667e-04,  6.414e-03,  3.663e-03, -1.744e-02,  6.054e-03, -2.675e-03,
         -2.213e-02, -1.450e-02, -1.223e-02, -3.476e-03],
        [ 9.735e-03,  1.104e-02, -4.437e-04, -1.803e-02,  3.526e-04, -2.693e-03,
          3.397e-03, -1.777e-02, -3.001e-03, -8.851e-03],
        [-1.151e-03,  1.546e-03,  1.735e-02, -5.894e-03,  3.286e-04,  8.066e-03,
          2.484e-03, -1.909e-02,  1.332e-02,  6.572e-03],
        [ 1.901e-02,  1.020e-02,  8.221e-04, -1.685e-02,  4.238e-03, -3.505e-02,
         -7.979e-03,  2.198e-03,  6.397e-03,  9.639e-03],
        [ 4.930e-03,  7.608e-03, -1.490e-02, -8.767e-03,  5.740e-03,  8.478e-03,
          3.504e-03,  2.385e-03, -3.065e-02, -2.331e-02],
        [-2.659e-02,  1.305e-03, -2.419e-03, -8.386e-04, -7.176e-03,  1.505e-02,
          2.463e-03, -1.242e-02,  6.892e-03,  1.149e-03],
        [ 4.229e-03, -6.305e-03,  3.693e-03,  9.899e-03, -6.578e-03, -2.536e-03,
         -3.619e-03,  2.093e-03, -1.827e-02, -1.681e-03],
        [ 2.260e-02,  6.642e-03,  9.002e-03,  7.309e-03,  7.028e-03,  1.351e-02,
          2.479e-03,  1.419e-02, -4.954e-03, -1.314e-02],
        [ 9.097e-03, -7.074e-03, -1.821e-03,  1.732e-02,  1.455e-04, -1.034e-02,
          1.843e-03, -1.394e-02,  8.063e-03,  1.175e-02],
        [ 1.242e-03,  3.263e-03, -1.462e-03,  8.769e-04,  1.637e-05, -1.188e-02,
         -9.220e-03, -2.792e-03, -6.042e-04,  3.101e-03],
        [ 4.965e-04,  7.886e-04, -7.379e-03,  1.374e-02,  8.325e-03, -4.698e-04,
         -1.060e-02,  6.099e-03,  1.052e-02, -1.946e-03],
        [-4.604e-05, -1.880e-02, -3.064e-03,  1.766e-02,  3.169e-03,  1.044e-03,
         -2.056e-02,  7.056e-03,  6.466e-03,  7.787e-03],
        [-2.579e-02,  2.425e-03, -1.253e-03, -1.953e-02, -1.614e-02, -4.065e-03,
          1.694e-02,  1.547e-02,  1.382e-02,  2.978e-03]], device='cuda:0')
s after update for 1 param tensor([[1.660, 0.929, 1.032, 0.599, 1.417, 1.836, 1.835, 1.712, 0.382, 1.736],
        [1.806, 1.802, 0.160, 1.208, 1.420, 1.436, 1.195, 1.412, 0.889, 1.402],
        [1.567, 0.232, 1.257, 0.579, 1.856, 0.989, 1.437, 0.829, 0.217, 1.394],
        [1.544, 0.285, 0.082, 1.757, 1.666, 1.475, 0.916, 0.794, 1.078, 1.702],
        [2.094, 1.918, 1.496, 0.865, 1.803, 1.491, 1.374, 1.475, 1.015, 1.615],
        [1.919, 1.240, 1.593, 1.591, 1.610, 1.113, 0.602, 1.585, 0.973, 1.176],
        [1.966, 1.707, 1.120, 0.236, 1.316, 1.640, 0.924, 1.950, 1.147, 1.823],
        [1.909, 1.648, 1.525, 1.153, 1.442, 1.481, 1.493, 1.645, 1.126, 1.214],
        [2.245, 1.236, 1.425, 1.733, 1.442, 1.249, 1.401, 1.426, 1.167, 1.114],
        [1.289, 1.984, 1.247, 1.002, 1.353, 1.824, 1.078, 1.434, 1.795, 1.437],
        [1.637, 1.127, 1.654, 1.646, 1.658, 1.089, 1.442, 1.826, 0.959, 1.299],
        [1.146, 1.821, 1.110, 2.213, 1.789, 1.263, 1.471, 1.559, 1.406, 0.410],
        [1.319, 1.531, 1.199, 0.109, 0.825, 1.476, 1.233, 0.665, 0.653, 1.390],
        [1.724, 2.000, 1.796, 1.529, 1.417, 1.363, 1.753, 1.412, 1.289, 1.100],
        [0.968, 2.099, 1.318, 1.207, 1.566, 2.076, 1.494, 1.091, 1.187, 1.084],
        [1.129, 1.902, 1.056, 0.812, 1.148, 1.161, 0.033, 1.189, 1.424, 1.468],
        [1.937, 1.327, 1.312, 1.151, 0.229, 1.232, 1.389, 1.633, 1.068, 0.771],
        [1.319, 1.390, 1.524, 1.247, 1.743, 1.601, 1.511, 1.543, 0.485, 1.319],
        [1.566, 1.862, 1.810, 0.150, 1.263, 0.863, 1.525, 1.274, 1.507, 1.423],
        [1.563, 1.528, 1.581, 1.753, 1.243, 1.664, 1.847, 0.377, 1.626, 1.304],
        [1.381, 1.107, 1.570, 0.919, 1.127, 1.464, 1.114, 1.892, 0.799, 1.557],
        [1.350, 1.388, 1.861, 0.821, 1.425, 0.896, 1.636, 1.456, 1.576, 1.506],
        [1.260, 1.473, 1.296, 0.795, 1.772, 1.522, 1.768, 1.837, 1.073, 0.306],
        [1.130, 1.668, 2.229, 1.711, 1.298, 1.224, 1.612, 1.265, 0.625, 1.482],
        [1.654, 1.876, 2.122, 1.403, 1.209, 1.025, 0.848, 1.185, 0.997, 1.908],
        [0.545, 1.752, 1.574, 1.994, 1.352, 1.599, 0.898, 1.241, 0.236, 1.032],
        [1.428, 1.245, 1.748, 1.624, 1.503, 1.568, 1.550, 1.612, 1.373, 1.438],
        [1.965, 1.568, 1.440, 0.901, 1.107, 0.815, 1.834, 1.098, 1.155, 1.394],
        [1.104, 0.341, 1.172, 1.179, 2.034, 1.808, 1.622, 0.352, 0.747, 1.721],
        [0.824, 1.221, 1.825, 1.376, 1.737, 1.146, 1.482, 0.642, 1.073, 1.727],
        [1.655, 1.343, 1.512, 2.119, 1.932, 1.698, 0.973, 1.674, 1.135, 1.014],
        [1.348, 1.282, 2.005, 1.387, 1.580, 1.968, 1.805, 1.237, 1.515, 1.043],
        [1.675, 1.782, 0.638, 1.305, 1.190, 2.093, 1.607, 1.554, 1.456, 0.735],
        [1.773, 1.124, 0.851, 1.356, 0.380, 1.664, 1.807, 1.506, 1.154, 1.431],
        [1.133, 1.097, 1.995, 1.128, 1.247, 1.327, 1.331, 1.369, 2.045, 0.959],
        [1.677, 1.822, 0.663, 1.880, 1.915, 0.755, 0.443, 0.589, 0.889, 1.352],
        [1.489, 0.948, 1.951, 1.724, 1.948, 1.981, 1.537, 0.966, 1.304, 1.375],
        [1.622, 1.775, 1.866, 1.328, 1.516, 1.674, 0.534, 1.986, 1.661, 1.434],
        [1.326, 1.745, 1.446, 1.764, 1.697, 1.972, 2.077, 1.167, 1.689, 1.259],
        [2.000, 2.021, 1.894, 1.258, 1.675, 2.026, 0.289, 1.645, 0.740, 1.669],
        [1.498, 0.869, 0.470, 0.402, 1.904, 1.448, 1.354, 0.934, 1.520, 1.352],
        [1.187, 1.338, 0.223, 1.641, 2.304, 1.712, 1.814, 1.316, 0.956, 0.553],
        [0.670, 1.516, 0.892, 1.264, 1.750, 1.987, 0.819, 1.505, 1.183, 2.054],
        [1.726, 1.819, 1.664, 1.547, 2.109, 1.642, 1.464, 1.281, 1.280, 0.754],
        [0.911, 1.153, 1.282, 1.459, 0.738, 0.972, 2.042, 0.958, 1.117, 0.464],
        [1.183, 1.639, 1.546, 1.637, 1.730, 0.734, 1.634, 2.044, 1.590, 1.228],
        [1.583, 1.320, 0.614, 1.239, 2.094, 1.877, 0.888, 0.541, 1.061, 1.927],
        [1.625, 0.365, 0.005, 1.398, 1.559, 0.259, 0.672, 1.767, 1.252, 1.603],
        [1.410, 1.092, 1.950, 1.487, 2.029, 0.888, 1.479, 1.497, 1.070, 1.570],
        [1.634, 0.167, 1.693, 1.036, 1.709, 1.410, 1.332, 1.245, 1.144, 1.589],
        [1.364, 0.071, 0.932, 1.981, 1.797, 1.851, 1.510, 1.549, 0.672, 0.910],
        [1.329, 1.279, 1.933, 1.439, 1.346, 1.430, 1.929, 1.055, 0.063, 1.997],
        [1.434, 1.754, 1.296, 1.559, 1.251, 1.263, 1.140, 1.222, 1.761, 1.307],
        [1.528, 1.207, 0.809, 1.255, 1.776, 1.816, 1.659, 0.918, 1.604, 1.734],
        [1.421, 1.130, 1.139, 0.645, 1.089, 2.034, 0.921, 1.256, 0.162, 0.640],
        [1.379, 1.320, 1.259, 1.175, 1.372, 1.410, 1.301, 1.581, 0.749, 1.846],
        [1.393, 1.278, 1.721, 0.803, 0.872, 1.371, 1.792, 1.506, 0.884, 1.454],
        [1.129, 0.879, 1.914, 1.503, 1.699, 2.039, 1.586, 1.213, 0.765, 0.877],
        [1.530, 2.061, 1.233, 1.150, 1.698, 1.658, 1.577, 1.596, 1.810, 1.126],
        [0.332, 1.575, 0.618, 1.349, 1.426, 1.771, 1.463, 1.462, 1.701, 1.207],
        [0.616, 0.488, 1.032, 1.653, 1.131, 1.206, 1.450, 0.934, 1.829, 1.317],
        [0.782, 1.281, 1.030, 1.900, 1.282, 1.042, 1.403, 0.286, 1.280, 1.738],
        [0.367, 0.839, 1.553, 0.760, 1.353, 1.197, 2.029, 1.538, 1.543, 1.714],
        [1.886, 1.219, 0.020, 0.342, 1.210, 1.181, 0.948, 1.798, 1.588, 1.710],
        [0.281, 1.092, 1.310, 1.192, 1.405, 1.618, 0.898, 1.483, 0.656, 1.229],
        [1.132, 1.558, 0.982, 0.187, 2.069, 1.150, 1.871, 2.124, 1.500, 1.568],
        [1.527, 1.050, 1.736, 0.886, 1.582, 1.696, 1.966, 1.692, 1.511, 1.510],
        [1.448, 1.203, 1.415, 0.525, 1.455, 1.357, 1.833, 1.290, 1.241, 1.470],
        [0.963, 1.644, 1.596, 1.516, 1.079, 1.590, 1.068, 1.926, 1.345, 1.909],
        [0.816, 0.784, 1.349, 1.008, 1.205, 1.566, 1.892, 1.632, 1.462, 1.472],
        [1.518, 1.859, 1.422, 1.270, 0.205, 2.177, 0.480, 1.707, 1.238, 1.371],
        [0.389, 1.470, 1.728, 1.463, 1.499, 1.148, 1.459, 1.815, 1.540, 1.627],
        [0.895, 0.763, 1.541, 1.713, 1.697, 1.222, 1.281, 1.755, 1.804, 0.366],
        [2.027, 1.036, 1.462, 0.970, 0.447, 1.778, 1.069, 2.180, 1.423, 1.464],
        [1.099, 1.813, 1.248, 1.462, 1.282, 1.038, 1.017, 1.729, 1.779, 1.019],
        [1.895, 0.651, 2.046, 1.585, 0.521, 1.752, 1.275, 2.076, 1.774, 0.902],
        [0.957, 1.612, 1.133, 0.781, 1.535, 2.040, 0.780, 0.977, 1.527, 2.017],
        [1.048, 1.724, 1.325, 1.253, 1.755, 0.894, 1.535, 1.756, 0.313, 0.410],
        [1.511, 1.699, 1.463, 1.504, 1.446, 1.760, 1.491, 1.543, 1.454, 0.484],
        [1.681, 0.832, 1.455, 1.689, 0.025, 1.369, 1.327, 2.269, 0.983, 1.463],
        [1.632, 1.373, 1.184, 1.656, 1.513, 0.600, 1.216, 0.584, 1.705, 0.794],
        [1.487, 1.890, 1.833, 0.900, 1.705, 1.542, 1.199, 0.537, 2.160, 1.832],
        [1.648, 1.835, 1.394, 0.516, 1.969, 0.759, 1.932, 1.444, 1.743, 0.958],
        [1.912, 1.399, 1.168, 1.913, 1.798, 1.109, 1.346, 1.987, 1.950, 2.006],
        [1.496, 1.597, 1.513, 0.780, 1.584, 1.577, 1.708, 1.350, 1.762, 1.462],
        [0.985, 1.427, 1.960, 0.823, 1.601, 0.249, 1.331, 0.888, 2.246, 1.729],
        [1.852, 1.573, 1.233, 0.672, 1.284, 1.401, 0.549, 1.806, 1.823, 1.542],
        [1.948, 0.884, 0.588, 1.731, 1.192, 0.503, 1.496, 1.591, 1.622, 0.999],
        [1.445, 1.856, 0.854, 1.074, 1.266, 1.590, 1.602, 1.637, 1.789, 1.609],
        [0.300, 1.631, 1.764, 1.230, 1.136, 1.257, 1.737, 1.659, 1.945, 1.041],
        [1.696, 1.842, 1.583, 1.905, 0.653, 1.634, 0.876, 1.288, 0.501, 1.471],
        [1.703, 1.263, 0.961, 0.487, 2.094, 0.947, 1.239, 1.489, 1.540, 1.986],
        [1.906, 1.510, 1.581, 1.085, 0.339, 1.563, 0.958, 1.490, 1.954, 1.142],
        [1.306, 2.137, 0.901, 1.660, 1.234, 0.763, 1.063, 1.060, 1.738, 2.140],
        [1.764, 1.936, 0.910, 1.587, 1.337, 1.324, 1.109, 1.765, 0.980, 1.785],
        [1.646, 1.774, 0.889, 1.424, 1.603, 1.447, 2.015, 1.553, 1.474, 1.281],
        [1.663, 1.352, 1.206, 0.915, 1.645, 1.757, 1.346, 1.269, 0.080, 1.647],
        [1.172, 1.357, 1.038, 1.827, 1.121, 1.798, 2.286, 1.377, 1.355, 1.708],
        [0.477, 1.605, 1.551, 1.430, 1.176, 1.528, 1.852, 1.380, 1.524, 1.185],
        [1.550, 0.194, 1.082, 1.266, 1.014, 1.255, 1.530, 1.323, 1.490, 1.636]],
       device='cuda:0')
b after update for 1 param tensor([[52.137, 38.996, 41.102, 31.311, 48.175, 54.833, 54.813, 52.948, 24.998,
         53.310],
        [54.381, 54.322, 16.196, 44.470, 48.226, 48.496, 44.226, 48.082, 38.157,
         47.917],
        [50.659, 19.487, 45.365, 30.797, 55.123, 40.234, 48.512, 36.840, 18.830,
         47.768],
        [50.272, 21.586, 11.593, 53.633, 52.222, 49.145, 38.733, 36.052, 42.008,
         52.786],
        [58.548, 56.042, 49.498, 37.627, 54.340, 49.411, 47.434, 49.146, 40.761,
         51.419],
        [56.047, 45.052, 51.064, 51.044, 51.340, 42.697, 31.405, 50.936, 39.921,
         43.886],
        [56.732, 52.865, 42.827, 19.643, 46.412, 51.819, 38.892, 56.508, 43.345,
         54.628],
        [55.911, 51.940, 49.971, 43.456, 48.585, 49.250, 49.443, 51.904, 42.941,
         44.577],
        [60.635, 44.984, 48.303, 53.276, 48.593, 45.231, 47.892, 48.327, 43.708,
         42.709],
        [45.944, 56.996, 45.183, 40.512, 47.075, 54.645, 42.005, 48.458, 54.210,
         48.506],
        [51.772, 42.960, 52.041, 51.910, 52.106, 42.223, 48.584, 54.676, 39.622,
         46.119],
        [43.325, 54.605, 42.624, 60.197, 54.120, 45.471, 49.071, 50.521, 47.982,
         25.924],
        [46.479, 50.060, 44.312, 13.332, 36.742, 49.162, 44.924, 32.998, 32.690,
         47.708],
        [53.130, 57.224, 54.231, 50.028, 48.173, 47.247, 53.568, 48.075, 45.936,
         42.433],
        [39.809, 58.622, 46.448, 44.463, 50.642, 58.298, 49.454, 42.264, 44.094,
         42.121],
        [42.998, 55.803, 41.578, 36.470, 43.363, 43.608,  7.325, 44.122, 48.288,
         49.020],
        [56.323, 46.619, 46.355, 43.402, 19.369, 44.908, 47.692, 51.716, 41.810,
         35.534],
        [46.468, 47.712, 49.952, 45.179, 53.427, 51.192, 49.734, 50.272, 28.183,
         46.480],
        [50.637, 55.211, 54.435, 15.682, 45.474, 37.589, 49.969, 45.678, 49.668,
         48.272],
        [50.585, 50.011, 50.873, 53.575, 45.122, 52.195, 54.998, 24.842, 51.597,
         46.215],
        [47.546, 42.571, 50.697, 38.786, 42.960, 48.960, 42.716, 55.664, 36.166,
         50.491],
        [47.023, 47.671, 55.199, 36.659, 48.302, 38.308, 51.759, 48.828, 50.802,
         49.662],
        [45.427, 49.105, 46.060, 36.074, 53.859, 49.913, 53.809, 54.848, 41.913,
         22.373],
        [43.015, 52.265, 60.418, 52.936, 46.100, 44.770, 51.368, 45.503, 31.995,
         49.253],
        [52.039, 55.415, 58.950, 47.920, 44.483, 40.969, 37.266, 44.053, 40.397,
         55.896],
        [29.871, 53.554, 50.767, 57.140, 47.055, 51.160, 38.352, 45.085, 19.644,
         41.104],
        [48.353, 45.145, 53.491, 51.569, 49.602, 50.677, 50.375, 51.375, 47.420,
         48.517],
        [56.715, 50.671, 48.550, 38.416, 42.580, 36.539, 54.800, 42.405, 43.484,
         47.781],
        [42.522, 23.631, 43.815, 43.934, 57.704, 54.408, 51.534, 24.015, 34.973,
         53.080],
        [36.736, 44.718, 54.664, 47.464, 53.333, 43.308, 49.253, 32.428, 41.910,
         53.177],
        [52.062, 46.885, 49.763, 58.900, 56.245, 52.728, 39.911, 52.358, 43.115,
         40.737],
        [46.980, 45.824, 57.298, 47.662, 50.859, 56.771, 54.369, 45.005, 49.800,
         41.331],
        [52.373, 54.013, 32.310, 46.228, 44.132, 58.547, 51.294, 50.438, 48.824,
         34.680],
        [53.873, 42.903, 37.337, 47.124, 24.956, 52.199, 54.394, 49.655, 43.463,
         48.398],
        [43.073, 42.386, 57.159, 42.973, 45.177, 46.617, 46.685, 47.348, 57.864,
         39.620],
        [52.393, 54.614, 32.948, 55.486, 56.002, 35.153, 26.938, 31.059, 38.151,
         47.055],
        [49.382, 39.392, 56.523, 53.129, 56.478, 56.954, 50.173, 39.771, 46.202,
         47.441],
        [51.538, 53.914, 55.281, 46.632, 49.819, 52.360, 29.564, 57.024, 52.144,
         48.449],
        [46.591, 53.454, 48.658, 53.739, 52.710, 56.820, 58.310, 43.704, 52.581,
         45.397],
        [57.226, 57.521, 55.683, 45.380, 52.372, 57.591, 21.751, 51.890, 34.807,
         52.282],
        [49.527, 37.713, 27.748, 25.660, 55.827, 48.698, 47.083, 39.103, 49.891,
         47.044],
        [44.080, 46.807, 19.117, 51.835, 61.417, 52.948, 54.493, 46.427, 39.564,
         30.093],
        [33.117, 49.829, 38.212, 45.486, 53.527, 57.031, 36.619, 49.641, 44.007,
         57.990],
        [53.160, 54.578, 52.194, 50.326, 58.760, 51.852, 48.968, 45.793, 45.782,
         35.127],
        [38.611, 43.444, 45.813, 48.879, 34.772, 39.885, 57.819, 39.602, 42.764,
         27.577],
        [44.007, 51.810, 50.304, 51.764, 53.217, 34.663, 51.729, 57.844, 51.021,
         44.843],
        [50.916, 46.492, 31.704, 45.046, 58.557, 55.441, 38.132, 29.751, 41.682,
         56.178],
        [51.589, 24.457,  2.954, 47.836, 50.522, 20.583, 33.177, 53.795, 45.272,
         51.231],
        [48.053, 42.285, 56.509, 49.350, 57.635, 38.132, 49.203, 49.510, 41.863,
         50.706],
        [51.718, 16.532, 52.647, 41.185, 52.903, 48.042, 46.697, 45.148, 43.273,
         51.005],
        [47.261, 10.816, 39.065, 56.958, 54.238, 55.048, 49.717, 50.360, 33.166,
         38.597],
        [46.648, 45.758, 56.265, 48.532, 46.939, 48.389, 56.194, 41.559, 10.195,
         57.182],
        [48.447, 53.586, 46.072, 50.519, 45.267, 45.481, 43.197, 44.726, 53.690,
         46.254],
        [50.020, 44.457, 36.387, 45.328, 53.928, 54.533, 52.119, 38.765, 51.246,
         53.291],
        [48.238, 43.018, 43.187, 32.492, 42.233, 57.705, 38.833, 45.356, 16.282,
         32.371],
        [47.519, 46.485, 45.400, 43.862, 47.402, 48.051, 46.146, 50.886, 35.031,
         54.973],
        [47.754, 45.744, 53.081, 36.267, 37.794, 47.374, 54.162, 49.659, 38.051,
         48.800],
        [42.989, 37.927, 55.977, 49.612, 52.738, 57.776, 50.958, 44.560, 35.396,
         37.903],
        [50.055, 58.095, 44.933, 43.401, 52.726, 52.097, 50.822, 51.124, 54.433,
         42.938],
        [23.330, 50.780, 31.810, 47.006, 48.312, 53.842, 48.935, 48.934, 52.777,
         44.450],
        [31.760, 28.268, 41.116, 52.031, 43.024, 44.437, 48.717, 39.098, 54.718,
         46.441],
        [35.775, 45.791, 41.067, 55.783, 45.808, 41.303, 47.932, 21.657, 45.775,
         53.347],
        [24.514, 37.074, 50.422, 35.284, 47.075, 44.267, 57.638, 50.184, 50.261,
         52.978],
        [55.572, 44.677,  5.708, 23.675, 44.507, 43.973, 39.408, 54.261, 50.989,
         52.912],
        [21.443, 42.293, 46.320, 44.170, 47.961, 51.466, 38.354, 49.274, 32.780,
         44.851],
        [43.056, 50.506, 40.089, 17.496, 58.201, 43.394, 55.352, 58.973, 49.551,
         50.672],
        [50.007, 41.465, 53.308, 38.094, 50.901, 52.701, 56.740, 52.631, 49.745,
         49.716],
        [48.684, 44.380, 48.138, 29.307, 48.806, 47.130, 54.791, 45.961, 45.073,
         49.057],
        [39.703, 51.883, 51.125, 49.827, 42.023, 51.018, 41.820, 56.159, 46.924,
         55.907],
        [36.550, 35.832, 47.004, 40.627, 44.410, 50.636, 55.654, 51.687, 48.931,
         49.093],
        [49.851, 55.165, 48.254, 45.609, 18.315, 59.701, 28.046, 52.862, 45.028,
         47.377],
        [25.252, 49.053, 53.192, 48.941, 49.542, 43.346, 48.881, 54.508, 50.211,
         51.614],
        [38.271, 35.350, 50.232, 52.953, 52.708, 44.723, 45.807, 53.605, 54.346,
         24.493],
        [57.612, 41.195, 48.921, 39.850, 27.060, 53.958, 41.845, 59.738, 48.276,
         48.967],
        [42.417, 54.478, 45.200, 48.921, 45.810, 41.227, 40.805, 53.204, 53.971,
         40.851],
        [55.697, 32.660, 57.882, 50.942, 29.210, 53.558, 45.683, 58.305, 53.889,
         38.427],
        [39.589, 51.374, 43.064, 35.770, 50.127, 57.792, 35.747, 39.992, 50.002,
         57.461],
        [41.427, 53.132, 46.581, 45.299, 53.609, 38.256, 50.138, 53.622, 22.627,
         25.901],
        [49.739, 52.740, 48.948, 49.621, 48.650, 53.686, 49.407, 50.260, 48.801,
         28.163],
        [52.459, 36.912, 48.803, 52.588,  6.446, 47.344, 46.607, 60.949, 40.122,
         48.942],
        [51.691, 47.421, 44.032, 52.071, 49.770, 31.345, 44.613, 30.920, 52.836,
         36.050],
        [49.344, 55.629, 54.788, 38.398, 52.829, 50.255, 44.308, 29.646, 59.468,
         54.767],
        [51.944, 54.819, 47.774, 29.071, 56.785, 35.242, 56.246, 48.621, 53.423,
         39.610],
        [55.951, 47.861, 43.728, 55.965, 54.260, 42.603, 46.943, 57.039, 56.505,
         57.313],
        [49.488, 51.138, 49.768, 35.747, 50.935, 50.818, 52.883, 47.014, 53.711,
         48.927],
        [40.163, 48.343, 56.653, 36.715, 51.205, 20.194, 46.677, 38.137, 60.640,
         53.205],
        [55.065, 50.744, 44.924, 33.165, 45.860, 47.898, 29.991, 54.385, 54.640,
         50.241],
        [56.474, 38.036, 31.032, 53.245, 44.186, 28.706, 49.500, 51.042, 51.540,
         40.443],
        [48.641, 55.128, 37.386, 41.932, 45.533, 51.017, 51.223, 51.775, 54.120,
         51.320],
        [22.162, 51.669, 53.741, 44.872, 43.124, 45.373, 53.324, 52.125, 56.433,
         41.291],
        [52.703, 54.913, 50.919, 55.852, 32.687, 51.717, 37.879, 45.928, 28.638,
         49.080],
        [52.802, 45.483, 39.675, 28.239, 58.556, 39.376, 45.033, 49.381, 50.222,
         57.026],
        [55.871, 49.729, 50.879, 42.147, 23.570, 50.592, 39.596, 49.401, 56.563,
         43.233],
        [46.249, 59.157, 38.409, 52.127, 44.948, 35.334, 41.717, 41.657, 53.341,
         59.187],
        [53.740, 56.303, 38.611, 50.980, 46.795, 46.553, 42.604, 53.756, 40.049,
         54.064],
        [51.908, 53.889, 38.151, 48.282, 51.224, 48.668, 57.440, 50.425, 49.132,
         45.801],
        [52.178, 47.045, 44.431, 38.702, 51.905, 53.632, 46.941, 45.578, 11.465,
         51.928],
        [43.797, 47.135, 41.229, 54.692, 42.850, 54.261, 61.180, 47.479, 47.101,
         52.884],
        [27.947, 51.270, 50.397, 48.382, 43.878, 50.013, 55.061, 47.541, 49.961,
         44.039],
        [50.374, 17.842, 42.082, 45.528, 40.736, 45.326, 50.059, 46.547, 49.394,
         51.761]], device='cuda:0')
clipping threshold 0.6649920925619305
a after update for 1 param tensor([ 6.925e-03,  1.689e-03,  1.343e-02,  1.457e-03,  2.385e-02, -6.553e-03,
         1.602e-03, -1.701e-02,  5.835e-03, -6.072e-03,  1.019e-03, -1.434e-02,
         1.564e-02, -2.382e-02, -2.422e-02,  4.134e-03, -1.964e-02,  9.614e-03,
        -4.466e-03, -9.277e-03,  1.688e-02, -7.379e-03, -3.632e-03,  1.862e-02,
         2.171e-02,  5.302e-03, -4.044e-03,  7.303e-03,  1.673e-02, -2.647e-02,
        -3.524e-02, -1.213e-02,  5.169e-03, -3.869e-04, -7.250e-03,  5.346e-03,
         3.019e-03, -1.424e-02, -8.602e-03,  2.099e-03, -1.571e-02, -1.987e-03,
        -1.597e-03, -5.358e-03,  1.885e-02,  2.859e-03,  6.996e-03,  2.927e-03,
        -9.371e-03,  5.267e-04,  6.123e-03, -1.180e-03,  4.075e-03, -5.324e-03,
        -1.490e-02, -8.527e-03,  2.504e-02,  2.213e-02, -1.229e-02, -7.765e-03,
        -9.779e-03, -7.151e-04, -2.927e-03,  5.118e-03, -1.535e-02, -2.731e-02,
        -4.177e-03,  6.017e-03,  9.169e-03, -4.489e-03,  8.418e-03, -1.235e-02,
         2.163e-03,  1.460e-02, -5.350e-03, -1.054e-02, -8.488e-03,  3.118e-03,
        -2.717e-02,  2.851e-03,  1.288e-02,  1.396e-02, -9.640e-05,  1.141e-03,
        -8.468e-03, -1.056e-02,  1.801e-03,  2.978e-02,  4.056e-06, -3.239e-02,
        -1.401e-02,  7.008e-03, -6.810e-03, -2.054e-03, -2.586e-02,  6.985e-03,
        -1.688e-03, -1.248e-03,  1.280e-03, -1.063e-02], device='cuda:0')
s after update for 1 param tensor([1.121, 0.849, 1.500, 0.028, 1.558, 1.136, 0.423, 1.222, 1.640, 1.270,
        1.454, 1.878, 1.565, 1.935, 1.827, 1.705, 1.278, 1.193, 1.186, 1.156,
        1.404, 1.203, 0.435, 1.820, 1.731, 0.807, 0.486, 0.836, 0.934, 1.309,
        1.763, 1.411, 1.335, 1.514, 2.250, 0.868, 1.512, 1.762, 1.236, 1.349,
        1.172, 0.733, 0.134, 1.273, 1.474, 1.902, 1.179, 0.266, 0.915, 1.485,
        1.234, 0.831, 1.731, 0.221, 1.397, 1.171, 1.150, 1.766, 1.295, 0.884,
        1.459, 0.448, 1.110, 1.603, 2.032, 1.959, 0.762, 1.244, 0.965, 1.170,
        1.155, 0.951, 0.305, 1.583, 1.106, 1.432, 0.856, 1.121, 1.877, 0.950,
        1.651, 1.415, 0.348, 1.186, 1.435, 1.530, 0.941, 1.369, 0.541, 1.953,
        1.751, 0.839, 1.836, 1.197, 1.792, 1.040, 1.311, 0.317, 0.805, 1.211],
       device='cuda:0')
b after update for 1 param tensor([42.838, 37.279, 49.562,  6.830, 50.513, 43.130, 26.306, 44.736, 51.817,
        45.602, 48.786, 55.452, 50.615, 56.285, 54.697, 52.837, 45.737, 44.198,
        44.064, 43.497, 47.948, 44.385, 26.682, 54.582, 53.243, 36.361, 28.202,
        37.004, 39.106, 46.304, 53.725, 48.061, 46.749, 49.795, 60.692, 37.697,
        49.749, 53.709, 44.985, 47.006, 43.801, 34.633, 14.813, 45.649, 49.119,
        55.811, 43.931, 20.861, 38.711, 49.306, 44.953, 36.897, 53.235, 19.029,
        47.818, 43.784, 43.386, 53.774, 46.045, 38.049, 48.882, 27.092, 42.625,
        51.227, 57.687, 56.628, 35.312, 45.135, 39.745, 43.770, 43.491, 39.465,
        22.347, 50.905, 42.561, 48.419, 37.430, 42.834, 55.439, 39.432, 51.998,
        48.139, 23.857, 44.072, 48.476, 50.055, 39.260, 47.352, 29.768, 56.556,
        53.540, 37.073, 54.827, 44.265, 54.169, 41.272, 46.336, 22.781, 36.295,
        44.537], device='cuda:0')
clipping threshold 0.6649920925619305
a after update for 1 param tensor([[[-2.413e-02],
         [-2.761e-03],
         [ 3.161e-03],
         [ 4.559e-05],
         [-1.859e-02],
         [-3.162e-03],
         [-3.636e-03],
         [-2.523e-02],
         [ 9.054e-03],
         [ 7.342e-03]],

        [[ 3.219e-03],
         [ 2.481e-03],
         [ 1.319e-02],
         [-1.748e-02],
         [-1.075e-02],
         [-5.855e-03],
         [ 7.460e-03],
         [ 7.630e-03],
         [ 1.717e-02],
         [ 1.936e-03]],

        [[-1.270e-02],
         [-6.114e-04],
         [-1.109e-02],
         [-4.436e-02],
         [-6.083e-03],
         [-4.686e-03],
         [-3.147e-03],
         [-5.443e-04],
         [-1.544e-02],
         [ 2.256e-04]],

        [[ 3.222e-02],
         [ 1.429e-02],
         [-2.926e-03],
         [-1.241e-02],
         [-2.501e-03],
         [ 2.894e-03],
         [-1.287e-02],
         [ 2.411e-03],
         [ 7.512e-03],
         [-7.914e-04]],

        [[ 5.850e-03],
         [-6.869e-04],
         [-2.645e-02],
         [-3.580e-02],
         [ 1.780e-03],
         [ 1.740e-02],
         [-4.747e-03],
         [-1.690e-02],
         [-1.632e-02],
         [ 1.825e-02]],

        [[-9.527e-03],
         [-3.641e-03],
         [ 1.290e-02],
         [-6.555e-03],
         [-7.103e-03],
         [ 9.753e-03],
         [-5.501e-03],
         [ 1.762e-02],
         [ 3.322e-03],
         [-6.240e-04]],

        [[ 9.329e-03],
         [ 7.901e-03],
         [ 4.410e-02],
         [ 2.359e-02],
         [ 2.765e-03],
         [ 2.380e-03],
         [ 1.530e-02],
         [ 1.305e-02],
         [-3.213e-03],
         [ 8.177e-03]],

        [[ 1.036e-02],
         [-5.669e-03],
         [-1.183e-03],
         [ 1.186e-02],
         [-5.496e-03],
         [ 5.818e-03],
         [ 4.370e-02],
         [ 1.096e-02],
         [ 7.774e-03],
         [ 1.167e-02]],

        [[ 7.397e-03],
         [-2.246e-03],
         [ 5.653e-03],
         [-1.451e-03],
         [ 6.775e-03],
         [ 7.097e-03],
         [ 3.231e-03],
         [ 1.486e-02],
         [ 9.223e-03],
         [-3.241e-02]],

        [[-2.638e-03],
         [-8.107e-03],
         [-1.156e-02],
         [-4.945e-03],
         [-1.590e-02],
         [ 1.584e-02],
         [-1.458e-02],
         [ 1.090e-02],
         [-1.816e-02],
         [-1.288e-02]]], device='cuda:0')
s after update for 1 param tensor([[[1.568],
         [1.787],
         [1.986],
         [0.405],
         [1.308],
         [1.567],
         [1.752],
         [1.749],
         [0.356],
         [2.072]],

        [[1.503],
         [1.428],
         [1.433],
         [1.348],
         [0.792],
         [1.887],
         [1.840],
         [1.557],
         [1.764],
         [1.790]],

        [[1.366],
         [1.181],
         [1.859],
         [1.885],
         [1.548],
         [1.736],
         [0.826],
         [1.349],
         [1.497],
         [1.634]],

        [[1.618],
         [1.691],
         [1.543],
         [1.620],
         [0.402],
         [1.019],
         [1.498],
         [1.190],
         [1.258],
         [1.569]],

        [[1.060],
         [1.652],
         [1.654],
         [1.741],
         [1.142],
         [1.726],
         [0.886],
         [1.911],
         [1.347],
         [0.954]],

        [[1.497],
         [1.129],
         [1.046],
         [1.346],
         [1.649],
         [1.625],
         [0.997],
         [1.976],
         [1.279],
         [1.481]],

        [[1.471],
         [1.321],
         [1.572],
         [1.523],
         [1.221],
         [1.574],
         [1.808],
         [0.692],
         [1.256],
         [1.575]],

        [[1.491],
         [2.090],
         [1.666],
         [1.372],
         [1.565],
         [1.414],
         [1.787],
         [1.497],
         [1.043],
         [1.635]],

        [[0.985],
         [0.969],
         [1.590],
         [1.493],
         [1.520],
         [1.000],
         [0.843],
         [1.474],
         [1.195],
         [2.169]],

        [[1.585],
         [1.391],
         [1.531],
         [1.662],
         [1.868],
         [2.002],
         [1.105],
         [1.659],
         [1.823],
         [1.327]]], device='cuda:0')
b after update for 1 param tensor([[[50.666],
         [54.095],
         [57.025],
         [25.739],
         [46.281],
         [50.659],
         [53.566],
         [53.521],
         [24.142],
         [58.247]],

        [[49.614],
         [48.357],
         [48.436],
         [46.983],
         [36.020],
         [55.582],
         [54.881],
         [50.490],
         [53.742],
         [54.136]],

        [[47.297],
         [43.979],
         [55.178],
         [55.556],
         [50.339],
         [53.311],
         [36.774],
         [47.005],
         [49.505],
         [51.721]],

        [[51.468],
         [52.626],
         [50.265],
         [51.498],
         [25.653],
         [40.848],
         [49.523],
         [44.135],
         [45.381],
         [50.683]],

        [[41.667],
         [52.001],
         [52.042],
         [53.384],
         [43.249],
         [53.166],
         [38.080],
         [55.936],
         [46.967],
         [39.524]],

        [[49.502],
         [43.004],
         [41.389],
         [46.952],
         [51.968],
         [51.581],
         [40.398],
         [56.876],
         [45.769],
         [49.245]],

        [[49.070],
         [46.513],
         [50.731],
         [49.940],
         [44.714],
         [50.765],
         [54.414],
         [33.664],
         [45.347],
         [50.787]],

        [[49.404],
         [58.494],
         [52.226],
         [47.405],
         [50.615],
         [48.110],
         [54.094],
         [49.507],
         [41.316],
         [51.740]],

        [[40.159],
         [39.830],
         [51.022],
         [49.446],
         [49.882],
         [40.467],
         [37.157],
         [49.121],
         [44.225],
         [59.591]],

        [[50.939],
         [47.724],
         [50.073],
         [52.163],
         [55.308],
         [57.258],
         [42.536],
         [52.126],
         [54.630],
         [46.615]]], device='cuda:0')
clipping threshold 0.6649920925619305
a after update for 1 param tensor([[ 0.008],
        [-0.011],
        [-0.001],
        [-0.005],
        [-0.029],
        [-0.006],
        [ 0.026],
        [ 0.018],
        [-0.004],
        [-0.003]], device='cuda:0')
s after update for 1 param tensor([[1.300],
        [1.710],
        [0.498],
        [1.298],
        [1.981],
        [1.416],
        [1.988],
        [1.627],
        [1.904],
        [1.615]], device='cuda:0')
b after update for 1 param tensor([[46.134],
        [52.919],
        [28.567],
        [46.105],
        [56.952],
        [48.148],
        [57.047],
        [51.611],
        [55.829],
        [51.426]], device='cuda:0')
clipping threshold 0.6649920925619305
||w||^2 0.04862225342218539
exp ma of ||w||^2 0.12685216265392718
||w|| 0.22050454286065263
exp ma of ||w|| 0.2911085977193279
||w||^2 0.15387495533773637
exp ma of ||w||^2 0.1859525175815193
||w|| 0.39226898340008526
exp ma of ||w|| 0.4059836135478254
cuda
Objective function 8.73 = squared loss an data 8.13 + 0.5*rho*h**2 0.054855 + alpha*h 0.227379 + L2reg 0.28 + L1reg 0.05 ; SHD = 9 ; DAG True
Proportion of microbatches that were clipped  0.7323347443829371
iteration 1 in inner loop, alpha 6.864780156316396 rho 100.0 h 0.03312250258652227
1210
cuda
Objective function 9.23 = squared loss an data 8.13 + 0.5*rho*h**2 0.548550 + alpha*h 0.227379 + L2reg 0.28 + L1reg 0.05 ; SHD = 9 ; DAG True
||w||^2 3303244716.8729544
exp ma of ||w||^2 2534708153.0642667
||w|| 57473.86116203569
exp ma of ||w|| 45271.829311744055
||w||^2 61944.003465216345
exp ma of ||w||^2 799918.5738724301
||w|| 248.8855228116259
exp ma of ||w|| 566.1946379679281
||w||^2 356.9526344486075
exp ma of ||w||^2 40152.488713071936
||w|| 18.89319016070625
exp ma of ||w|| 84.32897269684935
||w||^2 0.3125314265429446
exp ma of ||w||^2 0.36068715238005655
||w|| 0.5590451024228229
exp ma of ||w|| 0.5613215468531739
cuda
Objective function 8.79 = squared loss an data 8.27 + 0.5*rho*h**2 0.082603 + alpha*h 0.088235 + L2reg 0.30 + L1reg 0.05 ; SHD = 10 ; DAG True
Proportion of microbatches that were clipped  0.735969387755102
iteration 2 in inner loop, alpha 6.864780156316396 rho 1000.0 h 0.012853224232484806
iteration 3 in outer loop, alpha = 19.7180043888012, rho = 1000.0, h = 0.012853224232484806
cuda
1210
cuda
Objective function 8.96 = squared loss an data 8.27 + 0.5*rho*h**2 0.082603 + alpha*h 0.253440 + L2reg 0.30 + L1reg 0.05 ; SHD = 10 ; DAG True
||w||^2 1496492947.5886946
exp ma of ||w||^2 2855717030.1685514
||w|| 38684.53111501669
exp ma of ||w|| 48619.3911220074
||w||^2 409555028.6057005
exp ma of ||w||^2 845064340.6354365
||w|| 20237.465963052302
exp ma of ||w|| 25740.827388164285
||w||^2 0.12292307814134522
exp ma of ||w||^2 0.7363892223010553
||w|| 0.35060387639235424
exp ma of ||w|| 0.341010579185153
||w||^2 0.06715178688972945
exp ma of ||w||^2 0.13920174105624883
||w|| 0.2591366181953632
exp ma of ||w|| 0.302960112026745
||w||^2 0.2813617665682725
exp ma of ||w||^2 0.18277344216655095
||w|| 0.5304354499543489
exp ma of ||w|| 0.4036138811766789
cuda
Objective function 8.91 = squared loss an data 8.38 + 0.5*rho*h**2 0.028575 + alpha*h 0.149063 + L2reg 0.31 + L1reg 0.05 ; SHD = 10 ; DAG True
Proportion of microbatches that were clipped  0.734140435835351
iteration 1 in inner loop, alpha 19.7180043888012 rho 1000.0 h 0.007559722367481925
1210
cuda
Objective function 9.17 = squared loss an data 8.38 + 0.5*rho*h**2 0.285747 + alpha*h 0.149063 + L2reg 0.31 + L1reg 0.05 ; SHD = 10 ; DAG True
||w||^2 35072578527.819244
exp ma of ||w||^2 19303358422.088062
||w|| 187276.74315787115
exp ma of ||w|| 113622.28923186977
||w||^2 5621162.432181104
exp ma of ||w||^2 14012116.686857387
||w|| 2370.8990767599335
exp ma of ||w|| 2916.835256233327
||w||^2 306582.2440018722
exp ma of ||w||^2 1899544.4013898068
||w|| 553.6986942389084
exp ma of ||w|| 911.4044713077953
||w||^2 9802.760652641446
exp ma of ||w||^2 547095.2400545187
||w|| 99.00889178574542
exp ma of ||w|| 424.9347005692229
||w||^2 10636.652867218074
exp ma of ||w||^2 89688.35605308934
||w|| 103.13414985938496
exp ma of ||w|| 145.77843091956635
||w||^2 444.42436330084587
exp ma of ||w||^2 37257.175858601266
||w|| 21.0813747962709
exp ma of ||w|| 82.73865271021367
||w||^2 0.3470274120658299
exp ma of ||w||^2 0.12025319561269365
||w|| 0.5890903258973363
exp ma of ||w|| 0.33031854364717883
||w||^2 0.1351247841746365
exp ma of ||w||^2 0.15913895149683527
||w|| 0.36759323194889826
exp ma of ||w|| 0.3805215425461648
||w||^2 0.32972375548384614
exp ma of ||w||^2 0.17973012956959464
||w|| 0.5742157743251627
exp ma of ||w|| 0.3983106664024461
cuda
Objective function 8.93 = squared loss an data 8.49 + 0.5*rho*h**2 0.038426 + alpha*h 0.054663 + L2reg 0.31 + L1reg 0.04 ; SHD = 9 ; DAG True
Proportion of microbatches that were clipped  0.7386255170219536
iteration 2 in inner loop, alpha 19.7180043888012 rho 10000.0 h 0.0027722162477346046
iteration 4 in outer loop, alpha = 47.44016686614725, rho = 10000.0, h = 0.0027722162477346046
cuda
1210
cuda
Objective function 9.01 = squared loss an data 8.49 + 0.5*rho*h**2 0.038426 + alpha*h 0.131514 + L2reg 0.31 + L1reg 0.04 ; SHD = 9 ; DAG True
||w||^2 90225.0407149386
exp ma of ||w||^2 2826158.411494107
||w|| 300.37483369107105
exp ma of ||w|| 1107.80211922935
||w||^2 72272.48788395151
exp ma of ||w||^2 1514792.0514906077
||w|| 268.8354289969079
exp ma of ||w|| 774.4941209194632
||w||^2 6.248268670803172
exp ma of ||w||^2 853.0505347645274
||w|| 2.499653710177306
exp ma of ||w|| 5.600667406153232
||w||^2 0.20575079555869485
exp ma of ||w||^2 45.298205263849965
||w|| 0.4535976141457259
exp ma of ||w|| 1.1160567065942868
||w||^2 0.14061962588014837
exp ma of ||w||^2 0.1669345402043487
||w|| 0.37499283443840414
exp ma of ||w|| 0.3888738732174086
||w||^2 0.21803659974683864
exp ma of ||w||^2 0.23971267732375576
||w|| 0.46694389357484767
exp ma of ||w|| 0.45740255403001717
||w||^2 0.5213276174670554
exp ma of ||w||^2 0.33806364207452155
||w|| 0.7220302053702846
exp ma of ||w|| 0.5501491982296478
cuda
Objective function 8.97 = squared loss an data 8.51 + 0.5*rho*h**2 0.018585 + alpha*h 0.091461 + L2reg 0.31 + L1reg 0.04 ; SHD = 9 ; DAG True
Proportion of microbatches that were clipped  0.7303937520338432
iteration 1 in inner loop, alpha 47.44016686614725 rho 10000.0 h 0.001927927447376021
1210
cuda
Objective function 9.14 = squared loss an data 8.51 + 0.5*rho*h**2 0.185845 + alpha*h 0.091461 + L2reg 0.31 + L1reg 0.04 ; SHD = 9 ; DAG True
||w||^2 1041241716.0232693
exp ma of ||w||^2 150226264884.57578
||w|| 32268.27723977946
exp ma of ||w|| 140291.6049406832
||w||^2 812.266507000587
exp ma of ||w||^2 724239.6547648091
||w|| 28.500289595030207
exp ma of ||w|| 75.5060932856916
||w||^2 0.2808613260585066
exp ma of ||w||^2 0.14084452699261835
||w|| 0.5299635138936516
exp ma of ||w|| 0.3623254875833721
cuda
Objective function 8.97 = squared loss an data 8.57 + 0.5*rho*h**2 0.023402 + alpha*h 0.032456 + L2reg 0.30 + L1reg 0.04 ; SHD = 8 ; DAG True
Proportion of microbatches that were clipped  0.7336111556339157
iteration 2 in inner loop, alpha 47.44016686614725 rho 100000.0 h 0.0006841376188813086
iteration 5 in outer loop, alpha = 115.85392875427812, rho = 100000.0, h = 0.0006841376188813086
cuda
1210
cuda
Objective function 9.02 = squared loss an data 8.57 + 0.5*rho*h**2 0.023402 + alpha*h 0.079260 + L2reg 0.30 + L1reg 0.04 ; SHD = 8 ; DAG True
||w||^2 0.14541160583422916
exp ma of ||w||^2 2.6333656150620386
||w|| 0.3813287372258078
exp ma of ||w|| 0.4464589926493003
||w||^2 0.12605380304783942
exp ma of ||w||^2 1.8284441490397412
||w|| 0.35504056535533995
exp ma of ||w|| 0.4456466029609142
||w||^2 0.15915770056051218
exp ma of ||w||^2 0.16040940697148934
||w|| 0.3989457363608642
exp ma of ||w|| 0.38114880262911377
||w||^2 0.924871379677116
exp ma of ||w||^2 0.27181324615521435
||w|| 0.9617023342371152
exp ma of ||w|| 0.49488569457031345
||w||^2 0.14965446840302182
exp ma of ||w||^2 0.2619317708370427
||w|| 0.3868519980600098
exp ma of ||w|| 0.4853152280676876
cuda
Objective function 8.98 = squared loss an data 8.57 + 0.5*rho*h**2 0.009890 + alpha*h 0.051525 + L2reg 0.31 + L1reg 0.04 ; SHD = 6 ; DAG True
Proportion of microbatches that were clipped  0.7322658126501201
iteration 1 in inner loop, alpha 115.85392875427812 rho 100000.0 h 0.00044474472028355194
iteration 6 in outer loop, alpha = 560.59864903783, rho = 1000000.0, h = 0.00044474472028355194
Threshold 0.3
[[0.004 0.007 0.049 0.003 0.21  0.04  0.054 0.013 0.008 0.968]
 [0.406 0.003 0.385 0.003 0.234 0.093 0.409 0.014 0.002 0.397]
 [0.047 0.009 0.004 0.005 0.247 0.372 0.558 0.015 0.007 0.109]
 [0.852 0.62  0.369 0.003 0.134 1.354 0.273 0.02  0.058 0.319]
 [0.012 0.01  0.013 0.008 0.005 0.015 0.005 0.011 0.003 0.004]
 [0.069 0.026 0.009 0.002 0.133 0.003 0.397 0.007 0.012 0.08 ]
 [0.045 0.007 0.005 0.006 0.475 0.01  0.005 0.009 0.007 0.047]
 [0.081 0.155 0.157 0.116 0.183 0.398 0.158 0.003 0.057 0.109]
 [0.245 1.205 0.139 0.038 0.684 0.175 0.201 0.051 0.002 0.316]
 [0.003 0.006 0.027 0.005 0.432 0.027 0.065 0.02  0.004 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.968]
 [0.406 0.    0.385 0.    0.    0.    0.409 0.    0.    0.397]
 [0.    0.    0.    0.    0.    0.372 0.558 0.    0.    0.   ]
 [0.852 0.62  0.369 0.    0.    1.354 0.    0.    0.    0.319]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.397 0.    0.    0.   ]
 [0.    0.    0.    0.    0.475 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.398 0.    0.    0.    0.   ]
 [0.    1.205 0.    0.    0.684 0.    0.    0.    0.    0.316]
 [0.    0.    0.    0.    0.432 0.    0.    0.    0.    0.   ]]
{'fdr': 0.15789473684210525, 'tpr': 0.8, 'fpr': 0.12, 'f1': 0.8205128205128205, 'shd': 6, 'npred': 19, 'ntrue': 20}
[0.007 0.049 0.003 0.21  0.04  0.054 0.013 0.008 0.968 0.406 0.385 0.003
 0.234 0.093 0.409 0.014 0.002 0.397 0.047 0.009 0.005 0.247 0.372 0.558
 0.015 0.007 0.109 0.852 0.62  0.369 0.134 1.354 0.273 0.02  0.058 0.319
 0.012 0.01  0.013 0.008 0.015 0.005 0.011 0.003 0.004 0.069 0.026 0.009
 0.002 0.133 0.397 0.007 0.012 0.08  0.045 0.007 0.005 0.006 0.475 0.01
 0.009 0.007 0.047 0.081 0.155 0.157 0.116 0.183 0.398 0.158 0.057 0.109
 0.245 1.205 0.139 0.038 0.684 0.175 0.201 0.051 0.316 0.003 0.006 0.027
 0.005 0.432 0.027 0.065 0.02  0.004]
[[0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9285714285714286, 0.9001456582633054)
Iterations 693
Achieves (14.690186984748202, 1e-05)-DP
