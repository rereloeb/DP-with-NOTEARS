samples  5000  graph  20 40 ER mlp  minibatch size  100  noise  1.0  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.558077423622258
iteration 1 in outer loop, alpha = 1.558077423622258, rho = 1.0, h = 1.558077423622258
cuda
iteration 1 in inner loop,alpha 1.558077423622258 rho 1.0 h 1.034583104849741
iteration 2 in inner loop,alpha 1.558077423622258 rho 10.0 h 0.4453063278816174
iteration 3 in inner loop,alpha 1.558077423622258 rho 100.0 h 0.13879763090955777
iteration 2 in outer loop, alpha = 15.437840514578035, rho = 100.0, h = 0.13879763090955777
cuda
iteration 1 in inner loop,alpha 15.437840514578035 rho 100.0 h 0.0715566971707382
iteration 2 in inner loop,alpha 15.437840514578035 rho 1000.0 h 0.024352292407300524
iteration 3 in outer loop, alpha = 39.79013292187856, rho = 1000.0, h = 0.024352292407300524
cuda
iteration 1 in inner loop,alpha 39.79013292187856 rho 1000.0 h 0.010359599068490155
iteration 2 in inner loop,alpha 39.79013292187856 rho 10000.0 h 0.0026190391351050835
iteration 4 in outer loop, alpha = 65.98052427292939, rho = 10000.0, h = 0.0026190391351050835
cuda
iteration 1 in inner loop,alpha 65.98052427292939 rho 10000.0 h 0.001077941101538471
iteration 2 in inner loop,alpha 65.98052427292939 rho 100000.0 h 0.0004377690637014098
iteration 5 in outer loop, alpha = 109.75743064307036, rho = 100000.0, h = 0.0004377690637014098
cuda
iteration 1 in inner loop,alpha 109.75743064307036 rho 100000.0 h 0.00026113404248917504
iteration 6 in outer loop, alpha = 370.89147313224544, rho = 1000000.0, h = 0.00026113404248917504
Threshold 0.3
[[0.    0.011 0.055 2.325 0.052 0.032 0.042 0.002 1.994 0.014 1.861 0.003
  0.09  0.002 0.251 0.859 0.002 0.001 0.042 0.216]
 [0.036 0.006 0.209 0.057 0.365 0.105 0.029 0.002 3.047 0.048 0.015 0.013
  0.61  0.003 2.026 1.64  0.299 0.    0.204 0.326]
 [0.003 0.002 0.003 0.074 0.126 1.256 0.001 0.    2.295 0.077 0.061 0.
  2.29  0.002 1.438 0.904 0.001 0.    3.747 2.305]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.001 0.    0.    0.
  0.    0.    0.085 1.502 0.    0.    0.    0.001]
 [0.    0.    0.    1.708 0.001 0.198 0.001 0.    0.001 0.022 0.021 0.
  0.431 0.012 0.122 0.375 0.001 0.    0.036 0.189]
 [0.005 0.001 0.    0.137 0.002 0.001 0.    0.    0.001 0.026 0.069 0.
  0.305 0.    2.079 0.418 0.    0.    0.001 0.129]
 [0.004 0.017 0.56  0.055 0.228 0.101 0.006 0.    0.035 0.002 0.015 0.
  0.036 0.002 0.042 1.628 0.005 0.009 0.061 0.026]
 [0.003 0.014 0.778 0.044 0.18  0.09  4.509 0.001 0.15  0.01  0.002 0.054
  0.022 0.002 0.128 0.265 0.054 0.005 0.065 0.056]
 [0.    0.    0.    0.064 1.008 0.114 0.001 0.    0.003 0.046 0.046 0.
  0.37  0.005 0.429 0.448 0.    0.    0.018 0.266]
 [0.    0.    0.    0.096 0.    0.    0.    0.    0.    0.002 0.842 0.
  0.    0.    0.175 0.199 0.    0.    0.    0.193]
 [0.    0.001 0.    0.093 0.    0.005 0.    0.    0.001 0.    0.002 0.
  0.    0.    1.499 0.232 0.    0.    0.    2.325]
 [0.002 0.002 2.842 0.069 0.014 0.234 3.636 0.003 0.218 0.018 0.016 0.001
  0.233 0.002 0.624 0.337 0.007 0.02  0.208 0.169]
 [0.001 0.    0.    0.133 0.    0.002 0.    0.    0.001 3.226 0.121 0.
  0.002 0.    0.349 0.615 0.    0.    0.001 0.42 ]
 [0.002 0.005 0.014 0.055 0.026 0.127 0.003 0.004 0.096 0.032 0.076 0.025
  0.275 0.    0.191 0.18  0.004 0.024 2.627 0.228]
 [0.    0.    0.    0.013 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.005 0.243 0.    0.    0.    0.007]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.005 0.007 0.    0.    0.    0.   ]
 [0.009 0.006 0.327 0.066 0.323 0.198 0.034 0.003 0.273 0.044 0.056 0.011
  0.399 0.002 1.977 1.564 0.003 0.    0.329 0.479]
 [0.001 4.286 0.009 0.047 0.207 0.003 0.005 0.003 0.592 0.002 0.003 0.003
  1.337 0.003 0.204 0.184 2.966 0.001 1.422 1.035]
 [0.006 0.001 0.    0.07  0.014 1.253 0.    0.    0.082 0.049 0.017 0.
  0.94  0.    0.339 1.292 0.    0.    0.004 0.449]
 [0.    0.    0.    2.131 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.285 0.373 0.    0.    0.    0.003]]
[[0.    0.    0.    2.325 0.    0.    0.    0.    1.994 0.    1.861 0.
  0.    0.    0.    0.859 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.365 0.    0.    0.    3.047 0.    0.    0.
  0.61  0.    2.026 1.64  0.    0.    0.    0.326]
 [0.    0.    0.    0.    0.    1.256 0.    0.    2.295 0.    0.    0.
  2.29  0.    1.438 0.904 0.    0.    3.747 2.305]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.502 0.    0.    0.    0.   ]
 [0.    0.    0.    1.708 0.    0.    0.    0.    0.    0.    0.    0.
  0.431 0.    0.    0.375 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.305 0.    2.079 0.418 0.    0.    0.    0.   ]
 [0.    0.    0.56  0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.628 0.    0.    0.    0.   ]
 [0.    0.    0.778 0.    0.    0.    4.509 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.008 0.    0.    0.    0.    0.    0.    0.
  0.37  0.    0.429 0.448 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.842 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.499 0.    0.    0.    0.    2.325]
 [0.    0.    2.842 0.    0.    0.    3.636 0.    0.    0.    0.    0.
  0.    0.    0.624 0.337 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.226 0.    0.
  0.    0.    0.349 0.615 0.    0.    0.    0.42 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.627 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.327 0.    0.323 0.    0.    0.    0.    0.    0.    0.
  0.399 0.    1.977 1.564 0.    0.    0.329 0.479]
 [0.    4.286 0.    0.    0.    0.    0.    0.    0.592 0.    0.    0.
  1.337 0.    0.    0.    2.966 0.    1.422 1.035]
 [0.    0.    0.    0.    0.    1.253 0.    0.    0.    0.    0.    0.
  0.94  0.    0.339 1.292 0.    0.    0.    0.449]
 [0.    0.    0.    2.131 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.373 0.    0.    0.    0.   ]]
{'fdr': 0.375, 'tpr': 1.0, 'fpr': 0.16, 'f1': 0.7692307692307693, 'shd': 24, 'npred': 64, 'ntrue': 40}
[1.063e-02 5.535e-02 2.325e+00 5.173e-02 3.236e-02 4.189e-02 2.139e-03
 1.994e+00 1.377e-02 1.861e+00 2.786e-03 8.959e-02 2.105e-03 2.508e-01
 8.591e-01 2.262e-03 8.091e-04 4.247e-02 2.161e-01 3.637e-02 2.086e-01
 5.740e-02 3.652e-01 1.047e-01 2.929e-02 1.823e-03 3.047e+00 4.791e-02
 1.542e-02 1.343e-02 6.102e-01 2.880e-03 2.026e+00 1.640e+00 2.988e-01
 3.637e-04 2.038e-01 3.259e-01 2.640e-03 1.613e-03 7.354e-02 1.261e-01
 1.256e+00 5.566e-04 4.643e-05 2.295e+00 7.734e-02 6.095e-02 2.331e-04
 2.290e+00 2.483e-03 1.438e+00 9.035e-01 6.166e-04 6.434e-05 3.747e+00
 2.305e+00 8.893e-06 1.314e-04 1.870e-04 5.670e-04 1.227e-03 1.890e-04
 3.908e-05 5.207e-04 4.805e-05 5.567e-05 7.948e-05 7.055e-05 7.767e-05
 8.491e-02 1.502e+00 3.716e-04 8.426e-05 3.202e-04 9.392e-04 2.606e-05
 1.136e-04 1.582e-04 1.708e+00 1.977e-01 5.406e-04 3.336e-05 6.242e-04
 2.210e-02 2.067e-02 2.009e-05 4.313e-01 1.190e-02 1.221e-01 3.751e-01
 8.275e-04 3.200e-05 3.592e-02 1.894e-01 5.330e-03 8.664e-04 8.756e-05
 1.374e-01 1.699e-03 3.985e-04 8.780e-06 1.483e-03 2.605e-02 6.868e-02
 3.164e-05 3.049e-01 3.724e-05 2.079e+00 4.181e-01 1.439e-04 3.454e-05
 7.419e-04 1.286e-01 4.027e-03 1.683e-02 5.597e-01 5.486e-02 2.284e-01
 1.013e-01 1.628e-04 3.547e-02 1.538e-03 1.543e-02 1.640e-05 3.604e-02
 1.943e-03 4.247e-02 1.628e+00 5.027e-03 8.596e-03 6.056e-02 2.612e-02
 2.729e-03 1.379e-02 7.780e-01 4.393e-02 1.798e-01 9.012e-02 4.509e+00
 1.498e-01 9.785e-03 1.930e-03 5.447e-02 2.213e-02 2.143e-03 1.282e-01
 2.651e-01 5.435e-02 5.100e-03 6.510e-02 5.589e-02 4.833e-05 2.611e-04
 4.153e-04 6.372e-02 1.008e+00 1.145e-01 6.594e-04 2.213e-05 4.573e-02
 4.634e-02 8.290e-05 3.704e-01 4.747e-03 4.286e-01 4.481e-01 3.367e-04
 3.927e-05 1.808e-02 2.663e-01 2.839e-04 1.256e-05 5.142e-05 9.555e-02
 3.786e-05 2.414e-04 1.998e-04 8.471e-06 1.709e-04 8.421e-01 2.169e-05
 1.368e-04 4.046e-05 1.747e-01 1.988e-01 7.685e-05 1.081e-05 4.960e-05
 1.934e-01 6.595e-05 9.788e-04 7.750e-06 9.329e-02 4.992e-04 4.687e-03
 2.907e-04 4.133e-05 5.244e-04 6.654e-05 8.355e-06 2.910e-05 1.572e-04
 1.499e+00 2.317e-01 2.040e-04 5.979e-05 1.342e-04 2.325e+00 1.850e-03
 2.355e-03 2.842e+00 6.880e-02 1.356e-02 2.341e-01 3.636e+00 2.878e-03
 2.177e-01 1.837e-02 1.555e-02 2.329e-01 2.110e-03 6.239e-01 3.366e-01
 7.102e-03 1.957e-02 2.084e-01 1.693e-01 1.371e-03 1.302e-04 1.723e-04
 1.333e-01 3.922e-04 2.384e-03 9.344e-05 2.154e-05 9.269e-04 3.226e+00
 1.207e-01 4.586e-05 2.338e-04 3.490e-01 6.146e-01 4.910e-04 4.186e-05
 6.230e-04 4.199e-01 2.321e-03 5.328e-03 1.395e-02 5.483e-02 2.639e-02
 1.269e-01 3.436e-03 3.703e-03 9.586e-02 3.247e-02 7.622e-02 2.536e-02
 2.745e-01 1.912e-01 1.801e-01 3.747e-03 2.406e-02 2.627e+00 2.284e-01
 9.303e-06 3.457e-04 8.084e-05 1.343e-02 3.286e-04 1.412e-04 1.132e-04
 2.019e-05 2.312e-04 1.344e-04 1.479e-04 2.087e-05 7.822e-05 8.994e-06
 2.433e-01 3.373e-04 6.226e-05 4.278e-05 6.943e-03 1.039e-05 2.244e-04
 5.166e-05 2.698e-04 1.623e-04 1.911e-04 4.589e-04 3.153e-05 1.677e-04
 1.451e-04 5.849e-05 9.760e-06 1.395e-04 3.704e-05 4.861e-03 3.605e-05
 2.838e-05 9.392e-05 1.109e-04 9.483e-03 6.441e-03 3.269e-01 6.647e-02
 3.225e-01 1.981e-01 3.373e-02 3.387e-03 2.732e-01 4.420e-02 5.626e-02
 1.097e-02 3.985e-01 2.124e-03 1.977e+00 1.564e+00 4.222e-04 3.288e-01
 4.788e-01 1.144e-03 4.286e+00 8.709e-03 4.701e-02 2.070e-01 3.405e-03
 5.093e-03 2.841e-03 5.920e-01 2.286e-03 2.944e-03 2.910e-03 1.337e+00
 2.724e-03 2.042e-01 1.839e-01 2.966e+00 1.422e+00 1.035e+00 5.917e-03
 1.254e-03 4.285e-04 6.995e-02 1.443e-02 1.253e+00 1.424e-04 2.433e-05
 8.242e-02 4.891e-02 1.700e-02 8.331e-05 9.404e-01 3.227e-05 3.393e-01
 1.292e+00 6.581e-05 1.004e-04 4.493e-01 1.661e-05 1.476e-04 1.818e-04
 2.131e+00 1.256e-03 7.382e-04 1.983e-04 2.277e-05 1.839e-04 7.303e-05
 6.424e-05 5.548e-05 7.363e-05 1.016e-04 2.855e-01 3.733e-01 2.549e-04
 7.876e-05 1.458e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9994117647058823, 0.9949617734555005)
cuda
4420
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
||w||^2 0.19425933147330315
exp ma of ||w||^2 61.27814330404632
||w|| 0.4407486034842347
exp ma of ||w|| 0.49229717508724224
||w||^2 0.17844242957504935
exp ma of ||w||^2 0.20963618686636762
||w|| 0.4224244661179669
exp ma of ||w|| 0.44605090363756567
||w||^2 0.10368518538732177
exp ma of ||w||^2 0.20760751931257287
||w|| 0.3220018406582822
exp ma of ||w|| 0.44388201650994735
||w||^2 0.18166420540015094
exp ma of ||w||^2 0.17112459668862293
||w|| 0.4262208411142643
exp ma of ||w|| 0.40521012291385977
||w||^2 0.05897064821445623
exp ma of ||w||^2 0.16784975918032793
||w|| 0.2428387288190585
exp ma of ||w|| 0.4000929025572447
cuda
Objective function 181.10 = squared loss an data 178.58 + 0.5*rho*h**2 1.897765 + alpha*h 0.000000 + L2reg 0.36 + L1reg 0.27 ; SHD = 104 ; DAG False
Proportion of microbatches that were clipped  0.72533418204965
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.9482122544159317
iteration 1 in outer loop, alpha = 1.9482122544159317, rho = 1.0, h = 1.9482122544159317
cuda
4420
cuda
Objective function 184.90 = squared loss an data 178.58 + 0.5*rho*h**2 1.897765 + alpha*h 3.795531 + L2reg 0.36 + L1reg 0.27 ; SHD = 104 ; DAG False
||w||^2 2.7145192196938663
exp ma of ||w||^2 2108.310587698072
||w|| 1.64757980677534
exp ma of ||w|| 7.686424557530955
||w||^2 1.7065766292345041
exp ma of ||w||^2 330.7446812902226
||w|| 1.3063600687538273
exp ma of ||w|| 2.460650600382531
||w||^2 0.27848034631143176
exp ma of ||w||^2 0.33348268882741144
||w|| 0.5277123708152309
exp ma of ||w|| 0.5432437155370433
||w||^2 0.47877226006027607
exp ma of ||w||^2 0.36114121207895733
||w|| 0.6919337107413369
exp ma of ||w|| 0.5720410902715053
||w||^2 0.6211564256728911
exp ma of ||w||^2 0.39417710813936374
||w|| 0.7881347763377093
exp ma of ||w|| 0.6015082875959522
cuda
Objective function 88.10 = squared loss an data 82.30 + 0.5*rho*h**2 1.508219 + alpha*h 3.383634 + L2reg 0.66 + L1reg 0.25 ; SHD = 84 ; DAG False
Proportion of microbatches that were clipped  0.7422964644826467
iteration 1 in inner loop, alpha 1.9482122544159317 rho 1.0 h 1.736789398481065
4420
cuda
Objective function 101.68 = squared loss an data 82.30 + 0.5*rho*h**2 15.082187 + alpha*h 3.383634 + L2reg 0.66 + L1reg 0.25 ; SHD = 84 ; DAG False
||w||^2 116566.81304107203
exp ma of ||w||^2 6025866.896524623
||w|| 341.41882350138815
exp ma of ||w|| 1154.8244115240627
||w||^2 0.3962812267553608
exp ma of ||w||^2 1.1341869089078567
||w|| 0.6295087185697755
exp ma of ||w|| 1.0038945762975433
||w||^2 0.48186306200007256
exp ma of ||w||^2 1.1408827876403387
||w|| 0.6941635700611727
exp ma of ||w|| 1.0231363683991128
||w||^2 0.6637977658873767
exp ma of ||w||^2 1.1675125807499604
||w|| 0.8147378510216502
exp ma of ||w|| 1.0428869010240356
||w||^2 0.9954009351297335
exp ma of ||w||^2 1.1987666142564841
||w|| 0.9976978175428337
exp ma of ||w|| 1.0471196761270856
cuda
Objective function 60.46 = squared loss an data 52.71 + 0.5*rho*h**2 4.788346 + alpha*h 1.906532 + L2reg 0.83 + L1reg 0.22 ; SHD = 55 ; DAG False
Proportion of microbatches that were clipped  0.7677084969373331
iteration 2 in inner loop, alpha 1.9482122544159317 rho 10.0 h 0.9786057160429635
4420
cuda
Objective function 103.56 = squared loss an data 52.71 + 0.5*rho*h**2 47.883457 + alpha*h 1.906532 + L2reg 0.83 + L1reg 0.22 ; SHD = 55 ; DAG False
||w||^2 3938110532.205524
exp ma of ||w||^2 15223022922.683077
||w|| 62754.366638549734
exp ma of ||w|| 102104.39411259518
||w||^2 1.999126083669528
exp ma of ||w||^2 2.361796497034953
||w|| 1.41390455253158
exp ma of ||w|| 1.4820225662212856
||w||^2 1.7976478628914248
exp ma of ||w||^2 2.5550561308846564
||w|| 1.3407639101987436
exp ma of ||w|| 1.5352679921052066
||w||^2 1.923468155997018
exp ma of ||w||^2 2.292328292644735
||w|| 1.386891544424804
exp ma of ||w|| 1.4579526132147076
cuda
Objective function 55.13 = squared loss an data 47.08 + 0.5*rho*h**2 6.214540 + alpha*h 0.686840 + L2reg 0.97 + L1reg 0.19 ; SHD = 42 ; DAG True
Proportion of microbatches that were clipped  0.7809717231682549
iteration 3 in inner loop, alpha 1.9482122544159317 rho 100.0 h 0.3525490139394165
iteration 2 in outer loop, alpha = 37.203113648357586, rho = 100.0, h = 0.3525490139394165
cuda
4420
cuda
Objective function 67.56 = squared loss an data 47.08 + 0.5*rho*h**2 6.214540 + alpha*h 13.115921 + L2reg 0.97 + L1reg 0.19 ; SHD = 42 ; DAG True
||w||^2 16685302197.684607
exp ma of ||w||^2 34368781138.01312
||w|| 129171.59981081216
exp ma of ||w|| 155527.1980498287
||w||^2 28845195235.174847
exp ma of ||w||^2 32790699476.323753
||w|| 169838.73302393316
exp ma of ||w|| 152329.01790710597
||w||^2 67024995.892008334
exp ma of ||w||^2 1788412292.5213878
||w|| 8186.879496609702
exp ma of ||w|| 30104.967757076534
||w||^2 370.5142190100349
exp ma of ||w||^2 84645.67675087674
||w|| 19.248745907461995
exp ma of ||w|| 54.12605956789314
||w||^2 103.88231333391535
exp ma of ||w||^2 6763.739634106167
||w|| 10.19226733037921
exp ma of ||w|| 12.885834829954
||w||^2 3.2451689281265503
exp ma of ||w||^2 15.139720700409692
||w|| 1.80143524116926
exp ma of ||w|| 2.0079505230361856
||w||^2 4.653586498027703
exp ma of ||w||^2 5.234819565111599
||w|| 2.1572173043130594
exp ma of ||w|| 1.8307932013176516
||w||^2 4.094846451091772
exp ma of ||w||^2 3.0754429938711474
||w|| 2.0235726947880504
exp ma of ||w|| 1.6758844095784704
||w||^2 1.1152417285538736
exp ma of ||w||^2 2.966194907220995
||w|| 1.0560500596817717
exp ma of ||w|| 1.6666598642097024
||w||^2 2.075212569293545
exp ma of ||w||^2 2.9309087459538334
||w|| 1.4405598110781603
exp ma of ||w|| 1.6596476383950662
cuda
Objective function 57.60 = squared loss an data 44.51 + 0.5*rho*h**2 2.884867 + alpha*h 8.936288 + L2reg 1.09 + L1reg 0.18 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.7799902072792557
iteration 1 in inner loop, alpha 37.203113648357586 rho 100.0 h 0.24020269496887892
4420
cuda
Objective function 83.57 = squared loss an data 44.51 + 0.5*rho*h**2 28.848667 + alpha*h 8.936288 + L2reg 1.09 + L1reg 0.18 ; SHD = 36 ; DAG True
||w||^2 1230525951.5253756
exp ma of ||w||^2 5627681308.277242
||w|| 35078.85333823464
exp ma of ||w|| 57293.47048943275
||w||^2 310.7599385396828
exp ma of ||w||^2 140531.9550109718
||w|| 17.628384456315978
exp ma of ||w|| 72.11908838353712
||w||^2 6.6055358552383705
exp ma of ||w||^2 21.737131415342493
||w|| 2.570123704267631
exp ma of ||w|| 2.4635511877849887
cuda
Objective function 56.13 = squared loss an data 45.88 + 0.5*rho*h**2 5.148201 + alpha*h 3.775044 + L2reg 1.16 + L1reg 0.16 ; SHD = 44 ; DAG True
Proportion of microbatches that were clipped  0.7887772995377013
iteration 2 in inner loop, alpha 37.203113648357586 rho 1000.0 h 0.1014711858394648
4420
cuda
Objective function 102.46 = squared loss an data 45.88 + 0.5*rho*h**2 51.482008 + alpha*h 3.775044 + L2reg 1.16 + L1reg 0.16 ; SHD = 44 ; DAG True
||w||^2 2899.132382465517
exp ma of ||w||^2 1204913.921347568
||w|| 53.8435918421637
exp ma of ||w|| 254.06163928430186
||w||^2 4.100083931432034
exp ma of ||w||^2 4.377646350969483
||w|| 2.0248663984154693
exp ma of ||w|| 2.030911503943317
||w||^2 7.809205326490162
exp ma of ||w||^2 4.302846116256654
||w|| 2.7944955406101766
exp ma of ||w|| 2.0112935931269504
||w||^2 3.8107320592360665
exp ma of ||w||^2 4.0111678849018855
||w|| 1.9521096432414002
exp ma of ||w|| 1.9463860031822247
||w||^2 6.277761536846974
exp ma of ||w||^2 4.189890640014996
||w|| 2.5055461554014475
exp ma of ||w|| 1.955405830524445
cuda
Objective function 55.19 = squared loss an data 45.95 + 0.5*rho*h**2 6.514224 + alpha*h 1.342844 + L2reg 1.25 + L1reg 0.14 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.7891167447117714
iteration 3 in inner loop, alpha 37.203113648357586 rho 10000.0 h 0.03609494010785497
iteration 3 in outer loop, alpha = 398.15251472690727, rho = 10000.0, h = 0.03609494010785497
cuda
4420
cuda
Objective function 68.22 = squared loss an data 45.95 + 0.5*rho*h**2 6.514224 + alpha*h 14.371291 + L2reg 1.25 + L1reg 0.14 ; SHD = 40 ; DAG True
||w||^2 1301923132893.355
exp ma of ||w||^2 388411154142.115
||w|| 1141018.4629940721
exp ma of ||w|| 307332.32713459485
||w||^2 46269915194.65081
exp ma of ||w||^2 199576099725.58737
||w|| 215104.42857981983
exp ma of ||w|| 373500.1676825324
||w||^2 1.8337045940243963
exp ma of ||w||^2 4.981195601550479
||w|| 1.3541434909286372
exp ma of ||w|| 2.1802797409453425
||w||^2 3.046306960140719
exp ma of ||w||^2 4.643496680589216
||w|| 1.74536728516972
exp ma of ||w|| 2.107432896496357
||w||^2 6.314377897926256
exp ma of ||w||^2 4.391596891028453
||w|| 2.512842593145511
exp ma of ||w|| 2.051127391230464
cuda
Objective function 56.06 = squared loss an data 44.46 + 0.5*rho*h**2 2.055121 + alpha*h 8.072038 + L2reg 1.34 + L1reg 0.14 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.7879318579843974
iteration 1 in inner loop, alpha 398.15251472690727 rho 10000.0 h 0.020273732696086455
4420
cuda
Objective function 74.56 = squared loss an data 44.46 + 0.5*rho*h**2 20.551212 + alpha*h 8.072038 + L2reg 1.34 + L1reg 0.14 ; SHD = 43 ; DAG True
||w||^2 2879234240439.704
exp ma of ||w||^2 520522353934.71436
||w|| 1696830.6457745582
exp ma of ||w|| 292275.6014061983
||w||^2 11129572111.365889
exp ma of ||w||^2 74036588009.75446
||w|| 105496.78720873868
exp ma of ||w|| 140252.49845015293
v before min max tensor([[-2.560e-02, -3.731e-05, -3.449e-03,  ..., -1.483e-04,  1.608e-02,
         -1.918e-04],
        [-1.323e-02, -3.077e-04, -1.591e-04,  ...,  3.805e-05, -5.164e-04,
          2.219e-03],
        [ 2.613e-02, -3.999e-03, -6.731e-05,  ...,  5.689e-05,  2.317e-04,
         -1.723e-04],
        ...,
        [ 1.243e-04, -2.642e-04, -1.174e-02,  ...,  8.033e-03,  3.401e-03,
         -2.558e-02],
        [-2.197e-04, -1.325e-02, -6.028e-04,  ...,  2.758e-03,  9.333e-03,
          9.267e-03],
        [ 2.377e-03, -2.973e-03, -1.805e-03,  ..., -1.107e-02, -1.960e-02,
          2.789e-02]], device='cuda:0')
v tensor([[1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.608e-02,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 3.805e-05, 1.000e-12,
         2.219e-03],
        [2.613e-02, 1.000e-12, 1.000e-12,  ..., 5.689e-05, 2.317e-04,
         1.000e-12],
        ...,
        [1.243e-04, 1.000e-12, 1.000e-12,  ..., 8.033e-03, 3.401e-03,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 2.758e-03, 9.333e-03,
         9.267e-03],
        [2.377e-03, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         2.789e-02]], device='cuda:0')
v before min max tensor([-3.483e-06, -1.517e-05, -2.788e-04, -1.643e-03, -1.379e-04, -1.452e-05,
        -2.626e-04,  1.826e-04,  1.611e-03,  2.638e-02, -3.566e-05, -1.342e-05,
         4.939e-04, -2.224e-03, -2.063e-04, -2.878e-04, -6.844e-04, -9.874e-04,
         5.626e-04,  1.781e-06, -1.724e-02, -8.368e-04,  5.012e-03,  3.331e-04,
         9.394e-06, -7.116e-05, -1.442e-03, -4.367e-04, -2.604e-03, -3.438e-04,
        -2.618e-03,  2.178e-04, -5.712e-05, -2.494e-06,  1.552e-04, -2.466e-05,
        -1.251e-04, -2.799e-05, -2.224e-05, -8.467e-05,  7.150e-05, -2.881e-02,
        -4.836e-04, -5.035e-06, -1.375e-03,  1.533e-03, -2.003e-05,  1.144e-05,
        -4.070e-04, -4.785e-04, -6.124e-04, -2.554e-03, -1.182e-05,  3.521e-05,
        -2.527e-04, -8.041e-05,  1.367e-03, -1.382e-05, -6.698e-04, -6.495e-03,
         9.846e-04, -1.995e-03,  4.363e-03, -4.470e-03, -3.399e-05, -1.477e-03,
        -1.450e-03,  3.231e-03,  2.543e-04,  8.517e-03, -6.746e-06, -1.173e-04,
         1.528e-03,  6.525e-04,  1.276e-02, -6.680e-03, -5.589e-06, -2.589e-04,
        -9.477e-04, -1.112e-04, -1.818e-02,  1.809e-03, -2.336e-05, -3.046e-04,
        -2.950e-04, -1.707e-03, -2.041e-03, -3.948e-06,  1.402e-03, -1.052e-05,
        -7.349e-05,  2.439e-04, -3.384e-04, -2.165e-05, -2.218e-04,  1.244e-02,
         3.305e-03, -1.210e-04, -5.413e-04, -2.529e-04, -7.867e-04, -7.863e-05,
        -4.520e-04,  5.120e-03, -2.095e-05, -1.557e-04, -9.947e-04,  1.124e-03,
        -4.583e-03,  9.590e-07,  6.886e-04,  2.476e-03, -1.122e-04,  2.851e-05,
        -9.929e-04, -1.044e-05, -2.969e-05, -2.043e-05, -4.031e-04,  6.022e-03,
         3.381e-03, -1.820e-03, -3.983e-03, -6.664e-05, -1.120e-05, -5.707e-04,
        -3.234e-03, -8.811e-04, -3.201e-02,  1.006e-03, -6.487e-03, -3.062e-04,
        -3.132e-04,  1.811e-04,  6.706e-04, -2.246e-05, -3.508e-05, -1.612e-03,
        -5.792e-04, -1.461e-04,  1.187e-04, -7.629e-05, -2.054e-03,  7.817e-06,
        -5.177e-03, -1.722e-05, -3.455e-04, -4.060e-03, -1.208e-03, -8.571e-04,
        -2.737e-02, -1.316e-03, -5.654e-04, -3.573e-05, -2.481e-04, -1.116e-04,
         2.175e-02,  5.794e-04, -2.835e-04,  7.874e-03, -3.847e-05,  1.871e-04,
        -3.895e-04, -1.987e-03, -7.532e-04, -3.399e-05, -2.921e-03,  3.903e-03,
        -2.034e-03, -7.231e-06, -2.192e-05,  1.191e-03, -5.624e-03, -6.813e-04,
        -1.403e-03, -1.870e-06, -2.695e-03, -2.119e-03, -4.543e-05, -6.073e-04,
        -5.377e-04,  2.600e-04,  5.393e-04,  3.361e-06, -8.617e-06, -1.625e-04,
        -7.366e-04,  3.048e-02, -5.523e-04, -8.461e-05,  2.114e-02, -1.758e-03,
         8.280e-03,  2.669e-05,  4.807e-03,  1.181e-03, -2.553e-05,  3.403e-03,
        -2.354e-03, -6.040e-03], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.826e-04, 1.611e-03, 2.638e-02, 1.000e-12, 1.000e-12,
        4.939e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        5.626e-04, 1.781e-06, 1.000e-12, 1.000e-12, 5.012e-03, 3.331e-04,
        9.394e-06, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.178e-04, 1.000e-12, 1.000e-12, 1.552e-04, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 7.150e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.533e-03, 1.000e-12, 1.144e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.521e-05,
        1.000e-12, 1.000e-12, 1.367e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        9.846e-04, 1.000e-12, 4.363e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 3.231e-03, 2.543e-04, 8.517e-03, 1.000e-12, 1.000e-12,
        1.528e-03, 6.525e-04, 1.276e-02, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.809e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.402e-03, 1.000e-12,
        1.000e-12, 2.439e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.244e-02,
        3.305e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 5.120e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.124e-03,
        1.000e-12, 9.590e-07, 6.886e-04, 2.476e-03, 1.000e-12, 2.851e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 6.022e-03,
        3.381e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.006e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.811e-04, 6.706e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.187e-04, 1.000e-12, 1.000e-12, 7.817e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        2.175e-02, 5.794e-04, 1.000e-12, 7.874e-03, 1.000e-12, 1.871e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.903e-03,
        1.000e-12, 1.000e-12, 1.000e-12, 1.191e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.600e-04, 5.393e-04, 3.361e-06, 1.000e-12, 1.000e-12,
        1.000e-12, 3.048e-02, 1.000e-12, 1.000e-12, 2.114e-02, 1.000e-12,
        8.280e-03, 2.669e-05, 4.807e-03, 1.181e-03, 1.000e-12, 3.403e-03,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 6.614e-04],
         [ 9.367e-03],
         [ 6.758e-06],
         [-1.374e-03],
         [-2.466e-04],
         [ 5.254e-06],
         [ 8.178e-05],
         [ 1.732e-04],
         [-3.135e-03],
         [ 1.114e-05]],

        [[-3.029e-05],
         [-1.523e-04],
         [-1.913e-04],
         [-3.804e-05],
         [-1.306e-04],
         [-2.797e-04],
         [-7.320e-05],
         [-4.733e-04],
         [-3.433e-04],
         [-3.319e-04]],

        [[-6.735e-03],
         [ 8.942e-05],
         [-8.990e-05],
         [-3.099e-04],
         [-5.765e-03],
         [ 4.753e-04],
         [ 1.577e-03],
         [ 2.987e-03],
         [-4.306e-05],
         [-1.087e-03]],

        [[-1.120e-04],
         [ 1.156e-04],
         [ 1.146e-02],
         [-4.821e-04],
         [-8.709e-05],
         [-9.370e-04],
         [-1.521e-04],
         [-1.977e-04],
         [-1.609e-05],
         [-1.829e-05]],

        [[-1.262e-03],
         [ 1.575e-02],
         [ 3.752e-04],
         [-4.122e-05],
         [-1.356e-04],
         [-1.463e-03],
         [-8.252e-04],
         [-3.513e-07],
         [ 8.968e-04],
         [-5.357e-04]],

        [[-3.821e-03],
         [ 2.856e-02],
         [ 3.014e-05],
         [-7.539e-05],
         [-3.538e-03],
         [-3.451e-04],
         [-4.115e-05],
         [-1.105e-04],
         [-8.694e-05],
         [ 1.034e-03]],

        [[-5.007e-04],
         [ 4.312e-03],
         [-2.182e-05],
         [-4.778e-05],
         [-2.762e-03],
         [-9.076e-04],
         [-2.030e-05],
         [-7.703e-04],
         [-2.768e-03],
         [-3.783e-04]],

        [[-1.161e-03],
         [-6.341e-03],
         [ 3.695e-03],
         [-7.537e-04],
         [ 1.511e-04],
         [ 5.873e-03],
         [-1.998e-02],
         [-9.060e-06],
         [-4.555e-05],
         [-1.453e-04]],

        [[-5.365e-05],
         [-1.402e-04],
         [-3.672e-04],
         [-1.718e-04],
         [-2.876e-05],
         [-3.980e-06],
         [ 1.580e-05],
         [-4.286e-04],
         [-1.338e-04],
         [ 8.052e-04]],

        [[-2.393e-03],
         [-1.164e-03],
         [-2.463e-05],
         [-1.757e-05],
         [-8.662e-04],
         [-3.514e-03],
         [ 4.935e-03],
         [-1.049e-03],
         [-1.951e-03],
         [-5.505e-04]],

        [[ 5.740e-04],
         [-2.272e-06],
         [ 1.967e-04],
         [-5.958e-04],
         [-2.583e-03],
         [-1.157e-02],
         [-1.125e-04],
         [-3.215e-05],
         [ 2.261e-03],
         [-1.159e-03]],

        [[-4.388e-03],
         [-1.149e-03],
         [-4.327e-03],
         [ 3.043e-07],
         [-1.437e-03],
         [-1.478e-02],
         [-2.625e-03],
         [ 2.013e-05],
         [-8.555e-03],
         [-1.048e-04]],

        [[ 1.978e-05],
         [-3.899e-03],
         [ 2.173e-06],
         [ 6.618e-03],
         [ 2.209e-04],
         [-2.885e-06],
         [-1.826e-05],
         [ 2.382e-04],
         [ 5.155e-04],
         [-5.544e-04]],

        [[-5.056e-06],
         [-1.492e-03],
         [ 4.929e-04],
         [-1.636e-03],
         [-2.977e-02],
         [ 4.703e-03],
         [ 3.251e-05],
         [-1.534e-04],
         [-5.929e-06],
         [-1.117e-03]],

        [[-1.926e-03],
         [ 6.791e-06],
         [-9.067e-04],
         [-2.884e-04],
         [ 2.984e-04],
         [-1.430e-04],
         [ 1.386e-03],
         [ 1.343e-03],
         [ 2.368e-04],
         [ 2.888e-03]],

        [[-2.370e-04],
         [-2.950e-04],
         [ 6.737e-04],
         [ 1.881e-03],
         [ 3.592e-05],
         [-2.779e-05],
         [-4.455e-03],
         [ 6.166e-05],
         [-4.075e-03],
         [-2.032e-04]],

        [[ 2.773e-05],
         [ 1.197e-04],
         [-5.390e-03],
         [-1.331e-03],
         [-2.881e-04],
         [-9.138e-06],
         [-5.968e-03],
         [ 2.879e-04],
         [ 4.461e-05],
         [ 1.211e-02]],

        [[ 2.915e-04],
         [ 2.495e-03],
         [-1.235e-04],
         [-1.575e-04],
         [ 6.921e-03],
         [-3.456e-03],
         [ 1.011e-03],
         [-1.017e-03],
         [-2.996e-06],
         [-5.952e-04]],

        [[-1.206e-03],
         [-9.299e-03],
         [-9.486e-07],
         [-1.049e-03],
         [-1.758e-05],
         [ 1.801e-06],
         [-7.635e-04],
         [ 3.674e-04],
         [ 6.084e-03],
         [ 1.204e-04]],

        [[-4.558e-03],
         [-6.043e-04],
         [-7.880e-05],
         [-4.280e-05],
         [-1.620e-04],
         [-3.759e-05],
         [ 5.780e-04],
         [-3.040e-03],
         [-1.044e-02],
         [-3.269e-05]]], device='cuda:0')
v tensor([[[6.614e-04],
         [9.367e-03],
         [6.758e-06],
         [1.000e-12],
         [1.000e-12],
         [5.254e-06],
         [8.178e-05],
         [1.732e-04],
         [1.000e-12],
         [1.114e-05]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [8.942e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.753e-04],
         [1.577e-03],
         [2.987e-03],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.156e-04],
         [1.146e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.575e-02],
         [3.752e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.968e-04],
         [1.000e-12]],

        [[1.000e-12],
         [2.856e-02],
         [3.014e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.034e-03]],

        [[1.000e-12],
         [4.312e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [3.695e-03],
         [1.000e-12],
         [1.511e-04],
         [5.873e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.580e-05],
         [1.000e-12],
         [1.000e-12],
         [8.052e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.935e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[5.740e-04],
         [1.000e-12],
         [1.967e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.261e-03],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.043e-07],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.013e-05],
         [1.000e-12],
         [1.000e-12]],

        [[1.978e-05],
         [1.000e-12],
         [2.173e-06],
         [6.618e-03],
         [2.209e-04],
         [1.000e-12],
         [1.000e-12],
         [2.382e-04],
         [5.155e-04],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [4.929e-04],
         [1.000e-12],
         [1.000e-12],
         [4.703e-03],
         [3.251e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [6.791e-06],
         [1.000e-12],
         [1.000e-12],
         [2.984e-04],
         [1.000e-12],
         [1.386e-03],
         [1.343e-03],
         [2.368e-04],
         [2.888e-03]],

        [[1.000e-12],
         [1.000e-12],
         [6.737e-04],
         [1.881e-03],
         [3.592e-05],
         [1.000e-12],
         [1.000e-12],
         [6.166e-05],
         [1.000e-12],
         [1.000e-12]],

        [[2.773e-05],
         [1.197e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.879e-04],
         [4.461e-05],
         [1.211e-02]],

        [[2.915e-04],
         [2.495e-03],
         [1.000e-12],
         [1.000e-12],
         [6.921e-03],
         [1.000e-12],
         [1.011e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.801e-06],
         [1.000e-12],
         [3.674e-04],
         [6.084e-03],
         [1.204e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.780e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-2.441e-04],
        [-6.409e-04],
        [-5.772e-06],
        [-8.421e-07],
        [-2.088e-05],
        [-8.647e-04],
        [-1.515e-04],
        [-2.073e-06],
        [-4.678e-04],
        [-1.756e-03],
        [ 2.825e-03],
        [-1.124e-02],
        [-7.748e-05],
        [-1.068e-02],
        [-2.807e-04],
        [-3.692e-04],
        [ 9.672e-06],
        [-1.410e-04],
        [-1.376e-02],
        [-1.473e-03]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [2.825e-03],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [9.672e-06],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-9.298e-05,  4.875e-08, -3.310e-05,  ...,  2.976e-06, -1.244e-04,
          3.707e-06],
        [-1.075e-04,  1.370e-05,  8.377e-06,  ...,  7.376e-06,  8.251e-06,
         -6.532e-05],
        [-1.569e-04, -3.833e-05, -7.551e-06,  ..., -1.410e-05,  1.803e-05,
          4.456e-06],
        ...,
        [-3.869e-05, -2.491e-05, -8.333e-05,  ...,  1.406e-04, -7.217e-05,
         -9.174e-05],
        [ 2.220e-05, -4.546e-05,  1.959e-06,  ..., -6.047e-05,  9.667e-05,
          1.037e-04],
        [-7.791e-05,  6.211e-05, -4.530e-05,  ..., -2.209e-06, -3.488e-05,
         -1.674e-04]], device='cuda:0')
s after update for 1 param tensor([[1.928e-02, 3.199e-05, 2.628e-03,  ..., 1.552e-04, 4.021e-02,
         1.806e-04],
        [1.067e-02, 2.637e-04, 1.335e-04,  ..., 1.951e-03, 4.566e-04,
         1.493e-02],
        [5.138e-02, 3.419e-03, 1.376e-04,  ..., 2.386e-03, 4.817e-03,
         1.287e-04],
        ...,
        [3.926e-03, 2.693e-04, 1.001e-02,  ..., 2.882e-02, 1.859e-02,
         2.065e-02],
        [2.558e-04, 9.858e-03, 4.486e-04,  ..., 1.661e-02, 3.056e-02,
         3.091e-02],
        [1.656e-02, 7.198e-03, 1.575e-03,  ..., 8.873e-03, 1.475e-02,
         5.286e-02]], device='cuda:0')
b after update for 1 param tensor([[0.850, 0.035, 0.314,  ..., 0.076, 1.228, 0.082],
        [0.633, 0.099, 0.071,  ..., 0.271, 0.131, 0.749],
        [1.388, 0.358, 0.072,  ..., 0.299, 0.425, 0.069],
        ...,
        [0.384, 0.101, 0.613,  ..., 1.040, 0.835, 0.880],
        [0.098, 0.608, 0.130,  ..., 0.789, 1.071, 1.077],
        [0.788, 0.520, 0.243,  ..., 0.577, 0.744, 1.408]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 1.123e-06,  1.550e-05,  7.344e-06, -2.647e-06,  1.945e-05, -5.617e-08,
        -8.220e-07,  2.710e-05, -2.939e-05,  2.485e-04,  4.602e-06,  2.011e-06,
        -3.874e-05,  4.531e-06,  2.528e-06,  1.685e-07,  6.626e-06, -1.383e-05,
        -2.823e-05, -1.160e-05, -2.443e-05, -2.819e-05, -1.064e-04, -3.224e-05,
         4.002e-06, -1.439e-05,  2.019e-05, -3.044e-05,  1.733e-05, -9.079e-06,
        -6.978e-06, -4.126e-05,  1.057e-05, -7.689e-07, -1.141e-05,  1.043e-07,
        -1.552e-05, -6.597e-07, -9.391e-06, -1.592e-06, -2.437e-05, -1.114e-04,
         3.528e-06, -1.863e-06,  2.195e-05,  6.134e-05, -5.225e-06, -7.095e-06,
         1.910e-05,  1.030e-05, -4.675e-05,  1.387e-05,  2.380e-06, -8.986e-06,
        -7.151e-06, -3.167e-06,  5.465e-05,  4.255e-06, -1.868e-06,  8.813e-06,
         3.318e-05,  1.006e-05, -1.120e-04, -4.441e-06, -1.204e-05,  3.122e-05,
         7.232e-05, -8.414e-05,  5.931e-05,  1.141e-04,  9.480e-06, -7.349e-06,
         6.662e-05,  2.719e-05, -1.457e-04, -2.933e-05, -3.290e-07,  2.518e-06,
         8.430e-06, -7.217e-06,  9.324e-05, -9.046e-05, -1.240e-06, -1.495e-05,
        -5.119e-06,  2.229e-05,  1.600e-06, -1.631e-06,  3.680e-05, -1.767e-04,
        -1.856e-05,  1.781e-05, -6.595e-08, -6.479e-06,  1.540e-05, -1.202e-04,
         7.179e-05,  2.192e-05,  3.828e-05,  1.598e-06,  4.228e-05, -7.627e-06,
        -4.924e-06, -1.038e-04, -3.144e-06,  1.141e-05, -1.817e-05, -3.698e-05,
         6.720e-06, -1.461e-06, -3.796e-05, -4.800e-05, -2.324e-05, -6.772e-06,
        -1.965e-05,  3.398e-06,  7.855e-06,  1.153e-05, -3.421e-05, -9.290e-05,
         6.621e-05, -2.588e-05,  6.025e-05, -2.952e-06, -4.560e-07,  4.126e-05,
        -8.605e-06,  1.498e-05, -7.574e-05,  6.107e-05,  2.506e-05,  1.427e-05,
        -7.226e-05, -1.219e-05, -4.401e-05,  2.694e-06,  4.146e-06, -6.557e-07,
        -1.301e-06, -4.173e-06, -2.503e-05, -6.838e-07,  3.160e-05,  7.463e-06,
        -4.280e-05,  1.565e-05, -1.874e-05, -4.902e-05, -4.533e-06,  9.851e-06,
         7.776e-05,  2.413e-05,  5.486e-05, -1.232e-06,  8.814e-06, -2.020e-05,
        -1.297e-04, -3.352e-05,  9.543e-05, -8.976e-05, -5.879e-06,  1.307e-05,
        -3.104e-05,  1.487e-05, -4.624e-05, -1.719e-06, -4.645e-05,  1.087e-04,
        -5.473e-05, -1.377e-06,  6.059e-06,  7.393e-05,  1.487e-05,  2.287e-05,
        -2.496e-05, -2.596e-06,  2.193e-06,  3.044e-05, -1.729e-06, -3.928e-05,
         1.997e-05,  2.022e-05,  3.068e-05,  1.699e-06, -1.015e-05, -6.853e-05,
         1.186e-05, -2.473e-04, -4.887e-06,  3.634e-06,  1.682e-04, -7.186e-05,
         1.164e-04,  2.593e-05,  1.443e-04,  4.747e-05,  7.350e-06, -9.999e-05,
        -1.576e-04, -9.257e-05], device='cuda:0')
s after update for 1 param tensor([3.160e-06, 4.028e-04, 2.082e-04, 1.311e-03, 1.937e-04, 1.243e-05,
        3.129e-04, 4.296e-03, 1.286e-02, 5.260e-02, 4.753e-05, 1.002e-05,
        7.090e-03, 1.686e-03, 1.574e-04, 2.539e-04, 5.169e-04, 1.104e-03,
        7.502e-03, 4.269e-04, 1.890e-02, 8.772e-04, 2.269e-02, 5.797e-03,
        9.693e-04, 1.597e-04, 1.238e-03, 8.368e-04, 2.231e-03, 2.948e-04,
        2.170e-03, 4.722e-03, 7.449e-05, 1.882e-06, 3.940e-03, 2.466e-04,
        2.209e-04, 2.220e-05, 1.061e-04, 6.346e-05, 2.710e-03, 2.165e-02,
        3.601e-04, 1.171e-05, 1.024e-03, 1.287e-02, 2.052e-05, 1.069e-03,
        3.293e-04, 6.073e-04, 2.648e-03, 2.380e-03, 9.145e-06, 1.877e-03,
        2.035e-04, 6.005e-05, 1.393e-02, 2.829e-05, 5.454e-04, 4.866e-03,
        1.071e-02, 1.531e-03, 2.172e-02, 4.009e-03, 6.880e-05, 2.562e-03,
        1.956e-03, 1.870e-02, 5.298e-03, 2.927e-02, 2.402e-05, 1.505e-04,
        1.238e-02, 8.085e-03, 3.587e-02, 5.098e-03, 4.734e-06, 2.291e-04,
        8.492e-04, 1.398e-04, 1.357e-02, 1.390e-02, 1.798e-05, 2.526e-04,
        2.641e-04, 1.460e-03, 1.776e-03, 4.009e-06, 1.185e-02, 1.202e-02,
        3.102e-04, 4.940e-03, 2.679e-04, 9.413e-05, 2.727e-04, 3.602e-02,
        1.822e-02, 3.450e-04, 8.657e-04, 1.999e-04, 8.753e-04, 7.214e-05,
        3.364e-04, 2.278e-02, 1.560e-05, 1.643e-04, 9.182e-04, 1.061e-02,
        3.646e-03, 3.097e-04, 8.494e-03, 1.582e-02, 5.651e-04, 1.689e-03,
        9.042e-04, 8.698e-06, 5.266e-05, 7.126e-05, 6.675e-04, 2.474e-02,
        1.843e-02, 1.435e-03, 4.164e-03, 6.244e-05, 8.702e-06, 6.596e-04,
        2.713e-03, 6.687e-04, 2.761e-02, 1.052e-02, 8.704e-03, 3.109e-04,
        2.494e-03, 4.256e-03, 8.216e-03, 1.035e-04, 4.441e-05, 1.210e-03,
        5.077e-04, 1.300e-04, 3.449e-03, 1.041e-04, 2.286e-03, 8.844e-04,
        4.195e-03, 1.876e-04, 4.351e-04, 3.857e-03, 1.280e-03, 6.965e-04,
        2.036e-02, 1.032e-03, 1.153e-03, 2.663e-05, 2.267e-04, 3.126e-04,
        4.702e-02, 7.617e-03, 2.537e-03, 2.813e-02, 4.237e-05, 4.328e-03,
        2.070e-03, 1.568e-03, 1.016e-03, 2.917e-05, 3.184e-03, 1.988e-02,
        2.837e-03, 7.994e-06, 3.005e-05, 1.111e-02, 4.372e-03, 6.160e-04,
        1.063e-03, 4.390e-06, 2.051e-03, 2.064e-03, 3.385e-05, 2.103e-03,
        5.724e-04, 5.101e-03, 7.347e-03, 5.798e-04, 8.184e-05, 4.087e-03,
        5.816e-04, 5.578e-02, 5.338e-04, 6.597e-05, 4.916e-02, 2.098e-03,
        2.882e-02, 1.782e-03, 2.217e-02, 1.093e-02, 5.765e-05, 1.877e-02,
        1.270e-02, 5.948e-03], device='cuda:0')
b after update for 1 param tensor([0.011, 0.123, 0.088, 0.222, 0.085, 0.022, 0.108, 0.401, 0.695, 1.405,
        0.042, 0.019, 0.516, 0.251, 0.077, 0.098, 0.139, 0.204, 0.531, 0.127,
        0.842, 0.181, 0.923, 0.466, 0.191, 0.077, 0.215, 0.177, 0.289, 0.105,
        0.285, 0.421, 0.053, 0.008, 0.384, 0.096, 0.091, 0.029, 0.063, 0.049,
        0.319, 0.901, 0.116, 0.021, 0.196, 0.695, 0.028, 0.200, 0.111, 0.151,
        0.315, 0.299, 0.019, 0.265, 0.087, 0.047, 0.723, 0.033, 0.143, 0.427,
        0.634, 0.240, 0.903, 0.388, 0.051, 0.310, 0.271, 0.838, 0.446, 1.048,
        0.030, 0.075, 0.681, 0.551, 1.160, 0.437, 0.013, 0.093, 0.178, 0.072,
        0.713, 0.722, 0.026, 0.097, 0.100, 0.234, 0.258, 0.012, 0.667, 0.671,
        0.108, 0.431, 0.100, 0.059, 0.101, 1.163, 0.827, 0.114, 0.180, 0.087,
        0.181, 0.052, 0.112, 0.925, 0.024, 0.079, 0.186, 0.631, 0.370, 0.108,
        0.565, 0.770, 0.146, 0.252, 0.184, 0.018, 0.044, 0.052, 0.158, 0.963,
        0.831, 0.232, 0.395, 0.048, 0.018, 0.157, 0.319, 0.158, 1.018, 0.628,
        0.571, 0.108, 0.306, 0.400, 0.555, 0.062, 0.041, 0.213, 0.138, 0.070,
        0.360, 0.063, 0.293, 0.182, 0.397, 0.084, 0.128, 0.380, 0.219, 0.162,
        0.874, 0.197, 0.208, 0.032, 0.092, 0.108, 1.328, 0.535, 0.309, 1.027,
        0.040, 0.403, 0.279, 0.243, 0.195, 0.033, 0.346, 0.864, 0.326, 0.017,
        0.034, 0.646, 0.405, 0.152, 0.200, 0.013, 0.277, 0.278, 0.036, 0.281,
        0.147, 0.437, 0.525, 0.147, 0.055, 0.392, 0.148, 1.447, 0.142, 0.050,
        1.358, 0.281, 1.040, 0.259, 0.912, 0.640, 0.047, 0.839, 0.690, 0.472],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 7.469e-05],
         [-1.062e-04],
         [ 1.087e-05],
         [ 2.623e-05],
         [-6.717e-06],
         [ 5.914e-06],
         [ 3.384e-05],
         [ 1.492e-05],
         [-2.507e-05],
         [-3.084e-06]],

        [[ 9.924e-06],
         [ 1.135e-05],
         [ 9.380e-06],
         [-1.261e-05],
         [ 1.401e-05],
         [ 7.260e-06],
         [-9.034e-08],
         [-2.298e-07],
         [ 1.341e-05],
         [ 4.430e-05]],

        [[ 8.468e-06],
         [-2.145e-05],
         [-1.323e-05],
         [-4.873e-05],
         [ 2.059e-05],
         [ 2.773e-05],
         [ 3.409e-05],
         [-1.398e-04],
         [ 3.755e-06],
         [-1.754e-05]],

        [[-3.033e-06],
         [ 1.925e-05],
         [ 1.157e-04],
         [ 3.306e-06],
         [ 1.240e-06],
         [ 1.754e-05],
         [ 5.983e-06],
         [ 1.765e-05],
         [ 1.851e-06],
         [ 4.070e-06]],

        [[-2.861e-05],
         [ 1.285e-04],
         [ 1.871e-05],
         [-3.090e-06],
         [ 3.482e-07],
         [-1.854e-05],
         [-2.506e-06],
         [ 3.018e-06],
         [-4.430e-05],
         [ 2.016e-05]],

        [[ 3.622e-05],
         [-2.450e-04],
         [ 7.515e-06],
         [ 7.905e-07],
         [-2.122e-05],
         [ 1.561e-05],
         [-1.804e-05],
         [-5.138e-06],
         [ 1.826e-05],
         [ 4.676e-05]],

        [[ 4.585e-05],
         [ 8.382e-05],
         [-3.384e-05],
         [ 1.516e-06],
         [-2.285e-05],
         [ 6.884e-06],
         [ 4.554e-06],
         [ 2.486e-05],
         [-3.722e-05],
         [-4.162e-06]],

        [[-3.656e-06],
         [-2.599e-06],
         [ 8.019e-05],
         [ 3.982e-05],
         [-2.366e-05],
         [-8.176e-05],
         [-7.784e-05],
         [ 1.342e-06],
         [-9.323e-07],
         [-1.102e-06]],

        [[-1.855e-05],
         [-6.197e-06],
         [-6.909e-05],
         [-1.625e-05],
         [-2.606e-06],
         [-1.895e-06],
         [-6.498e-06],
         [-4.609e-07],
         [-5.357e-06],
         [ 2.566e-05]],

        [[ 1.240e-06],
         [-1.891e-05],
         [ 2.098e-06],
         [ 1.006e-06],
         [-1.361e-05],
         [ 2.135e-05],
         [-1.280e-04],
         [-3.499e-05],
         [-3.935e-05],
         [-1.071e-05]],

        [[-2.981e-05],
         [ 1.040e-06],
         [-3.374e-05],
         [-3.526e-05],
         [-4.297e-05],
         [ 3.037e-05],
         [ 9.361e-06],
         [-7.164e-06],
         [-6.803e-05],
         [ 8.287e-06]],

        [[ 1.030e-04],
         [ 1.045e-04],
         [-1.551e-05],
         [-2.590e-05],
         [ 3.191e-05],
         [ 7.632e-05],
         [-6.110e-06],
         [-5.916e-06],
         [-8.688e-05],
         [ 3.139e-06]],

        [[ 6.714e-06],
         [ 3.377e-05],
         [-3.800e-06],
         [ 7.979e-05],
         [ 2.177e-05],
         [-2.015e-06],
         [-1.383e-06],
         [-2.761e-05],
         [ 1.837e-05],
         [-3.441e-05]],

        [[-2.081e-05],
         [ 5.838e-05],
         [ 3.018e-05],
         [-5.411e-05],
         [-4.212e-05],
         [ 1.294e-04],
         [ 1.390e-05],
         [-1.051e-05],
         [-5.372e-07],
         [-1.141e-05]],

        [[ 1.825e-05],
         [ 2.604e-06],
         [ 1.165e-05],
         [-2.243e-06],
         [ 1.449e-05],
         [-5.752e-05],
         [ 2.700e-05],
         [-5.063e-05],
         [-3.721e-05],
         [-1.351e-04]],

        [[-2.326e-05],
         [-1.192e-05],
         [ 3.539e-05],
         [ 3.478e-05],
         [-8.199e-06],
         [-1.481e-06],
         [ 1.368e-05],
         [-9.217e-06],
         [-9.367e-05],
         [-2.567e-06]],

        [[ 5.768e-06],
         [-1.137e-05],
         [-1.247e-04],
         [-3.412e-05],
         [ 6.209e-06],
         [ 2.047e-06],
         [ 6.894e-05],
         [-4.532e-05],
         [-7.714e-06],
         [-1.152e-04]],

        [[-1.810e-05],
         [-4.236e-05],
         [ 1.054e-05],
         [ 2.348e-06],
         [ 9.178e-05],
         [-1.283e-05],
         [ 6.822e-05],
         [-1.348e-05],
         [-1.069e-06],
         [ 2.003e-05]],

        [[ 1.902e-05],
         [-1.298e-05],
         [-4.242e-07],
         [ 9.783e-06],
         [ 2.984e-06],
         [-2.681e-06],
         [-7.015e-06],
         [-3.013e-05],
         [-9.540e-05],
         [ 3.960e-05]],

        [[-5.047e-06],
         [ 2.519e-05],
         [-4.787e-08],
         [-1.278e-05],
         [-8.579e-06],
         [ 1.728e-05],
         [ 3.828e-05],
         [ 7.646e-07],
         [-1.983e-05],
         [ 1.299e-05]]], device='cuda:0')
s after update for 1 param tensor([[[8.462e-03],
         [3.070e-02],
         [8.271e-04],
         [1.238e-03],
         [1.884e-04],
         [7.249e-04],
         [2.921e-03],
         [4.162e-03],
         [2.428e-03],
         [1.056e-03]],

        [[1.687e-04],
         [2.433e-04],
         [2.699e-04],
         [5.994e-05],
         [1.409e-04],
         [2.271e-04],
         [9.832e-05],
         [3.623e-04],
         [3.164e-04],
         [2.800e-03]],

        [[6.012e-03],
         [3.002e-03],
         [1.961e-04],
         [7.588e-04],
         [4.318e-03],
         [6.897e-03],
         [1.256e-02],
         [1.788e-02],
         [3.208e-05],
         [8.116e-04]],

        [[1.376e-04],
         [3.405e-03],
         [3.408e-02],
         [3.844e-04],
         [6.551e-05],
         [7.518e-04],
         [1.219e-04],
         [1.864e-04],
         [1.312e-05],
         [6.411e-05]],

        [[1.941e-03],
         [3.985e-02],
         [6.126e-03],
         [3.464e-05],
         [1.010e-04],
         [1.458e-03],
         [6.834e-04],
         [1.882e-06],
         [9.640e-03],
         [5.453e-04]],

        [[5.129e-03],
         [5.459e-02],
         [1.736e-03],
         [6.339e-05],
         [2.990e-03],
         [6.118e-04],
         [1.162e-04],
         [1.198e-04],
         [1.852e-04],
         [1.018e-02]],

        [[1.458e-03],
         [2.081e-02],
         [4.801e-04],
         [7.557e-05],
         [2.082e-03],
         [6.972e-04],
         [2.311e-05],
         [6.580e-04],
         [3.739e-03],
         [6.490e-04]],

        [[8.645e-04],
         [4.841e-03],
         [1.951e-02],
         [1.249e-03],
         [3.897e-03],
         [2.432e-02],
         [1.490e-02],
         [2.070e-05],
         [3.449e-05],
         [1.265e-04]],

        [[1.265e-04],
         [1.521e-04],
         [2.344e-03],
         [2.245e-04],
         [3.151e-05],
         [6.644e-06],
         [1.257e-03],
         [3.760e-04],
         [1.007e-04],
         [8.993e-03]],

        [[1.896e-03],
         [8.676e-04],
         [1.887e-05],
         [1.319e-05],
         [6.671e-04],
         [2.614e-03],
         [2.285e-02],
         [2.523e-03],
         [1.557e-03],
         [4.125e-04]],

        [[7.579e-03],
         [1.882e-06],
         [4.483e-03],
         [1.371e-03],
         [5.437e-03],
         [1.232e-02],
         [1.351e-04],
         [2.753e-05],
         [1.505e-02],
         [1.761e-03]],

        [[4.180e-03],
         [2.947e-03],
         [3.679e-03],
         [1.520e-03],
         [2.394e-03],
         [1.100e-02],
         [2.083e-03],
         [1.419e-03],
         [8.795e-03],
         [1.032e-04]],

        [[1.406e-03],
         [3.024e-03],
         [4.665e-04],
         [2.577e-02],
         [4.758e-03],
         [4.122e-06],
         [1.513e-05],
         [4.972e-03],
         [7.204e-03],
         [2.998e-03]],

        [[2.281e-04],
         [2.374e-03],
         [7.022e-03],
         [2.575e-03],
         [2.293e-02],
         [2.227e-02],
         [1.878e-03],
         [1.670e-04],
         [4.432e-06],
         [8.763e-04]],

        [[1.447e-03],
         [8.241e-04],
         [6.935e-04],
         [2.842e-04],
         [5.464e-03],
         [1.158e-03],
         [1.189e-02],
         [1.160e-02],
         [4.878e-03],
         [1.879e-02]],

        [[2.733e-04],
         [3.691e-04],
         [8.222e-03],
         [1.378e-02],
         [1.895e-03],
         [2.607e-05],
         [3.352e-03],
         [2.483e-03],
         [6.567e-03],
         [1.782e-04]],

        [[1.667e-03],
         [3.460e-03],
         [9.529e-03],
         [1.441e-03],
         [2.408e-04],
         [8.007e-06],
         [5.274e-03],
         [5.631e-03],
         [2.112e-03],
         [3.554e-02]],

        [[5.400e-03],
         [1.590e-02],
         [1.824e-04],
         [1.173e-04],
         [2.645e-02],
         [2.603e-03],
         [1.009e-02],
         [9.041e-04],
         [2.759e-06],
         [6.619e-04]],

        [[9.597e-04],
         [7.543e-03],
         [1.882e-06],
         [1.086e-03],
         [1.318e-05],
         [4.244e-04],
         [5.905e-04],
         [6.063e-03],
         [2.496e-02],
         [4.025e-03]],

        [[6.070e-03],
         [4.718e-04],
         [5.868e-05],
         [7.858e-05],
         [1.480e-04],
         [1.702e-04],
         [7.659e-03],
         [3.276e-03],
         [7.785e-03],
         [8.970e-05]]], device='cuda:0')
b after update for 1 param tensor([[[0.563],
         [1.073],
         [0.176],
         [0.216],
         [0.084],
         [0.165],
         [0.331],
         [0.395],
         [0.302],
         [0.199]],

        [[0.080],
         [0.096],
         [0.101],
         [0.047],
         [0.073],
         [0.092],
         [0.061],
         [0.117],
         [0.109],
         [0.324]],

        [[0.475],
         [0.336],
         [0.086],
         [0.169],
         [0.402],
         [0.509],
         [0.687],
         [0.819],
         [0.035],
         [0.174]],

        [[0.072],
         [0.357],
         [1.131],
         [0.120],
         [0.050],
         [0.168],
         [0.068],
         [0.084],
         [0.022],
         [0.049]],

        [[0.270],
         [1.223],
         [0.479],
         [0.036],
         [0.062],
         [0.234],
         [0.160],
         [0.008],
         [0.601],
         [0.143]],

        [[0.439],
         [1.431],
         [0.255],
         [0.049],
         [0.335],
         [0.151],
         [0.066],
         [0.067],
         [0.083],
         [0.618]],

        [[0.234],
         [0.884],
         [0.134],
         [0.053],
         [0.279],
         [0.162],
         [0.029],
         [0.157],
         [0.375],
         [0.156]],

        [[0.180],
         [0.426],
         [0.856],
         [0.216],
         [0.382],
         [0.955],
         [0.748],
         [0.028],
         [0.036],
         [0.069]],

        [[0.069],
         [0.076],
         [0.297],
         [0.092],
         [0.034],
         [0.016],
         [0.217],
         [0.119],
         [0.061],
         [0.581]],

        [[0.267],
         [0.180],
         [0.027],
         [0.022],
         [0.158],
         [0.313],
         [0.926],
         [0.308],
         [0.242],
         [0.124]],

        [[0.533],
         [0.008],
         [0.410],
         [0.227],
         [0.452],
         [0.680],
         [0.071],
         [0.032],
         [0.751],
         [0.257]],

        [[0.396],
         [0.332],
         [0.372],
         [0.239],
         [0.300],
         [0.642],
         [0.280],
         [0.231],
         [0.574],
         [0.062]],

        [[0.230],
         [0.337],
         [0.132],
         [0.983],
         [0.422],
         [0.012],
         [0.024],
         [0.432],
         [0.520],
         [0.335]],

        [[0.093],
         [0.298],
         [0.513],
         [0.311],
         [0.928],
         [0.914],
         [0.265],
         [0.079],
         [0.013],
         [0.181]],

        [[0.233],
         [0.176],
         [0.161],
         [0.103],
         [0.453],
         [0.208],
         [0.668],
         [0.660],
         [0.428],
         [0.840]],

        [[0.101],
         [0.118],
         [0.555],
         [0.719],
         [0.267],
         [0.031],
         [0.355],
         [0.305],
         [0.496],
         [0.082]],

        [[0.250],
         [0.360],
         [0.598],
         [0.232],
         [0.095],
         [0.017],
         [0.445],
         [0.460],
         [0.281],
         [1.155]],

        [[0.450],
         [0.772],
         [0.083],
         [0.066],
         [0.996],
         [0.312],
         [0.615],
         [0.184],
         [0.010],
         [0.158]],

        [[0.190],
         [0.532],
         [0.008],
         [0.202],
         [0.022],
         [0.126],
         [0.149],
         [0.477],
         [0.968],
         [0.389]],

        [[0.477],
         [0.133],
         [0.047],
         [0.054],
         [0.075],
         [0.080],
         [0.536],
         [0.351],
         [0.540],
         [0.058]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 2.380e-06],
        [ 4.279e-05],
        [ 2.945e-06],
        [ 1.713e-05],
        [ 1.088e-05],
        [-7.152e-06],
        [ 3.819e-06],
        [ 8.672e-07],
        [ 1.621e-05],
        [ 1.647e-05],
        [ 4.657e-05],
        [ 7.013e-05],
        [-6.671e-06],
        [ 3.108e-05],
        [-1.767e-05],
        [ 1.223e-06],
        [-8.429e-06],
        [-1.748e-05],
        [-9.137e-05],
        [ 4.724e-05]], device='cuda:0')
s after update for 1 param tensor([[3.373e-04],
        [5.436e-03],
        [7.995e-06],
        [9.170e-05],
        [4.088e-05],
        [6.460e-04],
        [1.591e-04],
        [1.882e-06],
        [1.173e-03],
        [3.880e-03],
        [1.681e-02],
        [1.106e-02],
        [5.837e-05],
        [9.274e-03],
        [2.102e-04],
        [2.812e-04],
        [1.006e-03],
        [3.515e-04],
        [1.154e-02],
        [7.395e-03]], device='cuda:0')
b after update for 1 param tensor([[0.112],
        [0.452],
        [0.017],
        [0.059],
        [0.039],
        [0.156],
        [0.077],
        [0.008],
        [0.210],
        [0.382],
        [0.794],
        [0.644],
        [0.047],
        [0.590],
        [0.089],
        [0.103],
        [0.194],
        [0.115],
        [0.658],
        [0.527]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 6.475135061740562
exp ma of ||w||^2 231.7778831645401
||w|| 2.544628668733527
exp ma of ||w|| 3.43665850020582
||w||^2 5.014382728806139
exp ma of ||w||^2 6.374218060273374
||w|| 2.2392817439541055
exp ma of ||w|| 2.4602216574091353
||w||^2 5.473615328913923
exp ma of ||w||^2 4.687358233792419
||w|| 2.3395758865473724
exp ma of ||w|| 2.12444621814578
cuda
Objective function 52.94 = squared loss an data 42.66 + 0.5*rho*h**2 4.815969 + alpha*h 3.907566 + L2reg 1.41 + L1reg 0.13 ; SHD = 41 ; DAG True
Proportion of microbatches that were clipped  0.7955172973850901
iteration 2 in inner loop, alpha 398.15251472690727 rho 100000.0 h 0.00981424361148342
iteration 4 in outer loop, alpha = 10212.396126210328, rho = 1000000.0, h = 0.00981424361148342
Threshold 0.3
[[0.004 0.057 0.04  0.148 0.043 0.028 0.244 0.065 0.031 0.111 0.124 0.077
  0.02  0.176 0.103 0.089 0.054 0.025 0.231 0.03 ]
 [0.089 0.008 0.035 0.072 0.09  0.068 0.188 0.025 0.548 0.085 0.091 0.037
  0.038 0.057 0.085 0.077 0.019 0.004 0.109 0.021]
 [0.167 0.115 0.006 0.38  0.174 0.23  0.165 0.064 0.422 0.287 0.16  0.071
  0.113 0.176 0.271 0.247 0.103 0.063 1.606 0.291]
 [0.061 0.076 0.01  0.007 0.006 0.059 0.104 0.035 0.102 0.082 0.018 0.051
  0.034 0.09  0.061 0.073 0.048 0.041 0.067 0.007]
 [0.205 0.081 0.028 0.832 0.006 0.081 0.228 0.051 0.428 0.084 0.093 0.122
  0.059 0.166 0.152 0.071 0.039 0.05  0.143 0.024]
 [0.161 0.045 0.034 0.114 0.078 0.005 0.139 0.037 0.149 0.092 0.08  0.095
  0.036 0.095 0.141 0.098 0.06  0.039 0.649 0.038]
 [0.025 0.035 0.025 0.069 0.027 0.047 0.007 0.003 0.045 0.019 0.039 0.025
  0.01  0.027 0.069 0.022 0.009 0.012 0.048 0.012]
 [0.097 0.198 0.096 0.211 0.107 0.179 2.126 0.004 0.326 0.18  0.1   0.209
  0.101 0.184 0.288 0.768 0.115 0.148 0.208 0.114]
 [0.134 0.009 0.02  0.093 0.015 0.05  0.094 0.02  0.004 0.043 0.027 0.036
  0.044 0.064 0.064 0.06  0.029 0.014 0.102 0.025]
 [0.073 0.048 0.033 0.083 0.048 0.071 0.414 0.029 0.08  0.007 0.036 0.04
  0.003 0.065 0.063 0.15  0.04  0.033 0.081 0.045]
 [0.041 0.062 0.031 0.252 0.057 0.065 0.189 0.042 0.167 0.143 0.005 0.152
  0.089 0.192 0.154 0.104 0.041 0.035 0.139 0.038]
 [0.099 0.224 0.095 0.159 0.055 0.082 0.255 0.024 0.163 0.126 0.055 0.005
  0.061 0.088 0.089 0.113 0.025 0.019 0.409 0.054]
 [0.127 0.143 0.066 0.21  0.083 0.181 0.427 0.07  0.152 0.77  0.058 0.091
  0.006 0.129 0.117 0.211 0.075 0.054 0.439 0.224]
 [0.032 0.071 0.055 0.08  0.035 0.06  0.245 0.028 0.082 0.096 0.035 0.091
  0.045 0.006 0.072 0.025 0.032 0.045 0.193 0.039]
 [0.055 0.133 0.031 0.097 0.047 0.062 0.091 0.024 0.093 0.074 0.055 0.054
  0.049 0.109 0.008 0.061 0.01  0.012 0.127 0.04 ]
 [0.071 0.085 0.04  0.08  0.099 0.061 0.41  0.007 0.099 0.057 0.054 0.088
  0.033 0.159 0.152 0.008 0.043 0.028 0.15  0.029]
 [0.096 0.705 0.064 0.101 0.17  0.082 0.411 0.039 0.24  0.224 0.2   0.206
  0.103 0.131 0.946 0.157 0.004 0.01  0.228 0.043]
 [0.199 0.928 0.099 0.157 0.141 0.164 0.367 0.041 0.345 0.25  0.266 0.164
  0.093 0.128 0.254 0.17  0.664 0.003 0.432 0.053]
 [0.028 0.086 0.004 0.077 0.03  0.008 0.06  0.028 0.042 0.032 0.062 0.017
  0.01  0.039 0.043 0.039 0.026 0.015 0.005 0.015]
 [0.251 0.372 0.029 1.149 0.331 0.207 0.278 0.066 0.205 0.195 0.192 0.158
  0.041 0.133 0.08  0.245 0.177 0.153 0.417 0.006]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.548 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.38  0.    0.    0.    0.    0.422 0.    0.    0.
  0.    0.    0.    0.    0.    0.    1.606 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.832 0.    0.    0.    0.    0.428 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.649 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    2.126 0.    0.326 0.    0.    0.
  0.    0.    0.    0.768 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.414 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.409 0.   ]
 [0.    0.    0.    0.    0.    0.    0.427 0.    0.    0.77  0.    0.
  0.    0.    0.    0.    0.    0.    0.439 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.41  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.705 0.    0.    0.    0.    0.411 0.    0.    0.    0.    0.
  0.    0.    0.946 0.    0.    0.    0.    0.   ]
 [0.    0.928 0.    0.    0.    0.    0.367 0.    0.345 0.    0.    0.
  0.    0.    0.    0.    0.664 0.    0.432 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.372 0.    1.149 0.331 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.417 0.   ]]
{'fdr': 0.5714285714285714, 'tpr': 0.3, 'fpr': 0.10666666666666667, 'f1': 0.35294117647058826, 'shd': 41, 'npred': 28, 'ntrue': 40}
[0.057 0.04  0.148 0.043 0.028 0.244 0.065 0.031 0.111 0.124 0.077 0.02
 0.176 0.103 0.089 0.054 0.025 0.231 0.03  0.089 0.035 0.072 0.09  0.068
 0.188 0.025 0.548 0.085 0.091 0.037 0.038 0.057 0.085 0.077 0.019 0.004
 0.109 0.021 0.167 0.115 0.38  0.174 0.23  0.165 0.064 0.422 0.287 0.16
 0.071 0.113 0.176 0.271 0.247 0.103 0.063 1.606 0.291 0.061 0.076 0.01
 0.006 0.059 0.104 0.035 0.102 0.082 0.018 0.051 0.034 0.09  0.061 0.073
 0.048 0.041 0.067 0.007 0.205 0.081 0.028 0.832 0.081 0.228 0.051 0.428
 0.084 0.093 0.122 0.059 0.166 0.152 0.071 0.039 0.05  0.143 0.024 0.161
 0.045 0.034 0.114 0.078 0.139 0.037 0.149 0.092 0.08  0.095 0.036 0.095
 0.141 0.098 0.06  0.039 0.649 0.038 0.025 0.035 0.025 0.069 0.027 0.047
 0.003 0.045 0.019 0.039 0.025 0.01  0.027 0.069 0.022 0.009 0.012 0.048
 0.012 0.097 0.198 0.096 0.211 0.107 0.179 2.126 0.326 0.18  0.1   0.209
 0.101 0.184 0.288 0.768 0.115 0.148 0.208 0.114 0.134 0.009 0.02  0.093
 0.015 0.05  0.094 0.02  0.043 0.027 0.036 0.044 0.064 0.064 0.06  0.029
 0.014 0.102 0.025 0.073 0.048 0.033 0.083 0.048 0.071 0.414 0.029 0.08
 0.036 0.04  0.003 0.065 0.063 0.15  0.04  0.033 0.081 0.045 0.041 0.062
 0.031 0.252 0.057 0.065 0.189 0.042 0.167 0.143 0.152 0.089 0.192 0.154
 0.104 0.041 0.035 0.139 0.038 0.099 0.224 0.095 0.159 0.055 0.082 0.255
 0.024 0.163 0.126 0.055 0.061 0.088 0.089 0.113 0.025 0.019 0.409 0.054
 0.127 0.143 0.066 0.21  0.083 0.181 0.427 0.07  0.152 0.77  0.058 0.091
 0.129 0.117 0.211 0.075 0.054 0.439 0.224 0.032 0.071 0.055 0.08  0.035
 0.06  0.245 0.028 0.082 0.096 0.035 0.091 0.045 0.072 0.025 0.032 0.045
 0.193 0.039 0.055 0.133 0.031 0.097 0.047 0.062 0.091 0.024 0.093 0.074
 0.055 0.054 0.049 0.109 0.061 0.01  0.012 0.127 0.04  0.071 0.085 0.04
 0.08  0.099 0.061 0.41  0.007 0.099 0.057 0.054 0.088 0.033 0.159 0.152
 0.043 0.028 0.15  0.029 0.096 0.705 0.064 0.101 0.17  0.082 0.411 0.039
 0.24  0.224 0.2   0.206 0.103 0.131 0.946 0.157 0.01  0.228 0.043 0.199
 0.928 0.099 0.157 0.141 0.164 0.367 0.041 0.345 0.25  0.266 0.164 0.093
 0.128 0.254 0.17  0.664 0.432 0.053 0.028 0.086 0.004 0.077 0.03  0.008
 0.06  0.028 0.042 0.032 0.062 0.017 0.01  0.039 0.043 0.039 0.026 0.015
 0.015 0.251 0.372 0.029 1.149 0.331 0.207 0.278 0.066 0.205 0.195 0.192
 0.158 0.041 0.133 0.08  0.245 0.177 0.153 0.417]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.6970588235294117, 0.4040275330460189)
Iterations 567
Achieves (3.8714582201520527, 1e-05)-DP
