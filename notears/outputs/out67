samples  5000  graph  15 30 ER mlp  minibatch size  50  noise  0.5  minibatches per NN training  250 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006610539212474009
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0013085613332552981
iteration 4 in outer loop, alpha = 171.48074067169492, rho = 100000.0, h = 0.0013085613332552981
cuda
iteration 1 in inner loop,alpha 171.48074067169492 rho 100000.0 h 0.000287732824038045
iteration 5 in outer loop, alpha = 200.25402307549942, rho = 100000.0, h = 0.000287732824038045
cuda
iteration 1 in inner loop,alpha 200.25402307549942 rho 100000.0 h 0.00015606770021392435
iteration 6 in outer loop, alpha = 356.32172328942374, rho = 1000000.0, h = 0.00015606770021392435
Threshold 0.3
[[0.002 0.    0.    0.004 0.211 0.    0.    0.    0.    2.744 0.239 0.
  0.    0.    0.004]
 [2.523 0.001 0.002 0.263 0.209 0.01  0.009 0.003 0.001 2.197 0.222 1.354
  0.001 0.001 0.099]
 [0.028 0.07  0.    0.146 0.155 0.033 0.129 0.006 0.068 0.126 0.118 0.157
  0.006 0.006 0.007]
 [0.166 0.001 0.001 0.001 0.191 0.    0.    0.    0.    0.222 0.332 0.001
  0.    0.    1.023]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.001 0.
  0.    0.    0.001]
 [2.645 0.03  0.002 0.97  0.362 0.001 1.363 0.    0.056 1.752 1.28  1.483
  0.012 0.005 1.924]
 [0.417 0.046 0.002 0.338 0.409 0.    0.001 0.    0.055 0.145 1.772 0.095
  0.005 0.006 0.163]
 [2.688 0.005 0.007 2.091 0.143 0.607 0.048 0.    0.005 0.315 0.148 0.061
  0.023 0.024 0.118]
 [0.108 0.384 0.002 0.646 0.408 0.002 0.001 0.001 0.003 2.326 2.277 1.046
  0.    0.    0.37 ]
 [0.    0.    0.    0.    2.801 0.    0.    0.    0.    0.002 0.199 0.
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.113 0.    0.    0.    0.001 0.004 0.009 0.
  0.    0.    0.002]
 [0.375 0.    0.    0.185 0.143 0.    0.003 0.    0.001 0.428 0.324 0.001
  0.    0.001 0.034]
 [0.04  0.078 0.006 0.104 3.056 0.017 0.007 0.004 1.224 0.109 1.909 0.034
  0.001 0.017 0.016]
 [0.127 1.42  0.008 1.126 1.233 0.005 0.008 0.002 3.46  0.379 0.457 0.155
  0.011 0.002 0.562]
 [0.102 0.    0.002 0.    0.098 0.    0.001 0.    0.    0.112 0.146 0.002
  0.    0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.744 0.    0.
  0.    0.    0.   ]
 [2.523 0.    0.    0.    0.    0.    0.    0.    0.    2.197 0.    1.354
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.332 0.
  0.    0.    1.023]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.645 0.    0.    0.97  0.362 0.    1.363 0.    0.    1.752 1.28  1.483
  0.    0.    1.924]
 [0.417 0.    0.    0.338 0.409 0.    0.    0.    0.    0.    1.772 0.
  0.    0.    0.   ]
 [2.688 0.    0.    2.091 0.    0.607 0.    0.    0.    0.315 0.    0.
  0.    0.    0.   ]
 [0.    0.384 0.    0.646 0.408 0.    0.    0.    0.    2.326 2.277 1.046
  0.    0.    0.37 ]
 [0.    0.    0.    0.    2.801 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.113 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.375 0.    0.    0.    0.    0.    0.    0.    0.    0.428 0.324 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.056 0.    0.    0.    1.224 0.    1.909 0.
  0.    0.    0.   ]
 [0.    1.42  0.    1.126 1.233 0.    0.    0.    3.46  0.379 0.457 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.3409090909090909, 'tpr': 0.9666666666666667, 'fpr': 0.2, 'f1': 0.7837837837837838, 'shd': 16, 'npred': 44, 'ntrue': 30}
[4.983e-05 4.863e-04 3.571e-03 2.114e-01 5.948e-05 5.304e-05 4.128e-05
 1.612e-04 2.744e+00 2.388e-01 4.391e-04 6.647e-05 9.869e-05 4.005e-03
 2.523e+00 2.037e-03 2.629e-01 2.093e-01 9.692e-03 8.831e-03 2.752e-03
 1.242e-03 2.197e+00 2.223e-01 1.354e+00 8.802e-04 7.457e-04 9.853e-02
 2.804e-02 6.961e-02 1.457e-01 1.554e-01 3.339e-02 1.287e-01 5.538e-03
 6.823e-02 1.265e-01 1.179e-01 1.575e-01 5.823e-03 5.919e-03 6.855e-03
 1.659e-01 7.805e-04 1.410e-03 1.912e-01 2.061e-04 3.877e-04 9.021e-05
 3.181e-04 2.220e-01 3.319e-01 9.598e-04 4.266e-05 7.248e-05 1.023e+00
 5.361e-05 4.191e-06 8.741e-06 2.017e-04 8.903e-06 1.016e-04 3.667e-06
 1.418e-04 2.997e-04 9.048e-04 4.940e-05 1.815e-05 5.430e-05 6.148e-04
 2.645e+00 3.014e-02 1.563e-03 9.699e-01 3.623e-01 1.363e+00 1.102e-04
 5.649e-02 1.752e+00 1.280e+00 1.483e+00 1.239e-02 5.462e-03 1.924e+00
 4.165e-01 4.560e-02 1.636e-03 3.377e-01 4.086e-01 2.177e-04 5.471e-05
 5.536e-02 1.453e-01 1.772e+00 9.487e-02 4.694e-03 6.078e-03 1.633e-01
 2.688e+00 4.990e-03 7.011e-03 2.091e+00 1.431e-01 6.070e-01 4.795e-02
 5.261e-03 3.152e-01 1.481e-01 6.149e-02 2.251e-02 2.409e-02 1.178e-01
 1.078e-01 3.843e-01 1.631e-03 6.456e-01 4.083e-01 1.894e-03 1.453e-03
 1.062e-03 2.326e+00 2.277e+00 1.046e+00 7.208e-05 4.536e-04 3.696e-01
 1.951e-04 3.905e-06 9.500e-05 3.636e-04 2.801e+00 5.161e-06 2.889e-04
 1.524e-05 3.551e-04 1.987e-01 2.961e-04 1.853e-05 9.356e-05 1.685e-03
 5.671e-04 8.529e-05 3.299e-04 6.071e-04 3.113e+00 1.618e-04 4.296e-04
 1.379e-05 5.573e-04 4.405e-03 4.476e-04 1.258e-04 1.501e-04 2.450e-03
 3.751e-01 3.325e-04 3.181e-04 1.847e-01 1.432e-01 2.345e-04 3.331e-03
 5.054e-05 1.366e-03 4.278e-01 3.245e-01 2.333e-04 5.066e-04 3.384e-02
 4.045e-02 7.837e-02 6.284e-03 1.037e-01 3.056e+00 1.672e-02 7.167e-03
 4.197e-03 1.224e+00 1.089e-01 1.909e+00 3.390e-02 1.667e-02 1.627e-02
 1.272e-01 1.420e+00 7.506e-03 1.126e+00 1.233e+00 5.401e-03 8.229e-03
 2.052e-03 3.460e+00 3.791e-01 4.571e-01 1.549e-01 1.143e-02 5.622e-01
 1.022e-01 2.167e-04 1.511e-03 1.242e-04 9.821e-02 5.704e-05 1.420e-03
 3.614e-05 3.768e-04 1.116e-01 1.460e-01 1.936e-03 3.999e-05 6.669e-05]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9803703703703704, 0.9718688168353634)
cuda
2565
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
||w||^2 2665199887.8523684
exp ma of ||w||^2 549743435320.9347
||w|| 51625.57397116635
exp ma of ||w|| 373433.15245411685
||w||^2 0.21041360219733735
exp ma of ||w||^2 16225314.140499998
||w|| 0.4587086245072544
exp ma of ||w|| 13.663513610944262
||w||^2 0.16373536230981336
exp ma of ||w||^2 4873.154888873428
||w|| 0.4046422646113643
exp ma of ||w|| 0.4033641462689814
||w||^2 0.2154600094661321
exp ma of ||w||^2 137.66816042913172
||w|| 0.46417670069288497
exp ma of ||w|| 0.40520938822128644
||w||^2 0.13645635999697822
exp ma of ||w||^2 69.60141019137106
||w|| 0.3693999999959099
exp ma of ||w|| 0.4160141044105001
||w||^2 0.1751413954130898
exp ma of ||w||^2 22.502871259252636
||w|| 0.41849897898691435
exp ma of ||w|| 0.4377165581559299
||w||^2 0.13078557065898735
exp ma of ||w||^2 1.9276475429662716
||w|| 0.36164287724077654
exp ma of ||w|| 0.44976302465730045
||w||^2 0.21558601857680296
exp ma of ||w||^2 0.2881588526234804
||w|| 0.4643124148424237
exp ma of ||w|| 0.4952257234266427
||w||^2 0.33240551690541364
exp ma of ||w||^2 0.2856817184451344
||w|| 0.5765461966793413
exp ma of ||w|| 0.496299440506345
||w||^2 0.12063906835852961
exp ma of ||w||^2 0.3227219046152766
||w|| 0.3473313523978646
exp ma of ||w|| 0.5207987014781817
||w||^2 0.4651865197794963
exp ma of ||w||^2 0.4619551574024655
||w|| 0.682045834075318
exp ma of ||w|| 0.5998839930018847
||w||^2 0.13434970272858845
exp ma of ||w||^2 0.49350515627249525
||w|| 0.366537450649437
exp ma of ||w|| 0.6255549145437109
||w||^2 0.7615582659594765
exp ma of ||w||^2 0.5683077940423422
||w|| 0.8726730578856416
exp ma of ||w|| 0.6956670117343995
||w||^2 0.28505942879645746
exp ma of ||w||^2 0.5021331798821187
||w|| 0.5339095698678358
exp ma of ||w|| 0.6531303627730028
||w||^2 0.2022430137719628
exp ma of ||w||^2 0.4232240769243879
||w|| 0.44971436909661094
exp ma of ||w|| 0.6146869365511327
||w||^2 0.14447074724001985
exp ma of ||w||^2 0.462766813981097
||w|| 0.38009307707457635
exp ma of ||w|| 0.6282577562074944
||w||^2 0.5235492869906309
exp ma of ||w||^2 0.4665089271950616
||w|| 0.7235670577013791
exp ma of ||w|| 0.6353561875398637
cuda
Objective function 13.54 = squared loss an data 11.15 + 0.5*rho*h**2 1.447341 + alpha*h 0.000000 + L2reg 0.78 + L1reg 0.16 ; SHD = 56 ; DAG False
Proportion of microbatches that were clipped  0.7652608660591887
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.7013764668420528
iteration 1 in outer loop, alpha = 1.7013764668420528, rho = 1.0, h = 1.7013764668420528
cuda
2565
cuda
Objective function 16.43 = squared loss an data 11.15 + 0.5*rho*h**2 1.447341 + alpha*h 2.894682 + L2reg 0.78 + L1reg 0.16 ; SHD = 56 ; DAG False
||w||^2 2.260191851224286
exp ma of ||w||^2 1522341.644141143
||w|| 1.5033934452512043
exp ma of ||w|| 56.24240090018876
||w||^2 0.11251558652313819
exp ma of ||w||^2 297.30430876591413
||w|| 0.33543343083708604
exp ma of ||w|| 0.6738326015144764
||w||^2 0.11128175248427578
exp ma of ||w||^2 3.382688573110561
||w|| 0.33358919719360786
exp ma of ||w|| 0.6752764312792947
||w||^2 0.9405376109109064
exp ma of ||w||^2 0.6664025808954919
||w|| 0.9698131835105699
exp ma of ||w|| 0.744411778762737
||w||^2 0.44000036810261356
exp ma of ||w||^2 0.525463814000857
||w|| 0.6633252355388068
exp ma of ||w|| 0.6716182499128265
||w||^2 0.40208112911810917
exp ma of ||w||^2 0.5647196883300999
||w|| 0.6340986745910365
exp ma of ||w|| 0.706978484622886
||w||^2 0.3172625384627465
exp ma of ||w||^2 0.5621084485257714
||w|| 0.563260631025058
exp ma of ||w|| 0.7055778655686299
||w||^2 0.43306610599076983
exp ma of ||w||^2 0.6417335426653578
||w|| 0.6580775835650154
exp ma of ||w|| 0.7472075897306237
||w||^2 0.8284214321899429
exp ma of ||w||^2 0.5890479782016196
||w|| 0.9101765939585257
exp ma of ||w|| 0.7211169998373365
||w||^2 0.8645198929895591
exp ma of ||w||^2 0.5359669629748713
||w|| 0.9297956189343759
exp ma of ||w|| 0.6979037578369409
||w||^2 0.2952688580655001
exp ma of ||w||^2 0.6324415793337055
||w|| 0.5433864721038794
exp ma of ||w|| 0.7508577908549434
||w||^2 2.305360908517753
exp ma of ||w||^2 0.5489265361709081
||w|| 1.518341499306975
exp ma of ||w|| 0.6978522435919066
||w||^2 0.45093441296012593
exp ma of ||w||^2 0.5632448402034237
||w|| 0.6715165023736394
exp ma of ||w|| 0.7111224638758716
||w||^2 0.6716546422444093
exp ma of ||w||^2 0.677934004900656
||w|| 0.8195453875438561
exp ma of ||w|| 0.773049725055989
||w||^2 0.40789243156244115
exp ma of ||w||^2 0.6836506892224008
||w|| 0.6386645688954736
exp ma of ||w|| 0.7676820668864183
cuda
Objective function 13.03 = squared loss an data 9.53 + 0.5*rho*h**2 0.538834 + alpha*h 1.766213 + L2reg 1.04 + L1reg 0.15 ; SHD = 49 ; DAG False
Proportion of microbatches that were clipped  0.7641317608784058
iteration 1 in inner loop, alpha 1.7013764668420528 rho 1.0 h 1.0381083490841405
2565
cuda
Objective function 17.88 = squared loss an data 9.53 + 0.5*rho*h**2 5.388345 + alpha*h 1.766213 + L2reg 1.04 + L1reg 0.15 ; SHD = 49 ; DAG False
||w||^2 0.5051047770385633
exp ma of ||w||^2 365308.5265067233
||w|| 0.7107072372211805
exp ma of ||w|| 12.196279456224108
||w||^2 0.565321959808578
exp ma of ||w||^2 3.9797397489595494
||w|| 0.7518789528963941
exp ma of ||w|| 0.8274203293772207
||w||^2 1.2440265140373656
exp ma of ||w||^2 0.7906904848779844
||w|| 1.1153593654232548
exp ma of ||w|| 0.8314213326341199
||w||^2 0.41498841349099863
exp ma of ||w||^2 0.7671603206657318
||w|| 0.6441959433984342
exp ma of ||w|| 0.819297431822616
||w||^2 0.947876663830967
exp ma of ||w||^2 0.803904342823518
||w|| 0.9735895766856623
exp ma of ||w|| 0.839883352393437
||w||^2 0.7570791623037016
exp ma of ||w||^2 0.7281426162219846
||w|| 0.8701029607487275
exp ma of ||w|| 0.8090233180091716
||w||^2 0.13943485146976362
exp ma of ||w||^2 0.8355415233102371
||w|| 0.37340976349014177
exp ma of ||w|| 0.8543833783735159
||w||^2 0.45052731474714414
exp ma of ||w||^2 0.8746447819258827
||w|| 0.6712133153827806
exp ma of ||w|| 0.869668880046839
||w||^2 0.18229061067094207
exp ma of ||w||^2 0.8015390487809583
||w|| 0.42695504525762673
exp ma of ||w|| 0.8461698229695445
||w||^2 0.2124055598263044
exp ma of ||w||^2 0.8216636293777319
||w|| 0.46087477673040905
exp ma of ||w|| 0.8552603008540437
||w||^2 0.6439579863113635
exp ma of ||w||^2 0.7608342633711754
||w|| 0.8024699286025386
exp ma of ||w|| 0.8201661184911224
cuda
Objective function 13.81 = squared loss an data 10.38 + 0.5*rho*h**2 1.204998 + alpha*h 0.835235 + L2reg 1.24 + L1reg 0.14 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.7700104577266511
iteration 2 in inner loop, alpha 1.7013764668420528 rho 10.0 h 0.49091710228701047
2565
cuda
Objective function 24.65 = squared loss an data 10.38 + 0.5*rho*h**2 12.049980 + alpha*h 0.835235 + L2reg 1.24 + L1reg 0.14 ; SHD = 43 ; DAG True
||w||^2 3.3841999777482226
exp ma of ||w||^2 1.2172681092818012
||w|| 1.8396195198323546
exp ma of ||w|| 1.0332008378772248
||w||^2 0.7386370563820056
exp ma of ||w||^2 1.286631766079405
||w|| 0.8594399667120477
exp ma of ||w|| 1.028497616509086
||w||^2 0.8859332927217167
exp ma of ||w||^2 1.0036255685547413
||w|| 0.9412402948884608
exp ma of ||w|| 0.9372720055324429
||w||^2 1.155231774359986
exp ma of ||w||^2 1.1542136549925428
||w|| 1.0748170887923145
exp ma of ||w|| 1.0110186940552681
||w||^2 1.3549577025362762
exp ma of ||w||^2 1.2726727433937013
||w|| 1.164026504224142
exp ma of ||w|| 1.0385576609062337
||w||^2 2.3828364160410747
exp ma of ||w||^2 1.3362144374168745
||w|| 1.5436438760417102
exp ma of ||w|| 1.048644118086598
cuda
Objective function 15.69 = squared loss an data 12.58 + 0.5*rho*h**2 1.313631 + alpha*h 0.275773 + L2reg 1.39 + L1reg 0.14 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.7682699326790494
iteration 3 in inner loop, alpha 1.7013764668420528 rho 100.0 h 0.16208831811674784
iteration 2 in outer loop, alpha = 17.910208278516837, rho = 100.0, h = 0.16208831811674784
cuda
2565
cuda
Objective function 18.32 = squared loss an data 12.58 + 0.5*rho*h**2 1.313631 + alpha*h 2.903036 + L2reg 1.39 + L1reg 0.14 ; SHD = 40 ; DAG True
||w||^2 7342.207392533177
exp ma of ||w||^2 41018984.12878241
||w|| 85.68668153530733
exp ma of ||w|| 727.4027873642716
v before min max tensor([[ 40.279,  24.140,  65.634,  ..., -11.665,  -5.487, -14.878],
        [-23.192,  -0.465, -15.511,  ...,  -7.803,  -5.289,  -7.458],
        [ 42.956,  14.388, -18.465,  ..., -16.698,  38.253, -17.830],
        ...,
        [ 18.740,  16.867, -16.400,  ...,  -6.009,  56.211,  71.510],
        [-21.166, -13.364,  31.648,  ...,  14.555, -19.435, -16.343],
        [ -4.144,  21.553, -23.988,  ...,  -0.922,  -7.760, -11.244]],
       device='cuda:0')
v tensor([[1.000e+01, 1.000e+01, 1.000e+01,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12],
        [1.000e+01, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e-12],
        ...,
        [1.000e+01, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e+01, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.000e-12]], device='cuda:0')
v before min max tensor([-6.583e+00, -1.197e+01, -1.825e+01,  4.300e+01, -8.286e+00, -1.645e+01,
         5.823e+01, -1.400e+01, -2.092e+01,  2.134e+01, -3.584e+00, -1.687e+01,
         8.743e+01,  1.545e+01,  4.129e+01, -1.341e+01, -9.049e+00, -1.690e+01,
        -1.861e+01,  4.970e+01,  1.441e+01,  5.687e+01,  3.385e+01, -1.340e+01,
        -8.258e+00, -2.124e+01, -2.096e+01, -6.128e+00,  4.883e+00,  3.817e+01,
        -5.753e+00,  4.680e+00, -1.506e+01, -1.088e+00, -1.772e+01, -1.636e+01,
        -7.178e-01,  4.875e+01, -9.653e+00, -9.277e+00, -1.426e+01, -4.657e+00,
        -1.293e+01,  4.429e+01,  5.315e+01, -1.620e+01, -1.180e+01, -1.995e+01,
         6.364e+01,  6.768e-01, -2.181e+01,  1.352e+01, -8.503e+00, -1.581e+01,
         3.110e+01,  5.232e+01, -1.488e+01,  1.195e+01,  1.907e+02, -1.390e+01,
        -9.934e+00,  4.305e+00,  5.817e+01,  4.213e+01, -1.341e+01, -1.032e+01,
        -4.252e+00, -1.862e+01,  1.344e+01,  6.190e-01, -1.314e+01, -9.657e+00,
        -1.417e+01, -1.384e+01, -8.445e+00, -1.276e+01,  3.180e+01,  4.145e+01,
         4.099e+00, -1.971e+00, -1.518e+01,  4.455e+00, -1.081e+01,  1.323e+00,
        -3.830e+00, -1.446e+01, -1.680e+01, -1.608e+01, -2.121e+00,  2.361e+01,
        -1.701e+01,  3.104e+01, -1.549e+01, -1.443e+01, -8.450e+00, -3.066e+00,
         1.374e+01, -7.031e+00,  3.514e+01,  7.313e+00, -6.508e+00, -1.669e+01,
         1.535e+01, -1.652e+01,  2.715e+00, -1.762e+01, -1.962e+01, -1.549e+01,
         2.651e+00, -1.765e+01, -2.143e+01, -1.187e+00, -1.139e+01, -1.745e+01,
         1.491e+01, -1.072e+01, -8.993e-01, -1.608e+01, -9.873e+00,  5.066e+01,
        -1.647e-01, -1.568e+01, -1.706e+01, -1.313e+01, -7.471e+00, -1.500e+01,
        -1.187e+01,  2.365e+01, -9.519e+00, -1.453e+01, -2.286e+01, -2.088e+01,
        -1.838e+01,  3.411e+01,  5.632e+01,  2.847e+00, -8.098e+00, -1.024e+01,
        -1.215e+01, -9.037e+00, -1.537e+01, -9.756e+00,  2.711e+01, -1.382e+01,
        -1.084e+01, -1.131e+01,  5.652e+00,  5.253e+01, -5.728e+00,  9.555e+00],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.883e+00, 1.000e+01,
        1.000e-12, 4.680e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 6.768e-01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 4.305e+00, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 6.190e-01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        4.099e+00, 1.000e-12, 1.000e-12, 4.455e+00, 1.000e-12, 1.323e+00,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 7.313e+00, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 2.715e+00, 1.000e-12, 1.000e-12, 1.000e-12,
        2.651e+00, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 2.847e+00, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 5.652e+00, 1.000e+01, 1.000e-12, 9.555e+00],
       device='cuda:0')
v before min max tensor([[[-11.619],
         [-23.183],
         [  5.615],
         [  5.387],
         [-12.402],
         [-11.255],
         [128.642],
         [-12.868],
         [ 28.439],
         [ -5.430]],

        [[-11.108],
         [-11.457],
         [  3.473],
         [-17.621],
         [-11.646],
         [-16.921],
         [ 33.832],
         [  2.480],
         [ -0.266],
         [ -9.091]],

        [[-10.634],
         [-10.717],
         [ -1.924],
         [ 34.088],
         [-20.727],
         [ 19.962],
         [ 13.615],
         [ -1.414],
         [ -6.170],
         [  1.574]],

        [[ 26.043],
         [ -7.436],
         [  7.493],
         [-13.379],
         [-14.409],
         [ 21.361],
         [ -6.500],
         [ -7.033],
         [ 25.997],
         [ -7.053]],

        [[-17.427],
         [-16.668],
         [109.801],
         [ -3.276],
         [ -9.445],
         [-19.312],
         [ -7.456],
         [-16.730],
         [  9.280],
         [ 12.636]],

        [[ -5.148],
         [ 10.028],
         [ 47.373],
         [-11.361],
         [-12.238],
         [ -8.579],
         [  3.463],
         [ -7.748],
         [  8.058],
         [ -7.460]],

        [[-23.337],
         [ -1.772],
         [-16.476],
         [ 53.302],
         [-12.830],
         [ 18.535],
         [-18.656],
         [104.932],
         [ -8.745],
         [ -9.115]],

        [[ 15.176],
         [  9.964],
         [  7.203],
         [-13.754],
         [ -4.810],
         [-16.189],
         [  3.621],
         [ -8.285],
         [ -0.742],
         [-17.535]],

        [[ 23.582],
         [-16.215],
         [-20.175],
         [ -4.057],
         [-20.628],
         [-22.729],
         [ -9.249],
         [  3.326],
         [ 34.389],
         [-12.934]],

        [[-16.859],
         [-16.686],
         [-14.573],
         [ 17.384],
         [ -9.691],
         [ 22.251],
         [ 30.319],
         [ 61.964],
         [-20.501],
         [-16.571]],

        [[ -9.622],
         [-18.560],
         [-19.392],
         [-14.809],
         [-14.092],
         [ 23.133],
         [-16.695],
         [-17.433],
         [ 45.914],
         [ 25.678]],

        [[-10.991],
         [ -5.084],
         [ 18.027],
         [ -2.303],
         [-18.159],
         [-10.542],
         [ -5.556],
         [ 25.744],
         [-13.726],
         [-15.379]],

        [[ -3.250],
         [-14.457],
         [-23.633],
         [-15.110],
         [  6.817],
         [-11.868],
         [-14.429],
         [-12.366],
         [ 52.052],
         [-10.518]],

        [[-13.399],
         [-12.613],
         [  0.325],
         [-19.583],
         [-14.983],
         [ -0.353],
         [-20.116],
         [  4.961],
         [ -8.924],
         [ 32.984]],

        [[ -8.369],
         [ 10.063],
         [ -4.434],
         [-22.198],
         [ 28.618],
         [-13.041],
         [  7.303],
         [  2.561],
         [  3.353],
         [ -6.057]]], device='cuda:0')
v tensor([[[1.000e-12],
         [1.000e-12],
         [5.615e+00],
         [5.387e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [3.473e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [2.480e+00],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.574e+00]],

        [[1.000e+01],
         [1.000e-12],
         [7.493e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [9.280e+00],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.463e+00],
         [1.000e-12],
         [8.058e+00],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [9.964e+00],
         [7.203e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.621e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.326e+00],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.817e+00],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [3.253e-01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.961e+00],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [7.303e+00],
         [2.561e+00],
         [3.353e+00],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[ 31.313],
        [-15.071],
        [  2.950],
        [ 34.431],
        [ 10.670],
        [-18.219],
        [-18.846],
        [ 13.298],
        [ -4.895],
        [-22.417],
        [111.535],
        [  9.795],
        [ 20.449],
        [-11.332],
        [-13.853]], device='cuda:0')
v tensor([[1.000e+01],
        [1.000e-12],
        [2.950e+00],
        [1.000e+01],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [9.795e+00],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[ 0.030, -0.002, -0.063,  ..., -0.135,  0.129, -0.009],
        [ 0.027,  0.141, -0.037,  ..., -0.015, -0.044,  0.005],
        [ 0.030,  0.001,  0.046,  ..., -0.061,  0.018, -0.005],
        ...,
        [ 0.028, -0.056,  0.013,  ...,  0.079, -0.065, -0.022],
        [-0.011,  0.043,  0.006,  ..., -0.014,  0.019,  0.000],
        [ 0.080,  0.007,  0.037,  ..., -0.029, -0.020, -0.016]],
       device='cuda:0')
s after update for 1 param tensor([[2.051, 1.874, 1.840,  ..., 1.144, 1.503, 1.756],
        [2.046, 1.995, 1.479,  ..., 1.800, 1.131, 1.791],
        [1.862, 1.952, 1.674,  ..., 1.463, 1.942, 1.558],
        ...,
        [1.906, 2.198, 1.455,  ..., 0.976, 1.575, 2.405],
        [1.977, 1.601, 1.933,  ..., 1.248, 1.760, 1.431],
        [1.624, 1.623, 2.107,  ..., 1.741, 1.623, 1.480]], device='cuda:0')
b after update for 1 param tensor([[ 95.209,  90.997,  90.181,  ...,  71.113,  81.506,  88.097],
        [ 95.082,  93.896,  80.857,  ...,  89.190,  70.696,  88.961],
        [ 90.701,  92.868,  86.017,  ...,  80.404,  92.634,  82.981],
        ...,
        [ 91.768,  98.559,  80.192,  ...,  65.682,  83.435, 103.094],
        [ 93.461,  84.115,  92.429,  ...,  74.279,  88.197,  79.515],
        [ 84.715,  84.688,  96.498,  ...,  87.711,  84.688,  80.872]],
       device='cuda:0')
clipping threshold 0.6966352924520083
a after update for 1 param tensor([ 1.861e-03, -1.634e-01,  2.102e-02, -7.084e-02,  5.293e-02, -2.864e-02,
        -1.080e-01,  4.139e-02,  5.753e-03,  1.111e-01,  4.762e-02,  4.874e-02,
        -3.739e-02,  3.998e-02,  4.099e-02, -9.191e-04,  3.519e-02,  1.556e-02,
        -7.859e-02, -2.979e-02,  7.932e-02,  2.508e-02, -1.401e-01, -2.590e-02,
        -1.300e-03, -1.147e-01,  7.783e-02, -1.039e-02,  9.894e-02,  2.857e-02,
         6.213e-03,  1.077e-02,  3.159e-02, -1.383e-02, -3.042e-02, -9.232e-02,
         1.963e-02,  1.158e-01,  4.170e-02, -5.466e-02,  1.016e-02, -4.526e-02,
         3.484e-02,  4.096e-02,  5.441e-02, -8.740e-02, -8.179e-03, -7.660e-04,
         5.181e-02,  9.287e-02, -1.493e-01,  5.996e-02,  2.622e-02, -2.417e-02,
        -4.769e-02,  5.520e-02, -5.309e-02, -4.880e-02, -1.117e-02, -3.765e-02,
        -9.858e-02, -1.683e-01, -4.970e-02, -2.676e-03,  3.422e-02, -3.206e-02,
         1.528e-01,  8.090e-03, -2.328e-02, -1.081e-02,  6.516e-02,  1.572e-02,
         4.194e-02,  5.851e-02,  1.301e-02, -2.892e-02, -3.514e-02,  9.593e-02,
        -1.305e-01,  4.268e-02, -1.195e-01,  1.729e-02, -2.413e-02, -2.330e-02,
         5.086e-02,  1.360e-02,  3.805e-02,  1.252e-02,  9.072e-02, -6.564e-02,
        -4.068e-02,  5.028e-02, -7.328e-02,  1.644e-02,  3.869e-02,  1.153e-01,
        -3.390e-02, -3.530e-02,  9.913e-03, -1.459e-03,  1.189e-02,  1.031e-04,
        -1.998e-02, -3.117e-02,  2.944e-02, -4.510e-02, -1.713e-02,  5.760e-05,
         8.020e-02, -3.214e-03, -2.418e-02,  1.635e-02,  1.269e-02,  3.920e-02,
        -5.428e-02,  3.752e-02,  1.939e-02, -1.043e-02,  1.422e-01, -4.681e-02,
         4.075e-03,  2.574e-02, -2.058e-02,  7.456e-02, -4.398e-02,  1.792e-01,
         5.929e-02, -2.406e-02, -6.976e-02, -1.343e-02,  7.063e-02, -4.092e-03,
        -2.229e-02,  1.140e-02, -9.357e-02, -5.293e-03, -5.583e-03,  1.108e-02,
         5.871e-02,  3.970e-02, -4.338e-02,  2.078e-02, -5.741e-02, -4.460e-02,
        -9.378e-03, -1.704e-02,  4.131e-02,  4.208e-02, -5.015e-02, -2.288e-02],
       device='cuda:0')
s after update for 1 param tensor([0.576, 1.998, 1.627, 1.694, 0.728, 1.438, 2.264, 1.275, 1.967, 2.434,
        1.116, 1.609, 1.415, 1.902, 2.002, 1.756, 1.574, 1.712, 1.653, 1.906,
        1.995, 1.381, 1.738, 1.185, 2.106, 1.858, 1.847, 1.248, 1.974, 1.945,
        1.207, 2.189, 1.380, 0.642, 1.614, 1.868, 1.594, 1.980, 1.985, 1.245,
        1.266, 1.091, 1.469, 2.027, 1.733, 1.507, 1.371, 1.749, 1.986, 1.609,
        1.906, 2.022, 1.710, 1.388, 2.455, 1.636, 1.681, 1.871, 2.411, 1.217,
        1.806, 1.196, 1.727, 1.972, 1.541, 0.944, 1.530, 1.832, 1.635, 1.462,
        1.497, 1.058, 1.438, 1.266, 1.896, 1.487, 1.469, 2.192, 1.566, 1.550,
        1.764, 1.530, 1.790, 1.378, 1.264, 1.264, 1.469, 1.828, 1.591, 2.020,
        1.536, 1.466, 1.425, 1.263, 1.663, 1.662, 1.841, 1.243, 1.893, 1.980,
        1.128, 1.459, 1.954, 1.597, 1.973, 1.567, 1.752, 1.363, 1.497, 1.571,
        1.965, 1.601, 1.564, 1.530, 1.937, 1.242, 0.848, 1.584, 1.742, 2.333,
        1.527, 1.459, 1.722, 1.316, 1.510, 1.347, 1.041, 2.078, 1.728, 1.585,
        2.187, 1.957, 1.607, 2.095, 1.868, 1.684, 1.300, 1.548, 1.835, 1.028,
        1.343, 1.033, 2.309, 1.372, 1.836, 1.831, 1.930, 1.720, 1.685, 1.596],
       device='cuda:0')
b after update for 1 param tensor([ 50.433,  93.962,  84.796,  86.527,  56.725,  79.714, 100.035,  75.070,
         93.236, 103.717,  70.227,  84.326,  79.078,  91.670,  94.062,  88.099,
         83.415,  86.991,  85.467,  91.785,  93.891,  78.130,  87.639,  72.352,
         96.471,  90.627,  90.336,  74.279,  93.402,  92.708,  73.032,  98.359,
         78.088,  53.276,  84.468,  90.848,  83.940,  93.551,  93.658,  74.182,
         74.789,  69.445,  80.567,  94.645,  87.507,  81.602,  77.851,  87.920,
         93.692,  84.330,  91.779,  94.527,  86.942,  78.310, 104.166,  85.036,
         86.184,  90.928, 103.226,  73.332,  89.329,  72.703,  87.368,  93.346,
         82.516,  64.594,  82.240,  89.969,  85.013,  80.381,  81.342,  68.367,
         79.720,  74.793,  91.531,  81.068,  80.564,  98.414,  83.185,  82.770,
         88.290,  82.218,  88.936,  78.042,  74.754,  74.741,  80.561,  89.890,
         83.863,  94.492,  82.396,  80.491,  79.360,  74.723,  85.725,  85.708,
         90.189,  74.121,  91.461,  93.532,  70.596,  80.293,  92.921,  83.997,
         93.385,  83.210,  88.000,  77.599,  81.349,  83.332,  93.188,  84.114,
         83.135,  82.226,  92.520,  74.086,  61.219,  83.672,  87.735, 101.550,
         82.158,  80.298,  87.244,  76.272,  81.701,  77.161,  67.817,  95.825,
         87.400,  83.683,  98.322,  92.991,  84.267,  96.219,  90.856,  86.272,
         75.785,  82.706,  90.050,  67.399,  77.034,  67.573, 101.017,  77.880,
         90.076,  89.960,  92.347,  87.191,  86.289,  83.991], device='cuda:0')
clipping threshold 0.6966352924520083
a after update for 1 param tensor([[[ 0.067],
         [-0.069],
         [ 0.014],
         [-0.036],
         [-0.016],
         [ 0.019],
         [-0.004],
         [ 0.026],
         [-0.033],
         [ 0.014]],

        [[-0.025],
         [ 0.055],
         [-0.050],
         [ 0.009],
         [ 0.020],
         [ 0.065],
         [ 0.053],
         [-0.004],
         [-0.023],
         [ 0.009]],

        [[-0.067],
         [-0.021],
         [ 0.055],
         [-0.154],
         [ 0.112],
         [-0.004],
         [-0.073],
         [-0.089],
         [ 0.034],
         [-0.045]],

        [[ 0.059],
         [ 0.003],
         [-0.048],
         [ 0.013],
         [ 0.088],
         [ 0.039],
         [-0.011],
         [ 0.014],
         [ 0.020],
         [-0.057]],

        [[ 0.066],
         [ 0.058],
         [ 0.045],
         [-0.049],
         [ 0.061],
         [-0.081],
         [-0.002],
         [-0.015],
         [-0.002],
         [ 0.140]],

        [[ 0.026],
         [ 0.005],
         [-0.018],
         [ 0.009],
         [-0.070],
         [-0.038],
         [-0.011],
         [ 0.116],
         [ 0.032],
         [ 0.121]],

        [[ 0.034],
         [-0.042],
         [-0.070],
         [-0.014],
         [-0.196],
         [-0.027],
         [ 0.005],
         [ 0.101],
         [-0.105],
         [ 0.012]],

        [[-0.014],
         [ 0.001],
         [ 0.064],
         [ 0.036],
         [-0.029],
         [ 0.014],
         [-0.107],
         [-0.057],
         [-0.045],
         [ 0.012]],

        [[-0.040],
         [-0.001],
         [-0.106],
         [-0.032],
         [ 0.009],
         [ 0.041],
         [-0.012],
         [ 0.029],
         [ 0.018],
         [ 0.107]],

        [[-0.057],
         [ 0.036],
         [-0.019],
         [ 0.043],
         [-0.059],
         [ 0.019],
         [-0.015],
         [-0.006],
         [ 0.009],
         [ 0.037]],

        [[ 0.048],
         [-0.020],
         [-0.013],
         [ 0.052],
         [ 0.034],
         [ 0.040],
         [ 0.017],
         [-0.072],
         [-0.043],
         [ 0.044]],

        [[ 0.199],
         [ 0.073],
         [ 0.056],
         [ 0.095],
         [-0.041],
         [ 0.056],
         [ 0.002],
         [ 0.005],
         [ 0.009],
         [ 0.010]],

        [[ 0.002],
         [-0.025],
         [-0.021],
         [-0.038],
         [ 0.002],
         [-0.001],
         [-0.072],
         [-0.029],
         [-0.022],
         [ 0.002]],

        [[-0.008],
         [ 0.071],
         [-0.005],
         [ 0.038],
         [-0.057],
         [-0.086],
         [ 0.102],
         [ 0.030],
         [-0.014],
         [-0.021]],

        [[ 0.132],
         [ 0.004],
         [ 0.005],
         [ 0.016],
         [ 0.029],
         [ 0.053],
         [ 0.003],
         [-0.032],
         [ 0.022],
         [-0.023]]], device='cuda:0')
s after update for 1 param tensor([[[1.037],
         [2.059],
         [1.869],
         [1.842],
         [1.274],
         [1.123],
         [2.054],
         [1.129],
         [2.120],
         [1.043]],

        [[2.063],
         [1.144],
         [1.221],
         [1.559],
         [1.876],
         [1.479],
         [2.390],
         [1.926],
         [1.158],
         [1.914]],

        [[1.588],
         [1.342],
         [1.279],
         [1.556],
         [1.857],
         [1.645],
         [1.639],
         [1.348],
         [1.219],
         [1.836]],

        [[1.647],
         [0.968],
         [1.999],
         [1.222],
         [1.368],
         [1.450],
         [0.979],
         [1.688],
         [1.711],
         [1.255]],

        [[1.872],
         [1.470],
         [1.766],
         [1.439],
         [1.418],
         [1.690],
         [1.497],
         [1.699],
         [1.680],
         [2.124]],

        [[1.715],
         [2.473],
         [1.497],
         [1.614],
         [1.166],
         [1.180],
         [1.863],
         [1.562],
         [1.698],
         [1.779]],

        [[2.039],
         [1.429],
         [1.467],
         [1.649],
         [1.186],
         [1.715],
         [1.768],
         [2.030],
         [0.801],
         [1.402]],

        [[1.830],
         [1.450],
         [1.386],
         [1.462],
         [1.424],
         [1.466],
         [1.523],
         [1.702],
         [1.607],
         [1.653]],

        [[1.740],
         [1.418],
         [1.838],
         [1.417],
         [1.854],
         [2.070],
         [0.881],
         [1.203],
         [1.699],
         [1.469]],

        [[1.558],
         [1.580],
         [1.626],
         [1.635],
         [1.284],
         [2.053],
         [2.012],
         [2.248],
         [1.792],
         [1.530]],

        [[1.815],
         [1.668],
         [1.697],
         [1.301],
         [1.236],
         [1.943],
         [1.681],
         [1.527],
         [2.288],
         [2.228]],

        [[1.969],
         [1.222],
         [2.028],
         [1.298],
         [1.885],
         [1.529],
         [1.079],
         [1.929],
         [1.255],
         [1.508]],

        [[1.117],
         [1.854],
         [2.067],
         [1.405],
         [1.543],
         [1.053],
         [1.263],
         [1.208],
         [2.335],
         [1.432]],

        [[1.395],
         [1.601],
         [0.961],
         [1.719],
         [1.622],
         [1.004],
         [1.877],
         [1.835],
         [1.325],
         [1.682]],

        [[1.324],
         [2.368],
         [1.409],
         [1.944],
         [1.623],
         [1.665],
         [1.864],
         [1.361],
         [1.424],
         [1.478]]], device='cuda:0')
b after update for 1 param tensor([[[ 67.684],
         [ 95.394],
         [ 90.873],
         [ 90.218],
         [ 75.046],
         [ 70.463],
         [ 95.267],
         [ 70.641],
         [ 96.797],
         [ 67.883]],

        [[ 95.472],
         [ 71.106],
         [ 73.451],
         [ 83.011],
         [ 91.047],
         [ 80.834],
         [102.782],
         [ 92.265],
         [ 71.550],
         [ 91.971]],

        [[ 83.760],
         [ 76.999],
         [ 75.173],
         [ 82.913],
         [ 90.579],
         [ 85.255],
         [ 85.096],
         [ 77.188],
         [ 73.386],
         [ 90.084]],

        [[ 85.326],
         [ 65.400],
         [ 93.984],
         [ 73.488],
         [ 77.757],
         [ 80.057],
         [ 65.767],
         [ 86.381],
         [ 86.951],
         [ 74.485]],

        [[ 90.958],
         [ 80.606],
         [ 88.332],
         [ 79.745],
         [ 79.163],
         [ 86.413],
         [ 81.345],
         [ 86.640],
         [ 86.175],
         [ 96.885]],

        [[ 87.050],
         [104.538],
         [ 81.324],
         [ 84.452],
         [ 71.791],
         [ 72.204],
         [ 90.749],
         [ 83.094],
         [ 86.638],
         [ 88.669]],

        [[ 94.935],
         [ 79.480],
         [ 80.518],
         [ 85.358],
         [ 72.408],
         [ 87.066],
         [ 88.394],
         [ 94.727],
         [ 59.503],
         [ 78.713]],

        [[ 89.920],
         [ 80.039],
         [ 78.255],
         [ 80.387],
         [ 79.341],
         [ 80.493],
         [ 82.053],
         [ 86.722],
         [ 84.285],
         [ 85.474]],

        [[ 87.687],
         [ 79.158],
         [ 90.132],
         [ 79.126],
         [ 90.518],
         [ 95.638],
         [ 62.397],
         [ 72.908],
         [ 86.640],
         [ 80.571]],

        [[ 82.974],
         [ 83.561],
         [ 84.781],
         [ 84.995],
         [ 75.328],
         [ 95.249],
         [ 94.285],
         [ 99.662],
         [ 88.988],
         [ 82.235]],

        [[ 89.552],
         [ 85.865],
         [ 86.592],
         [ 75.836],
         [ 73.910],
         [ 92.656],
         [ 86.199],
         [ 82.136],
         [100.551],
         [ 99.230]],

        [[ 93.278],
         [ 73.488],
         [ 94.658],
         [ 75.738],
         [ 91.264],
         [ 82.201],
         [ 69.042],
         [ 92.320],
         [ 74.473],
         [ 81.632]],

        [[ 70.260],
         [ 90.524],
         [ 95.582],
         [ 78.798],
         [ 82.589],
         [ 68.210],
         [ 74.714],
         [ 73.064],
         [101.585],
         [ 79.565]],

        [[ 78.504],
         [ 84.128],
         [ 65.177],
         [ 87.168],
         [ 84.654],
         [ 66.596],
         [ 91.079],
         [ 90.047],
         [ 76.514],
         [ 86.218]],

        [[ 76.485],
         [102.299],
         [ 78.897],
         [ 92.686],
         [ 84.694],
         [ 85.779],
         [ 90.754],
         [ 77.552],
         [ 79.343],
         [ 80.813]]], device='cuda:0')
clipping threshold 0.6966352924520083
a after update for 1 param tensor([[ 0.037],
        [-0.069],
        [ 0.013],
        [ 0.090],
        [ 0.022],
        [-0.036],
        [ 0.032],
        [-0.021],
        [ 0.023],
        [-0.174],
        [-0.033],
        [ 0.014],
        [-0.022],
        [ 0.024],
        [-0.059]], device='cuda:0')
s after update for 1 param tensor([[1.931],
        [1.496],
        [2.009],
        [1.558],
        [1.892],
        [1.732],
        [1.944],
        [1.602],
        [1.460],
        [1.960],
        [2.273],
        [1.897],
        [1.505],
        [1.442],
        [1.214]], device='cuda:0')
b after update for 1 param tensor([[ 92.373],
        [ 81.321],
        [ 94.233],
        [ 82.965],
        [ 91.449],
        [ 87.481],
        [ 92.692],
        [ 84.142],
        [ 80.335],
        [ 93.071],
        [100.226],
        [ 91.568],
        [ 81.560],
        [ 79.837],
        [ 73.255]], device='cuda:0')
clipping threshold 0.6966352924520083
||w||^2 0.808944927857233
exp ma of ||w||^2 1.3628114901327837
||w|| 0.8994136578111503
exp ma of ||w|| 1.0404550034715694
||w||^2 2.1258348285179256
exp ma of ||w||^2 1.1871204717805741
||w|| 1.4580242894128772
exp ma of ||w|| 1.0176604230915778
||w||^2 0.2569789714949325
exp ma of ||w||^2 1.2876103735022013
||w|| 0.5069309336536216
exp ma of ||w|| 1.0446240821488928
||w||^2 1.4169169276981581
exp ma of ||w||^2 1.2540137944407805
||w|| 1.1903431974427199
exp ma of ||w|| 1.041952886926398
cuda
Objective function 16.53 = squared loss an data 12.59 + 0.5*rho*h**2 0.491206 + alpha*h 1.775201 + L2reg 1.54 + L1reg 0.14 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.7688946431461402
iteration 1 in inner loop, alpha 17.910208278516837 rho 100.0 h 0.09911673069342797
2565
cuda
Objective function 20.95 = squared loss an data 12.59 + 0.5*rho*h**2 4.912063 + alpha*h 1.775201 + L2reg 1.54 + L1reg 0.14 ; SHD = 37 ; DAG True
||w||^2 5172856886.954511
exp ma of ||w||^2 34488722908.35099
||w|| 71922.57564182828
exp ma of ||w|| 141671.8317380667
||w||^2 98403468.35093799
exp ma of ||w||^2 5797967223.64655
||w|| 9919.852234329803
exp ma of ||w|| 42792.070008895265
||w||^2 415576.9264954252
exp ma of ||w||^2 339483418.08213234
||w|| 644.6525626222432
exp ma of ||w|| 4308.535288931174
||w||^2 1427.215080800089
exp ma of ||w||^2 60918708.074282385
||w|| 37.77850024551119
exp ma of ||w|| 906.265074620997
||w||^2 445.9692323584723
exp ma of ||w||^2 39147392.63837363
||w|| 21.11798362435373
exp ma of ||w|| 592.5183367647054
||w||^2 1.0103334242648652
exp ma of ||w||^2 1.5846812037102453
||w|| 1.0051534331955818
exp ma of ||w|| 1.1342343827110892
||w||^2 0.7158615011129834
exp ma of ||w||^2 1.4829689012900866
||w|| 0.8460859891955329
exp ma of ||w|| 1.1199124144062973
||w||^2 1.930468960445598
exp ma of ||w||^2 1.538812615921688
||w|| 1.3894131712509414
exp ma of ||w|| 1.1562138653104563
||w||^2 1.0231492937033932
exp ma of ||w||^2 1.4442644400989437
||w|| 1.0115084249295174
exp ma of ||w|| 1.0887620312188615
||w||^2 0.81966681653121
exp ma of ||w||^2 1.43315843968499
||w|| 0.9053545253276254
exp ma of ||w|| 1.1125131630249028
||w||^2 1.3912598097654068
exp ma of ||w||^2 1.5200855254899264
||w|| 1.179516769599062
exp ma of ||w|| 1.1390100501740636
||w||^2 2.764794912168569
exp ma of ||w||^2 1.6375408452111988
||w|| 1.662767245337894
exp ma of ||w|| 1.1316601192717106
cuda
Objective function 16.67 = squared loss an data 13.34 + 0.5*rho*h**2 0.836069 + alpha*h 0.732380 + L2reg 1.61 + L1reg 0.14 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.7747396868293458
iteration 2 in inner loop, alpha 17.910208278516837 rho 1000.0 h 0.040891778102398746
2565
cuda
Objective function 24.19 = squared loss an data 13.34 + 0.5*rho*h**2 8.360688 + alpha*h 0.732380 + L2reg 1.61 + L1reg 0.14 ; SHD = 40 ; DAG True
||w||^2 67882760204.627075
exp ma of ||w||^2 1028550536951.6405
||w|| 260543.20218464168
exp ma of ||w|| 778301.4588299275
||w||^2 1.9648099615381511
exp ma of ||w||^2 373.63465426181097
||w|| 1.401716790774139
exp ma of ||w|| 1.1549912853588087
||w||^2 2.773545776296507
exp ma of ||w||^2 1.3170992655489928
||w|| 1.6653965822879868
exp ma of ||w|| 1.0626910796520135
||w||^2 0.6414763081679764
exp ma of ||w||^2 1.5019564234630118
||w|| 0.8009221611167819
exp ma of ||w|| 1.1063256812579128
||w||^2 0.4606513969595003
exp ma of ||w||^2 1.515662899375905
||w|| 0.6787130446363178
exp ma of ||w|| 1.1360846286727972
||w||^2 1.3896068200181717
exp ma of ||w||^2 1.3025981234185224
||w|| 1.1788158550079701
exp ma of ||w|| 1.044678991136176
||w||^2 0.8091569877265465
exp ma of ||w||^2 1.239616599331947
||w|| 0.8995315379276851
exp ma of ||w|| 1.0339557417208378
||w||^2 1.3864836611278226
exp ma of ||w||^2 1.5642813408808278
||w|| 1.1774904080831499
exp ma of ||w|| 1.1017932880245587
||w||^2 0.7670370865689689
exp ma of ||w||^2 1.4916713631323353
||w|| 0.8758065348973875
exp ma of ||w|| 1.115621382250417
||w||^2 0.5316715066627477
exp ma of ||w||^2 1.211917026802192
||w|| 0.7291580807086675
exp ma of ||w|| 1.0274817529328937
||w||^2 1.1132102659375378
exp ma of ||w||^2 1.234452736757107
||w|| 1.0550878001083785
exp ma of ||w|| 1.0340198227831818
||w||^2 0.37263794689917
exp ma of ||w||^2 1.1374065294100897
||w|| 0.6104407808290416
exp ma of ||w|| 1.0048031698035012
||w||^2 1.679620087060828
exp ma of ||w||^2 1.2709725946174941
||w|| 1.2960015767971997
exp ma of ||w|| 1.0536406132615637
||w||^2 0.3402243292927704
exp ma of ||w||^2 1.2083936580179724
||w|| 0.5832875185470459
exp ma of ||w|| 1.0274572328133111
cuda
Objective function 16.27 = squared loss an data 13.18 + 0.5*rho*h**2 0.984026 + alpha*h 0.251257 + L2reg 1.71 + L1reg 0.14 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.7720025510204082
iteration 3 in inner loop, alpha 17.910208278516837 rho 10000.0 h 0.0140287252895277
iteration 3 in outer loop, alpha = 158.19746117379384, rho = 10000.0, h = 0.0140287252895277
cuda
2565
cuda
Objective function 18.24 = squared loss an data 13.18 + 0.5*rho*h**2 0.984026 + alpha*h 2.219309 + L2reg 1.71 + L1reg 0.14 ; SHD = 39 ; DAG True
||w||^2 295180185207.91046
exp ma of ||w||^2 223066473313.1468
||w|| 543304.8731678288
exp ma of ||w|| 270416.17004136863
||w||^2 22608.443133873025
exp ma of ||w||^2 5955760419.507418
||w|| 150.3610426070298
exp ma of ||w|| 4618.121111854369
||w||^2 3.160471589388079
exp ma of ||w||^2 7378684.989362316
||w|| 1.7777715233932845
exp ma of ||w|| 8.064389755761516
||w||^2 1.0600978708527247
exp ma of ||w||^2 1.4205387799458777
||w|| 1.0296105432894151
exp ma of ||w|| 1.0993587320701768
||w||^2 0.913213558823985
exp ma of ||w||^2 1.2010737375826004
||w|| 0.9556220794979493
exp ma of ||w|| 1.025758176475328
||w||^2 0.25976314699575664
exp ma of ||w||^2 1.447960526356159
||w|| 0.5096696449620642
exp ma of ||w|| 1.0953582111284481
||w||^2 1.928789706150223
exp ma of ||w||^2 1.2914871107630934
||w|| 1.38880873634573
exp ma of ||w|| 1.058227723663331
||w||^2 1.2549843704454728
exp ma of ||w||^2 1.269619998842801
||w|| 1.1202608492871082
exp ma of ||w|| 1.0397228490484962
||w||^2 1.1289486568437754
exp ma of ||w||^2 1.1798240078981526
||w|| 1.0625199559743692
exp ma of ||w|| 1.0148591224089363
||w||^2 0.9854318117316326
exp ma of ||w||^2 1.2338936773671614
||w|| 0.9926891818346932
exp ma of ||w|| 1.0262984572596876
cuda
Objective function 16.74 = squared loss an data 13.28 + 0.5*rho*h**2 0.304023 + alpha*h 1.233581 + L2reg 1.77 + L1reg 0.15 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.7732622872594243
iteration 1 in inner loop, alpha 158.19746117379384 rho 10000.0 h 0.0077977308126300215
2565
cuda
Objective function 19.47 = squared loss an data 13.28 + 0.5*rho*h**2 3.040230 + alpha*h 1.233581 + L2reg 1.77 + L1reg 0.15 ; SHD = 39 ; DAG True
||w||^2 341527731.5086893
exp ma of ||w||^2 41599952382422.98
||w|| 18480.4689201516
exp ma of ||w|| 1361165.1903860483
||w||^2 8602936.303284723
exp ma of ||w||^2 10928932169012.596
||w|| 2933.076252552041
exp ma of ||w|| 364514.3736501412
||w||^2 87236.21527406781
exp ma of ||w||^2 1435117613298.5305
||w|| 295.3577750357485
exp ma of ||w|| 48616.08591196804
||w||^2 1.9100275860386908
exp ma of ||w||^2 2211.3792785092596
||w|| 1.3820374763510181
exp ma of ||w|| 1.2134071084723894
||w||^2 1.1243019383730222
exp ma of ||w||^2 13.607763969818174
||w|| 1.0603310513104018
exp ma of ||w|| 1.0628162157027154
||w||^2 0.6122773744136256
exp ma of ||w||^2 1.2337030011983787
||w|| 0.7824815489285518
exp ma of ||w|| 1.0522175848125142
||w||^2 0.49520449232538016
exp ma of ||w||^2 1.271459617035136
||w|| 0.7037076753349931
exp ma of ||w|| 1.0661377355211195
||w||^2 3.9804729132515115
exp ma of ||w||^2 1.1595873714313596
||w|| 1.9951122558020418
exp ma of ||w|| 1.018146622122174
||w||^2 1.0443351611697052
exp ma of ||w||^2 1.3107479030994376
||w|| 1.0219271799740455
exp ma of ||w|| 1.0781144721792066
||w||^2 0.6417340164449296
exp ma of ||w||^2 1.3334300218491282
||w|| 0.8010830271856529
exp ma of ||w|| 1.0699528887276073
cuda
Objective function 16.46 = squared loss an data 13.10 + 0.5*rho*h**2 0.766324 + alpha*h 0.619328 + L2reg 1.82 + L1reg 0.15 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.7739737010904426
iteration 2 in inner loop, alpha 158.19746117379384 rho 100000.0 h 0.003914903925471336
iteration 4 in outer loop, alpha = 4073.1013866451294, rho = 1000000.0, h = 0.003914903925471336
Threshold 0.3
[[0.004 0.02  0.109 0.168 0.444 0.006 0.04  0.002 0.035 1.012 0.159 0.305
  0.007 0.005 0.335]
 [0.331 0.005 0.655 0.482 0.518 0.03  0.035 0.028 0.102 0.951 0.351 0.996
  0.03  0.003 0.261]
 [0.038 0.006 0.004 0.061 0.344 0.012 0.008 0.016 0.042 0.185 0.05  0.065
  0.006 0.005 0.119]
 [0.043 0.014 0.087 0.004 0.391 0.008 0.02  0.006 0.036 0.252 0.06  0.062
  0.005 0.006 0.308]
 [0.004 0.004 0.014 0.015 0.006 0.004 0.001 0.002 0.002 0.003 0.002 0.013
  0.002 0.001 0.008]
 [0.806 0.144 0.517 0.766 0.624 0.005 0.445 0.033 0.326 0.805 0.453 0.932
  0.183 0.044 0.366]
 [0.14  0.118 0.695 0.313 0.43  0.013 0.006 0.025 0.151 0.191 1.007 0.235
  0.019 0.027 0.597]
 [1.391 0.157 0.438 0.938 1.028 0.21  0.139 0.004 0.105 0.746 0.291 0.946
  0.044 0.017 0.817]
 [0.188 0.092 0.15  0.24  0.769 0.021 0.057 0.042 0.006 1.163 0.806 0.765
  0.012 0.001 0.255]
 [0.006 0.008 0.029 0.026 2.003 0.004 0.02  0.004 0.004 0.01  0.136 0.055
  0.005 0.002 0.052]
 [0.02  0.019 0.08  0.085 2.612 0.01  0.005 0.014 0.009 0.045 0.01  0.014
  0.003 0.003 0.179]
 [0.022 0.005 0.123 0.095 0.246 0.007 0.021 0.007 0.009 0.098 0.23  0.004
  0.007 0.003 0.09 ]
 [0.59  0.168 0.705 0.656 1.99  0.034 0.328 0.1   0.564 0.267 1.396 0.661
  0.003 0.051 0.692]
 [0.677 1.376 0.519 0.703 0.864 0.158 0.234 0.304 2.786 1.106 0.955 0.844
  0.149 0.005 0.931]
 [0.015 0.019 0.052 0.022 0.439 0.014 0.013 0.006 0.02  0.087 0.04  0.071
  0.007 0.004 0.003]]
[[0.    0.    0.    0.    0.444 0.    0.    0.    0.    1.012 0.    0.305
  0.    0.    0.335]
 [0.331 0.    0.655 0.482 0.518 0.    0.    0.    0.    0.951 0.351 0.996
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.344 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.391 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.308]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.806 0.    0.517 0.766 0.624 0.    0.445 0.    0.326 0.805 0.453 0.932
  0.    0.    0.366]
 [0.    0.    0.695 0.313 0.43  0.    0.    0.    0.    0.    1.007 0.
  0.    0.    0.597]
 [1.391 0.    0.438 0.938 1.028 0.    0.    0.    0.    0.746 0.    0.946
  0.    0.    0.817]
 [0.    0.    0.    0.    0.769 0.    0.    0.    0.    1.163 0.806 0.765
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.003 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.612 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.59  0.    0.705 0.656 1.99  0.    0.328 0.    0.564 0.    1.396 0.661
  0.    0.    0.692]
 [0.677 1.376 0.519 0.703 0.864 0.    0.    0.304 2.786 1.106 0.955 0.844
  0.    0.    0.931]
 [0.    0.    0.    0.    0.439 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.5555555555555556, 'tpr': 0.9333333333333333, 'fpr': 0.4666666666666667, 'f1': 0.6021505376344086, 'shd': 37, 'npred': 63, 'ntrue': 30}
[2.007e-02 1.087e-01 1.682e-01 4.441e-01 6.482e-03 4.045e-02 2.202e-03
 3.482e-02 1.012e+00 1.594e-01 3.046e-01 7.067e-03 5.102e-03 3.354e-01
 3.312e-01 6.552e-01 4.821e-01 5.179e-01 3.034e-02 3.468e-02 2.838e-02
 1.022e-01 9.505e-01 3.513e-01 9.959e-01 3.029e-02 3.308e-03 2.610e-01
 3.816e-02 6.439e-03 6.054e-02 3.437e-01 1.184e-02 7.780e-03 1.592e-02
 4.184e-02 1.850e-01 4.957e-02 6.525e-02 6.384e-03 5.334e-03 1.193e-01
 4.304e-02 1.387e-02 8.705e-02 3.913e-01 8.416e-03 1.980e-02 5.852e-03
 3.611e-02 2.516e-01 5.991e-02 6.222e-02 5.269e-03 6.113e-03 3.081e-01
 4.400e-03 4.466e-03 1.418e-02 1.539e-02 4.025e-03 1.470e-03 1.927e-03
 1.926e-03 2.926e-03 1.943e-03 1.254e-02 1.785e-03 1.186e-03 7.666e-03
 8.062e-01 1.438e-01 5.167e-01 7.663e-01 6.238e-01 4.446e-01 3.298e-02
 3.258e-01 8.048e-01 4.532e-01 9.316e-01 1.829e-01 4.393e-02 3.655e-01
 1.404e-01 1.177e-01 6.949e-01 3.135e-01 4.303e-01 1.329e-02 2.510e-02
 1.508e-01 1.915e-01 1.007e+00 2.348e-01 1.879e-02 2.722e-02 5.973e-01
 1.391e+00 1.569e-01 4.379e-01 9.378e-01 1.028e+00 2.101e-01 1.386e-01
 1.054e-01 7.457e-01 2.912e-01 9.459e-01 4.438e-02 1.735e-02 8.166e-01
 1.882e-01 9.228e-02 1.497e-01 2.398e-01 7.691e-01 2.055e-02 5.653e-02
 4.222e-02 1.163e+00 8.059e-01 7.648e-01 1.186e-02 1.379e-03 2.547e-01
 5.534e-03 7.580e-03 2.899e-02 2.581e-02 2.003e+00 3.955e-03 1.970e-02
 3.860e-03 3.849e-03 1.361e-01 5.499e-02 5.171e-03 2.078e-03 5.244e-02
 1.974e-02 1.919e-02 8.047e-02 8.521e-02 2.612e+00 9.547e-03 5.056e-03
 1.397e-02 9.092e-03 4.462e-02 1.390e-02 3.267e-03 2.704e-03 1.792e-01
 2.245e-02 4.887e-03 1.232e-01 9.533e-02 2.461e-01 6.720e-03 2.112e-02
 6.695e-03 9.120e-03 9.824e-02 2.301e-01 7.155e-03 2.547e-03 9.013e-02
 5.905e-01 1.683e-01 7.046e-01 6.562e-01 1.990e+00 3.431e-02 3.283e-01
 1.001e-01 5.638e-01 2.667e-01 1.396e+00 6.607e-01 5.102e-02 6.916e-01
 6.773e-01 1.376e+00 5.189e-01 7.034e-01 8.636e-01 1.577e-01 2.337e-01
 3.036e-01 2.786e+00 1.106e+00 9.551e-01 8.437e-01 1.493e-01 9.313e-01
 1.540e-02 1.870e-02 5.215e-02 2.170e-02 4.395e-01 1.436e-02 1.262e-02
 5.966e-03 1.982e-02 8.704e-02 4.020e-02 7.122e-02 7.454e-03 4.045e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9377777777777778, 0.7533408413405114)
Iterations 2250
Achieves (23.071438786011726, 1e-05)-DP
