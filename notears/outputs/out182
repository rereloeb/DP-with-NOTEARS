samples  5000  graph  20 80 ER mlp  minibatch size  100  noise  0.6  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7622777646747636
iteration 1 in outer loop, alpha = 1.7622777646747636, rho = 1.0, h = 1.7622777646747636
cuda
iteration 1 in inner loop,alpha 1.7622777646747636 rho 1.0 h 1.1769941079911774
iteration 2 in inner loop,alpha 1.7622777646747636 rho 10.0 h 0.546699080645368
iteration 3 in inner loop,alpha 1.7622777646747636 rho 100.0 h 0.18576399615164618
iteration 2 in outer loop, alpha = 20.338677379839382, rho = 100.0, h = 0.18576399615164618
cuda
iteration 1 in inner loop,alpha 20.338677379839382 rho 100.0 h 0.1095608426621908
iteration 2 in inner loop,alpha 20.338677379839382 rho 1000.0 h 0.04370983218316127
iteration 3 in outer loop, alpha = 64.04850956300065, rho = 1000.0, h = 0.04370983218316127
cuda
iteration 1 in inner loop,alpha 64.04850956300065 rho 1000.0 h 0.023592627798716848
iteration 2 in inner loop,alpha 64.04850956300065 rho 10000.0 h 0.007833952965441426
iteration 4 in outer loop, alpha = 142.38803921741493, rho = 10000.0, h = 0.007833952965441426
cuda
iteration 1 in inner loop,alpha 142.38803921741493 rho 10000.0 h 0.004050807260291833
iteration 2 in inner loop,alpha 142.38803921741493 rho 100000.0 h 0.0013211948154001618
iteration 5 in outer loop, alpha = 274.5075207574311, rho = 100000.0, h = 0.0013211948154001618
cuda
iteration 1 in inner loop,alpha 274.5075207574311 rho 100000.0 h 0.0006431026761362091
iteration 6 in outer loop, alpha = 917.6101968936402, rho = 1000000.0, h = 0.0006431026761362091
Threshold 0.3
[[0.001 3.151 0.08  0.248 0.909 1.121 0.318 0.    1.96  1.137 0.123 1.133
  0.274 0.001 0.366 0.449 0.296 1.219 1.309 0.475]
 [0.    0.001 0.024 0.499 0.487 1.667 1.798 0.    0.566 0.466 0.09  0.676
  0.318 0.001 1.194 0.3   0.346 0.865 0.489 1.908]
 [0.002 0.009 0.    0.062 0.447 0.556 1.521 0.    0.19  0.488 0.096 0.188
  1.31  0.001 1.508 0.314 0.623 0.001 1.205 0.338]
 [0.    0.    0.    0.02  0.    0.003 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.209 0.219 0.17  0.    0.001 0.725]
 [0.    0.    0.001 0.596 0.004 0.325 0.22  0.    0.    0.415 0.002 0.001
  0.003 0.    0.22  0.331 1.24  0.    0.339 1.073]
 [0.    0.    0.    0.526 0.002 0.003 0.001 0.    0.    0.    0.    0.
  0.    0.    0.361 1.717 1.844 0.    0.293 0.298]
 [0.    0.    0.    0.185 0.001 0.228 0.002 0.    0.    0.01  0.    0.
  0.    0.    0.332 0.334 0.302 0.    0.169 0.206]
 [2.545 0.133 0.002 0.105 0.71  0.221 0.731 0.001 0.302 0.461 0.135 0.387
  1.977 0.695 0.221 0.215 0.179 1.138 0.495 0.455]
 [0.    0.    0.002 0.339 1.601 0.951 1.304 0.    0.001 0.836 0.452 0.296
  1.784 0.    1.229 1.303 0.826 0.    1.345 0.689]
 [0.    0.    0.001 0.108 0.016 2.218 0.12  0.    0.    0.008 0.001 0.001
  0.    0.    1.19  1.438 1.761 0.    0.828 0.226]
 [0.    0.    0.    0.965 0.111 0.593 2.331 0.    0.    0.262 0.001 0.
  0.    0.    1.522 0.482 1.038 0.    0.277 1.33 ]
 [0.    0.    0.    0.151 0.442 0.254 1.675 0.    0.    0.255 0.128 0.001
  0.118 0.    0.16  0.882 0.286 0.    0.123 0.221]
 [0.    0.    0.    0.1   0.141 1.402 1.086 0.    0.    0.18  2.157 0.002
  0.001 0.    0.305 0.336 0.301 0.    0.488 1.098]
 [0.158 0.037 0.032 0.757 0.339 1.645 0.322 0.001 0.289 0.357 1.719 0.533
  2.069 0.001 0.435 0.437 1.046 0.355 0.738 1.984]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.298 0.003 0.    0.001 0.002]
 [0.    0.    0.    0.003 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.001 0.006 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.003 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.132 1.268 0.004 0.    0.001 0.001]
 [0.    0.    0.032 0.048 0.462 0.296 1.389 0.    3.191 0.558 0.095 0.73
  0.087 0.    0.345 0.205 0.19  0.001 0.421 0.251]
 [0.    0.    0.    0.769 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.338 1.116 0.387 0.    0.001 0.251]
 [0.    0.    0.    0.003 0.    0.    0.001 0.    0.    0.001 0.    0.
  0.    0.    0.113 1.776 0.28  0.    0.001 0.002]]
[[0.    3.151 0.    0.    0.909 1.121 0.318 0.    1.96  1.137 0.    1.133
  0.    0.    0.366 0.449 0.    1.219 1.309 0.475]
 [0.    0.    0.    0.499 0.487 1.667 1.798 0.    0.566 0.466 0.    0.676
  0.318 0.    1.194 0.    0.346 0.865 0.489 1.908]
 [0.    0.    0.    0.    0.447 0.556 1.521 0.    0.    0.488 0.    0.
  1.31  0.    1.508 0.314 0.623 0.    1.205 0.338]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.725]
 [0.    0.    0.    0.596 0.    0.325 0.    0.    0.    0.415 0.    0.
  0.    0.    0.    0.331 1.24  0.    0.339 1.073]
 [0.    0.    0.    0.526 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.361 1.717 1.844 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.332 0.334 0.302 0.    0.    0.   ]
 [2.545 0.    0.    0.    0.71  0.    0.731 0.    0.302 0.461 0.    0.387
  1.977 0.695 0.    0.    0.    1.138 0.495 0.455]
 [0.    0.    0.    0.339 1.601 0.951 1.304 0.    0.    0.836 0.452 0.
  1.784 0.    1.229 1.303 0.826 0.    1.345 0.689]
 [0.    0.    0.    0.    0.    2.218 0.    0.    0.    0.    0.    0.
  0.    0.    1.19  1.438 1.761 0.    0.828 0.   ]
 [0.    0.    0.    0.965 0.    0.593 2.331 0.    0.    0.    0.    0.
  0.    0.    1.522 0.482 1.038 0.    0.    1.33 ]
 [0.    0.    0.    0.    0.442 0.    1.675 0.    0.    0.    0.    0.
  0.    0.    0.    0.882 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.402 1.086 0.    0.    0.    2.157 0.
  0.    0.    0.305 0.336 0.301 0.    0.488 1.098]
 [0.    0.    0.    0.757 0.339 1.645 0.322 0.    0.    0.357 1.719 0.533
  2.069 0.    0.435 0.437 1.046 0.355 0.738 1.984]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.132 1.268 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.462 0.    1.389 0.    3.191 0.558 0.    0.73
  0.    0.    0.345 0.    0.    0.    0.421 0.   ]
 [0.    0.    0.    0.769 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.338 1.116 0.387 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.776 0.    0.    0.    0.   ]]
{'fdr': 0.43548387096774194, 'tpr': 0.875, 'fpr': 0.4909090909090909, 'f1': 0.6862745098039216, 'shd': 59, 'npred': 124, 'ntrue': 80}
[3.151e+00 7.965e-02 2.482e-01 9.093e-01 1.121e+00 3.177e-01 2.724e-04
 1.960e+00 1.137e+00 1.227e-01 1.133e+00 2.735e-01 1.348e-03 3.661e-01
 4.492e-01 2.957e-01 1.219e+00 1.309e+00 4.751e-01 3.994e-04 2.398e-02
 4.990e-01 4.872e-01 1.667e+00 1.798e+00 1.734e-04 5.663e-01 4.662e-01
 8.975e-02 6.764e-01 3.177e-01 1.135e-03 1.194e+00 2.997e-01 3.463e-01
 8.653e-01 4.887e-01 1.908e+00 2.307e-03 8.888e-03 6.163e-02 4.472e-01
 5.557e-01 1.521e+00 4.605e-04 1.901e-01 4.883e-01 9.623e-02 1.881e-01
 1.310e+00 5.191e-04 1.508e+00 3.143e-01 6.231e-01 9.470e-04 1.205e+00
 3.377e-01 7.560e-05 1.452e-05 8.153e-05 2.799e-04 2.945e-03 1.216e-03
 4.047e-05 4.173e-04 1.812e-03 3.941e-04 2.413e-04 8.378e-05 1.274e-05
 2.091e-01 2.189e-01 1.697e-01 1.331e-04 1.164e-03 7.252e-01 6.319e-06
 2.498e-05 7.986e-04 5.957e-01 3.245e-01 2.202e-01 1.826e-05 1.989e-04
 4.151e-01 1.563e-03 1.235e-03 2.588e-03 7.069e-05 2.199e-01 3.306e-01
 1.240e+00 1.954e-05 3.388e-01 1.073e+00 8.432e-05 1.954e-04 7.530e-05
 5.259e-01 2.056e-03 1.442e-03 4.869e-05 7.578e-05 4.242e-04 5.411e-05
 3.646e-04 2.599e-04 3.642e-05 3.608e-01 1.717e+00 1.844e+00 1.276e-05
 2.925e-01 2.981e-01 5.791e-05 1.478e-04 2.112e-05 1.851e-01 6.655e-04
 2.276e-01 3.103e-05 7.244e-05 9.773e-03 1.430e-04 2.195e-04 4.111e-05
 4.119e-06 3.317e-01 3.336e-01 3.018e-01 2.229e-06 1.692e-01 2.062e-01
 2.545e+00 1.329e-01 1.849e-03 1.054e-01 7.105e-01 2.210e-01 7.307e-01
 3.019e-01 4.605e-01 1.351e-01 3.869e-01 1.977e+00 6.954e-01 2.207e-01
 2.147e-01 1.792e-01 1.138e+00 4.954e-01 4.555e-01 1.081e-05 2.004e-05
 1.836e-03 3.392e-01 1.601e+00 9.508e-01 1.304e+00 1.064e-05 8.363e-01
 4.517e-01 2.958e-01 1.784e+00 2.917e-05 1.229e+00 1.303e+00 8.261e-01
 1.204e-04 1.345e+00 6.890e-01 2.410e-05 1.585e-04 5.190e-04 1.081e-01
 1.552e-02 2.218e+00 1.201e-01 1.119e-04 1.621e-04 5.206e-04 1.205e-03
 3.139e-04 1.286e-04 1.190e+00 1.438e+00 1.761e+00 4.199e-05 8.280e-01
 2.259e-01 7.399e-06 1.144e-05 9.159e-05 9.645e-01 1.114e-01 5.928e-01
 2.331e+00 2.182e-05 7.159e-05 2.621e-01 4.249e-04 2.272e-04 3.300e-05
 1.522e+00 4.816e-01 1.038e+00 1.394e-05 2.773e-01 1.330e+00 2.889e-05
 4.243e-05 3.314e-04 1.510e-01 4.418e-01 2.537e-01 1.675e+00 9.387e-06
 2.242e-04 2.548e-01 1.282e-01 1.177e-01 1.477e-04 1.595e-01 8.819e-01
 2.855e-01 1.688e-04 1.228e-01 2.207e-01 5.196e-06 1.815e-05 1.943e-04
 1.002e-01 1.412e-01 1.402e+00 1.086e+00 3.085e-05 2.417e-04 1.805e-01
 2.157e+00 1.574e-03 7.650e-05 3.055e-01 3.365e-01 3.008e-01 3.265e-05
 4.883e-01 1.098e+00 1.581e-01 3.743e-02 3.152e-02 7.566e-01 3.392e-01
 1.645e+00 3.220e-01 6.780e-04 2.886e-01 3.568e-01 1.719e+00 5.327e-01
 2.069e+00 4.353e-01 4.368e-01 1.046e+00 3.547e-01 7.382e-01 1.984e+00
 4.349e-05 5.157e-05 3.730e-05 3.446e-03 1.238e-03 7.798e-04 3.672e-04
 2.885e-05 7.556e-06 8.270e-05 1.062e-04 2.392e-04 7.322e-05 1.150e-04
 2.984e-01 2.694e-03 1.943e-06 6.202e-04 1.944e-03 2.899e-05 2.159e-05
 4.930e-05 2.763e-03 5.344e-04 1.037e-04 3.901e-04 3.229e-05 2.048e-05
 2.567e-04 8.750e-05 6.330e-05 1.099e-05 1.280e-05 8.623e-04 1.105e-03
 5.146e-06 6.350e-04 2.561e-04 5.055e-05 8.550e-05 2.873e-05 3.015e-03
 8.954e-04 5.979e-04 8.255e-04 4.075e-05 4.971e-05 6.741e-05 7.750e-05
 9.318e-05 2.454e-05 3.149e-05 1.132e+00 1.268e+00 5.316e-06 6.851e-04
 1.490e-03 9.260e-05 1.373e-05 3.176e-02 4.811e-02 4.623e-01 2.956e-01
 1.389e+00 8.761e-05 3.191e+00 5.580e-01 9.532e-02 7.301e-01 8.712e-02
 1.233e-04 3.450e-01 2.051e-01 1.898e-01 4.205e-01 2.506e-01 7.040e-05
 1.590e-05 1.571e-04 7.693e-01 1.040e-03 7.839e-04 9.645e-04 4.579e-05
 1.613e-04 5.305e-04 2.079e-04 3.017e-04 1.475e-04 2.256e-05 3.378e-01
 1.116e+00 3.868e-01 4.761e-06 2.512e-01 3.799e-05 7.420e-05 6.691e-05
 2.842e-03 2.794e-04 3.272e-04 5.689e-04 1.886e-05 7.466e-05 6.150e-04
 3.413e-04 3.122e-04 2.312e-04 8.974e-05 1.125e-01 1.776e+00 2.801e-01
 1.522e-05 6.057e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.9383333333333334, 0.8902759432084234)
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
total norm for a microbatch 66.85582121868109 clip 9.735638618462225
total norm for a microbatch 60.1668582793219 clip 24.671505942188162
total norm for a microbatch 53.03002762584454 clip 26.799095292333973
total norm for a microbatch 49.70646997653814 clip 56.419080316143535
total norm for a microbatch 70.57150749209013 clip 53.31392940576015
cuda
Objective function 126.19 = squared loss an data 122.74 + 0.5*rho*h**2 2.615633 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.28 ; SHD = 122 ; DAG False
Proportion of microbatches that were clipped  0.9342775302355187
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.2871961313799787
iteration 1 in outer loop, alpha = 2.2871961313799787, rho = 1.0, h = 2.2871961313799787
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 131.42 = squared loss an data 122.74 + 0.5*rho*h**2 2.615633 + alpha*h 5.231266 + L2reg 0.55 + L1reg 0.28 ; SHD = 122 ; DAG False
total norm for a microbatch 68.3676900966791 clip 5.796128526196119
total norm for a microbatch 145.7099683585764 clip 7.00071832024005
total norm for a microbatch 85.87352764714349 clip 38.02246464080932
total norm for a microbatch 58.727572327558605 clip 55.72961023280375
total norm for a microbatch 64.23257821774291 clip 64.34586731276751
cuda
Objective function 47.44 = squared loss an data 40.60 + 0.5*rho*h**2 1.488207 + alpha*h 3.945936 + L2reg 1.14 + L1reg 0.26 ; SHD = 100 ; DAG False
Proportion of microbatches that were clipped  0.9459941615309764
iteration 1 in inner loop, alpha 2.2871961313799787 rho 1.0 h 1.7252286809310533
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 60.83 = squared loss an data 40.60 + 0.5*rho*h**2 14.882070 + alpha*h 3.945936 + L2reg 1.14 + L1reg 0.26 ; SHD = 100 ; DAG False
total norm for a microbatch 102.7801080390364 clip 3.856639643488005
total norm for a microbatch 84.58758957300229 clip 20.829888840481726
total norm for a microbatch 161.85951301275878 clip 39.41742534075676
total norm for a microbatch 117.83679525417428 clip 85.09589826851433
total norm for a microbatch 105.05695129544236 clip 105.74318352098776
cuda
Objective function 37.55 = squared loss an data 30.13 + 0.5*rho*h**2 3.730850 + alpha*h 1.975706 + L2reg 1.48 + L1reg 0.23 ; SHD = 75 ; DAG False
Proportion of microbatches that were clipped  0.9619915187686509
iteration 2 in inner loop, alpha 2.2871961313799787 rho 10.0 h 0.8638112668302682
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 71.13 = squared loss an data 30.13 + 0.5*rho*h**2 37.308495 + alpha*h 1.975706 + L2reg 1.48 + L1reg 0.23 ; SHD = 75 ; DAG False
total norm for a microbatch 299.4986091764294 clip 1.7191142432582052
total norm for a microbatch 206.31112638199357 clip 23.116110703380883
total norm for a microbatch 155.53524746677488 clip 32.907788644336755
total norm for a microbatch 126.36735127644026 clip 126.17565363250522
cuda
Objective function 40.63 = squared loss an data 32.50 + 0.5*rho*h**2 5.500720 + alpha*h 0.758627 + L2reg 1.68 + L1reg 0.20 ; SHD = 66 ; DAG True
Proportion of microbatches that were clipped  0.9723480706139666
iteration 3 in inner loop, alpha 2.2871961313799787 rho 100.0 h 0.33168419281475536
iteration 2 in outer loop, alpha = 35.455615412855515, rho = 100.0, h = 0.33168419281475536
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 51.64 = squared loss an data 32.50 + 0.5*rho*h**2 5.500720 + alpha*h 11.760067 + L2reg 1.68 + L1reg 0.20 ; SHD = 66 ; DAG True
total norm for a microbatch 138.4236278566741 clip 1.7234391714497566
total norm for a microbatch 217.27361615077035 clip 1.7234391714497566
total norm for a microbatch 110.06220411802819 clip 2.4961687087424425
total norm for a microbatch 192.78672049603068 clip 6.133355975213887
total norm for a microbatch 135.98639758922045 clip 7.901057120076323
total norm for a microbatch 171.79963844808637 clip 13.659422633934094
total norm for a microbatch 196.4804956442057 clip 16.47420604951095
total norm for a microbatch 212.62991737098474 clip 91.14236758354487
total norm for a microbatch 170.4310190914999 clip 140.8861738549283
total norm for a microbatch 122.58600925994932 clip 139.17558468552792
cuda
Objective function 44.38 = squared loss an data 32.52 + 0.5*rho*h**2 2.263860 + alpha*h 7.544402 + L2reg 1.86 + L1reg 0.19 ; SHD = 67 ; DAG True
Proportion of microbatches that were clipped  0.9699689897176432
iteration 1 in inner loop, alpha 35.455615412855515 rho 100.0 h 0.21278441298614226
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 64.75 = squared loss an data 32.52 + 0.5*rho*h**2 22.638603 + alpha*h 7.544402 + L2reg 1.86 + L1reg 0.19 ; SHD = 67 ; DAG True
total norm for a microbatch 115.35648936089629 clip 2.08949831268089
total norm for a microbatch 147.199128527965 clip 6.143904674117205
total norm for a microbatch 404.41877824645724 clip 13.769505993346549
cuda
Objective function 43.42 = squared loss an data 33.78 + 0.5*rho*h**2 4.222136 + alpha*h 3.258113 + L2reg 1.98 + L1reg 0.18 ; SHD = 65 ; DAG True
Proportion of microbatches that were clipped  0.9727403156384505
iteration 2 in inner loop, alpha 35.455615412855515 rho 1000.0 h 0.09189271752700989
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 81.42 = squared loss an data 33.78 + 0.5*rho*h**2 42.221358 + alpha*h 3.258113 + L2reg 1.98 + L1reg 0.18 ; SHD = 65 ; DAG True
total norm for a microbatch 133.9015316288596 clip 5.06717997469573
total norm for a microbatch 260.9228860077967 clip 47.46975445010847
total norm for a microbatch 219.1519890438218 clip 52.02686497924699
total norm for a microbatch 203.8636110981214 clip 108.16866203959832
total norm for a microbatch 273.19543526143485 clip 145.04768476548935
cuda
Objective function 44.44 = squared loss an data 34.93 + 0.5*rho*h**2 6.031606 + alpha*h 1.231449 + L2reg 2.07 + L1reg 0.18 ; SHD = 66 ; DAG True
Proportion of microbatches that were clipped  0.9807847569836913
iteration 3 in inner loop, alpha 35.455615412855515 rho 10000.0 h 0.03473213594739022
iteration 3 in outer loop, alpha = 382.7769748867577, rho = 10000.0, h = 0.03473213594739022
cuda
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 56.50 = squared loss an data 34.93 + 0.5*rho*h**2 6.031606 + alpha*h 13.294662 + L2reg 2.07 + L1reg 0.18 ; SHD = 66 ; DAG True
total norm for a microbatch 207.96121678738604 clip 1.0
total norm for a microbatch 313.89453134851533 clip 1.401524849263198
total norm for a microbatch 205.22842688801808 clip 24.48492849976962
total norm for a microbatch 196.1833321467219 clip 31.635876672410046
total norm for a microbatch 169.60448125413313 clip 60.189203515601456
cuda
Objective function 47.83 = squared loss an data 35.34 + 0.5*rho*h**2 2.166516 + alpha*h 7.967861 + L2reg 2.18 + L1reg 0.18 ; SHD = 72 ; DAG True
Proportion of microbatches that were clipped  0.9840789683171469
iteration 1 in inner loop, alpha 382.7769748867577 rho 10000.0 h 0.0208159362080238
noise_multiplier  0.6  noise_multiplier_b  5.0  noise_multiplier_delta  0.6010829247756457
cuda
Objective function 67.33 = squared loss an data 35.34 + 0.5*rho*h**2 21.665160 + alpha*h 7.967861 + L2reg 2.18 + L1reg 0.18 ; SHD = 72 ; DAG True
total norm for a microbatch 452.6203092737995 clip 1.0
total norm for a microbatch 290.00327684696106 clip 1.7444983489094892
total norm for a microbatch 142.86466367881013 clip 10.755457944377467
total norm for a microbatch 224.47564123356122 clip 20.31047702811882
total norm for a microbatch 242.80827989693896 clip 54.65658902491213
cuda
Objective function 47.49 = squared loss an data 35.25 + 0.5*rho*h**2 5.705450 + alpha*h 4.088893 + L2reg 2.26 + L1reg 0.19 ; SHD = 74 ; DAG True
Proportion of microbatches that were clipped  0.9887932434627253
iteration 2 in inner loop, alpha 382.7769748867577 rho 100000.0 h 0.010682181853560735
iteration 4 in outer loop, alpha = 11064.958828447492, rho = 1000000.0, h = 0.010682181853560735
Threshold 0.3
[[0.007 2.346 0.581 0.927 0.599 0.626 0.754 0.315 0.286 0.792 0.374 0.476
  0.686 0.292 0.311 0.516 0.446 0.712 0.845 0.536]
 [0.003 0.008 0.135 0.616 0.341 0.437 0.766 0.097 0.086 0.417 0.103 0.225
  0.114 0.091 0.133 0.256 0.186 0.131 0.251 0.223]
 [0.009 0.045 0.006 0.169 0.261 0.023 0.31  0.058 0.035 0.22  0.032 0.292
  0.036 0.028 0.07  0.323 0.092 0.29  0.132 0.068]
 [0.005 0.01  0.034 0.008 0.028 0.011 0.04  0.03  0.012 0.018 0.008 0.049
  0.023 0.005 0.067 0.064 0.038 0.043 0.019 0.007]
 [0.007 0.014 0.028 0.131 0.006 0.01  0.027 0.007 0.005 0.017 0.013 0.02
  0.022 0.01  0.029 0.048 0.032 0.01  0.015 0.011]
 [0.005 0.017 0.237 0.549 0.447 0.005 0.213 0.144 0.017 0.619 0.078 0.224
  0.073 0.022 0.313 1.085 1.065 0.277 0.299 0.164]
 [0.004 0.005 0.023 0.169 0.172 0.036 0.005 0.059 0.009 0.169 0.007 0.226
  0.042 0.023 0.082 0.094 0.094 0.074 0.064 0.032]
 [0.018 0.058 0.102 0.143 0.606 0.049 0.123 0.005 0.031 0.19  0.115 0.38
  0.076 0.028 0.109 0.19  0.106 0.112 0.056 0.116]
 [0.026 0.047 0.224 0.436 0.795 0.441 0.487 0.172 0.007 0.372 0.322 0.518
  0.316 0.062 0.585 0.528 0.616 0.711 0.748 0.266]
 [0.007 0.014 0.044 0.479 0.507 0.01  0.037 0.034 0.013 0.007 0.022 0.092
  0.028 0.014 0.074 0.239 0.032 0.06  0.019 0.05 ]
 [0.018 0.036 0.22  0.81  0.428 0.081 1.033 0.042 0.024 0.254 0.007 0.307
  0.56  0.015 0.313 0.317 0.146 0.375 0.143 0.665]
 [0.007 0.027 0.025 0.106 0.29  0.022 0.043 0.013 0.012 0.091 0.021 0.007
  0.053 0.013 0.072 0.188 0.029 0.059 0.037 0.021]
 [0.006 0.056 0.156 0.326 0.208 0.086 0.16  0.067 0.026 0.254 0.011 0.183
  0.007 0.01  0.27  0.305 0.142 0.095 0.103 0.128]
 [0.024 0.055 0.205 0.697 0.471 0.32  0.315 0.292 0.131 0.509 0.409 0.319
  0.438 0.006 0.536 0.48  0.47  0.302 0.244 0.741]
 [0.015 0.035 0.09  0.121 0.266 0.016 0.076 0.054 0.009 0.1   0.031 0.13
  0.019 0.014 0.008 0.27  0.011 0.076 0.055 0.039]
 [0.005 0.018 0.015 0.099 0.154 0.005 0.059 0.044 0.005 0.038 0.009 0.04
  0.019 0.009 0.028 0.006 0.009 0.041 0.008 0.008]
 [0.008 0.022 0.065 0.193 0.251 0.006 0.096 0.032 0.012 0.269 0.035 0.278
  0.045 0.009 0.68  0.689 0.007 0.085 0.058 0.048]
 [0.009 0.038 0.031 0.218 0.577 0.025 0.107 0.066 0.008 0.173 0.011 0.165
  0.084 0.025 0.093 0.181 0.07  0.007 0.039 0.023]
 [0.008 0.039 0.066 0.324 0.531 0.022 0.115 0.085 0.007 0.418 0.032 0.129
  0.072 0.035 0.077 1.053 0.13  0.167 0.009 0.064]
 [0.014 0.028 0.086 0.899 0.474 0.043 0.174 0.064 0.024 0.25  0.011 0.345
  0.036 0.011 0.162 1.028 0.106 0.223 0.134 0.008]]
[[0.    2.346 0.581 0.927 0.599 0.626 0.754 0.315 0.    0.792 0.374 0.476
  0.686 0.    0.311 0.516 0.446 0.712 0.845 0.536]
 [0.    0.    0.    0.616 0.341 0.437 0.766 0.    0.    0.417 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.31  0.    0.    0.    0.    0.
  0.    0.    0.    0.323 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.549 0.447 0.    0.    0.    0.    0.619 0.    0.
  0.    0.    0.313 1.085 1.065 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.606 0.    0.    0.    0.    0.    0.    0.38
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.436 0.795 0.441 0.487 0.    0.    0.372 0.322 0.518
  0.316 0.    0.585 0.528 0.616 0.711 0.748 0.   ]
 [0.    0.    0.    0.479 0.507 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.81  0.428 0.    1.033 0.    0.    0.    0.    0.307
  0.56  0.    0.313 0.317 0.    0.375 0.    0.665]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.326 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.305 0.    0.    0.    0.   ]
 [0.    0.    0.    0.697 0.471 0.32  0.315 0.    0.    0.509 0.409 0.319
  0.438 0.    0.536 0.48  0.47  0.302 0.    0.741]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.68  0.689 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.577 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.324 0.531 0.    0.    0.    0.    0.418 0.    0.
  0.    0.    0.    1.053 0.    0.    0.    0.   ]
 [0.    0.    0.    0.899 0.474 0.    0.    0.    0.    0.    0.    0.345
  0.    0.    0.    1.028 0.    0.    0.    0.   ]]
{'fdr': 0.5, 'tpr': 0.5125, 'fpr': 0.37272727272727274, 'f1': 0.5061728395061729, 'shd': 74, 'npred': 82, 'ntrue': 80}
[2.346 0.581 0.927 0.599 0.626 0.754 0.315 0.286 0.792 0.374 0.476 0.686
 0.292 0.311 0.516 0.446 0.712 0.845 0.536 0.003 0.135 0.616 0.341 0.437
 0.766 0.097 0.086 0.417 0.103 0.225 0.114 0.091 0.133 0.256 0.186 0.131
 0.251 0.223 0.009 0.045 0.169 0.261 0.023 0.31  0.058 0.035 0.22  0.032
 0.292 0.036 0.028 0.07  0.323 0.092 0.29  0.132 0.068 0.005 0.01  0.034
 0.028 0.011 0.04  0.03  0.012 0.018 0.008 0.049 0.023 0.005 0.067 0.064
 0.038 0.043 0.019 0.007 0.007 0.014 0.028 0.131 0.01  0.027 0.007 0.005
 0.017 0.013 0.02  0.022 0.01  0.029 0.048 0.032 0.01  0.015 0.011 0.005
 0.017 0.237 0.549 0.447 0.213 0.144 0.017 0.619 0.078 0.224 0.073 0.022
 0.313 1.085 1.065 0.277 0.299 0.164 0.004 0.005 0.023 0.169 0.172 0.036
 0.059 0.009 0.169 0.007 0.226 0.042 0.023 0.082 0.094 0.094 0.074 0.064
 0.032 0.018 0.058 0.102 0.143 0.606 0.049 0.123 0.031 0.19  0.115 0.38
 0.076 0.028 0.109 0.19  0.106 0.112 0.056 0.116 0.026 0.047 0.224 0.436
 0.795 0.441 0.487 0.172 0.372 0.322 0.518 0.316 0.062 0.585 0.528 0.616
 0.711 0.748 0.266 0.007 0.014 0.044 0.479 0.507 0.01  0.037 0.034 0.013
 0.022 0.092 0.028 0.014 0.074 0.239 0.032 0.06  0.019 0.05  0.018 0.036
 0.22  0.81  0.428 0.081 1.033 0.042 0.024 0.254 0.307 0.56  0.015 0.313
 0.317 0.146 0.375 0.143 0.665 0.007 0.027 0.025 0.106 0.29  0.022 0.043
 0.013 0.012 0.091 0.021 0.053 0.013 0.072 0.188 0.029 0.059 0.037 0.021
 0.006 0.056 0.156 0.326 0.208 0.086 0.16  0.067 0.026 0.254 0.011 0.183
 0.01  0.27  0.305 0.142 0.095 0.103 0.128 0.024 0.055 0.205 0.697 0.471
 0.32  0.315 0.292 0.131 0.509 0.409 0.319 0.438 0.536 0.48  0.47  0.302
 0.244 0.741 0.015 0.035 0.09  0.121 0.266 0.016 0.076 0.054 0.009 0.1
 0.031 0.13  0.019 0.014 0.27  0.011 0.076 0.055 0.039 0.005 0.018 0.015
 0.099 0.154 0.005 0.059 0.044 0.005 0.038 0.009 0.04  0.019 0.009 0.028
 0.009 0.041 0.008 0.008 0.008 0.022 0.065 0.193 0.251 0.006 0.096 0.032
 0.012 0.269 0.035 0.278 0.045 0.009 0.68  0.689 0.085 0.058 0.048 0.009
 0.038 0.031 0.218 0.577 0.025 0.107 0.066 0.008 0.173 0.011 0.165 0.084
 0.025 0.093 0.181 0.07  0.039 0.023 0.008 0.039 0.066 0.324 0.531 0.022
 0.115 0.085 0.007 0.418 0.032 0.129 0.072 0.035 0.077 1.053 0.13  0.167
 0.064 0.014 0.028 0.086 0.899 0.474 0.043 0.174 0.064 0.024 0.25  0.011
 0.345 0.036 0.011 0.162 1.028 0.106 0.223 0.134]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.7645833333333334, 0.562896306094483)
Iterations 567
Achieves (13.598573656938171, 1e-05)-DP
