samples  5000  graph  20 60 ER mim  minibatch size  50  noise  0.6  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.895209681015416
iteration 1 in outer loop, alpha = 1.895209681015416, rho = 1.0, h = 1.895209681015416
cuda
iteration 1 in inner loop,alpha 1.895209681015416 rho 1.0 h 1.1840756057740833
iteration 2 in inner loop,alpha 1.895209681015416 rho 10.0 h 0.459598151425638
iteration 2 in outer loop, alpha = 6.491191195271796, rho = 10.0, h = 0.459598151425638
cuda
iteration 1 in inner loop,alpha 6.491191195271796 rho 10.0 h 0.2595113229236361
iteration 2 in inner loop,alpha 6.491191195271796 rho 100.0 h 0.09244780263232144
iteration 3 in outer loop, alpha = 15.73597145850394, rho = 100.0, h = 0.09244780263232144
cuda
iteration 1 in inner loop,alpha 15.73597145850394 rho 100.0 h 0.053410540875525214
iteration 2 in inner loop,alpha 15.73597145850394 rho 1000.0 h 0.020149957930769347
iteration 4 in outer loop, alpha = 35.88592938927329, rho = 1000.0, h = 0.020149957930769347
cuda
iteration 1 in inner loop,alpha 35.88592938927329 rho 1000.0 h 0.010802106314571347
iteration 2 in inner loop,alpha 35.88592938927329 rho 10000.0 h 0.0034416016153002715
iteration 5 in outer loop, alpha = 70.301945542276, rho = 10000.0, h = 0.0034416016153002715
cuda
iteration 1 in inner loop,alpha 70.301945542276 rho 10000.0 h 0.001431393238430445
iteration 2 in inner loop,alpha 70.301945542276 rho 100000.0 h 0.000548923019778158
iteration 6 in outer loop, alpha = 125.1942475200918, rho = 100000.0, h = 0.000548923019778158
cuda
iteration 1 in inner loop,alpha 125.1942475200918 rho 100000.0 h 0.00032243468063342107
iteration 7 in outer loop, alpha = 447.6289281535129, rho = 1000000.0, h = 0.00032243468063342107
Threshold 0.3
[[0.    0.027 0.015 0.042 0.15  0.188 0.058 0.002 0.04  0.071 0.492 0.756
  1.107 0.003 0.086 0.009 0.076 2.59  0.523 0.019]
 [0.    0.001 0.    0.041 0.082 0.465 0.189 0.    0.008 0.043 0.042 0.
  0.    0.    0.072 0.144 0.018 0.    0.102 0.798]
 [0.008 0.288 0.002 0.129 0.54  0.129 0.101 0.004 1.49  0.865 0.116 0.009
  0.011 0.    0.03  0.216 0.057 0.039 0.136 0.114]
 [0.    0.006 0.    0.002 0.001 0.004 0.    0.    0.    0.001 0.002 0.
  0.    0.    0.048 0.001 0.    0.    0.002 0.021]
 [0.    0.003 0.    0.184 0.002 0.007 0.004 0.    0.    0.01  0.008 0.
  0.    0.    0.039 0.103 0.003 0.    0.001 0.007]
 [0.    0.002 0.005 0.082 0.032 0.005 0.009 0.    0.026 0.084 0.004 0.
  0.003 0.    1.197 0.118 0.006 0.    0.364 0.042]
 [0.    0.002 0.001 0.231 0.305 0.231 0.003 0.    0.001 0.004 0.002 0.001
  0.002 0.    0.096 0.012 0.001 0.    0.    0.013]
 [0.002 0.098 0.005 1.42  0.045 0.062 0.195 0.    0.39  0.071 0.077 2.293
  1.369 0.023 0.108 0.013 0.029 2.298 0.047 0.039]
 [0.    0.02  0.    1.465 1.389 0.008 0.097 0.    0.003 0.466 1.258 0.
  0.001 0.    0.422 0.15  0.002 0.    0.005 0.018]
 [0.    0.003 0.001 0.053 0.015 0.074 0.116 0.    0.003 0.004 1.083 0.001
  0.002 0.    0.088 0.006 0.041 0.    0.004 0.668]
 [0.    0.008 0.    0.026 0.009 0.019 0.354 0.    0.    0.001 0.002 0.
  0.    0.    0.014 0.004 0.352 0.    0.002 0.738]
 [0.    2.089 0.004 1.584 1.302 0.05  0.618 0.    1.046 0.043 1.115 0.001
  0.001 0.002 0.01  0.922 0.409 0.    0.069 0.116]
 [0.    0.034 0.003 0.187 0.077 0.137 0.495 0.    0.01  0.618 0.057 0.604
  0.003 0.    1.6   0.153 0.254 0.    0.008 0.056]
 [0.003 1.038 4.154 0.101 0.031 0.177 0.107 0.011 0.857 0.189 0.048 0.032
  0.052 0.    0.055 0.687 0.007 0.097 0.71  1.196]
 [0.    0.001 0.    0.016 0.02  0.001 0.001 0.    0.002 0.007 0.001 0.
  0.001 0.    0.002 0.009 0.001 0.    0.001 0.003]
 [0.    0.002 0.002 0.091 0.006 0.01  0.006 0.    0.004 0.323 0.034 0.
  0.    0.    0.039 0.003 0.006 0.    0.001 0.084]
 [0.    0.011 0.001 1.032 0.107 0.006 0.956 0.    0.001 0.002 0.002 0.
  0.002 0.    0.554 0.002 0.002 0.    0.003 0.021]
 [0.    0.046 0.005 0.278 0.051 0.055 0.065 0.    0.1   0.055 0.108 1.106
  2.16  0.008 0.007 0.077 0.05  0.002 0.024 1.253]
 [0.    0.003 0.009 0.098 0.771 0.012 0.009 0.    0.435 0.303 0.086 0.
  0.004 0.    0.699 1.072 0.006 0.004 0.004 0.063]
 [0.    0.    0.    0.018 0.01  0.023 0.072 0.    0.001 0.001 0.001 0.
  0.    0.    0.055 0.001 0.002 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.492 0.756
  1.107 0.    0.    0.    0.    2.59  0.523 0.   ]
 [0.    0.    0.    0.    0.    0.465 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.798]
 [0.    0.    0.    0.    0.54  0.    0.    0.    1.49  0.865 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.197 0.    0.    0.    0.364 0.   ]
 [0.    0.    0.    0.    0.305 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.42  0.    0.    0.    0.    0.39  0.    0.    2.293
  1.369 0.    0.    0.    0.    2.298 0.    0.   ]
 [0.    0.    0.    1.465 1.389 0.    0.    0.    0.    0.466 1.258 0.
  0.    0.    0.422 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.083 0.
  0.    0.    0.    0.    0.    0.    0.    0.668]
 [0.    0.    0.    0.    0.    0.    0.354 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.352 0.    0.    0.738]
 [0.    2.089 0.    1.584 1.302 0.    0.618 0.    1.046 0.    1.115 0.
  0.    0.    0.    0.922 0.409 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.495 0.    0.    0.618 0.    0.604
  0.    0.    1.6   0.    0.    0.    0.    0.   ]
 [0.    1.038 4.154 0.    0.    0.    0.    0.    0.857 0.    0.    0.
  0.    0.    0.    0.687 0.    0.    0.71  1.196]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.323 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.032 0.    0.    0.956 0.    0.    0.    0.    0.
  0.    0.    0.554 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.106
  2.16  0.    0.    0.    0.    0.    0.    1.253]
 [0.    0.    0.    0.    0.771 0.    0.    0.    0.435 0.303 0.    0.
  0.    0.    0.699 1.072 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.15517241379310345, 'tpr': 0.8166666666666667, 'fpr': 0.06923076923076923, 'f1': 0.8305084745762712, 'shd': 14, 'npred': 58, 'ntrue': 60}
[2.713e-02 1.537e-02 4.227e-02 1.501e-01 1.879e-01 5.790e-02 2.171e-03
 4.049e-02 7.132e-02 4.924e-01 7.558e-01 1.107e+00 2.943e-03 8.611e-02
 8.610e-03 7.635e-02 2.590e+00 5.225e-01 1.851e-02 4.394e-06 3.617e-04
 4.054e-02 8.202e-02 4.648e-01 1.893e-01 6.822e-06 8.434e-03 4.332e-02
 4.214e-02 9.468e-05 1.208e-04 7.354e-05 7.170e-02 1.442e-01 1.794e-02
 2.776e-05 1.020e-01 7.984e-01 7.795e-03 2.875e-01 1.293e-01 5.398e-01
 1.291e-01 1.014e-01 3.516e-03 1.490e+00 8.646e-01 1.162e-01 8.733e-03
 1.065e-02 3.476e-05 2.959e-02 2.164e-01 5.704e-02 3.930e-02 1.360e-01
 1.141e-01 4.475e-06 6.475e-03 1.618e-04 1.286e-03 4.402e-03 1.035e-04
 6.265e-05 3.249e-04 6.272e-04 1.728e-03 1.152e-04 2.196e-04 5.167e-06
 4.783e-02 9.265e-04 3.295e-04 1.359e-04 1.816e-03 2.109e-02 1.313e-05
 2.951e-03 1.809e-04 1.838e-01 6.686e-03 3.765e-03 5.103e-06 4.626e-04
 1.041e-02 8.132e-03 1.438e-04 2.226e-04 8.293e-06 3.926e-02 1.032e-01
 2.999e-03 6.976e-05 6.620e-04 6.889e-03 4.357e-05 1.872e-03 5.182e-03
 8.192e-02 3.180e-02 9.327e-03 9.672e-05 2.630e-02 8.374e-02 3.580e-03
 3.934e-04 2.680e-03 1.290e-04 1.197e+00 1.177e-01 6.063e-03 3.655e-04
 3.638e-01 4.235e-02 1.069e-04 2.320e-03 1.154e-03 2.309e-01 3.048e-01
 2.307e-01 2.481e-05 1.252e-03 3.676e-03 2.404e-03 6.176e-04 1.940e-03
 1.402e-04 9.560e-02 1.243e-02 1.006e-03 6.257e-05 3.628e-04 1.306e-02
 2.141e-03 9.844e-02 5.026e-03 1.420e+00 4.495e-02 6.213e-02 1.951e-01
 3.903e-01 7.130e-02 7.717e-02 2.293e+00 1.369e+00 2.348e-02 1.081e-01
 1.265e-02 2.948e-02 2.298e+00 4.693e-02 3.920e-02 5.530e-05 2.007e-02
 1.812e-04 1.465e+00 1.389e+00 8.413e-03 9.675e-02 7.036e-06 4.664e-01
 1.258e+00 2.668e-04 5.208e-04 1.540e-04 4.222e-01 1.501e-01 2.359e-03
 9.925e-05 5.477e-03 1.763e-02 4.130e-05 2.918e-03 5.221e-04 5.266e-02
 1.541e-02 7.398e-02 1.161e-01 5.049e-05 2.993e-03 1.083e+00 9.888e-04
 1.590e-03 5.671e-06 8.821e-02 6.026e-03 4.070e-02 1.036e-04 3.529e-03
 6.676e-01 1.116e-05 8.014e-03 2.417e-04 2.634e-02 9.039e-03 1.853e-02
 3.541e-01 3.531e-06 4.149e-04 8.781e-04 7.106e-05 9.766e-05 4.850e-06
 1.396e-02 3.819e-03 3.517e-01 6.570e-05 2.377e-03 7.384e-01 2.158e-06
 2.089e+00 4.021e-03 1.584e+00 1.302e+00 5.048e-02 6.180e-01 7.597e-06
 1.046e+00 4.275e-02 1.115e+00 1.065e-03 1.822e-03 9.942e-03 9.218e-01
 4.085e-01 1.231e-04 6.948e-02 1.158e-01 5.299e-06 3.417e-02 2.756e-03
 1.874e-01 7.702e-02 1.368e-01 4.949e-01 1.654e-05 1.047e-02 6.181e-01
 5.688e-02 6.036e-01 4.683e-04 1.600e+00 1.528e-01 2.543e-01 7.154e-05
 7.677e-03 5.566e-02 2.607e-03 1.038e+00 4.154e+00 1.010e-01 3.079e-02
 1.775e-01 1.066e-01 1.074e-02 8.565e-01 1.890e-01 4.814e-02 3.219e-02
 5.193e-02 5.505e-02 6.875e-01 7.197e-03 9.678e-02 7.103e-01 1.196e+00
 1.575e-05 8.455e-04 2.646e-04 1.561e-02 1.976e-02 8.835e-04 1.269e-03
 6.195e-06 1.527e-03 7.370e-03 1.331e-03 2.790e-04 5.457e-04 1.343e-04
 9.358e-03 7.511e-04 3.093e-05 1.451e-03 2.963e-03 4.840e-05 2.133e-03
 1.841e-03 9.057e-02 6.100e-03 1.018e-02 5.723e-03 2.022e-05 4.050e-03
 3.228e-01 3.389e-02 4.398e-04 3.891e-04 2.329e-04 3.883e-02 6.291e-03
 1.065e-04 1.387e-03 8.352e-02 1.135e-04 1.083e-02 9.255e-04 1.032e+00
 1.073e-01 5.582e-03 9.561e-01 9.057e-05 5.544e-04 1.838e-03 1.921e-03
 4.935e-04 2.083e-03 1.247e-04 5.537e-01 2.086e-03 1.367e-04 3.182e-03
 2.099e-02 4.763e-06 4.616e-02 5.031e-03 2.782e-01 5.060e-02 5.453e-02
 6.537e-02 1.118e-05 1.003e-01 5.469e-02 1.076e-01 1.106e+00 2.160e+00
 7.947e-03 6.630e-03 7.667e-02 5.026e-02 2.404e-02 1.253e+00 1.062e-04
 3.063e-03 9.014e-03 9.761e-02 7.710e-01 1.169e-02 9.148e-03 3.825e-04
 4.352e-01 3.032e-01 8.618e-02 4.564e-04 4.123e-03 1.220e-04 6.986e-01
 1.072e+00 5.859e-03 4.035e-03 6.311e-02 2.760e-06 3.828e-04 1.046e-04
 1.754e-02 1.021e-02 2.253e-02 7.244e-02 4.528e-06 6.324e-04 5.653e-04
 6.708e-04 4.629e-05 4.618e-04 1.106e-05 5.511e-02 8.487e-04 1.869e-03
 3.604e-04 1.657e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.92796875, 0.8568818235322516)
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
total norm for a microbatch 17.484786439508838 clip 12.99431658025008
total norm for a microbatch 17.12531845647121 clip 13.529700216216884
total norm for a microbatch 11.354223870923171 clip 12.818648245629625
total norm for a microbatch 15.434590080101016 clip 13.423080127185813
total norm for a microbatch 21.954778583215 clip 13.797493574704607
total norm for a microbatch 17.907467840650696 clip 15.039994888944575
total norm for a microbatch 22.89280290954648 clip 15.202934840926662
total norm for a microbatch 25.755369460348746 clip 16.708028239473112
total norm for a microbatch 20.49291607717357 clip 15.697493204224227
total norm for a microbatch 30.181617393750706 clip 16.36787298682538
total norm for a microbatch 13.319244269649342 clip 16.100073726799657
total norm for a microbatch 24.549594576001947 clip 16.28019007592897
cuda
Objective function 16.51 = squared loss an data 14.05 + 0.5*rho*h**2 1.618649 + alpha*h 0.000000 + L2reg 0.60 + L1reg 0.24 ; SHD = 49 ; DAG False
Proportion of microbatches that were clipped  0.7984978194152802
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.7992492223176058
iteration 1 in outer loop, alpha = 1.7992492223176058, rho = 1.0, h = 1.7992492223176058
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 19.75 = squared loss an data 14.05 + 0.5*rho*h**2 1.618649 + alpha*h 3.237298 + L2reg 0.60 + L1reg 0.24 ; SHD = 49 ; DAG False
total norm for a microbatch 11.230797071304618 clip 1.2048967501798744
total norm for a microbatch 26.594202655101178 clip 5.734361006279214
total norm for a microbatch 19.750192415011934 clip 12.067532816689715
total norm for a microbatch 25.092482349422955 clip 19.8953977590271
total norm for a microbatch 18.758888449403447 clip 18.64026751557073
total norm for a microbatch 19.15932300638644 clip 20.954149776090578
total norm for a microbatch 21.318691609972095 clip 20.02452899619657
total norm for a microbatch 15.03167022729265 clip 19.091237816404412
total norm for a microbatch 17.224391511959098 clip 20.358322904895672
total norm for a microbatch 39.04112683805553 clip 19.274464185234063
total norm for a microbatch 19.183464272108885 clip 19.845050280887662
total norm for a microbatch 23.77138249494128 clip 21.77045533847111
cuda
Objective function 17.42 = squared loss an data 13.40 + 0.5*rho*h**2 0.690601 + alpha*h 2.114560 + L2reg 0.99 + L1reg 0.22 ; SHD = 56 ; DAG True
Proportion of microbatches that were clipped  0.8010258080273549
iteration 1 in inner loop, alpha 1.7992492223176058 rho 1.0 h 1.1752459067434202
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 23.63 = squared loss an data 13.40 + 0.5*rho*h**2 6.906015 + alpha*h 2.114560 + L2reg 0.99 + L1reg 0.22 ; SHD = 56 ; DAG True
total norm for a microbatch 35.57910298477336 clip 6.535292006935008
total norm for a microbatch 27.998473518624834 clip 24.0786776753973
total norm for a microbatch 39.37375525093309 clip 24.016599504160695
total norm for a microbatch 25.739279092059366 clip 24.028095347602225
total norm for a microbatch 15.5580475167076 clip 23.87367629917782
total norm for a microbatch 26.346532066437973 clip 23.790864261605712
total norm for a microbatch 36.23814585306454 clip 24.192537159346493
total norm for a microbatch 37.336685912801876 clip 25.547778689462945
total norm for a microbatch 38.79121209370726 clip 25.06872303994216
total norm for a microbatch 31.06884764534561 clip 24.414070481781042
total norm for a microbatch 30.449214536603957 clip 24.349973635924623
cuda
Objective function 18.50 = squared loss an data 15.15 + 0.5*rho*h**2 1.134386 + alpha*h 0.857012 + L2reg 1.17 + L1reg 0.19 ; SHD = 62 ; DAG True
Proportion of microbatches that were clipped  0.8052229811169144
iteration 2 in inner loop, alpha 1.7992492223176058 rho 10.0 h 0.4763162177891047
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 28.71 = squared loss an data 15.15 + 0.5*rho*h**2 11.343857 + alpha*h 0.857012 + L2reg 1.17 + L1reg 0.19 ; SHD = 62 ; DAG True
total norm for a microbatch 43.798357011343896 clip 2.948653418152897
total norm for a microbatch 27.729200082882215 clip 26.497696371261625
total norm for a microbatch 40.68294712864591 clip 26.498390847584215
total norm for a microbatch 30.56974516090843 clip 25.254988239360355
total norm for a microbatch 19.43129153003051 clip 25.119389157400928
total norm for a microbatch 45.26145237957094 clip 25.76778776431332
total norm for a microbatch 27.24986165505751 clip 27.518361080381336
total norm for a microbatch 44.752348394841626 clip 28.639862196701486
cuda
Objective function 20.01 = squared loss an data 16.98 + 0.5*rho*h**2 1.350151 + alpha*h 0.295663 + L2reg 1.21 + L1reg 0.17 ; SHD = 60 ; DAG True
Proportion of microbatches that were clipped  0.8071741600389547
iteration 3 in inner loop, alpha 1.7992492223176058 rho 100.0 h 0.16432595903277303
iteration 2 in outer loop, alpha = 18.23184512559491, rho = 100.0, h = 0.16432595903277303
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 22.71 = squared loss an data 16.98 + 0.5*rho*h**2 1.350151 + alpha*h 2.995965 + L2reg 1.21 + L1reg 0.17 ; SHD = 60 ; DAG True
total norm for a microbatch 19.481106649237045 clip 1.8892737511837898
total norm for a microbatch 49.20437871100069 clip 4.232688379914981
total norm for a microbatch 29.912796202178832 clip 12.870221525249661
total norm for a microbatch 44.30158034909476 clip 22.210721372664192
total norm for a microbatch 22.618242781973446 clip 28.994534566903535
total norm for a microbatch 40.535448483506634 clip 27.950118201675934
total norm for a microbatch 28.084107419976707 clip 27.89179459698222
total norm for a microbatch 32.094052209833414 clip 29.0998878756496
total norm for a microbatch 36.119214676538775 clip 29.68551216549389
total norm for a microbatch 19.45334730628484 clip 29.60017557111651
total norm for a microbatch 27.08275840703917 clip 26.99787216037004
total norm for a microbatch 40.43592170179322 clip 28.88758580412602
total norm for a microbatch 52.708746041002485 clip 28.758548700486422
total norm for a microbatch 32.52259157504777 clip 29.338257544671745
total norm for a microbatch 26.551189764471935 clip 28.063008589822
cuda
Objective function 21.28 = squared loss an data 17.44 + 0.5*rho*h**2 0.522605 + alpha*h 1.863942 + L2reg 1.29 + L1reg 0.16 ; SHD = 64 ; DAG True
Proportion of microbatches that were clipped  0.8075740411069753
iteration 1 in inner loop, alpha 18.23184512559491 rho 100.0 h 0.10223549973605373
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 25.98 = squared loss an data 17.44 + 0.5*rho*h**2 5.226049 + alpha*h 1.863942 + L2reg 1.29 + L1reg 0.16 ; SHD = 64 ; DAG True
total norm for a microbatch 23.11998730946367 clip 13.748719700092343
total norm for a microbatch 44.13836382708289 clip 29.240812789470294
total norm for a microbatch 47.12538053122265 clip 30.76429411595975
total norm for a microbatch 40.22936039078387 clip 30.793300864695546
total norm for a microbatch 42.33347806824336 clip 30.64118455854792
total norm for a microbatch 47.837840308192916 clip 30.218847432967618
cuda
Objective function 21.58 = squared loss an data 18.36 + 0.5*rho*h**2 0.944409 + alpha*h 0.792366 + L2reg 1.34 + L1reg 0.15 ; SHD = 56 ; DAG True
Proportion of microbatches that were clipped  0.8101487314085739
iteration 2 in inner loop, alpha 18.23184512559491 rho 1000.0 h 0.043460533047571914
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 30.08 = squared loss an data 18.36 + 0.5*rho*h**2 9.444090 + alpha*h 0.792366 + L2reg 1.34 + L1reg 0.15 ; SHD = 56 ; DAG True
total norm for a microbatch 129.60394072397585 clip 1.1023523130427815
total norm for a microbatch 40.267606388227634 clip 1.724667458972577
total norm for a microbatch 36.48616707050798 clip 5.069901930891112
total norm for a microbatch 40.34343672442906 clip 24.459772588059636
total norm for a microbatch 47.01437639554323 clip 35.78051244912642
total norm for a microbatch 42.190886069135956 clip 34.8390200022521
total norm for a microbatch 46.97757749122703 clip 34.5543948451131
total norm for a microbatch 44.00862815663065 clip 32.74124683650228
total norm for a microbatch 40.271587493840485 clip 34.25098946356673
total norm for a microbatch 54.068889869778495 clip 30.50002031705127
total norm for a microbatch 46.47617516262655 clip 32.20206839420448
total norm for a microbatch 39.39446126548409 clip 31.295542899608403
cuda
Objective function 21.78 = squared loss an data 18.94 + 0.5*rho*h**2 1.072970 + alpha*h 0.267079 + L2reg 1.35 + L1reg 0.15 ; SHD = 55 ; DAG True
Proportion of microbatches that were clipped  0.810192200334955
iteration 3 in inner loop, alpha 18.23184512559491 rho 10000.0 h 0.014649027906347811
iteration 3 in outer loop, alpha = 164.722124189073, rho = 10000.0, h = 0.014649027906347811
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 23.92 = squared loss an data 18.94 + 0.5*rho*h**2 1.072970 + alpha*h 2.413019 + L2reg 1.35 + L1reg 0.15 ; SHD = 55 ; DAG True
total norm for a microbatch 48.27252610059737 clip 25.04474568717045
total norm for a microbatch 45.91242246236695 clip 43.94467538558351
total norm for a microbatch 59.87074557593855 clip 43.73777884065457
total norm for a microbatch 51.16348926682865 clip 39.222676270254986
total norm for a microbatch 30.016685851267027 clip 36.236404006922164
total norm for a microbatch 30.042061962707336 clip 35.70730317504092
total norm for a microbatch 42.83278394986241 clip 34.06528719025319
total norm for a microbatch 37.62935347590206 clip 32.62831710241798
total norm for a microbatch 42.58725349662212 clip 32.62831710241798
total norm for a microbatch 36.49929741446987 clip 32.06931394046198
total norm for a microbatch 52.165084513551065 clip 37.071361931105535
total norm for a microbatch 50.93204484108815 clip 34.54789729239357
cuda
Objective function 22.53 = squared loss an data 19.01 + 0.5*rho*h**2 0.462641 + alpha*h 1.584488 + L2reg 1.31 + L1reg 0.16 ; SHD = 62 ; DAG True
Proportion of microbatches that were clipped  0.8101114649681529
iteration 1 in inner loop, alpha 164.722124189073 rho 10000.0 h 0.00961915616843001
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 26.69 = squared loss an data 19.01 + 0.5*rho*h**2 4.626408 + alpha*h 1.584488 + L2reg 1.31 + L1reg 0.16 ; SHD = 62 ; DAG True
total norm for a microbatch 58.660744262534 clip 7.1683676437658574
total norm for a microbatch 88.24017550762521 clip 73.62502321720677
total norm for a microbatch 89.09981750985204 clip 75.36753189016872
total norm for a microbatch 53.73209582967834 clip 46.23532274830055
total norm for a microbatch 51.06539373548459 clip 35.50219159984072
total norm for a microbatch 69.34719329417808 clip 34.19725856466894
total norm for a microbatch 34.65243284916796 clip 38.98240468170555
total norm for a microbatch 49.7585110538756 clip 35.91793667101865
total norm for a microbatch 39.91421680605388 clip 37.48758464144007
total norm for a microbatch 43.5684235449546 clip 37.74584256833508
total norm for a microbatch 44.37689189063793 clip 39.04726423671597
cuda
Objective function 21.87 = squared loss an data 19.02 + 0.5*rho*h**2 0.737571 + alpha*h 0.632658 + L2reg 1.32 + L1reg 0.16 ; SHD = 62 ; DAG True
Proportion of microbatches that were clipped  0.8114800225098481
iteration 2 in inner loop, alpha 164.722124189073 rho 100000.0 h 0.0038407587702522505
iteration 4 in outer loop, alpha = 4005.4808944413235, rho = 1000000.0, h = 0.0038407587702522505
Threshold 0.3
[[0.003 0.136 0.183 0.417 0.471 0.486 0.22  0.021 0.212 0.26  0.064 0.123
  0.421 0.023 0.261 0.545 0.203 0.703 0.348 0.432]
 [0.023 0.003 0.209 0.614 0.439 0.509 0.266 0.028 0.227 0.392 0.179 0.262
  0.13  0.014 0.29  0.416 0.132 0.384 0.1   0.591]
 [0.019 0.017 0.003 0.11  0.332 0.091 0.163 0.014 0.032 0.229 0.058 0.027
  0.09  0.001 0.101 0.103 0.014 0.115 0.076 0.117]
 [0.007 0.008 0.029 0.004 0.029 0.021 0.058 0.003 0.007 0.043 0.016 0.006
  0.01  0.008 0.048 0.026 0.009 0.06  0.014 0.043]
 [0.008 0.009 0.016 0.133 0.003 0.031 0.026 0.005 0.008 0.028 0.008 0.006
  0.029 0.004 0.074 0.038 0.008 0.024 0.013 0.042]
 [0.01  0.012 0.046 0.271 0.194 0.004 0.187 0.008 0.047 0.236 0.031 0.073
  0.039 0.006 0.475 0.109 0.051 0.106 0.006 0.161]
 [0.01  0.01  0.022 0.09  0.138 0.018 0.004 0.007 0.015 0.088 0.025 0.01
  0.012 0.006 0.106 0.054 0.006 0.116 0.016 0.096]
 [0.177 0.203 0.157 1.038 0.689 0.473 0.479 0.004 0.165 0.532 0.246 0.915
  0.695 0.024 0.394 0.341 0.501 1.224 0.244 0.693]
 [0.022 0.019 0.1   0.536 0.555 0.09  0.267 0.015 0.004 0.313 0.008 0.012
  0.028 0.004 0.243 0.095 0.02  0.417 0.014 0.123]
 [0.014 0.005 0.024 0.121 0.116 0.019 0.039 0.005 0.014 0.005 0.004 0.01
  0.012 0.005 0.089 0.012 0.008 0.036 0.007 0.033]
 [0.044 0.033 0.096 0.287 0.414 0.182 0.187 0.025 0.329 0.837 0.003 0.037
  0.123 0.011 0.25  0.181 0.027 0.377 0.05  0.25 ]
 [0.025 0.02  0.167 0.614 0.936 0.069 0.397 0.004 0.339 0.536 0.136 0.004
  0.259 0.019 0.246 0.455 0.134 0.363 0.054 0.375]
 [0.012 0.028 0.039 0.324 0.139 0.089 0.418 0.007 0.072 0.342 0.053 0.011
  0.003 0.008 0.683 0.187 0.098 0.207 0.045 0.135]
 [0.217 0.376 2.19  0.459 0.567 0.522 0.599 0.152 0.749 0.622 0.41  0.229
  0.465 0.002 0.518 0.516 0.334 0.718 0.659 0.739]
 [0.01  0.009 0.04  0.139 0.064 0.01  0.048 0.007 0.025 0.04  0.019 0.012
  0.005 0.007 0.003 0.048 0.011 0.111 0.008 0.021]
 [0.005 0.012 0.04  0.198 0.118 0.061 0.035 0.01  0.053 0.33  0.017 0.006
  0.014 0.007 0.131 0.001 0.018 0.077 0.008 0.071]
 [0.017 0.026 0.281 0.434 0.342 0.083 0.662 0.01  0.202 0.352 0.217 0.043
  0.067 0.012 0.244 0.287 0.004 0.292 0.099 0.168]
 [0.005 0.007 0.023 0.109 0.246 0.03  0.03  0.003 0.01  0.113 0.009 0.017
  0.021 0.004 0.048 0.068 0.013 0.002 0.02  0.022]
 [0.007 0.052 0.047 0.278 0.239 0.511 0.263 0.017 0.323 0.375 0.089 0.052
  0.09  0.007 0.397 0.636 0.043 0.245 0.005 0.158]
 [0.011 0.005 0.037 0.114 0.099 0.025 0.056 0.005 0.039 0.157 0.019 0.015
  0.027 0.004 0.144 0.07  0.023 0.229 0.009 0.003]]
[[0.    0.    0.    0.417 0.471 0.486 0.    0.    0.    0.    0.    0.
  0.421 0.    0.    0.545 0.    0.703 0.348 0.432]
 [0.    0.    0.    0.614 0.439 0.509 0.    0.    0.    0.392 0.    0.
  0.    0.    0.    0.416 0.    0.384 0.    0.591]
 [0.    0.    0.    0.    0.332 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.475 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.038 0.689 0.473 0.479 0.    0.    0.532 0.    0.915
  0.695 0.    0.394 0.341 0.501 1.224 0.    0.693]
 [0.    0.    0.    0.536 0.555 0.    0.    0.    0.    0.313 0.    0.
  0.    0.    0.    0.    0.    0.417 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.414 0.    0.    0.    0.329 0.837 0.    0.
  0.    0.    0.    0.    0.    0.377 0.    0.   ]
 [0.    0.    0.    0.614 0.936 0.    0.397 0.    0.339 0.536 0.    0.
  0.    0.    0.    0.455 0.    0.363 0.    0.375]
 [0.    0.    0.    0.324 0.    0.    0.418 0.    0.    0.342 0.    0.
  0.    0.    0.683 0.    0.    0.    0.    0.   ]
 [0.    0.376 2.19  0.459 0.567 0.522 0.599 0.    0.749 0.622 0.41  0.
  0.465 0.    0.518 0.516 0.334 0.718 0.659 0.739]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.33  0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.434 0.342 0.    0.662 0.    0.    0.352 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.511 0.    0.    0.323 0.375 0.    0.
  0.    0.    0.397 0.636 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5466666666666666, 'tpr': 0.5666666666666667, 'fpr': 0.3153846153846154, 'f1': 0.5037037037037037, 'shd': 62, 'npred': 75, 'ntrue': 60}
[1.359e-01 1.833e-01 4.171e-01 4.714e-01 4.861e-01 2.197e-01 2.073e-02
 2.118e-01 2.597e-01 6.380e-02 1.228e-01 4.212e-01 2.331e-02 2.614e-01
 5.447e-01 2.034e-01 7.027e-01 3.480e-01 4.316e-01 2.282e-02 2.086e-01
 6.138e-01 4.387e-01 5.091e-01 2.657e-01 2.785e-02 2.268e-01 3.923e-01
 1.787e-01 2.623e-01 1.299e-01 1.359e-02 2.898e-01 4.158e-01 1.323e-01
 3.839e-01 9.964e-02 5.907e-01 1.863e-02 1.711e-02 1.102e-01 3.323e-01
 9.072e-02 1.632e-01 1.355e-02 3.190e-02 2.286e-01 5.772e-02 2.657e-02
 8.955e-02 1.499e-03 1.006e-01 1.033e-01 1.442e-02 1.146e-01 7.648e-02
 1.165e-01 6.988e-03 7.762e-03 2.855e-02 2.944e-02 2.148e-02 5.757e-02
 2.562e-03 7.380e-03 4.284e-02 1.622e-02 6.430e-03 9.987e-03 8.342e-03
 4.770e-02 2.593e-02 8.888e-03 6.013e-02 1.425e-02 4.271e-02 7.648e-03
 9.131e-03 1.626e-02 1.329e-01 3.123e-02 2.550e-02 4.575e-03 7.922e-03
 2.797e-02 8.112e-03 5.556e-03 2.916e-02 3.502e-03 7.416e-02 3.841e-02
 8.112e-03 2.439e-02 1.349e-02 4.207e-02 1.034e-02 1.240e-02 4.609e-02
 2.710e-01 1.945e-01 1.868e-01 7.827e-03 4.706e-02 2.356e-01 3.145e-02
 7.261e-02 3.866e-02 6.498e-03 4.748e-01 1.087e-01 5.106e-02 1.060e-01
 6.233e-03 1.614e-01 9.609e-03 1.047e-02 2.173e-02 8.966e-02 1.377e-01
 1.783e-02 7.177e-03 1.461e-02 8.812e-02 2.505e-02 9.601e-03 1.225e-02
 5.891e-03 1.055e-01 5.422e-02 5.587e-03 1.155e-01 1.605e-02 9.561e-02
 1.769e-01 2.032e-01 1.569e-01 1.038e+00 6.892e-01 4.729e-01 4.793e-01
 1.654e-01 5.318e-01 2.456e-01 9.148e-01 6.948e-01 2.423e-02 3.939e-01
 3.409e-01 5.011e-01 1.224e+00 2.444e-01 6.925e-01 2.228e-02 1.914e-02
 9.950e-02 5.357e-01 5.551e-01 8.990e-02 2.675e-01 1.462e-02 3.131e-01
 8.476e-03 1.196e-02 2.811e-02 3.590e-03 2.430e-01 9.549e-02 1.960e-02
 4.174e-01 1.446e-02 1.234e-01 1.373e-02 5.270e-03 2.406e-02 1.208e-01
 1.162e-01 1.870e-02 3.928e-02 5.478e-03 1.352e-02 3.971e-03 9.908e-03
 1.180e-02 4.948e-03 8.873e-02 1.159e-02 8.230e-03 3.592e-02 7.134e-03
 3.267e-02 4.440e-02 3.341e-02 9.632e-02 2.868e-01 4.136e-01 1.818e-01
 1.871e-01 2.499e-02 3.294e-01 8.368e-01 3.713e-02 1.232e-01 1.136e-02
 2.496e-01 1.812e-01 2.677e-02 3.769e-01 4.970e-02 2.503e-01 2.528e-02
 1.998e-02 1.668e-01 6.142e-01 9.358e-01 6.889e-02 3.965e-01 4.260e-03
 3.389e-01 5.356e-01 1.359e-01 2.589e-01 1.916e-02 2.464e-01 4.554e-01
 1.338e-01 3.632e-01 5.374e-02 3.755e-01 1.152e-02 2.764e-02 3.935e-02
 3.242e-01 1.391e-01 8.890e-02 4.182e-01 6.849e-03 7.179e-02 3.417e-01
 5.264e-02 1.069e-02 8.178e-03 6.828e-01 1.872e-01 9.843e-02 2.065e-01
 4.517e-02 1.346e-01 2.169e-01 3.765e-01 2.190e+00 4.585e-01 5.671e-01
 5.220e-01 5.987e-01 1.518e-01 7.494e-01 6.220e-01 4.102e-01 2.286e-01
 4.650e-01 5.185e-01 5.156e-01 3.342e-01 7.182e-01 6.586e-01 7.390e-01
 1.017e-02 9.272e-03 4.042e-02 1.388e-01 6.437e-02 9.627e-03 4.847e-02
 6.624e-03 2.460e-02 4.033e-02 1.917e-02 1.183e-02 5.166e-03 6.556e-03
 4.817e-02 1.068e-02 1.114e-01 8.244e-03 2.059e-02 4.919e-03 1.152e-02
 3.974e-02 1.984e-01 1.178e-01 6.054e-02 3.483e-02 9.971e-03 5.257e-02
 3.301e-01 1.736e-02 5.794e-03 1.396e-02 7.451e-03 1.314e-01 1.835e-02
 7.719e-02 7.752e-03 7.089e-02 1.662e-02 2.649e-02 2.806e-01 4.339e-01
 3.423e-01 8.335e-02 6.624e-01 1.037e-02 2.021e-01 3.517e-01 2.171e-01
 4.286e-02 6.650e-02 1.228e-02 2.442e-01 2.874e-01 2.924e-01 9.918e-02
 1.685e-01 5.081e-03 7.297e-03 2.274e-02 1.094e-01 2.460e-01 3.030e-02
 3.004e-02 3.159e-03 1.036e-02 1.127e-01 9.112e-03 1.682e-02 2.058e-02
 4.354e-03 4.821e-02 6.793e-02 1.284e-02 2.017e-02 2.166e-02 7.101e-03
 5.150e-02 4.690e-02 2.780e-01 2.393e-01 5.106e-01 2.634e-01 1.652e-02
 3.228e-01 3.751e-01 8.939e-02 5.200e-02 9.025e-02 6.666e-03 3.971e-01
 6.363e-01 4.290e-02 2.446e-01 1.575e-01 1.115e-02 4.775e-03 3.731e-02
 1.142e-01 9.855e-02 2.501e-02 5.587e-02 5.231e-03 3.927e-02 1.572e-01
 1.931e-02 1.534e-02 2.736e-02 4.219e-03 1.439e-01 7.005e-02 2.293e-02
 2.292e-01 9.181e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7678125, 0.49177762201273245)
Iterations 2250
Achieves (13.034436291409676, 1e-05)-DP
