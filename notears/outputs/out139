samples  5000  graph  30 90 ER mlp  minibatch size  100  noise  0.8  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 2.115034010050131
iteration 1 in outer loop, alpha = 2.115034010050131, rho = 1.0, h = 2.115034010050131
cuda
iteration 1 in inner loop,alpha 2.115034010050131 rho 1.0 h 1.3374026753201562
iteration 2 in inner loop,alpha 2.115034010050131 rho 10.0 h 0.5764720350935555
iteration 3 in inner loop,alpha 2.115034010050131 rho 100.0 h 0.17913910773663844
iteration 2 in outer loop, alpha = 20.028944783713975, rho = 100.0, h = 0.17913910773663844
cuda
iteration 1 in inner loop,alpha 20.028944783713975 rho 100.0 h 0.09446373324962565
iteration 2 in inner loop,alpha 20.028944783713975 rho 1000.0 h 0.034174148204588306
iteration 3 in outer loop, alpha = 54.20309298830228, rho = 1000.0, h = 0.034174148204588306
cuda
iteration 1 in inner loop,alpha 54.20309298830228 rho 1000.0 h 0.017532235887088632
iteration 2 in inner loop,alpha 54.20309298830228 rho 10000.0 h 0.005864051479548493
iteration 4 in outer loop, alpha = 112.8436077837872, rho = 10000.0, h = 0.005864051479548493
cuda
iteration 1 in inner loop,alpha 112.8436077837872 rho 10000.0 h 0.0027945561423159404
iteration 2 in inner loop,alpha 112.8436077837872 rho 100000.0 h 0.0009127079402979632
iteration 5 in outer loop, alpha = 204.11440181358353, rho = 100000.0, h = 0.0009127079402979632
cuda
iteration 1 in inner loop,alpha 204.11440181358353 rho 100000.0 h 0.0004413627823360855
iteration 6 in outer loop, alpha = 645.477184149669, rho = 1000000.0, h = 0.0004413627823360855
Threshold 0.3
[[0.006 0.001 0.001 1.931 0.    0.001 0.001 0.001 0.001 0.    0.    0.002
  0.    0.    0.001 0.188 0.    0.    0.964 0.292 0.    0.    1.641 0.
  0.    0.    0.066 0.    0.108 0.   ]
 [1.023 0.003 0.362 0.506 0.    0.135 0.002 0.415 0.001 0.    0.    1.05
  0.    0.    0.    1.594 0.    0.001 0.955 0.368 0.001 0.001 0.078 0.001
  0.    0.001 0.447 0.    0.199 0.001]
 [0.35  0.001 0.003 0.326 0.001 0.112 0.    0.225 0.    0.    0.    0.284
  0.    0.    0.    0.224 0.    0.001 0.161 0.226 0.    0.    0.123 0.
  0.    0.    0.192 0.    0.202 0.   ]
 [0.    0.    0.    0.007 0.    0.    0.002 0.    0.    0.    0.    0.001
  0.    0.    0.    1.711 0.    0.    0.874 1.162 0.    0.    0.235 0.
  0.    0.    0.034 0.    0.149 0.   ]
 [0.198 0.16  0.23  0.197 0.002 0.126 0.009 0.141 0.001 0.    0.    0.258
  0.    0.    0.001 0.249 0.    0.    0.116 0.259 0.    0.    0.093 0.
  0.    0.    0.054 0.    0.192 0.   ]
 [0.413 0.    0.001 0.274 0.001 0.002 0.002 0.001 0.    0.    0.    0.001
  0.    0.    0.    0.147 0.    0.001 0.858 1.222 0.    0.    0.08  0.
  0.    0.    0.043 0.    1.181 0.001]
 [0.076 0.201 0.151 0.142 0.057 0.121 0.001 0.073 0.577 0.006 0.031 0.095
  0.    0.005 0.    0.152 0.    0.001 0.661 0.107 0.033 0.062 0.073 0.
  0.    0.002 0.069 0.    0.049 0.001]
 [0.988 0.001 0.004 0.434 0.001 1.964 0.003 0.003 0.    0.    0.    0.002
  0.    0.    0.    0.201 0.002 0.    0.127 1.215 0.    0.001 0.09  0.
  0.    0.    0.027 0.    0.196 0.001]
 [0.145 0.236 0.108 0.164 0.141 0.088 0.001 0.166 0.001 0.005 0.096 0.117
  0.    0.019 0.    0.251 0.    0.001 0.112 1.129 0.119 0.17  0.094 0.011
  0.    0.004 0.507 0.001 0.109 0.001]
 [0.158 0.111 0.172 1.636 0.054 0.322 0.022 2.573 0.016 0.003 0.428 0.574
  0.    3.38  0.017 0.322 0.001 0.463 0.139 0.225 0.278 0.102 0.387 0.112
  0.001 0.457 0.175 0.    0.271 0.285]
 [1.013 0.11  0.119 2.034 0.145 0.179 0.032 0.213 0.004 0.001 0.001 0.243
  0.006 0.085 0.    1.132 0.    0.075 0.252 0.153 2.78  1.123 1.475 0.075
  0.001 0.283 0.117 0.    0.094 0.078]
 [0.202 0.    0.002 0.274 0.001 0.111 0.002 0.296 0.001 0.    0.    0.003
  0.    0.    0.    0.2   0.    0.    0.154 0.335 0.    0.    1.371 0.001
  0.    0.    0.012 0.    0.14  0.001]
 [1.053 0.833 0.071 0.207 0.008 1.753 0.623 0.095 0.173 0.028 0.059 0.24
  0.001 0.    0.    0.105 0.01  0.167 0.591 0.075 0.032 0.025 0.173 0.044
  0.003 3.156 0.114 0.004 1.049 0.464]
 [0.954 2.174 1.866 0.691 0.027 0.278 0.011 1.819 0.002 0.    0.005 1.876
  0.004 0.001 0.    0.305 0.001 0.197 0.339 0.824 0.125 0.011 0.226 0.003
  0.    0.025 0.118 0.    0.262 0.074]
 [0.147 0.068 1.615 0.147 0.087 0.147 0.406 0.079 0.001 0.001 0.004 0.075
  0.007 0.037 0.    1.401 0.016 0.061 0.078 0.794 0.026 0.017 0.124 0.012
  0.005 0.076 0.013 0.027 0.048 0.058]
 [0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.003 0.    0.    0.073 0.273 0.    0.    0.004 0.
  0.    0.    0.001 0.    0.066 0.   ]
 [0.148 0.042 1.408 0.161 0.019 0.084 0.687 0.118 0.044 0.022 0.004 0.151
  0.005 0.    0.    0.102 0.001 0.039 0.083 0.801 0.053 0.022 0.079 0.027
  0.011 0.016 0.449 0.005 0.155 1.967]
 [0.136 0.129 0.159 1.525 1.354 0.093 0.188 0.289 0.152 0.    0.001 0.317
  0.    0.    0.    1.168 0.    0.001 0.141 0.252 0.024 0.003 0.176 0.
  0.    0.    0.076 0.    0.983 0.   ]
 [0.002 0.    0.    0.005 0.001 0.    0.001 0.    0.    0.    0.    0.001
  0.    0.    0.002 0.023 0.    0.    0.008 0.011 0.    0.    0.009 0.
  0.    0.    0.021 0.    0.006 0.   ]
 [0.    0.    0.    0.001 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.036 0.003 0.    0.    0.005 0.
  0.    0.    0.002 0.    1.183 0.   ]
 [0.916 1.236 1.828 0.31  2.114 0.091 0.005 0.119 0.001 0.    0.    1.283
  0.001 0.001 0.    0.628 0.002 0.01  0.136 1.018 0.002 0.    0.161 0.001
  0.    0.005 0.153 0.    0.129 0.004]
 [0.769 0.241 0.192 0.302 0.13  0.03  0.01  0.13  0.001 0.001 0.    0.261
  0.004 0.03  0.002 0.318 0.    0.048 0.116 0.139 0.461 0.001 0.079 0.037
  0.    0.112 0.067 0.    0.099 0.023]
 [0.    0.001 0.    0.002 0.001 0.001 0.001 0.    0.001 0.    0.    0.001
  0.    0.    0.    0.149 0.    0.    0.119 0.112 0.    0.    0.004 0.
  0.    0.    0.003 0.    0.244 0.   ]
 [1.185 0.081 0.084 0.242 0.11  0.038 0.07  0.287 0.007 0.002 0.001 0.119
  0.    0.04  0.    0.09  0.006 0.192 0.101 0.112 0.081 0.002 0.088 0.001
  0.001 0.001 0.011 0.    0.171 0.093]
 [0.051 0.254 0.031 0.282 0.106 0.163 0.075 0.441 0.    4.098 1.033 0.223
  0.013 0.336 0.    0.037 0.014 0.176 0.081 0.085 0.117 0.021 0.198 0.
  0.001 1.659 0.019 0.    0.727 0.158]
 [0.474 0.37  0.202 0.231 0.092 1.955 0.085 0.139 0.048 0.001 0.    0.066
  0.    0.013 0.    0.15  0.    0.388 0.204 0.202 0.099 0.003 0.052 0.246
  0.    0.001 0.596 0.014 1.219 1.895]
 [0.008 0.005 0.001 0.024 0.008 0.012 0.002 0.003 0.    0.    0.    0.076
  0.    0.    0.001 0.257 0.    0.005 0.268 0.221 0.001 0.001 1.571 0.002
  0.    0.    0.005 0.    1.157 0.001]
 [0.22  0.022 0.01  0.156 2.193 0.187 0.035 1.652 0.023 0.    0.012 2.016
  0.002 0.038 0.001 0.14  0.022 1.024 0.126 0.325 0.006 0.038 0.099 1.488
  0.006 0.    0.041 0.001 0.113 0.076]
 [0.    0.001 0.001 0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.    0.    0.    0.001 0.    0.    0.092 0.    0.    0.    0.002 0.
  0.    0.    0.002 0.    0.003 0.   ]
 [0.213 0.075 1.809 0.135 0.107 0.098 0.059 0.085 0.053 0.    0.    0.12
  0.    0.001 0.    0.173 0.    1.106 0.132 0.188 0.032 0.002 0.095 0.003
  0.    0.    0.718 0.    0.37  0.001]]
[[0.    0.    0.    1.931 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.964 0.    0.    0.    1.641 0.
  0.    0.    0.    0.    0.    0.   ]
 [1.023 0.    0.362 0.506 0.    0.    0.    0.415 0.    0.    0.    1.05
  0.    0.    0.    1.594 0.    0.    0.955 0.368 0.    0.    0.    0.
  0.    0.    0.447 0.    0.    0.   ]
 [0.35  0.    0.    0.326 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.711 0.    0.    0.874 1.162 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.413 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.858 1.222 0.    0.    0.    0.
  0.    0.    0.    0.    1.181 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.577 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.661 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.988 0.    0.    0.434 0.    1.964 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    1.215 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    1.129 0.    0.    0.    0.
  0.    0.    0.507 0.    0.    0.   ]
 [0.    0.    0.    1.636 0.    0.322 0.    2.573 0.    0.    0.428 0.574
  0.    3.38  0.    0.322 0.    0.463 0.    0.    0.    0.    0.387 0.
  0.    0.457 0.    0.    0.    0.   ]
 [1.013 0.    0.    2.034 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.132 0.    0.    0.    0.    2.78  1.123 1.475 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.335 0.    0.    1.371 0.
  0.    0.    0.    0.    0.    0.   ]
 [1.053 0.833 0.    0.    0.    1.753 0.623 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.591 0.    0.    0.    0.    0.
  0.    3.156 0.    0.    1.049 0.464]
 [0.954 2.174 1.866 0.691 0.    0.    0.    1.819 0.    0.    0.    1.876
  0.    0.    0.    0.305 0.    0.    0.339 0.824 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.615 0.    0.    0.    0.406 0.    0.    0.    0.    0.
  0.    0.    0.    1.401 0.    0.    0.    0.794 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.408 0.    0.    0.    0.687 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.801 0.    0.    0.    0.
  0.    0.    0.449 0.    0.    1.967]
 [0.    0.    0.    1.525 1.354 0.    0.    0.    0.    0.    0.    0.317
  0.    0.    0.    1.168 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.983 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    1.183 0.   ]
 [0.916 1.236 1.828 0.31  2.114 0.    0.    0.    0.    0.    0.    1.283
  0.    0.    0.    0.628 0.    0.    0.    1.018 0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.769 0.    0.    0.302 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.318 0.    0.    0.    0.    0.461 0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [1.185 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.441 0.    4.098 1.033 0.
  0.    0.336 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    1.659 0.    0.    0.727 0.   ]
 [0.474 0.37  0.    0.    0.    1.955 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.388 0.    0.    0.    0.    0.    0.
  0.    0.    0.596 0.    1.219 1.895]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.571 0.
  0.    0.    0.    0.    1.157 0.   ]
 [0.    0.    0.    0.    2.193 0.    0.    1.652 0.    0.    0.    2.016
  0.    0.    0.    0.    0.    1.024 0.    0.325 0.    0.    0.    1.488
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.809 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    1.106 0.    0.    0.    0.    0.    0.
  0.    0.    0.718 0.    0.37  0.   ]]
{'fdr': 0.29914529914529914, 'tpr': 0.9111111111111111, 'fpr': 0.10144927536231885, 'f1': 0.7922705314009661, 'shd': 42, 'npred': 117, 'ntrue': 90}
[9.199e-04 8.461e-04 1.931e+00 4.520e-04 6.396e-04 5.237e-04 7.155e-04
 6.066e-04 2.485e-05 4.195e-05 1.560e-03 6.339e-05 2.790e-05 1.334e-03
 1.878e-01 4.732e-05 3.123e-04 9.636e-01 2.919e-01 4.899e-04 1.253e-04
 1.641e+00 5.166e-05 2.007e-05 4.097e-05 6.610e-02 1.972e-05 1.080e-01
 1.058e-04 1.023e+00 3.620e-01 5.058e-01 3.060e-04 1.347e-01 1.524e-03
 4.147e-01 1.044e-03 6.269e-06 2.968e-05 1.050e+00 2.767e-04 6.990e-05
 1.543e-04 1.594e+00 6.411e-05 1.219e-03 9.554e-01 3.678e-01 7.620e-04
 5.063e-04 7.769e-02 7.902e-04 2.193e-06 7.017e-04 4.472e-01 1.090e-04
 1.994e-01 5.587e-04 3.503e-01 1.369e-03 3.263e-01 6.081e-04 1.122e-01
 3.694e-04 2.254e-01 3.148e-04 1.163e-04 1.089e-05 2.838e-01 9.269e-06
 2.076e-04 1.287e-05 2.238e-01 6.890e-05 1.216e-03 1.610e-01 2.263e-01
 3.273e-04 1.373e-04 1.231e-01 3.527e-04 5.299e-05 2.480e-05 1.920e-01
 4.784e-04 2.016e-01 1.050e-04 1.422e-04 1.196e-04 2.084e-04 3.105e-04
 1.643e-04 1.869e-03 8.946e-05 3.631e-04 4.512e-05 1.237e-04 8.693e-04
 7.996e-06 1.738e-05 3.104e-05 1.711e+00 2.256e-05 1.117e-04 8.736e-01
 1.162e+00 1.154e-04 5.994e-05 2.345e-01 4.190e-05 3.336e-06 4.926e-05
 3.359e-02 2.945e-05 1.490e-01 4.230e-05 1.981e-01 1.604e-01 2.297e-01
 1.968e-01 1.263e-01 8.998e-03 1.415e-01 7.742e-04 1.333e-04 1.222e-05
 2.576e-01 9.486e-05 3.615e-04 8.101e-04 2.490e-01 2.592e-05 2.727e-04
 1.157e-01 2.594e-01 4.671e-04 2.271e-04 9.264e-02 2.434e-04 2.039e-05
 1.367e-04 5.373e-02 2.504e-05 1.924e-01 1.542e-04 4.134e-01 4.900e-04
 1.348e-03 2.737e-01 9.718e-04 1.529e-03 6.232e-04 2.003e-04 8.428e-06
 3.244e-05 8.988e-04 1.450e-05 1.941e-04 9.873e-05 1.468e-01 1.115e-04
 1.102e-03 8.575e-01 1.222e+00 4.636e-05 4.423e-04 7.955e-02 1.594e-04
 1.034e-05 8.743e-05 4.323e-02 5.106e-05 1.181e+00 5.433e-04 7.580e-02
 2.007e-01 1.507e-01 1.424e-01 5.741e-02 1.212e-01 7.266e-02 5.771e-01
 6.182e-03 3.096e-02 9.506e-02 4.253e-04 4.805e-03 2.557e-04 1.517e-01
 4.657e-04 5.920e-04 6.612e-01 1.072e-01 3.336e-02 6.217e-02 7.315e-02
 2.472e-04 9.526e-05 1.756e-03 6.898e-02 3.227e-04 4.861e-02 7.020e-04
 9.876e-01 5.259e-04 4.286e-03 4.342e-01 9.651e-04 1.964e+00 2.907e-03
 4.538e-04 7.799e-06 3.709e-05 2.350e-03 2.830e-05 1.158e-04 1.225e-04
 2.013e-01 1.612e-03 4.220e-04 1.266e-01 1.215e+00 5.741e-05 6.762e-04
 9.049e-02 1.414e-04 5.982e-06 2.491e-04 2.726e-02 7.870e-05 1.958e-01
 5.583e-04 1.446e-01 2.357e-01 1.078e-01 1.638e-01 1.406e-01 8.822e-02
 9.476e-04 1.655e-01 5.366e-03 9.567e-02 1.172e-01 3.304e-04 1.870e-02
 1.855e-04 2.507e-01 8.360e-05 1.232e-03 1.122e-01 1.129e+00 1.190e-01
 1.702e-01 9.412e-02 1.126e-02 2.323e-04 4.255e-03 5.074e-01 9.449e-04
 1.093e-01 7.264e-04 1.582e-01 1.115e-01 1.717e-01 1.636e+00 5.371e-02
 3.219e-01 2.195e-02 2.573e+00 1.598e-02 4.277e-01 5.744e-01 3.529e-04
 3.380e+00 1.685e-02 3.222e-01 1.399e-03 4.626e-01 1.392e-01 2.252e-01
 2.779e-01 1.022e-01 3.870e-01 1.119e-01 5.225e-04 4.573e-01 1.749e-01
 2.602e-04 2.711e-01 2.853e-01 1.013e+00 1.104e-01 1.189e-01 2.034e+00
 1.449e-01 1.786e-01 3.235e-02 2.135e-01 4.342e-03 9.692e-04 2.433e-01
 6.141e-03 8.456e-02 2.380e-04 1.132e+00 4.215e-04 7.493e-02 2.518e-01
 1.529e-01 2.780e+00 1.123e+00 1.475e+00 7.476e-02 1.132e-03 2.833e-01
 1.170e-01 3.243e-04 9.423e-02 7.764e-02 2.018e-01 1.667e-04 2.131e-03
 2.735e-01 5.042e-04 1.112e-01 1.967e-03 2.962e-01 5.696e-04 2.808e-05
 3.277e-05 8.371e-05 5.448e-05 4.558e-04 1.996e-01 4.658e-05 2.940e-04
 1.544e-01 3.354e-01 2.405e-04 6.708e-05 1.371e+00 1.320e-03 2.135e-05
 2.588e-04 1.168e-02 1.799e-04 1.399e-01 7.094e-04 1.053e+00 8.328e-01
 7.055e-02 2.074e-01 7.518e-03 1.753e+00 6.226e-01 9.548e-02 1.733e-01
 2.789e-02 5.862e-02 2.404e-01 3.856e-04 4.524e-04 1.054e-01 1.005e-02
 1.669e-01 5.911e-01 7.502e-02 3.184e-02 2.453e-02 1.728e-01 4.449e-02
 2.583e-03 3.156e+00 1.145e-01 4.436e-03 1.049e+00 4.636e-01 9.542e-01
 2.174e+00 1.866e+00 6.911e-01 2.729e-02 2.781e-01 1.065e-02 1.819e+00
 1.703e-03 2.756e-04 5.169e-03 1.876e+00 4.231e-03 5.513e-05 3.052e-01
 5.106e-04 1.967e-01 3.389e-01 8.243e-01 1.249e-01 1.145e-02 2.258e-01
 3.395e-03 7.259e-05 2.543e-02 1.182e-01 4.063e-04 2.621e-01 7.433e-02
 1.465e-01 6.804e-02 1.615e+00 1.469e-01 8.660e-02 1.468e-01 4.065e-01
 7.897e-02 5.084e-04 5.029e-04 3.711e-03 7.455e-02 6.924e-03 3.701e-02
 1.401e+00 1.601e-02 6.077e-02 7.774e-02 7.943e-01 2.626e-02 1.657e-02
 1.240e-01 1.193e-02 5.260e-03 7.631e-02 1.290e-02 2.716e-02 4.830e-02
 5.850e-02 2.402e-05 1.148e-04 9.438e-05 7.775e-04 6.537e-05 5.076e-05
 2.749e-04 6.466e-05 1.070e-04 2.047e-05 1.267e-04 4.016e-04 9.184e-06
 1.047e-05 1.505e-05 1.105e-05 4.207e-05 7.322e-02 2.734e-01 5.982e-05
 5.168e-05 4.079e-03 1.644e-05 2.996e-05 3.651e-05 8.222e-04 8.215e-06
 6.558e-02 3.818e-05 1.485e-01 4.167e-02 1.408e+00 1.609e-01 1.856e-02
 8.435e-02 6.871e-01 1.180e-01 4.358e-02 2.181e-02 4.317e-03 1.507e-01
 5.273e-03 3.372e-04 4.478e-04 1.020e-01 3.887e-02 8.253e-02 8.010e-01
 5.254e-02 2.153e-02 7.918e-02 2.683e-02 1.064e-02 1.591e-02 4.491e-01
 5.433e-03 1.554e-01 1.967e+00 1.359e-01 1.293e-01 1.587e-01 1.525e+00
 1.354e+00 9.250e-02 1.882e-01 2.894e-01 1.521e-01 1.169e-04 1.374e-03
 3.166e-01 5.982e-06 2.299e-04 4.772e-04 1.168e+00 2.274e-05 1.413e-01
 2.518e-01 2.385e-02 3.042e-03 1.759e-01 1.752e-04 2.778e-05 9.254e-05
 7.620e-02 8.325e-05 9.828e-01 1.775e-04 1.773e-03 2.040e-04 4.864e-04
 5.168e-03 1.019e-03 1.416e-04 7.498e-04 3.161e-04 3.564e-04 2.813e-05
 1.139e-04 1.069e-03 5.566e-05 2.465e-05 2.235e-03 2.323e-02 9.656e-05
 2.881e-04 1.067e-02 1.113e-04 1.352e-04 9.022e-03 8.372e-05 1.303e-05
 2.723e-05 2.098e-02 1.913e-05 5.946e-03 1.047e-04 3.883e-05 6.468e-05
 1.386e-04 6.281e-04 8.659e-04 1.344e-04 5.590e-05 2.604e-05 2.221e-05
 5.819e-06 2.543e-05 2.345e-04 1.107e-05 1.557e-05 6.398e-05 3.504e-04
 8.553e-05 1.790e-05 3.578e-02 5.500e-05 2.282e-05 4.696e-03 3.330e-05
 1.538e-05 1.085e-05 1.810e-03 7.817e-06 1.183e+00 6.786e-05 9.164e-01
 1.236e+00 1.828e+00 3.097e-01 2.114e+00 9.119e-02 5.092e-03 1.191e-01
 8.386e-04 8.928e-05 1.286e-05 1.283e+00 6.597e-04 1.325e-03 6.958e-05
 6.277e-01 1.918e-03 1.004e-02 1.359e-01 1.018e+00 4.019e-04 1.610e-01
 7.688e-04 1.887e-05 5.422e-03 1.528e-01 1.978e-04 1.291e-01 3.729e-03
 7.686e-01 2.415e-01 1.924e-01 3.022e-01 1.303e-01 2.972e-02 9.575e-03
 1.302e-01 1.236e-03 5.213e-04 8.509e-05 2.609e-01 3.572e-03 2.982e-02
 2.217e-03 3.181e-01 6.524e-05 4.837e-02 1.156e-01 1.395e-01 4.615e-01
 7.938e-02 3.712e-02 3.381e-04 1.118e-01 6.663e-02 4.543e-04 9.879e-02
 2.350e-02 7.654e-05 5.855e-04 3.527e-04 2.457e-03 8.035e-04 6.326e-04
 1.004e-03 4.712e-05 5.269e-04 1.818e-05 9.909e-05 1.111e-03 6.528e-05
 1.673e-05 8.382e-05 1.491e-01 2.905e-04 2.565e-04 1.190e-01 1.121e-01
 3.322e-04 9.087e-05 4.498e-05 3.450e-05 1.314e-04 3.123e-03 7.901e-05
 2.445e-01 3.676e-04 1.185e+00 8.071e-02 8.393e-02 2.423e-01 1.096e-01
 3.757e-02 7.038e-02 2.865e-01 6.698e-03 1.669e-03 1.124e-03 1.187e-01
 1.141e-04 4.004e-02 1.215e-04 9.002e-02 5.901e-03 1.924e-01 1.010e-01
 1.121e-01 8.097e-02 1.638e-03 8.809e-02 5.003e-04 5.099e-04 1.083e-02
 1.526e-04 1.710e-01 9.299e-02 5.111e-02 2.542e-01 3.118e-02 2.825e-01
 1.057e-01 1.629e-01 7.547e-02 4.414e-01 4.878e-04 4.098e+00 1.033e+00
 2.233e-01 1.267e-02 3.358e-01 3.033e-04 3.684e-02 1.398e-02 1.761e-01
 8.127e-02 8.469e-02 1.167e-01 2.107e-02 1.975e-01 4.699e-04 1.659e+00
 1.903e-02 2.676e-04 7.265e-01 1.577e-01 4.736e-01 3.697e-01 2.024e-01
 2.308e-01 9.203e-02 1.955e+00 8.508e-02 1.391e-01 4.793e-02 1.061e-03
 1.310e-04 6.566e-02 8.750e-06 1.303e-02 2.445e-04 1.500e-01 3.772e-04
 3.878e-01 2.039e-01 2.020e-01 9.901e-02 2.943e-03 5.217e-02 2.459e-01
 4.995e-04 5.964e-01 1.356e-02 1.219e+00 1.895e+00 7.759e-03 5.228e-03
 1.113e-03 2.422e-02 7.829e-03 1.242e-02 2.262e-03 3.344e-03 3.095e-04
 7.614e-06 4.431e-05 7.633e-02 8.823e-05 8.349e-05 6.718e-04 2.571e-01
 1.986e-04 5.147e-03 2.681e-01 2.213e-01 1.465e-03 5.166e-04 1.571e+00
 2.040e-03 4.584e-06 2.166e-04 1.427e-04 1.157e+00 6.300e-04 2.205e-01
 2.216e-02 1.031e-02 1.560e-01 2.193e+00 1.873e-01 3.479e-02 1.652e+00
 2.254e-02 4.395e-04 1.166e-02 2.016e+00 2.193e-03 3.807e-02 1.446e-03
 1.397e-01 2.201e-02 1.024e+00 1.264e-01 3.246e-01 6.333e-03 3.797e-02
 9.901e-02 1.488e+00 5.630e-03 3.494e-04 4.096e-02 1.132e-01 7.611e-02
 1.463e-04 6.425e-04 8.388e-04 2.558e-04 2.980e-04 6.397e-05 6.794e-04
 3.157e-05 9.284e-05 9.049e-06 1.595e-05 1.816e-04 1.158e-05 7.499e-06
 2.242e-04 5.877e-04 7.528e-05 5.128e-05 9.248e-02 2.051e-04 1.562e-04
 5.506e-05 1.509e-03 3.987e-05 2.398e-05 1.266e-04 1.928e-03 1.258e-05
 3.685e-05 2.130e-01 7.513e-02 1.809e+00 1.346e-01 1.072e-01 9.801e-02
 5.929e-02 8.517e-02 5.311e-02 5.343e-05 4.149e-04 1.195e-01 1.963e-05
 8.567e-04 3.493e-04 1.730e-01 9.427e-05 1.106e+00 1.316e-01 1.881e-01
 3.203e-02 1.549e-03 9.538e-02 2.609e-03 6.339e-05 1.837e-04 7.176e-01
 4.561e-04 3.698e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 1. 0. 0. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.979928774928775, 0.9380698876130998)
cuda
9630
cuda
Objective function 982.29 = squared loss an data 726.28 + 0.5*rho*h**2 254.631624 + alpha*h 0.000000 + L2reg 0.55 + L1reg 0.82 ; SHD = 415 ; DAG False
||w||^2 0.20451795494559757
exp ma of ||w||^2 6.0617758865073945
||w|| 0.4522366138932114
exp ma of ||w|| 0.3981470536150089
||w||^2 0.1384871866821988
exp ma of ||w||^2 0.11401532268062614
||w|| 0.37213866593273914
exp ma of ||w|| 0.3345383550561449
||w||^2 0.11339114544133805
exp ma of ||w||^2 0.1116045897290656
||w|| 0.33673601743997933
exp ma of ||w|| 0.33111357065277186
||w||^2 0.10630579495563353
exp ma of ||w||^2 0.10617294834791971
||w|| 0.3260456945822679
exp ma of ||w|| 0.32203250652082815
||w||^2 0.10914086657740382
exp ma of ||w||^2 0.10622706762852532
||w|| 0.3303647477825136
exp ma of ||w|| 0.322726439647176
||w||^2 0.06378400389961558
exp ma of ||w||^2 0.09575026036578099
||w|| 0.2525549522373608
exp ma of ||w|| 0.3040181912102574
||w||^2 0.03778482300781457
exp ma of ||w||^2 0.09109267573257702
||w|| 0.19438318602135982
exp ma of ||w|| 0.2952732173688957
cuda
Objective function 218.26 = squared loss an data 214.81 + 0.5*rho*h**2 2.377358 + alpha*h 0.000000 + L2reg 0.57 + L1reg 0.50 ; SHD = 139 ; DAG False
Proportion of microbatches that were clipped  0.7178469355350905
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.1805313520720446
iteration 1 in outer loop, alpha = 2.1805313520720446, rho = 1.0, h = 2.1805313520720446
cuda
9630
cuda
Objective function 223.01 = squared loss an data 214.81 + 0.5*rho*h**2 2.377358 + alpha*h 4.754717 + L2reg 0.57 + L1reg 0.50 ; SHD = 139 ; DAG False
||w||^2 5416.676447265865
exp ma of ||w||^2 921684.7172462159
||w|| 73.59807366545584
exp ma of ||w|| 384.13846241827537
||w||^2 4174.103309328552
exp ma of ||w||^2 533887.0684730719
||w|| 64.60730074324846
exp ma of ||w|| 257.86217697708145
||w||^2 76.40185233672196
exp ma of ||w||^2 22540.403135287237
||w|| 8.740815313042711
exp ma of ||w|| 30.406131146118707
||w||^2 0.19214035468107743
exp ma of ||w||^2 0.16090764813400826
||w|| 0.4383381738807121
exp ma of ||w|| 0.38671329556936274
||w||^2 0.10642087772997896
exp ma of ||w||^2 0.16710264028848051
||w|| 0.32622212943020734
exp ma of ||w|| 0.3892088058377546
cuda
Objective function 94.90 = squared loss an data 87.65 + 0.5*rho*h**2 1.721662 + alpha*h 4.046237 + L2reg 0.99 + L1reg 0.49 ; SHD = 141 ; DAG False
Proportion of microbatches that were clipped  0.7261074152198604
iteration 1 in inner loop, alpha 2.1805313520720446 rho 1.0 h 1.855619757521822
9630
cuda
Objective function 110.39 = squared loss an data 87.65 + 0.5*rho*h**2 17.216623 + alpha*h 4.046237 + L2reg 0.99 + L1reg 0.49 ; SHD = 141 ; DAG False
||w||^2 0.44842438333803875
exp ma of ||w||^2 0.3971253657876834
||w|| 0.6696449681271701
exp ma of ||w|| 0.6067405073948752
||w||^2 0.24823680164490622
exp ma of ||w||^2 0.40675768218478314
||w|| 0.4982336817647982
exp ma of ||w|| 0.6180185460137807
||w||^2 0.20312332688622675
exp ma of ||w||^2 0.38557641975263507
||w|| 0.4506920532760998
exp ma of ||w|| 0.5973671540800921
cuda
Objective function 69.05 = squared loss an data 61.75 + 0.5*rho*h**2 3.765801 + alpha*h 1.892370 + L2reg 1.22 + L1reg 0.43 ; SHD = 115 ; DAG False
Proportion of microbatches that were clipped  0.7491039426523297
iteration 2 in inner loop, alpha 2.1805313520720446 rho 10.0 h 0.8678480549436252
9630
cuda
Objective function 102.95 = squared loss an data 61.75 + 0.5*rho*h**2 37.658012 + alpha*h 1.892370 + L2reg 1.22 + L1reg 0.43 ; SHD = 115 ; DAG False
||w||^2 107083985559.34291
exp ma of ||w||^2 71928590522.32945
||w|| 327236.89516822965
exp ma of ||w|| 154547.52562470685
||w||^2 243336153593.80423
exp ma of ||w||^2 90641834407.82149
||w|| 493291.1448564673
exp ma of ||w|| 193822.13500019856
||w||^2 2932052184.499937
exp ma of ||w||^2 15183994923.811014
||w|| 54148.42735020046
exp ma of ||w|| 106694.69718712028
||w||^2 571990.9084283264
exp ma of ||w||^2 15744548.977100022
||w|| 756.3008055187607
exp ma of ||w|| 2047.5275349444241
||w||^2 1072.8925583336165
exp ma of ||w||^2 398605.6798790442
||w|| 32.75503867092232
exp ma of ||w|| 165.9264819174774
||w||^2 2.396313345308992
exp ma of ||w||^2 140.29530158447238
||w|| 1.5480030185077134
exp ma of ||w|| 2.4008616550720894
||w||^2 0.7946171949559196
exp ma of ||w||^2 23.19842978869737
||w|| 0.891413032749645
exp ma of ||w|| 1.398834190439293
||w||^2 0.616030426812258
exp ma of ||w||^2 5.648938165182808
||w|| 0.7848760582488536
exp ma of ||w|| 1.0862127142565736
||w||^2 1.585681558229635
exp ma of ||w||^2 0.8897780899099224
||w|| 1.259238483461189
exp ma of ||w|| 0.9095752367915041
cuda
Objective function 65.30 = squared loss an data 56.47 + 0.5*rho*h**2 6.342657 + alpha*h 0.776628 + L2reg 1.35 + L1reg 0.36 ; SHD = 96 ; DAG True
Proportion of microbatches that were clipped  0.7604574651417828
iteration 3 in inner loop, alpha 2.1805313520720446 rho 100.0 h 0.35616448565113856
iteration 2 in outer loop, alpha = 37.7969799171859, rho = 100.0, h = 0.35616448565113856
cuda
9630
cuda
Objective function 77.98 = squared loss an data 56.47 + 0.5*rho*h**2 6.342657 + alpha*h 13.461942 + L2reg 1.35 + L1reg 0.36 ; SHD = 96 ; DAG True
||w||^2 1744408016.8529568
exp ma of ||w||^2 11342869403.57231
||w|| 41766.11086578396
exp ma of ||w|| 87743.27199582131
||w||^2 23122.920699808215
exp ma of ||w||^2 4607377.053963147
||w|| 152.06222640685036
exp ma of ||w|| 753.3706030239624
||w||^2 3.469526521601842
exp ma of ||w||^2 331.98685420617215
||w|| 1.8626665084232985
exp ma of ||w|| 3.800148314673977
||w||^2 2.7685087335704703
exp ma of ||w||^2 248.82415962849183
||w|| 1.6638836298162412
exp ma of ||w|| 3.2751727437647458
||w||^2 2.281621486383641
exp ma of ||w||^2 87.61130444103087
||w|| 1.5105037194206576
exp ma of ||w|| 2.202992791545499
||w||^2 1.4979264823630691
exp ma of ||w||^2 9.448532498967209
||w|| 1.223898068616447
exp ma of ||w|| 1.3335827009205878
||w||^2 3.7488894317441113
exp ma of ||w||^2 1.5154559363454474
||w|| 1.9362049043797278
exp ma of ||w|| 1.139896162054417
||w||^2 1.8434389630398718
exp ma of ||w||^2 1.223228000860777
||w|| 1.3577330234769542
exp ma of ||w|| 1.0749315459564566
||w||^2 1.749115214463143
exp ma of ||w||^2 1.188179118217375
||w|| 1.322541195752761
exp ma of ||w|| 1.0574878324658394
cuda
Objective function 70.93 = squared loss an data 56.02 + 0.5*rho*h**2 3.339154 + alpha*h 9.767655 + L2reg 1.47 + L1reg 0.34 ; SHD = 102 ; DAG True
Proportion of microbatches that were clipped  0.7613469985358712
iteration 1 in inner loop, alpha 37.7969799171859 rho 100.0 h 0.2584242070117213
9630
cuda
Objective function 100.98 = squared loss an data 56.02 + 0.5*rho*h**2 33.391535 + alpha*h 9.767655 + L2reg 1.47 + L1reg 0.34 ; SHD = 102 ; DAG True
||w||^2 134771.90178792085
exp ma of ||w||^2 16078168.581467796
||w|| 367.1129278409041
exp ma of ||w|| 1661.4723973893745
||w||^2 290.6278688759637
exp ma of ||w||^2 28206.265816577117
||w|| 17.047811263501355
exp ma of ||w|| 23.538015340345186
||w||^2 3.563642890387222
exp ma of ||w||^2 4.725951880701585
||w|| 1.8877613435991378
exp ma of ||w|| 1.4661887952064334
v before min max tensor([[ 9.138e+02, -2.338e+02, -1.135e+02,  ...,  7.295e+00, -1.877e+02,
         -1.564e+02],
        [ 5.230e+02, -2.154e+02, -2.615e+02,  ...,  3.355e+02, -1.037e+02,
          3.323e+03],
        [ 3.392e+03, -2.440e+02, -8.696e+01,  ..., -1.803e+02,  1.410e+02,
          7.672e+02],
        ...,
        [ 2.482e+00, -2.030e+02, -1.726e+02,  ...,  1.959e+02,  8.402e+01,
          1.237e+04],
        [-2.679e+02, -2.030e+02,  7.929e+02,  ...,  9.067e+01, -2.647e+02,
          9.995e+02],
        [-3.138e+02, -2.548e+02,  2.559e+01,  ...,  6.355e+02,  2.647e+02,
          1.300e+03]], device='cuda:0')
v tensor([[1.000e+01, 1.000e-12, 1.000e-12,  ..., 7.295e+00, 1.000e-12,
         1.000e-12],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e+01, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e+01,
         1.000e+01],
        ...,
        [2.482e+00, 1.000e-12, 1.000e-12,  ..., 1.000e+01, 1.000e+01,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e-12,
         1.000e+01],
        [1.000e-12, 1.000e-12, 1.000e+01,  ..., 1.000e+01, 1.000e+01,
         1.000e+01]], device='cuda:0')
v before min max tensor([-1.277e+02, -8.213e+01,  9.983e+02, -9.193e+01, -2.334e+02, -1.768e+02,
         2.669e+02, -3.996e+01,  4.503e+02, -1.617e+02,  1.384e+03, -2.835e+02,
        -2.409e+02, -7.680e+00,  3.553e+01,  1.902e+03, -1.975e+02, -2.522e+02,
         6.018e+01, -2.604e+02, -8.190e+01, -1.311e+02, -2.495e+02, -5.789e+01,
        -2.543e+02, -1.457e+02,  3.723e+01,  4.613e+01, -1.018e+02, -1.312e+02,
         2.473e+01,  2.117e+01, -2.565e+02, -9.588e+01, -2.604e+02,  5.048e+02,
         1.210e+03,  2.013e+02, -1.670e+02,  2.805e+02,  3.030e+01, -2.816e+02,
         2.322e+01, -9.956e+00, -3.398e+00, -2.048e+02,  5.589e+01, -2.537e+02,
         3.548e+03, -1.014e+02,  8.432e+02, -2.684e+02, -2.149e+02, -1.696e+02,
         6.814e+02, -5.199e+01,  2.534e+02, -1.038e+02, -1.995e+02, -2.304e+02,
        -1.759e+02, -1.553e+02,  1.787e+03,  2.909e+02, -1.455e+02, -2.618e+01,
         1.549e+02,  3.793e+01,  3.721e+02,  3.326e+02, -1.730e+02, -2.068e+02,
        -1.180e+02,  2.728e+02, -1.265e+02,  4.147e+02,  6.378e+00, -1.506e+02,
        -2.465e+02, -1.466e+02,  3.405e+01, -2.050e+02, -1.574e+02, -1.921e+02,
        -6.853e+01, -2.094e+02, -1.905e+02, -2.084e+02,  2.165e+02, -8.613e+01,
        -2.857e+02,  1.012e+03,  1.117e+02, -2.171e+02,  5.044e+01, -3.027e+02,
        -3.072e+02, -6.744e+01,  3.086e+02, -2.162e+02,  1.259e+03,  1.164e+03,
         3.792e+02, -1.159e+02,  1.189e+02,  1.048e+03, -1.061e+02, -2.112e+02,
         5.252e+01,  3.261e+02,  1.870e+02, -1.775e+02, -2.797e+02, -2.592e+02,
         6.751e+02, -2.542e+02,  5.724e+02,  1.354e+02,  1.686e+02,  4.178e+02,
        -1.286e+02, -2.834e+02,  3.245e+01,  7.770e+01, -1.890e+02, -2.556e+02,
         4.676e+01, -2.051e+02,  1.765e+01,  9.076e+01, -1.198e+02, -2.290e+02,
        -2.602e+02,  4.114e+02, -2.411e+02, -2.223e+02,  8.551e+02, -3.059e+02,
         8.219e+02, -3.088e+01, -1.116e+02, -2.337e+02,  2.719e+02,  2.545e+02,
        -1.532e+02, -1.158e+02, -2.912e+02, -2.735e+01,  6.334e+02, -1.061e+02,
        -2.518e+02,  1.785e+02,  9.974e+01,  1.782e+03, -3.096e+01,  1.432e+02,
        -1.484e+02, -1.983e+02, -2.115e+02,  8.821e+01,  1.182e+02, -6.803e+00,
        -2.137e+02, -2.582e+02, -1.046e+02,  2.443e+02, -9.418e+01,  8.287e+01,
        -1.880e+02,  2.284e+02, -2.597e+02,  6.435e+02, -2.625e+02, -1.136e+02,
         1.067e+03,  4.785e+02, -1.917e+02, -1.315e+02,  1.523e+02, -1.804e+02,
        -1.754e+02, -2.289e+02,  2.260e+02,  1.040e+03,  1.081e+03, -2.141e+02,
        -3.131e+02,  3.403e+02, -8.035e+01,  1.496e+02, -2.471e+02, -1.562e+02,
        -1.158e+02, -2.237e+02,  3.695e+02,  1.093e+02,  1.026e+03, -9.285e+01,
         1.541e+01, -2.907e+02, -1.859e+02, -7.824e+01, -2.035e+02, -2.635e+02,
        -1.425e+02,  5.046e+02, -2.298e+02, -2.748e+02, -1.107e+02, -1.805e+02,
         1.085e+02, -2.576e+02, -2.548e+02,  4.498e+02, -2.744e+01,  1.890e+03,
        -1.811e+02,  2.679e+02, -1.625e+02, -1.314e+02,  3.419e+01, -2.021e+02,
        -1.952e+02, -3.306e+02, -2.092e+02, -1.739e+02,  1.066e+03,  9.618e+02,
        -1.156e+02, -2.278e+02,  4.703e+02, -1.517e+02, -2.094e+01, -1.140e+02,
        -2.661e+02,  1.831e+02, -6.356e+01,  7.940e+01, -2.090e+02, -2.302e+02,
        -3.892e+01, -2.153e+02,  8.241e+02, -2.248e+02, -2.188e+02, -1.829e+02,
         4.377e+02, -1.478e+02, -1.365e+02, -2.033e+02, -1.371e+02, -1.095e+02,
         1.730e+02,  2.149e+02, -1.389e+02, -5.292e+01,  7.718e+01, -2.117e+02,
        -1.850e+02,  2.907e+02, -1.837e+02,  3.755e+01, -2.733e+02, -2.174e+02,
        -2.152e+02, -5.700e+01, -2.984e+02, -2.264e+02, -1.693e+02, -2.588e+02,
         2.903e+03, -4.478e+01, -2.975e+02,  3.218e+02, -8.572e+01, -7.309e+01,
        -2.501e+02, -7.020e+01,  8.719e+01,  2.354e+01, -4.574e+01, -7.443e+01,
        -1.117e+02, -1.336e+02, -1.048e+02,  3.337e+02,  2.806e+02,  1.102e+03,
        -2.639e+02, -1.466e+02, -3.044e+02, -2.214e+02, -1.763e+02, -2.697e+02,
        -1.922e+02,  2.173e+02,  3.510e+02, -9.175e+01,  9.189e+02, -1.336e+02],
       device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 6.378e+00, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e+01, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e+01, 1.000e+01, 1.000e+01,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e+01, 1.000e+01, 1.000e-12, 1.000e+01, 1.000e-12],
       device='cuda:0')
v before min max tensor([[[ 916.522],
         [-201.047],
         [ 750.089],
         [-167.314],
         [  41.802],
         [ 448.623],
         [-311.055],
         [ 333.049],
         [-215.119],
         [ 192.756]],

        [[-171.463],
         [-294.682],
         [-163.557],
         [-266.695],
         [ -28.508],
         [  63.332],
         [ 215.836],
         [-109.064],
         [ 307.688],
         [ 454.405]],

        [[-198.823],
         [-121.794],
         [-146.900],
         [-257.476],
         [-223.637],
         [-149.141],
         [ -11.701],
         [-226.093],
         [ 243.587],
         [-122.035]],

        [[-303.537],
         [ 423.806],
         [ 261.701],
         [ 941.287],
         [ 101.592],
         [1929.298],
         [-120.364],
         [-179.342],
         [ 463.311],
         [ 472.929]],

        [[  33.722],
         [  50.344],
         [  70.646],
         [ 341.349],
         [ 254.846],
         [-330.211],
         [-221.633],
         [-295.547],
         [ -91.720],
         [-249.258]],

        [[ 206.656],
         [-236.738],
         [-127.768],
         [-184.675],
         [-217.426],
         [-102.347],
         [  75.415],
         [-197.458],
         [-256.420],
         [1498.206]],

        [[1124.871],
         [-175.551],
         [-304.351],
         [ 264.074],
         [ -16.825],
         [-104.125],
         [ -70.493],
         [-221.772],
         [-251.553],
         [ 320.667]],

        [[-174.879],
         [-248.666],
         [ 644.109],
         [-177.300],
         [ 131.287],
         [ 649.215],
         [ -29.419],
         [-167.249],
         [-111.432],
         [ 364.458]],

        [[  -7.745],
         [-217.957],
         [ 136.337],
         [-189.522],
         [-130.588],
         [-109.170],
         [  62.765],
         [ -86.730],
         [1771.224],
         [ -17.069]],

        [[-315.202],
         [ -52.649],
         [  11.316],
         [-218.316],
         [-119.086],
         [  12.800],
         [ 183.462],
         [-208.364],
         [ -81.353],
         [1239.678]],

        [[-205.195],
         [-196.338],
         [-168.845],
         [-196.885],
         [  31.793],
         [  94.365],
         [ -48.243],
         [ 132.917],
         [-137.793],
         [ 137.523]],

        [[ -87.598],
         [-281.577],
         [-152.416],
         [ 138.780],
         [  63.913],
         [-147.396],
         [ 229.854],
         [  80.480],
         [-184.932],
         [-240.746]],

        [[ 170.887],
         [-103.009],
         [ 133.484],
         [ -54.449],
         [-190.597],
         [ 930.089],
         [ -14.191],
         [ 208.391],
         [-223.563],
         [ 252.088]],

        [[-208.057],
         [1211.092],
         [-125.603],
         [ -99.743],
         [-131.954],
         [-245.738],
         [-270.284],
         [-172.485],
         [-237.811],
         [-246.711]],

        [[-250.321],
         [-113.816],
         [ 220.939],
         [ -77.103],
         [ -89.884],
         [-139.142],
         [  -8.963],
         [-147.469],
         [ 112.446],
         [  73.079]],

        [[-252.645],
         [-234.977],
         [-309.032],
         [ 466.595],
         [-227.621],
         [-123.914],
         [ -56.675],
         [ 289.113],
         [-188.454],
         [-230.257]],

        [[-193.659],
         [-278.386],
         [ -94.156],
         [  89.468],
         [-243.819],
         [-281.966],
         [ 155.790],
         [-156.058],
         [  60.873],
         [-214.911]],

        [[-245.134],
         [-144.249],
         [ 288.531],
         [-159.250],
         [1178.610],
         [-203.944],
         [-156.553],
         [-258.803],
         [  62.930],
         [-297.237]],

        [[ 283.110],
         [-251.477],
         [ 130.291],
         [ 223.747],
         [-173.334],
         [ -44.479],
         [ -19.200],
         [-211.705],
         [ 566.438],
         [ 734.079]],

        [[ 565.545],
         [-236.877],
         [-245.673],
         [-199.936],
         [ 672.107],
         [-308.413],
         [ 115.035],
         [-150.717],
         [ 120.877],
         [-138.370]],

        [[-253.821],
         [-175.025],
         [ 152.233],
         [ -90.771],
         [ 130.516],
         [-223.467],
         [ 281.520],
         [-286.429],
         [-233.151],
         [  48.867]],

        [[-105.767],
         [ -54.560],
         [ 204.764],
         [ 394.766],
         [-255.000],
         [-214.871],
         [-166.090],
         [  11.892],
         [-291.038],
         [-193.971]],

        [[-196.023],
         [-140.837],
         [  59.702],
         [-224.043],
         [-152.000],
         [-137.333],
         [  21.477],
         [-178.887],
         [  15.888],
         [-299.181]],

        [[-166.720],
         [-233.477],
         [-202.561],
         [ -74.233],
         [-187.880],
         [-230.155],
         [-265.475],
         [-216.782],
         [-226.090],
         [ 815.782]],

        [[-172.148],
         [-218.858],
         [-136.899],
         [ 494.939],
         [-302.815],
         [ -64.995],
         [-234.962],
         [-218.705],
         [ 400.685],
         [ 312.080]],

        [[-256.318],
         [-208.199],
         [2356.349],
         [-324.796],
         [-158.001],
         [ -17.105],
         [-311.669],
         [-302.843],
         [ 100.974],
         [-140.029]],

        [[ 454.195],
         [ -20.244],
         [ 421.781],
         [-187.461],
         [ 270.892],
         [-246.199],
         [ 203.047],
         [-135.987],
         [-228.288],
         [   4.489]],

        [[-139.519],
         [ 156.101],
         [-171.013],
         [ -21.922],
         [1109.822],
         [-144.599],
         [-102.523],
         [-112.092],
         [-202.483],
         [ 662.120]],

        [[-117.923],
         [ -54.770],
         [-194.197],
         [  58.119],
         [-215.995],
         [  36.596],
         [ -36.410],
         [ -94.986],
         [-204.251],
         [  37.957]],

        [[ 173.613],
         [-169.955],
         [  70.322],
         [-228.103],
         [-238.683],
         [-225.490],
         [-145.559],
         [-131.832],
         [ -89.279],
         [-272.745]]], device='cuda:0')
v tensor([[[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [4.489e+00]],

        [[1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e+01]],

        [[1.000e+01],
         [1.000e-12],
         [1.000e+01],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-132.148],
        [-235.796],
        [-263.761],
        [-279.091],
        [-242.994],
        [-296.639],
        [ -31.402],
        [-122.292],
        [-169.946],
        [  37.142],
        [ -70.487],
        [ -55.258],
        [-186.766],
        [ -79.997],
        [-212.109],
        [ -50.329],
        [ 132.209],
        [-209.970],
        [ -11.130],
        [ -50.985],
        [  84.792],
        [-227.097],
        [-291.778],
        [-249.267],
        [ 464.445],
        [-258.374],
        [ 802.890],
        [-110.443],
        [-117.553],
        [-252.127]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e+01],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-0.094, -0.055, -0.023,  ...,  0.006, -0.046,  0.039],
        [ 0.003, -0.094, -0.039,  ...,  0.182, -0.063,  0.086],
        [ 0.097,  0.032,  0.019,  ..., -0.043, -0.136, -0.011],
        ...,
        [ 0.112, -0.025,  0.000,  ..., -0.142, -0.023,  0.177],
        [-0.030, -0.157, -0.142,  ..., -0.067,  0.082, -0.033],
        [-0.040, -0.153, -0.016,  ..., -0.004,  0.011, -0.094]],
       device='cuda:0')
s after update for 1 param tensor([[1.862, 1.527, 1.681,  ..., 2.070, 1.657, 1.906],
        [1.668, 1.843, 1.698,  ..., 1.852, 1.441, 2.300],
        [2.080, 1.599, 2.038,  ..., 1.192, 2.174, 1.887],
        ...,
        [1.915, 1.370, 1.820,  ..., 1.873, 1.805, 2.597],
        [1.801, 1.630, 2.118,  ..., 2.268, 1.758, 2.267],
        [2.089, 1.935, 2.184,  ..., 2.136, 1.627, 2.182]], device='cuda:0')
b after update for 1 param tensor([[175.070, 158.527, 166.334,  ..., 184.567, 165.149, 177.096],
        [165.666, 174.156, 167.159,  ..., 174.587, 153.981, 194.568],
        [185.001, 162.236, 183.157,  ..., 140.054, 189.165, 176.220],
        ...,
        [177.533, 150.155, 173.054,  ..., 175.587, 172.349, 206.721],
        [172.176, 163.759, 186.678,  ..., 193.199, 170.102, 193.135],
        [185.431, 178.462, 189.595,  ..., 187.479, 163.642, 189.476]],
       device='cuda:0')
clipping threshold 1.1917976608417045
a after update for 1 param tensor([ 4.426e-02, -2.022e-01, -5.187e-02,  1.099e-01, -1.071e-01,  5.602e-02,
         1.143e-01,  7.653e-02,  1.868e-02,  2.438e-03, -7.281e-02, -8.906e-02,
         5.746e-02,  7.195e-02, -3.417e-02,  2.397e-03, -3.497e-02, -1.207e-01,
         2.431e-01,  1.405e-01,  9.437e-02,  5.395e-03, -5.167e-04, -7.906e-02,
        -8.174e-02, -8.072e-04,  6.366e-02, -1.638e-01, -1.839e-02, -9.356e-02,
        -5.699e-02, -1.177e-01, -9.500e-02, -1.124e-01,  9.860e-02, -2.628e-04,
         1.689e-01,  1.894e-02,  6.683e-02,  4.550e-02, -5.169e-02,  5.052e-02,
        -5.835e-02, -1.115e-01,  5.341e-03,  8.110e-02, -9.712e-02,  1.555e-01,
        -5.294e-03, -1.564e-02,  7.338e-02, -4.639e-02, -4.527e-02, -9.723e-02,
         1.308e-02, -4.103e-02, -6.675e-02,  3.114e-03,  1.010e-01,  6.639e-02,
        -6.139e-02, -1.439e-01, -3.007e-01,  8.224e-02,  4.889e-02, -4.317e-02,
         2.584e-01,  3.064e-01,  1.004e-02, -1.263e-01,  4.736e-02,  8.328e-03,
         4.296e-02, -2.768e-01, -2.000e-01, -4.793e-02, -2.548e-02, -2.078e-02,
        -3.139e-01, -6.542e-02,  2.464e-02, -3.353e-03, -5.018e-03,  4.730e-02,
         1.053e-02,  1.019e-01, -2.434e-01, -5.403e-02,  1.747e-01,  7.318e-02,
         1.212e-02,  1.561e-01, -2.620e-02, -5.524e-02,  3.044e-02, -4.039e-02,
         1.237e-01, -5.180e-02, -2.564e-01,  7.219e-02,  7.021e-02, -3.164e-01,
         1.665e-02,  3.969e-02,  4.850e-02,  1.981e-01,  8.727e-04, -5.835e-02,
        -3.552e-02, -2.468e-02,  1.046e-01,  5.623e-02,  1.209e-01, -2.596e-02,
        -1.232e-01, -6.613e-02,  1.315e-02, -4.358e-02, -3.116e-01,  4.216e-02,
        -9.121e-02, -2.373e-01,  4.443e-02, -3.552e-02, -1.174e-01,  1.548e-01,
        -3.207e-02,  2.336e-01,  2.167e-01,  9.049e-02,  5.203e-02,  2.464e-02,
         9.288e-02, -2.011e-01,  1.175e-01,  5.126e-02, -1.134e-01,  1.975e-02,
         1.845e-02,  2.384e-01,  5.921e-02, -4.145e-02, -6.977e-02,  1.635e-01,
        -2.160e-01, -4.713e-03, -2.840e-02,  2.115e-02,  2.457e-02,  1.125e-01,
         4.127e-03,  6.149e-02, -7.203e-02, -6.259e-02,  5.141e-02,  2.023e-02,
         1.216e-01, -4.202e-02, -1.080e-01, -2.210e-02,  2.971e-02, -2.125e-02,
         1.726e-01,  3.643e-01,  1.533e-02, -1.881e-03,  5.138e-02, -5.397e-02,
        -5.485e-02,  6.063e-02,  1.792e-02,  1.113e-01, -1.096e-01,  2.960e-01,
        -1.648e-01,  4.902e-02, -9.381e-02,  1.897e-01, -1.520e-01,  3.352e-02,
        -1.767e-01,  3.670e-02,  2.915e-02, -2.264e-03,  6.312e-02, -1.761e-01,
        -2.135e-01, -1.445e-01,  2.754e-02,  5.107e-02, -3.169e-03, -1.138e-01,
        -4.055e-02,  2.245e-02,  6.161e-02, -4.757e-02, -1.562e-01,  3.465e-03,
         2.049e-03,  1.556e-04,  1.993e-02,  6.449e-02, -1.485e-02, -1.064e-01,
        -9.746e-02, -5.886e-02, -1.351e-01,  3.437e-02,  1.767e-01,  7.856e-02,
        -1.567e-02, -1.438e-01,  8.374e-02, -5.433e-02,  1.025e-02, -3.319e-02,
         1.055e-01, -6.898e-02, -1.496e-02,  4.359e-02,  9.580e-02,  4.206e-02,
        -2.077e-02, -8.598e-02, -2.268e-02,  1.250e-01, -6.541e-02, -4.014e-01,
         1.211e-01,  1.855e-02, -6.869e-02,  1.196e-02,  4.368e-02, -2.880e-02,
         5.088e-02, -3.808e-02, -2.720e-02,  2.543e-01, -1.149e-01, -1.213e-01,
         1.657e-02, -3.751e-02,  8.580e-02, -7.135e-03,  4.491e-02, -6.647e-02,
        -1.303e-01, -2.894e-01, -1.066e-01, -1.210e-03,  9.994e-03, -2.240e-01,
        -5.340e-02,  1.051e-01, -9.673e-02,  5.351e-02, -8.064e-02, -9.616e-02,
        -2.063e-02, -1.228e-01, -3.628e-02, -2.514e-01,  2.965e-02, -8.944e-03,
        -2.125e-01, -2.207e-02,  2.487e-02,  2.871e-02,  9.601e-02, -1.254e-01,
         4.285e-02, -7.634e-02,  1.668e-02,  1.797e-01,  1.204e-03, -8.245e-04,
         1.089e-01, -6.738e-02,  2.288e-02,  1.625e-01,  5.865e-02, -3.306e-01,
         5.983e-03, -1.402e-01, -1.259e-01, -9.280e-02, -4.709e-02,  5.259e-02,
        -2.049e-02, -1.376e-02,  2.253e-01,  1.260e-01, -1.721e-01, -3.746e-01,
        -1.009e-01, -1.067e-01, -7.126e-02, -1.409e-01, -2.581e-01, -2.654e-03],
       device='cuda:0')
s after update for 1 param tensor([1.716, 2.004, 2.100, 1.590, 1.582, 1.592, 1.668, 1.827, 2.056, 1.780,
        1.908, 1.831, 1.878, 1.287, 1.888, 1.885, 1.421, 1.693, 1.762, 2.064,
        1.621, 0.921, 1.609, 1.567, 1.686, 0.949, 2.151, 2.110, 1.729, 1.449,
        2.232, 1.800, 1.693, 1.897, 1.670, 2.003, 1.901, 1.183, 1.329, 2.184,
        1.491, 1.998, 1.937, 1.928, 1.057, 1.433, 2.103, 1.792, 2.091, 1.225,
        2.340, 1.722, 1.670, 1.345, 2.041, 1.757, 1.969, 1.732, 1.995, 1.538,
        1.481, 1.803, 2.136, 1.814, 1.524, 1.723, 1.986, 2.035, 2.180, 1.390,
        1.215, 1.390, 1.375, 1.957, 1.464, 2.261, 2.144, 1.354, 1.631, 1.089,
        1.807, 1.339, 1.075, 1.233, 1.625, 1.382, 1.487, 1.481, 1.844, 1.923,
        1.832, 1.912, 2.371, 1.460, 1.788, 1.971, 2.035, 1.694, 2.279, 1.775,
        1.801, 1.983, 1.842, 0.748, 1.722, 1.972, 1.448, 1.361, 1.352, 1.446,
        1.186, 1.393, 1.919, 1.663, 2.212, 1.682, 1.732, 1.723, 2.041, 1.602,
        1.940, 1.968, 1.582, 1.497, 1.309, 1.964, 1.681, 1.996, 1.819, 2.235,
        1.061, 1.469, 1.832, 1.716, 1.551, 1.432, 1.829, 1.974, 1.562, 1.756,
        0.899, 1.500, 2.236, 1.864, 1.800, 1.783, 1.881, 0.505, 1.702, 1.422,
        1.615, 1.980, 1.480, 2.241, 1.481, 1.959, 1.284, 1.272, 1.564, 1.636,
        1.359, 1.630, 1.790, 1.657, 1.806, 1.673, 1.653, 1.401, 1.700, 2.072,
        1.723, 1.940, 1.832, 1.277, 1.657, 1.959, 1.445, 1.397, 1.934, 1.160,
        1.221, 1.891, 1.775, 1.797, 1.562, 1.579, 2.042, 1.712, 1.135, 2.021,
        1.918, 1.799, 1.598, 1.612, 2.053, 1.632, 1.572, 1.571, 1.069, 1.894,
        1.553, 1.395, 1.527, 1.793, 1.182, 1.870, 1.502, 1.764, 2.009, 1.950,
        1.274, 1.776, 1.790, 1.608, 0.265, 1.895, 1.162, 2.272, 1.257, 1.636,
        1.816, 2.046, 1.257, 2.175, 1.506, 1.272, 1.992, 2.272, 1.860, 1.466,
        2.032, 1.110, 1.397, 1.155, 1.708, 1.952, 1.335, 1.891, 1.381, 1.721,
        1.278, 1.412, 1.885, 1.450, 1.445, 1.230, 1.765, 1.765, 1.732, 1.386,
        1.368, 2.046, 2.259, 1.670, 1.862, 1.732, 1.627, 1.931, 1.305, 1.971,
        1.214, 2.233, 2.109, 1.768, 1.463, 0.623, 1.929, 1.455, 1.897, 1.714,
        1.672, 1.711, 1.948, 2.103, 1.471, 0.486, 1.607, 1.151, 1.617, 1.963,
        1.396, 2.305, 1.267, 1.681, 1.510, 1.929, 1.743, 1.402, 2.006, 1.439,
        1.959, 1.611, 1.200, 1.767, 1.232, 2.061, 1.863, 1.705, 1.947, 1.948],
       device='cuda:0')
b after update for 1 param tensor([168.025, 181.582, 185.907, 161.758, 161.328, 161.843, 165.682, 173.416,
        183.957, 171.163, 177.189, 173.578, 175.793, 145.521, 176.262, 176.131,
        152.945, 166.906, 170.289, 184.306, 163.316, 123.137, 162.748, 160.560,
        166.555, 124.956, 188.155, 186.346, 168.675, 154.405, 191.671, 172.093,
        166.923, 176.680, 165.771, 181.536, 176.888, 139.510, 147.907, 189.575,
        156.650, 181.323, 178.531, 178.116, 131.881, 153.563, 186.050, 171.727,
        185.502, 142.007, 196.219, 168.331, 165.757, 148.775, 183.250, 170.030,
        180.003, 168.840, 181.192, 159.083, 156.123, 172.238, 187.472, 172.761,
        158.370, 168.373, 180.800, 183.018, 189.424, 151.225, 141.430, 151.242,
        150.413, 179.475, 155.216, 192.888, 187.845, 149.263, 163.835, 133.877,
        172.464, 148.425, 133.026, 142.448, 163.526, 150.792, 156.456, 156.106,
        174.189, 177.901, 173.616, 177.376, 197.531, 154.986, 171.539, 180.121,
        183.019, 166.956, 193.682, 170.929, 172.158, 180.631, 174.125, 110.934,
        168.363, 180.159, 154.389, 149.639, 149.169, 154.243, 139.723, 151.409,
        177.723, 165.429, 190.797, 166.365, 168.845, 168.374, 183.259, 162.373,
        178.690, 179.957, 161.374, 156.959, 146.780, 179.763, 166.333, 181.235,
        172.994, 191.793, 132.124, 155.508, 173.620, 168.068, 159.783, 153.529,
        173.515, 180.221, 160.313, 170.005, 121.620, 157.131, 191.825, 175.155,
        172.089, 171.285, 175.948,  91.161, 167.372, 152.950, 163.008, 180.516,
        156.052, 192.027, 156.116, 179.571, 145.346, 144.667, 160.432, 164.078,
        149.568, 163.759, 171.652, 165.118, 172.395, 165.915, 164.942, 151.842,
        167.285, 184.664, 168.411, 178.693, 173.634, 144.989, 165.111, 179.539,
        154.181, 151.650, 178.413, 138.148, 141.776, 176.411, 170.924, 171.990,
        160.339, 161.202, 183.310, 167.841, 136.674, 182.380, 177.682, 172.071,
        162.184, 162.853, 183.786, 163.864, 160.861, 160.766, 132.632, 176.533,
        159.878, 151.539, 158.499, 171.792, 139.463, 175.405, 157.225, 170.374,
        181.844, 179.149, 144.809, 170.952, 171.608, 162.670,  66.006, 176.598,
        138.286, 193.381, 143.838, 164.081, 172.852, 183.504, 143.813, 189.178,
        157.451, 144.708, 181.061, 193.361, 174.961, 155.325, 182.858, 135.170,
        151.600, 137.875, 167.660, 179.210, 148.212, 176.407, 150.743, 168.305,
        145.024, 152.457, 176.134, 154.462, 154.184, 142.278, 170.435, 170.436,
        168.832, 151.026, 150.029, 183.512, 192.823, 165.776, 175.039, 168.836,
        163.655, 178.268, 146.565, 180.078, 141.359, 191.697, 186.311, 170.554,
        155.183, 101.283, 178.174, 154.725, 176.666, 167.967, 165.859, 167.795,
        179.064, 186.023, 155.586,  89.430, 162.601, 137.606, 163.111, 179.727,
        151.590, 194.767, 144.412, 166.318, 157.659, 178.164, 169.344, 151.916,
        181.678, 153.882, 179.531, 162.801, 140.537, 170.515, 142.417, 184.183,
        175.094, 167.521, 179.023, 179.057], device='cuda:0')
clipping threshold 1.1917976608417045
a after update for 1 param tensor([[[-4.528e-03],
         [ 1.157e-01],
         [-2.536e-01],
         [ 1.191e-01],
         [ 2.178e-02],
         [ 2.157e-02],
         [-7.608e-02],
         [ 2.123e-01],
         [-6.870e-02],
         [-1.978e-01]],

        [[-9.215e-03],
         [ 2.653e-01],
         [ 1.244e-01],
         [ 1.365e-02],
         [ 3.600e-01],
         [-8.852e-02],
         [ 1.168e-03],
         [-2.575e-02],
         [ 3.976e-02],
         [-1.557e-02]],

        [[ 1.372e-01],
         [ 8.327e-03],
         [ 6.420e-03],
         [-4.380e-02],
         [ 1.704e-01],
         [-1.272e-01],
         [-8.353e-02],
         [-9.947e-02],
         [ 9.809e-02],
         [-1.219e-02]],

        [[ 2.493e-02],
         [ 1.054e-01],
         [ 1.150e-01],
         [ 1.277e-01],
         [-7.150e-02],
         [-3.532e-02],
         [-3.730e-02],
         [-7.899e-02],
         [ 1.479e-02],
         [ 9.400e-02]],

        [[ 1.113e-01],
         [-1.494e-02],
         [ 5.607e-02],
         [ 3.047e-02],
         [-7.397e-02],
         [-5.630e-04],
         [-2.606e-02],
         [-7.078e-02],
         [-1.132e-01],
         [-6.757e-02]],

        [[-7.872e-02],
         [-5.876e-02],
         [-1.002e-01],
         [-1.089e-01],
         [ 7.876e-02],
         [ 2.390e-02],
         [ 1.440e-01],
         [-8.768e-02],
         [-2.428e-01],
         [ 1.017e-01]],

        [[-1.324e-01],
         [ 1.262e-01],
         [ 2.223e-01],
         [-1.837e-02],
         [-4.973e-02],
         [-2.269e-01],
         [ 8.455e-02],
         [ 2.455e-02],
         [-2.946e-02],
         [ 1.412e-02]],

        [[ 1.557e-01],
         [-4.405e-02],
         [ 3.132e-03],
         [-1.056e-01],
         [ 1.617e-01],
         [-3.980e-02],
         [ 9.317e-03],
         [-2.089e-01],
         [-1.091e-01],
         [-7.896e-02]],

        [[-3.419e-02],
         [-2.116e-02],
         [ 1.843e-01],
         [ 2.447e-02],
         [-1.287e-01],
         [ 1.480e-01],
         [ 1.172e-02],
         [ 3.722e-02],
         [ 2.685e-02],
         [ 8.403e-02]],

        [[ 1.616e-01],
         [-9.055e-02],
         [ 3.660e-02],
         [-1.088e-01],
         [-7.271e-02],
         [ 1.642e-01],
         [ 1.589e-01],
         [-1.518e-01],
         [ 2.226e-03],
         [-3.565e-02]],

        [[ 1.954e-01],
         [-1.972e-01],
         [ 1.157e-01],
         [ 5.169e-02],
         [-7.241e-02],
         [ 1.333e-01],
         [-1.945e-01],
         [-9.909e-02],
         [-7.712e-02],
         [-3.346e-02]],

        [[-3.753e-02],
         [-1.962e-01],
         [-9.370e-02],
         [-7.120e-02],
         [-1.287e-02],
         [ 1.547e-02],
         [ 6.380e-02],
         [-5.589e-02],
         [-1.649e-02],
         [-1.631e-01]],

        [[-1.396e-01],
         [ 9.512e-02],
         [ 2.303e-01],
         [-2.242e-02],
         [ 1.211e-01],
         [ 1.520e-01],
         [-2.488e-03],
         [ 4.063e-02],
         [ 2.548e-02],
         [-2.570e-01]],

        [[-8.332e-02],
         [-1.202e-01],
         [ 9.966e-03],
         [ 1.381e-01],
         [-2.011e-02],
         [-8.371e-02],
         [ 6.473e-02],
         [ 1.504e-02],
         [-6.718e-02],
         [-7.153e-02]],

        [[ 6.946e-03],
         [-2.750e-02],
         [-3.323e-02],
         [-4.890e-02],
         [-6.108e-02],
         [-1.912e-02],
         [ 2.138e-02],
         [ 4.924e-02],
         [ 5.070e-03],
         [-5.867e-02]],

        [[-1.993e-01],
         [-2.534e-02],
         [ 4.259e-02],
         [ 3.263e-02],
         [ 7.225e-02],
         [-1.838e-01],
         [-5.758e-02],
         [-2.055e-02],
         [-1.865e-01],
         [-2.355e-02]],

        [[-2.832e-02],
         [-1.779e-02],
         [-2.670e-02],
         [-2.617e-02],
         [-3.972e-03],
         [ 7.608e-02],
         [-1.106e-01],
         [ 3.368e-02],
         [ 1.726e-01],
         [ 7.101e-02]],

        [[ 4.897e-02],
         [ 6.716e-02],
         [ 2.509e-02],
         [ 1.395e-01],
         [ 1.987e-02],
         [ 9.962e-02],
         [-1.083e-02],
         [ 3.517e-02],
         [-2.078e-01],
         [ 5.304e-02]],

        [[ 1.189e-01],
         [ 1.831e-01],
         [ 1.754e-02],
         [ 1.906e-01],
         [-4.550e-02],
         [-1.243e-02],
         [-2.237e-02],
         [-7.570e-02],
         [ 1.844e-02],
         [ 1.665e-02]],

        [[ 2.635e-01],
         [ 3.793e-02],
         [ 2.186e-01],
         [-5.645e-02],
         [ 1.540e-02],
         [-3.251e-01],
         [ 1.999e-01],
         [-9.569e-02],
         [-2.816e-02],
         [-4.856e-02]],

        [[ 7.411e-02],
         [ 1.881e-01],
         [ 8.044e-02],
         [ 6.375e-02],
         [ 1.354e-03],
         [-6.565e-02],
         [ 7.304e-02],
         [ 5.502e-02],
         [ 8.160e-02],
         [ 1.580e-01]],

        [[ 1.106e-03],
         [-4.033e-02],
         [ 4.363e-03],
         [ 2.407e-01],
         [-4.060e-02],
         [-2.937e-03],
         [-3.781e-02],
         [ 6.430e-02],
         [ 1.274e-01],
         [-3.113e-01]],

        [[-8.327e-02],
         [ 8.715e-02],
         [ 4.866e-03],
         [ 1.886e-01],
         [-4.875e-03],
         [ 1.832e-01],
         [-3.557e-02],
         [ 5.822e-02],
         [ 5.947e-02],
         [-5.204e-02]],

        [[ 9.675e-02],
         [-7.802e-02],
         [-3.373e-02],
         [-4.214e-02],
         [ 6.424e-02],
         [ 8.134e-02],
         [ 9.725e-02],
         [-8.213e-02],
         [-1.312e-01],
         [-5.811e-03]],

        [[-9.829e-02],
         [-9.849e-03],
         [-5.931e-02],
         [ 4.197e-02],
         [-8.328e-03],
         [-5.223e-02],
         [-2.460e-01],
         [-1.393e-01],
         [ 7.325e-02],
         [ 6.099e-02]],

        [[-3.582e-02],
         [-5.294e-03],
         [-2.809e-01],
         [ 1.148e-02],
         [ 1.711e-02],
         [-9.318e-02],
         [ 4.189e-02],
         [-4.302e-02],
         [-1.917e-02],
         [ 2.330e-04]],

        [[ 8.621e-02],
         [ 1.123e-01],
         [-6.796e-02],
         [-3.762e-02],
         [ 4.289e-02],
         [-3.233e-02],
         [ 7.512e-02],
         [-1.639e-02],
         [ 1.486e-01],
         [ 4.620e-02]],

        [[-4.928e-03],
         [-7.835e-02],
         [ 7.863e-02],
         [ 7.569e-02],
         [-4.547e-02],
         [-4.630e-03],
         [ 1.145e-01],
         [-3.437e-02],
         [ 1.413e-01],
         [ 1.995e-02]],

        [[ 1.013e-01],
         [ 1.184e-01],
         [-6.949e-02],
         [-7.636e-02],
         [-6.491e-02],
         [-1.233e-01],
         [ 5.439e-02],
         [ 6.514e-02],
         [ 1.873e-01],
         [-1.974e-02]],

        [[ 5.532e-02],
         [-1.869e-01],
         [ 1.141e-01],
         [ 1.071e-01],
         [-1.061e-01],
         [-4.666e-02],
         [ 5.331e-02],
         [ 3.273e-02],
         [-9.195e-02],
         [ 2.501e-01]]], device='cuda:0')
s after update for 1 param tensor([[[1.952],
         [1.472],
         [2.298],
         [1.721],
         [1.537],
         [1.446],
         [1.997],
         [2.543],
         [1.689],
         [2.051]],

        [[1.377],
         [1.897],
         [1.617],
         [1.797],
         [1.545],
         [2.008],
         [1.847],
         [1.160],
         [1.703],
         [1.516]],

        [[1.987],
         [1.975],
         [1.471],
         [2.001],
         [1.530],
         [1.939],
         [1.258],
         [2.055],
         [1.774],
         [1.185]],

        [[1.948],
         [1.993],
         [1.939],
         [1.470],
         [1.641],
         [1.896],
         [0.962],
         [1.498],
         [1.801],
         [2.299]],

        [[1.714],
         [2.079],
         [1.752],
         [1.975],
         [1.731],
         [2.117],
         [1.424],
         [1.991],
         [2.199],
         [1.829]],

        [[2.078],
         [1.530],
         [1.316],
         [1.288],
         [1.577],
         [1.247],
         [2.224],
         [1.407],
         [2.066],
         [1.925]],

        [[2.066],
         [1.248],
         [2.170],
         [1.726],
         [0.873],
         [1.835],
         [1.441],
         [1.479],
         [1.640],
         [1.639]],

        [[1.259],
         [1.636],
         [1.931],
         [1.558],
         [1.839],
         [2.137],
         [0.837],
         [2.143],
         [1.403],
         [1.983]],

        [[0.457],
         [1.515],
         [1.553],
         [1.480],
         [1.588],
         [1.682],
         [1.655],
         [1.430],
         [2.198],
         [1.229]],

        [[2.038],
         [1.123],
         [1.798],
         [1.872],
         [1.554],
         [1.989],
         [1.290],
         [1.507],
         [1.278],
         [1.469]],

        [[1.503],
         [1.371],
         [1.641],
         [1.835],
         [2.081],
         [1.362],
         [1.790],
         [2.120],
         [1.477],
         [1.673]],

        [[1.465],
         [1.860],
         [1.232],
         [1.661],
         [1.583],
         [1.083],
         [2.028],
         [1.599],
         [1.305],
         [1.584]],

        [[2.188],
         [1.680],
         [1.982],
         [0.863],
         [1.241],
         [1.943],
         [1.253],
         [1.804],
         [1.434],
         [2.344]],

        [[1.838],
         [1.882],
         [1.100],
         [1.821],
         [1.381],
         [1.576],
         [1.857],
         [1.106],
         [1.714],
         [1.827]],

        [[1.657],
         [1.733],
         [1.088],
         [1.810],
         [0.737],
         [1.339],
         [0.669],
         [1.224],
         [1.677],
         [1.888]],

        [[1.752],
         [1.769],
         [1.986],
         [1.754],
         [1.668],
         [2.020],
         [0.921],
         [2.146],
         [1.510],
         [1.582]],

        [[1.402],
         [2.002],
         [1.228],
         [1.163],
         [1.636],
         [1.922],
         [2.212],
         [1.230],
         [2.388],
         [1.560]],

        [[1.735],
         [1.166],
         [2.062],
         [1.685],
         [1.976],
         [1.312],
         [1.224],
         [1.725],
         [2.171],
         [1.958]],

        [[1.699],
         [1.865],
         [1.771],
         [1.919],
         [1.117],
         [1.615],
         [1.689],
         [1.533],
         [1.731],
         [1.441]],

        [[2.016],
         [1.909],
         [1.590],
         [1.640],
         [2.095],
         [1.995],
         [1.807],
         [0.972],
         [1.505],
         [1.172]],

        [[2.009],
         [1.756],
         [1.874],
         [1.469],
         [1.618],
         [1.565],
         [1.418],
         [2.105],
         [1.662],
         [1.831]],

        [[1.057],
         [1.762],
         [1.192],
         [1.984],
         [1.693],
         [1.427],
         [1.838],
         [1.732],
         [2.028],
         [1.754]],

        [[1.260],
         [1.211],
         [2.144],
         [1.438],
         [1.454],
         [1.415],
         [2.169],
         [1.254],
         [1.461],
         [2.045]],

        [[1.830],
         [1.511],
         [1.331],
         [1.658],
         [1.205],
         [1.779],
         [2.014],
         [1.819],
         [1.451],
         [2.009]],

        [[1.207],
         [1.780],
         [1.151],
         [1.866],
         [1.971],
         [1.537],
         [2.050],
         [1.472],
         [1.795],
         [1.749]],

        [[1.722],
         [1.458],
         [1.537],
         [2.083],
         [1.255],
         [1.323],
         [2.164],
         [2.041],
         [2.147],
         [0.914]],

        [[2.027],
         [0.814],
         [1.635],
         [1.228],
         [1.817],
         [1.627],
         [1.699],
         [1.002],
         [1.475],
         [1.470]],

        [[1.589],
         [1.382],
         [2.041],
         [1.499],
         [1.680],
         [1.296],
         [1.671],
         [1.246],
         [1.951],
         [1.503]],

        [[1.628],
         [2.078],
         [1.563],
         [2.163],
         [1.528],
         [1.815],
         [1.597],
         [1.098],
         [1.315],
         [1.746]],

        [[1.439],
         [1.455],
         [1.902],
         [1.546],
         [1.791],
         [1.581],
         [1.338],
         [1.360],
         [1.043],
         [1.789]]], device='cuda:0')
b after update for 1 param tensor([[[179.209],
         [155.648],
         [194.461],
         [168.298],
         [159.030],
         [154.282],
         [181.276],
         [204.586],
         [166.726],
         [183.739]],

        [[150.550],
         [176.686],
         [163.137],
         [171.969],
         [159.434],
         [181.791],
         [174.353],
         [138.178],
         [167.393],
         [157.970]],

        [[180.818],
         [180.296],
         [155.596],
         [181.482],
         [158.670],
         [178.647],
         [143.882],
         [183.902],
         [170.863],
         [139.622]],

        [[179.045],
         [181.096],
         [178.612],
         [155.545],
         [164.320],
         [176.627],
         [125.811],
         [157.033],
         [172.158],
         [194.507]],

        [[167.939],
         [184.953],
         [169.778],
         [180.279],
         [168.800],
         [186.654],
         [153.063],
         [180.994],
         [190.215],
         [173.499]],

        [[184.919],
         [158.667],
         [147.154],
         [145.603],
         [161.104],
         [143.281],
         [191.309],
         [152.187],
         [184.395],
         [177.965]],

        [[184.383],
         [143.310],
         [188.978],
         [168.521],
         [119.830],
         [173.762],
         [154.001],
         [155.987],
         [164.296],
         [164.220]],

        [[143.943],
         [164.066],
         [178.246],
         [160.133],
         [173.975],
         [187.524],
         [117.336],
         [187.815],
         [151.926],
         [180.671]],

        [[ 86.722],
         [157.922],
         [159.890],
         [156.061],
         [161.673],
         [166.353],
         [165.011],
         [153.412],
         [190.179],
         [142.224]],

        [[183.136],
         [135.944],
         [172.003],
         [175.513],
         [159.899],
         [180.916],
         [145.700],
         [157.501],
         [145.014],
         [155.503]],

        [[157.285],
         [150.222],
         [164.326],
         [173.789],
         [185.037],
         [149.740],
         [171.626],
         [186.774],
         [155.928],
         [165.913]],

        [[155.278],
         [174.948],
         [142.392],
         [165.313],
         [161.391],
         [133.497],
         [182.673],
         [162.207],
         [146.566],
         [161.475]],

        [[189.753],
         [166.260],
         [180.597],
         [119.161],
         [142.923],
         [178.803],
         [143.589],
         [172.292],
         [153.636],
         [196.420]],

        [[173.926],
         [175.976],
         [134.564],
         [173.133],
         [150.771],
         [161.048],
         [174.828],
         [134.905],
         [167.946],
         [173.399]],

        [[165.130],
         [168.898],
         [133.814],
         [172.566],
         [110.138],
         [148.423],
         [104.961],
         [141.925],
         [166.117],
         [176.275]],

        [[169.817],
         [170.627],
         [180.772],
         [169.907],
         [165.668],
         [182.315],
         [123.094],
         [187.913],
         [157.617],
         [161.333]],

        [[151.882],
         [181.531],
         [142.155],
         [138.367],
         [164.071],
         [177.840],
         [190.798],
         [142.297],
         [198.247],
         [160.227]],

        [[168.961],
         [138.499],
         [184.210],
         [166.545],
         [180.309],
         [146.967],
         [141.915],
         [168.490],
         [189.029],
         [179.499]],

        [[167.201],
         [175.203],
         [170.721],
         [177.722],
         [135.552],
         [163.023],
         [166.734],
         [158.853],
         [168.789],
         [153.983]],

        [[182.130],
         [177.226],
         [161.777],
         [164.266],
         [185.698],
         [181.204],
         [172.458],
         [126.484],
         [157.380],
         [138.879]],

        [[181.816],
         [169.992],
         [175.635],
         [155.491],
         [163.184],
         [160.461],
         [152.775],
         [186.129],
         [165.375],
         [173.567]],

        [[131.865],
         [170.262],
         [140.086],
         [180.699],
         [166.937],
         [153.254],
         [173.935],
         [168.816],
         [182.684],
         [169.897]],

        [[143.990],
         [141.150],
         [187.851],
         [153.810],
         [154.688],
         [152.583],
         [188.914],
         [143.630],
         [155.057],
         [183.445]],

        [[173.535],
         [157.673],
         [148.008],
         [165.165],
         [140.844],
         [171.125],
         [182.064],
         [173.007],
         [154.507],
         [181.824]],

        [[140.916],
         [171.129],
         [137.632],
         [175.227],
         [180.104],
         [159.042],
         [183.669],
         [155.655],
         [171.870],
         [169.676]],

        [[168.333],
         [154.909],
         [159.017],
         [185.130],
         [143.722],
         [147.552],
         [188.720],
         [183.253],
         [187.984],
         [122.629]],

        [[182.629],
         [115.728],
         [164.034],
         [142.177],
         [172.911],
         [163.618],
         [167.214],
         [128.437],
         [155.780],
         [155.512]],

        [[161.691],
         [150.788],
         [183.275],
         [157.044],
         [166.261],
         [146.055],
         [165.833],
         [143.175],
         [179.192],
         [157.297]],

        [[163.703],
         [184.938],
         [160.356],
         [188.673],
         [158.573],
         [172.826],
         [162.125],
         [134.410],
         [147.125],
         [169.526]],

        [[153.863],
         [154.743],
         [176.909],
         [159.487],
         [171.691],
         [161.322],
         [148.413],
         [149.602],
         [131.013],
         [171.600]]], device='cuda:0')
clipping threshold 1.1917976608417045
a after update for 1 param tensor([[-0.004],
        [ 0.050],
        [ 0.083],
        [ 0.067],
        [-0.021],
        [ 0.146],
        [ 0.122],
        [ 0.071],
        [-0.137],
        [-0.034],
        [-0.090],
        [ 0.050],
        [-0.041],
        [ 0.104],
        [ 0.023],
        [ 0.018],
        [-0.041],
        [ 0.127],
        [ 0.062],
        [ 0.041],
        [-0.177],
        [ 0.171],
        [ 0.080],
        [ 0.001],
        [ 0.182],
        [ 0.148],
        [-0.188],
        [ 0.033],
        [ 0.085],
        [ 0.093]], device='cuda:0')
s after update for 1 param tensor([[1.153],
        [1.520],
        [1.746],
        [1.790],
        [1.794],
        [1.909],
        [1.901],
        [1.377],
        [1.715],
        [1.741],
        [1.671],
        [1.983],
        [1.253],
        [1.434],
        [1.868],
        [1.779],
        [1.486],
        [1.573],
        [0.656],
        [0.387],
        [2.094],
        [1.456],
        [1.934],
        [1.650],
        [2.136],
        [1.840],
        [2.303],
        [1.229],
        [1.375],
        [1.817]], device='cuda:0')
b after update for 1 param tensor([[137.732],
        [158.177],
        [169.510],
        [171.630],
        [171.821],
        [177.254],
        [176.896],
        [150.548],
        [168.008],
        [169.263],
        [165.849],
        [180.659],
        [143.599],
        [153.604],
        [175.319],
        [171.118],
        [156.390],
        [160.902],
        [103.932],
        [ 79.844],
        [185.623],
        [154.792],
        [178.413],
        [164.779],
        [187.474],
        [173.997],
        [194.692],
        [142.193],
        [150.423],
        [172.914]], device='cuda:0')
clipping threshold 1.1917976608417045
||w||^2 1.311932349620676
exp ma of ||w||^2 2.353474930045712
||w|| 1.1453961540099025
exp ma of ||w|| 1.3814014594293866
||w||^2 0.9651728259867703
exp ma of ||w||^2 1.4563452219306243
||w|| 0.9824320973923696
exp ma of ||w|| 1.1718147086837332
cuda
Objective function 69.07 = squared loss an data 55.52 + 0.5*rho*h**2 7.166495 + alpha*h 4.525072 + L2reg 1.56 + L1reg 0.30 ; SHD = 95 ; DAG True
Proportion of microbatches that were clipped  0.7713228194399743
iteration 2 in inner loop, alpha 37.7969799171859 rho 1000.0 h 0.11972046598256725
9630
cuda
Objective function 133.57 = squared loss an data 55.52 + 0.5*rho*h**2 71.664950 + alpha*h 4.525072 + L2reg 1.56 + L1reg 0.30 ; SHD = 95 ; DAG True
||w||^2 53.50812256087215
exp ma of ||w||^2 11464.165169242859
||w|| 7.314924644920969
exp ma of ||w|| 15.545651619918734
||w||^2 1.5990691744260732
exp ma of ||w||^2 2.253185661216792
||w|| 1.264543069423131
exp ma of ||w|| 1.468058865754117
||w||^2 2.982350862774557
exp ma of ||w||^2 1.8903057995533954
||w|| 1.7269484250476494
exp ma of ||w|| 1.3404613195511488
||w||^2 1.2572842140740892
exp ma of ||w||^2 1.7936905752793244
||w|| 1.12128685628348
exp ma of ||w|| 1.3084337049598713
||w||^2 2.168441090813364
exp ma of ||w||^2 1.803996477253364
||w|| 1.472562762945391
exp ma of ||w|| 1.3120566861541483
cuda
Objective function 70.80 = squared loss an data 57.52 + 0.5*rho*h**2 9.723240 + alpha*h 1.666777 + L2reg 1.64 + L1reg 0.25 ; SHD = 93 ; DAG True
Proportion of microbatches that were clipped  0.7758927128776862
iteration 3 in inner loop, alpha 37.7969799171859 rho 10000.0 h 0.04409816214194251
iteration 3 in outer loop, alpha = 478.778601336611, rho = 10000.0, h = 0.04409816214194251
cuda
9630
cuda
Objective function 90.25 = squared loss an data 57.52 + 0.5*rho*h**2 9.723240 + alpha*h 21.113256 + L2reg 1.64 + L1reg 0.25 ; SHD = 93 ; DAG True
||w||^2 10.827106446280792
exp ma of ||w||^2 2286.991811300109
||w|| 3.290456875006994
exp ma of ||w|| 6.568593727331359
||w||^2 9.480111937631863
exp ma of ||w||^2 556.8457548909087
||w|| 3.0789790414408253
exp ma of ||w|| 3.9975206539310126
||w||^2 3.1005467305368923
exp ma of ||w||^2 55.56627538334598
||w|| 1.7608369403601494
exp ma of ||w|| 2.3784310429913313
cuda
Objective function 76.94 = squared loss an data 56.73 + 0.5*rho*h**2 4.264039 + alpha*h 13.981711 + L2reg 1.73 + L1reg 0.24 ; SHD = 92 ; DAG True
Proportion of microbatches that were clipped  0.7817371937639198
iteration 1 in inner loop, alpha 478.778601336611 rho 10000.0 h 0.02920287414712064
9630
cuda
Objective function 115.31 = squared loss an data 56.73 + 0.5*rho*h**2 42.640393 + alpha*h 13.981711 + L2reg 1.73 + L1reg 0.24 ; SHD = 92 ; DAG True
||w||^2 6313260696128.523
exp ma of ||w||^2 3756144368564.826
||w|| 2512620.2849074756
exp ma of ||w|| 1662767.8816430196
||w||^2 760.3748609008054
exp ma of ||w||^2 806523.320031451
||w|| 27.57489548304409
exp ma of ||w|| 69.99798103536423
||w||^2 2.688736505345151
exp ma of ||w||^2 3.7726896913235173
||w|| 1.6397367183011884
exp ma of ||w|| 1.9025905521981097
cuda
Objective function 73.80 = squared loss an data 57.20 + 0.5*rho*h**2 8.408487 + alpha*h 6.208814 + L2reg 1.77 + L1reg 0.21 ; SHD = 91 ; DAG True
Proportion of microbatches that were clipped  0.7792880258899676
iteration 2 in inner loop, alpha 478.778601336611 rho 100000.0 h 0.0129680279127804
iteration 4 in outer loop, alpha = 13446.806514117012, rho = 1000000.0, h = 0.0129680279127804
Threshold 0.3
[[0.003 0.024 0.08  0.068 0.088 0.045 0.061 0.023 0.045 0.074 0.025 0.04
  0.052 0.158 0.059 0.017 0.15  0.073 0.42  0.027 0.127 0.043 0.095 0.039
  0.061 0.029 0.046 0.027 0.016 0.058]
 [0.226 0.004 0.094 0.059 0.078 0.064 0.071 0.023 0.042 0.052 0.035 0.072
  0.063 0.07  0.076 0.055 0.124 0.03  0.298 0.051 0.207 0.13  0.097 0.058
  0.048 0.069 0.067 0.131 0.049 0.044]
 [0.072 0.047 0.003 0.035 0.074 0.034 0.057 0.016 0.025 0.06  0.02  0.027
  0.042 0.006 0.041 0.084 0.061 0.041 0.139 0.033 0.06  0.081 0.069 0.029
  0.052 0.075 0.024 0.039 0.016 0.047]
 [0.08  0.088 0.126 0.004 0.139 0.096 0.078 0.053 0.17  0.087 0.029 0.23
  0.051 0.257 0.095 0.174 0.114 0.074 0.713 0.044 0.112 0.189 0.205 0.165
  0.135 0.06  0.084 0.127 0.025 0.042]
 [0.067 0.046 0.084 0.031 0.003 0.059 0.026 0.033 0.017 0.084 0.021 0.042
  0.06  0.036 0.025 0.044 0.046 0.028 0.139 0.032 0.229 0.03  0.067 0.063
  0.078 0.033 0.049 0.086 0.02  0.033]
 [0.121 0.071 0.155 0.063 0.094 0.005 0.065 0.011 0.113 0.09  0.04  0.123
  0.114 0.06  0.097 0.077 0.117 0.062 0.12  0.026 0.116 0.072 0.051 0.051
  0.044 0.069 0.054 0.069 0.05  0.093]
 [0.097 0.074 0.093 0.058 0.164 0.094 0.004 0.074 0.173 0.234 0.039 0.144
  0.057 0.185 0.059 0.208 0.063 0.061 0.239 0.102 0.245 0.084 0.15  0.109
  0.209 0.017 0.051 0.132 0.064 0.056]
 [0.227 0.217 0.195 0.089 0.219 0.381 0.071 0.002 0.127 0.084 0.065 0.079
  0.1   0.268 0.055 0.073 0.203 0.08  0.45  0.122 0.115 0.157 0.14  0.065
  0.092 0.049 0.089 0.057 0.171 0.072]
 [0.131 0.103 0.14  0.03  0.228 0.041 0.029 0.032 0.003 0.126 0.045 0.076
  0.054 0.063 0.088 0.132 0.384 0.107 0.411 0.099 0.261 0.167 0.086 0.053
  0.11  0.049 0.108 0.081 0.043 0.149]
 [0.092 0.094 0.074 0.039 0.045 0.046 0.024 0.038 0.026 0.004 0.003 0.091
  0.038 0.416 0.041 0.023 0.051 0.041 0.093 0.044 0.149 0.034 0.119 0.031
  0.012 0.009 0.037 0.028 0.067 0.063]
 [0.247 0.205 0.304 0.196 0.249 0.112 0.137 0.076 0.096 0.929 0.003 0.458
  0.218 0.348 0.063 0.294 0.127 0.106 0.453 0.189 0.242 0.137 0.472 0.072
  0.087 0.141 0.071 0.091 0.098 0.272]
 [0.128 0.05  0.214 0.031 0.086 0.045 0.022 0.052 0.056 0.094 0.013 0.005
  0.056 0.361 0.027 0.194 0.099 0.041 0.094 0.037 0.115 0.038 0.47  0.034
  0.065 0.061 0.05  0.025 0.029 0.015]
 [0.098 0.092 0.128 0.076 0.071 0.04  0.079 0.058 0.061 0.13  0.025 0.087
  0.005 0.064 0.119 0.066 0.043 0.057 0.15  0.098 0.118 0.129 0.101 0.034
  0.061 0.068 0.06  0.047 0.041 0.111]
 [0.027 0.068 0.602 0.028 0.126 0.108 0.019 0.025 0.061 0.012 0.013 0.018
  0.079 0.004 0.056 0.049 0.044 0.035 0.151 0.019 0.11  0.068 0.045 0.041
  0.029 0.045 0.037 0.044 0.029 0.047]
 [0.113 0.077 0.116 0.089 0.123 0.044 0.054 0.042 0.045 0.154 0.069 0.131
  0.043 0.093 0.002 0.064 0.16  0.071 0.296 0.097 0.166 0.093 0.175 0.063
  0.046 0.037 0.06  0.217 0.038 0.045]
 [0.166 0.067 0.092 0.035 0.104 0.055 0.023 0.061 0.038 0.237 0.018 0.031
  0.055 0.167 0.08  0.003 0.079 0.037 0.239 0.015 0.068 0.035 0.093 0.081
  0.033 0.016 0.043 0.076 0.077 0.079]
 [0.045 0.032 0.063 0.069 0.13  0.048 0.039 0.021 0.015 0.105 0.037 0.038
  0.082 0.089 0.024 0.046 0.004 0.032 0.161 0.046 0.127 0.047 0.069 0.038
  0.063 0.038 0.04  0.057 0.036 0.024]
 [0.082 0.192 0.08  0.073 0.175 0.061 0.115 0.072 0.049 0.102 0.052 0.103
  0.071 0.09  0.081 0.106 0.149 0.003 0.139 0.078 0.113 0.062 0.166 0.171
  0.085 0.043 0.082 0.116 0.033 0.103]
 [0.01  0.015 0.04  0.004 0.034 0.037 0.021 0.014 0.014 0.055 0.01  0.034
  0.028 0.035 0.015 0.022 0.026 0.03  0.005 0.021 0.064 0.026 0.024 0.011
  0.03  0.038 0.015 0.021 0.023 0.012]
 [0.179 0.083 0.211 0.084 0.179 0.138 0.06  0.027 0.062 0.132 0.039 0.214
  0.05  0.326 0.054 0.223 0.067 0.07  0.203 0.003 0.336 0.112 0.161 0.08
  0.053 0.073 0.062 0.099 0.051 0.066]
 [0.039 0.035 0.08  0.032 0.03  0.037 0.014 0.041 0.023 0.043 0.006 0.035
  0.042 0.048 0.033 0.061 0.045 0.048 0.084 0.018 0.003 0.049 0.05  0.018
  0.031 0.021 0.025 0.038 0.019 0.042]
 [0.137 0.033 0.066 0.042 0.159 0.056 0.053 0.044 0.041 0.07  0.031 0.059
  0.05  0.06  0.061 0.084 0.095 0.082 0.158 0.035 0.094 0.004 0.105 0.061
  0.035 0.047 0.036 0.044 0.032 0.057]
 [0.072 0.048 0.088 0.022 0.086 0.059 0.029 0.036 0.039 0.055 0.01  0.009
  0.04  0.112 0.022 0.077 0.059 0.04  0.211 0.038 0.107 0.071 0.006 0.038
  0.037 0.051 0.004 0.033 0.037 0.022]
 [0.089 0.081 0.147 0.041 0.061 0.094 0.038 0.075 0.084 0.158 0.054 0.11
  0.105 0.11  0.057 0.06  0.107 0.022 0.339 0.095 0.28  0.09  0.15  0.002
  0.125 0.053 0.055 0.064 0.031 0.09 ]
 [0.114 0.101 0.073 0.042 0.074 0.075 0.029 0.055 0.045 0.4   0.06  0.073
  0.077 0.17  0.121 0.12  0.066 0.046 0.1   0.067 0.154 0.085 0.083 0.046
  0.004 0.089 0.06  0.128 0.069 0.056]
 [0.212 0.07  0.089 0.097 0.081 0.084 0.199 0.104 0.093 0.5   0.045 0.083
  0.071 0.123 0.135 0.208 0.127 0.103 0.099 0.054 0.214 0.134 0.092 0.1
  0.086 0.002 0.045 0.175 0.067 0.046]
 [0.099 0.144 0.131 0.074 0.06  0.083 0.09  0.053 0.054 0.139 0.081 0.107
  0.06  0.132 0.106 0.085 0.147 0.068 0.247 0.057 0.178 0.131 0.954 0.104
  0.053 0.054 0.003 0.142 0.059 0.05 ]
 [0.131 0.038 0.118 0.05  0.032 0.075 0.038 0.057 0.08  0.185 0.043 0.2
  0.078 0.104 0.025 0.058 0.089 0.032 0.165 0.061 0.154 0.151 0.131 0.084
  0.05  0.026 0.033 0.004 0.028 0.041]
 [0.187 0.143 0.193 0.164 0.228 0.14  0.066 0.045 0.053 0.111 0.081 0.186
  0.092 0.2   0.158 0.058 0.15  0.143 0.192 0.106 0.179 0.181 0.091 0.199
  0.078 0.074 0.101 0.176 0.003 0.205]
 [0.079 0.085 0.108 0.106 0.169 0.051 0.091 0.063 0.036 0.078 0.018 0.194
  0.035 0.076 0.115 0.063 0.149 0.051 0.201 0.068 0.143 0.114 0.147 0.043
  0.073 0.108 0.087 0.103 0.026 0.005]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.42  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.713 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.381 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.45  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.384 0.    0.411 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.416 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.304 0.    0.    0.    0.    0.    0.    0.929 0.    0.458
  0.    0.348 0.    0.    0.    0.    0.453 0.    0.    0.    0.472 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.361 0.    0.    0.    0.    0.    0.    0.    0.    0.47  0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.602 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.326 0.    0.    0.    0.    0.    0.    0.336 0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.339 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.4   0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.954 0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5909090909090909, 'tpr': 0.1, 'fpr': 0.03768115942028986, 'f1': 0.16071428571428573, 'shd': 91, 'npred': 22, 'ntrue': 90}
[0.024 0.08  0.068 0.088 0.045 0.061 0.023 0.045 0.074 0.025 0.04  0.052
 0.158 0.059 0.017 0.15  0.073 0.42  0.027 0.127 0.043 0.095 0.039 0.061
 0.029 0.046 0.027 0.016 0.058 0.226 0.094 0.059 0.078 0.064 0.071 0.023
 0.042 0.052 0.035 0.072 0.063 0.07  0.076 0.055 0.124 0.03  0.298 0.051
 0.207 0.13  0.097 0.058 0.048 0.069 0.067 0.131 0.049 0.044 0.072 0.047
 0.035 0.074 0.034 0.057 0.016 0.025 0.06  0.02  0.027 0.042 0.006 0.041
 0.084 0.061 0.041 0.139 0.033 0.06  0.081 0.069 0.029 0.052 0.075 0.024
 0.039 0.016 0.047 0.08  0.088 0.126 0.139 0.096 0.078 0.053 0.17  0.087
 0.029 0.23  0.051 0.257 0.095 0.174 0.114 0.074 0.713 0.044 0.112 0.189
 0.205 0.165 0.135 0.06  0.084 0.127 0.025 0.042 0.067 0.046 0.084 0.031
 0.059 0.026 0.033 0.017 0.084 0.021 0.042 0.06  0.036 0.025 0.044 0.046
 0.028 0.139 0.032 0.229 0.03  0.067 0.063 0.078 0.033 0.049 0.086 0.02
 0.033 0.121 0.071 0.155 0.063 0.094 0.065 0.011 0.113 0.09  0.04  0.123
 0.114 0.06  0.097 0.077 0.117 0.062 0.12  0.026 0.116 0.072 0.051 0.051
 0.044 0.069 0.054 0.069 0.05  0.093 0.097 0.074 0.093 0.058 0.164 0.094
 0.074 0.173 0.234 0.039 0.144 0.057 0.185 0.059 0.208 0.063 0.061 0.239
 0.102 0.245 0.084 0.15  0.109 0.209 0.017 0.051 0.132 0.064 0.056 0.227
 0.217 0.195 0.089 0.219 0.381 0.071 0.127 0.084 0.065 0.079 0.1   0.268
 0.055 0.073 0.203 0.08  0.45  0.122 0.115 0.157 0.14  0.065 0.092 0.049
 0.089 0.057 0.171 0.072 0.131 0.103 0.14  0.03  0.228 0.041 0.029 0.032
 0.126 0.045 0.076 0.054 0.063 0.088 0.132 0.384 0.107 0.411 0.099 0.261
 0.167 0.086 0.053 0.11  0.049 0.108 0.081 0.043 0.149 0.092 0.094 0.074
 0.039 0.045 0.046 0.024 0.038 0.026 0.003 0.091 0.038 0.416 0.041 0.023
 0.051 0.041 0.093 0.044 0.149 0.034 0.119 0.031 0.012 0.009 0.037 0.028
 0.067 0.063 0.247 0.205 0.304 0.196 0.249 0.112 0.137 0.076 0.096 0.929
 0.458 0.218 0.348 0.063 0.294 0.127 0.106 0.453 0.189 0.242 0.137 0.472
 0.072 0.087 0.141 0.071 0.091 0.098 0.272 0.128 0.05  0.214 0.031 0.086
 0.045 0.022 0.052 0.056 0.094 0.013 0.056 0.361 0.027 0.194 0.099 0.041
 0.094 0.037 0.115 0.038 0.47  0.034 0.065 0.061 0.05  0.025 0.029 0.015
 0.098 0.092 0.128 0.076 0.071 0.04  0.079 0.058 0.061 0.13  0.025 0.087
 0.064 0.119 0.066 0.043 0.057 0.15  0.098 0.118 0.129 0.101 0.034 0.061
 0.068 0.06  0.047 0.041 0.111 0.027 0.068 0.602 0.028 0.126 0.108 0.019
 0.025 0.061 0.012 0.013 0.018 0.079 0.056 0.049 0.044 0.035 0.151 0.019
 0.11  0.068 0.045 0.041 0.029 0.045 0.037 0.044 0.029 0.047 0.113 0.077
 0.116 0.089 0.123 0.044 0.054 0.042 0.045 0.154 0.069 0.131 0.043 0.093
 0.064 0.16  0.071 0.296 0.097 0.166 0.093 0.175 0.063 0.046 0.037 0.06
 0.217 0.038 0.045 0.166 0.067 0.092 0.035 0.104 0.055 0.023 0.061 0.038
 0.237 0.018 0.031 0.055 0.167 0.08  0.079 0.037 0.239 0.015 0.068 0.035
 0.093 0.081 0.033 0.016 0.043 0.076 0.077 0.079 0.045 0.032 0.063 0.069
 0.13  0.048 0.039 0.021 0.015 0.105 0.037 0.038 0.082 0.089 0.024 0.046
 0.032 0.161 0.046 0.127 0.047 0.069 0.038 0.063 0.038 0.04  0.057 0.036
 0.024 0.082 0.192 0.08  0.073 0.175 0.061 0.115 0.072 0.049 0.102 0.052
 0.103 0.071 0.09  0.081 0.106 0.149 0.139 0.078 0.113 0.062 0.166 0.171
 0.085 0.043 0.082 0.116 0.033 0.103 0.01  0.015 0.04  0.004 0.034 0.037
 0.021 0.014 0.014 0.055 0.01  0.034 0.028 0.035 0.015 0.022 0.026 0.03
 0.021 0.064 0.026 0.024 0.011 0.03  0.038 0.015 0.021 0.023 0.012 0.179
 0.083 0.211 0.084 0.179 0.138 0.06  0.027 0.062 0.132 0.039 0.214 0.05
 0.326 0.054 0.223 0.067 0.07  0.203 0.336 0.112 0.161 0.08  0.053 0.073
 0.062 0.099 0.051 0.066 0.039 0.035 0.08  0.032 0.03  0.037 0.014 0.041
 0.023 0.043 0.006 0.035 0.042 0.048 0.033 0.061 0.045 0.048 0.084 0.018
 0.049 0.05  0.018 0.031 0.021 0.025 0.038 0.019 0.042 0.137 0.033 0.066
 0.042 0.159 0.056 0.053 0.044 0.041 0.07  0.031 0.059 0.05  0.06  0.061
 0.084 0.095 0.082 0.158 0.035 0.094 0.105 0.061 0.035 0.047 0.036 0.044
 0.032 0.057 0.072 0.048 0.088 0.022 0.086 0.059 0.029 0.036 0.039 0.055
 0.01  0.009 0.04  0.112 0.022 0.077 0.059 0.04  0.211 0.038 0.107 0.071
 0.038 0.037 0.051 0.004 0.033 0.037 0.022 0.089 0.081 0.147 0.041 0.061
 0.094 0.038 0.075 0.084 0.158 0.054 0.11  0.105 0.11  0.057 0.06  0.107
 0.022 0.339 0.095 0.28  0.09  0.15  0.125 0.053 0.055 0.064 0.031 0.09
 0.114 0.101 0.073 0.042 0.074 0.075 0.029 0.055 0.045 0.4   0.06  0.073
 0.077 0.17  0.121 0.12  0.066 0.046 0.1   0.067 0.154 0.085 0.083 0.046
 0.089 0.06  0.128 0.069 0.056 0.212 0.07  0.089 0.097 0.081 0.084 0.199
 0.104 0.093 0.5   0.045 0.083 0.071 0.123 0.135 0.208 0.127 0.103 0.099
 0.054 0.214 0.134 0.092 0.1   0.086 0.045 0.175 0.067 0.046 0.099 0.144
 0.131 0.074 0.06  0.083 0.09  0.053 0.054 0.139 0.081 0.107 0.06  0.132
 0.106 0.085 0.147 0.068 0.247 0.057 0.178 0.131 0.954 0.104 0.053 0.054
 0.142 0.059 0.05  0.131 0.038 0.118 0.05  0.032 0.075 0.038 0.057 0.08
 0.185 0.043 0.2   0.078 0.104 0.025 0.058 0.089 0.032 0.165 0.061 0.154
 0.151 0.131 0.084 0.05  0.026 0.033 0.028 0.041 0.187 0.143 0.193 0.164
 0.228 0.14  0.066 0.045 0.053 0.111 0.081 0.186 0.092 0.2   0.158 0.058
 0.15  0.143 0.192 0.106 0.179 0.181 0.091 0.199 0.078 0.074 0.101 0.176
 0.205 0.079 0.085 0.108 0.106 0.169 0.051 0.091 0.063 0.036 0.078 0.018
 0.194 0.035 0.076 0.115 0.063 0.149 0.051 0.201 0.068 0.143 0.114 0.147
 0.043 0.073 0.108 0.087 0.103 0.026]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  0. 1. 0. 0. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 1. 0.]
 [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
  0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
  0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.5559401709401709, 0.20261980053963732)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
