samples  5000  SCM  10 2 RE mim  minibatch size  50  noise  0.6  minibatches per NN training  250  DP methodology  plain_vanilla 10.0  box penalty  1
cpu
max degree  2.0  complexity indicator 1  625.0  complexity indicator 2  1086.9565217391305
model created with box penalty
model created with box penalty
cpu
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 0.2855731205864114
iteration 1 in outer loop, alpha = 0.2855731205864114, rho = 1.0, h = 0.2855731205864114
model created with box penalty
cpu
iteration 1 in inner loop,alpha 0.2855731205864114 rho 1.0 h 0.15778945700017388
iteration 2 in inner loop,alpha 0.2855731205864114 rho 10.0 h 0.057422677149482126
iteration 2 in outer loop, alpha = 0.8597998920812326, rho = 10.0, h = 0.057422677149482126
model created with box penalty
cpu
iteration 1 in inner loop,alpha 0.8597998920812326 rho 10.0 h 0.0378950349318643
iteration 2 in inner loop,alpha 0.8597998920812326 rho 100.0 h 0.016471852091774153
iteration 3 in inner loop,alpha 0.8597998920812326 rho 1000.0 h 0.001737601982998882
iteration 3 in outer loop, alpha = 2.5974018750801147, rho = 1000.0, h = 0.001737601982998882
model created with box penalty
cpu
iteration 1 in inner loop,alpha 2.5974018750801147 rho 1000.0 h 0.0009355762507770038
iteration 2 in inner loop,alpha 2.5974018750801147 rho 10000.0 h 0.0003313582199417908
iteration 4 in outer loop, alpha = 5.910984074498023, rho = 10000.0, h = 0.0003313582199417908
model created with box penalty
cpu
iteration 1 in inner loop,alpha 5.910984074498023 rho 10000.0 h 0.0001902268167022214
iteration 2 in inner loop,alpha 5.910984074498023 rho 100000.0 h 7.034699317998161e-05
iteration 5 in outer loop, alpha = 12.945683392496184, rho = 100000.0, h = 7.034699317998161e-05
model created with box penalty
cpu
iteration 1 in inner loop,alpha 12.945683392496184 rho 100000.0 h 4.035102040589322e-05
iteration 6 in outer loop, alpha = 53.29670379838941, rho = 1000000.0, h = 4.035102040589322e-05
Threshold 0.3
[[0.    0.    2.425 0.    0.139 0.    0.096 0.    0.    0.013]
 [2.391 0.    0.    0.    0.    0.    0.    0.    0.    0.013]
 [0.002 0.    0.    0.    0.    0.    1.048 0.    0.    0.   ]
 [2.185 0.    0.026 0.    0.79  0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.003]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.001 0.    0.002 0.    0.    0.    0.    0.    0.    0.021]
 [0.001 0.    0.    0.    0.062 0.    0.    0.    0.    2.961]
 [0.015 0.    0.    0.    0.    0.    1.416 0.    0.    2.676]
 [0.    0.    0.    0.    1.826 0.    0.02  0.    0.    0.   ]]
[[0.    0.    2.425 0.    0.    0.    0.    0.    0.    0.   ]
 [2.391 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.048 0.    0.    0.   ]
 [2.185 0.    0.    0.    0.79  0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    2.961]
 [0.    0.    0.    0.    0.    0.    1.416 0.    0.    2.676]
 [0.    0.    0.    0.    1.826 0.    0.    0.    0.    0.   ]]
{'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'f1': 1.0, 'shd': 0, 'npred': 9, 'ntrue': 9}
[0.000e+00 2.425e+00 0.000e+00 1.392e-01 0.000e+00 9.592e-02 0.000e+00
 0.000e+00 1.287e-02 2.391e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
 0.000e+00 0.000e+00 0.000e+00 1.278e-02 1.605e-03 0.000e+00 0.000e+00
 0.000e+00 0.000e+00 1.048e+00 0.000e+00 0.000e+00 0.000e+00 2.185e+00
 0.000e+00 2.555e-02 7.895e-01 0.000e+00 0.000e+00 0.000e+00 0.000e+00
 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
 0.000e+00 0.000e+00 2.520e-03 0.000e+00 0.000e+00 0.000e+00 0.000e+00
 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.360e-03 0.000e+00
 1.796e-03 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.051e-02
 1.132e-03 0.000e+00 0.000e+00 0.000e+00 6.220e-02 0.000e+00 0.000e+00
 0.000e+00 2.961e+00 1.497e-02 0.000e+00 0.000e+00 0.000e+00 0.000e+00
 0.000e+00 1.416e+00 0.000e+00 2.676e+00 0.000e+00 0.000e+00 0.000e+00
 0.000e+00 1.826e+00 0.000e+00 2.029e-02 0.000e+00 0.000e+00]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (1.0, 1.0)
model created with box penalty
cpu
cpu
[0.677 0.971 0.689 0.72  0.572 0.805 0.858 0.756 0.945 0.892 0.832 0.999
 0.701 0.819 0.962 0.884 0.92  0.795 0.857 0.662 0.911 0.542 1.    0.831
 0.714 0.796 0.787 0.638 0.895 0.724 0.98  0.773 0.774 1.043 0.695 0.666
 0.874 0.634 0.669 0.644 1.119 0.724 0.793 0.718 0.874 0.939 0.946 0.811
 0.665 0.591 0.727 0.612 0.705 0.758 0.93  0.992 1.106 0.61  0.742 1.111
 0.988 0.962 0.805 0.585 0.422 0.708 0.696 1.144 0.799 0.772 0.964 0.851
 0.734 0.693 0.799 0.876 0.556 0.783 0.827 0.857 0.498 0.664 0.728 0.759
 0.857 0.843 0.79  1.055 0.884 0.778]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.5747599451303156, 0.13480677069860666)
Objective function 18342.84 = squared loss an data 10.47 + 0.5*rho*h**2 14845.719911 + alpha*h 0.000000 + L2reg 0.36 + L1reg -0.10 + box penalty 3486.40 ; SHD = 55 ; DAG False
microbatch grad before clipping 17782.4146098303
microbatch grad before clipping 711.9711161339665
microbatch grad before clipping 218.87130755880594
microbatch grad before clipping 9.277357726450024
microbatch grad before clipping 5.985680402513438
microbatch grad before clipping 7.44720560789631
microbatch grad before clipping 10.550136662260448
microbatch grad before clipping 7.171929015235418
microbatch grad before clipping 11.64064183431166
microbatch grad before clipping 7.448962170139205
microbatch grad before clipping 5.882846894953602
microbatch grad before clipping 6.848672690059136
microbatch grad before clipping 10.463252970708922
cpu
[0.254 0.285 0.413 0.288 0.16  0.157 0.106 0.174 0.237 0.386 0.268 0.174
 0.182 0.095 0.218 0.106 0.138 0.203 0.432 0.149 0.09  0.141 0.277 0.334
 0.203 0.127 0.157 0.352 0.099 0.202 0.192 0.172 0.133 0.121 0.085 0.117
 0.074 0.119 0.193 0.173 0.167 0.182 0.156 0.099 0.281 0.124 0.247 0.09
 0.148 0.172 0.248 0.177 0.139 0.078 0.182 0.13  0.276 0.28  0.237 0.125
 0.244 0.28  0.127 0.256 0.262 0.174 0.211 0.249 0.143 0.181 0.109 0.575
 0.155 0.289 0.199 0.313 0.32  0.079 0.413 0.099 0.36  0.127 0.154 0.199
 0.081 0.345 0.191 0.132 0.222 0.225]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9341563786008231, 0.6635361552028218)
Objective function 6.88 = squared loss an data 4.93 + 0.5*rho*h**2 0.479529 + alpha*h 0.000000 + L2reg 0.25 + L1reg 1.19 + box penalty 0.03 ; SHD = 10 ; DAG False
Proportion of microbatches that were clipped  0.5031020408163265
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 0.9793155103774609
iteration 1 in outer loop, alpha = 0.9793155103774609, rho = 1.0, h = 0.9793155103774609
cpu
cpu
[0.254 0.285 0.413 0.288 0.16  0.157 0.106 0.174 0.237 0.386 0.268 0.174
 0.182 0.095 0.218 0.106 0.138 0.203 0.432 0.149 0.09  0.141 0.277 0.334
 0.203 0.127 0.157 0.352 0.099 0.202 0.192 0.172 0.133 0.121 0.085 0.117
 0.074 0.119 0.193 0.173 0.167 0.182 0.156 0.099 0.281 0.124 0.247 0.09
 0.148 0.172 0.248 0.177 0.139 0.078 0.182 0.13  0.276 0.28  0.237 0.125
 0.244 0.28  0.127 0.256 0.262 0.174 0.211 0.249 0.143 0.181 0.109 0.575
 0.155 0.289 0.199 0.313 0.32  0.079 0.413 0.099 0.36  0.127 0.154 0.199
 0.081 0.345 0.191 0.132 0.222 0.225]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9341563786008231, 0.6635361552028218)
Objective function 7.84 = squared loss an data 4.93 + 0.5*rho*h**2 0.479529 + alpha*h 0.959059 + L2reg 0.25 + L1reg 1.19 + box penalty 0.03 ; SHD = 10 ; DAG False
microbatch grad before clipping 8.844715163279236
microbatch grad before clipping 11.089224470403604
microbatch grad before clipping 8.609597864344845
microbatch grad before clipping 21.788666118272282
microbatch grad before clipping 6.785960554631513
microbatch grad before clipping 19.48704689743825
cpu
[0.356 0.313 0.202 0.18  0.246 0.166 0.176 0.211 0.139 0.323 0.218 0.196
 0.15  0.107 0.089 0.163 0.155 0.193 0.24  0.096 0.133 0.213 0.239 0.249
 0.155 0.228 0.105 0.389 0.222 0.223 0.284 0.125 0.145 0.069 0.123 0.153
 0.125 0.172 0.1   0.154 0.263 0.171 0.184 0.112 0.247 0.11  0.216 0.116
 0.172 0.079 0.146 0.197 0.188 0.154 0.148 0.112 0.271 0.285 0.09  0.144
 0.18  0.303 0.117 0.162 0.187 0.206 0.156 0.207 0.099 0.147 0.257 0.313
 0.1   0.228 0.198 0.115 0.245 0.225 0.269 0.123 0.242 0.169 0.13  0.125
 0.132 0.304 0.126 0.184 0.319 0.256]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9519890260631002, 0.6423280423280424)
Objective function 6.96 = squared loss an data 4.51 + 0.5*rho*h**2 0.220368 + alpha*h 0.650147 + L2reg 0.45 + L1reg 1.11 + box penalty 0.02 ; SHD = 9 ; DAG False
Proportion of microbatches that were clipped  0.7375937450135631
iteration 1 in inner loop, alpha 0.9793155103774609 rho 1.0 h 0.6638794181739627
cpu
[0.356 0.313 0.202 0.18  0.246 0.166 0.176 0.211 0.139 0.323 0.218 0.196
 0.15  0.107 0.089 0.163 0.155 0.193 0.24  0.096 0.133 0.213 0.239 0.249
 0.155 0.228 0.105 0.389 0.222 0.223 0.284 0.125 0.145 0.069 0.123 0.153
 0.125 0.172 0.1   0.154 0.263 0.171 0.184 0.112 0.247 0.11  0.216 0.116
 0.172 0.079 0.146 0.197 0.188 0.154 0.148 0.112 0.271 0.285 0.09  0.144
 0.18  0.303 0.117 0.162 0.187 0.206 0.156 0.207 0.099 0.147 0.257 0.313
 0.1   0.228 0.198 0.115 0.245 0.225 0.269 0.123 0.242 0.169 0.13  0.125
 0.132 0.304 0.126 0.184 0.319 0.256]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9519890260631002, 0.6423280423280424)
Objective function 8.94 = squared loss an data 4.51 + 0.5*rho*h**2 2.203679 + alpha*h 0.650147 + L2reg 0.45 + L1reg 1.11 + box penalty 0.02 ; SHD = 9 ; DAG False
microbatch grad before clipping 12.071822330501474
microbatch grad before clipping 14.37557940573203
microbatch grad before clipping 11.769650576001776
microbatch grad before clipping 17.943960584933617
microbatch grad before clipping 19.298626411238278
microbatch grad before clipping 9.257311414311575
microbatch grad before clipping 20.326700300746037
microbatch grad before clipping 10.699732009881334
microbatch grad before clipping 26.230432658247857
cpu
[0.242 0.255 0.155 0.169 0.083 0.122 0.15  0.161 0.144 0.327 0.109 0.096
 0.182 0.106 0.164 0.135 0.153 0.171 0.283 0.26  0.137 0.204 0.134 0.219
 0.187 0.08  0.194 0.371 0.348 0.241 0.172 0.158 0.236 0.173 0.12  0.116
 0.222 0.154 0.082 0.188 0.177 0.217 0.207 0.103 0.216 0.214 0.215 0.216
 0.163 0.141 0.085 0.123 0.08  0.09  0.173 0.265 0.231 0.137 0.104 0.117
 0.132 0.218 0.127 0.124 0.196 0.168 0.122 0.11  0.21  0.129 0.113 0.378
 0.19  0.197 0.243 0.206 0.21  0.267 0.324 0.162 0.249 0.129 0.164 0.16
 0.157 0.348 0.153 0.103 0.213 0.17 ]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9218106995884773, 0.6830510619984305)
Objective function 7.68 = squared loss an data 5.31 + 0.5*rho*h**2 0.326040 + alpha*h 0.250077 + L2reg 0.56 + L1reg 1.18 + box penalty 0.05 ; SHD = 5 ; DAG True
Proportion of microbatches that were clipped  0.862077333764763
iteration 2 in inner loop, alpha 0.9793155103774609 rho 10.0 h 0.2553585139343113
cpu
[0.242 0.255 0.155 0.169 0.083 0.122 0.15  0.161 0.144 0.327 0.109 0.096
 0.182 0.106 0.164 0.135 0.153 0.171 0.283 0.26  0.137 0.204 0.134 0.219
 0.187 0.08  0.194 0.371 0.348 0.241 0.172 0.158 0.236 0.173 0.12  0.116
 0.222 0.154 0.082 0.188 0.177 0.217 0.207 0.103 0.216 0.214 0.215 0.216
 0.163 0.141 0.085 0.123 0.08  0.09  0.173 0.265 0.231 0.137 0.104 0.117
 0.132 0.218 0.127 0.124 0.196 0.168 0.122 0.11  0.21  0.129 0.113 0.378
 0.19  0.197 0.243 0.206 0.21  0.267 0.324 0.162 0.249 0.129 0.164 0.16
 0.157 0.348 0.153 0.103 0.213 0.17 ]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9218106995884773, 0.6830510619984305)
Objective function 10.61 = squared loss an data 5.31 + 0.5*rho*h**2 3.260399 + alpha*h 0.250077 + L2reg 0.56 + L1reg 1.18 + box penalty 0.05 ; SHD = 5 ; DAG True
microbatch grad before clipping 23.495910915450974
microbatch grad before clipping 31.49663267858505
microbatch grad before clipping 26.76709013768742
microbatch grad before clipping 17.618870360995917
microbatch grad before clipping 17.68662730526234
microbatch grad before clipping 16.343431184641737
microbatch grad before clipping 15.449758599708938
microbatch grad before clipping 13.218045460597882
microbatch grad before clipping 28.523609906888016
microbatch grad before clipping 23.799264030331013
microbatch grad before clipping 20.038827148443474
cpu
[0.16  0.27  0.037 0.146 0.12  0.139 0.101 0.121 0.097 0.321 0.078 0.074
 0.162 0.129 0.172 0.098 0.164 0.199 0.236 0.205 0.131 0.15  0.114 0.213
 0.123 0.198 0.072 0.533 0.197 0.124 0.184 0.144 0.152 0.129 0.108 0.151
 0.127 0.155 0.144 0.152 0.224 0.117 0.123 0.15  0.277 0.098 0.178 0.104
 0.107 0.089 0.144 0.146 0.075 0.146 0.102 0.112 0.18  0.122 0.147 0.128
 0.171 0.106 0.07  0.15  0.173 0.145 0.12  0.168 0.107 0.097 0.175 0.54
 0.121 0.061 0.127 0.173 0.152 0.131 0.342 0.125 0.244 0.165 0.09  0.163
 0.169 0.222 0.099 0.151 0.037 0.09 ]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9794238683127573, 0.8533609908609909)
Objective function 8.39 = squared loss an data 6.23 + 0.5*rho*h**2 0.280314 + alpha*h 0.073326 + L2reg 0.59 + L1reg 1.19 + box penalty 0.03 ; SHD = 5 ; DAG True
Proportion of microbatches that were clipped  0.9366388977891701
iteration 3 in inner loop, alpha 0.9793155103774609 rho 100.0 h 0.07487508809104426
iteration 2 in outer loop, alpha = 8.466824319481887, rho = 100.0, h = 0.07487508809104426
cpu
cpu
[0.16  0.27  0.037 0.146 0.12  0.139 0.101 0.121 0.097 0.321 0.078 0.074
 0.162 0.129 0.172 0.098 0.164 0.199 0.236 0.205 0.131 0.15  0.114 0.213
 0.123 0.198 0.072 0.533 0.197 0.124 0.184 0.144 0.152 0.129 0.108 0.151
 0.127 0.155 0.144 0.152 0.224 0.117 0.123 0.15  0.277 0.098 0.178 0.104
 0.107 0.089 0.144 0.146 0.075 0.146 0.102 0.112 0.18  0.122 0.147 0.128
 0.171 0.106 0.07  0.15  0.173 0.145 0.12  0.168 0.107 0.097 0.175 0.54
 0.121 0.061 0.127 0.173 0.152 0.131 0.342 0.125 0.244 0.165 0.09  0.163
 0.169 0.222 0.099 0.151 0.037 0.09 ]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9794238683127573, 0.8533609908609909)
Objective function 8.95 = squared loss an data 6.23 + 0.5*rho*h**2 0.280314 + alpha*h 0.633954 + L2reg 0.59 + L1reg 1.19 + box penalty 0.03 ; SHD = 5 ; DAG True
microbatch grad before clipping 14.332777914306005
microbatch grad before clipping 10.050399902262448
microbatch grad before clipping 23.608415001569522
microbatch grad before clipping 16.23159831049573
microbatch grad before clipping 13.44602838390435
microbatch grad before clipping 14.104376056118294
microbatch grad before clipping 15.325372799285459
microbatch grad before clipping 18.227919984028826
microbatch grad before clipping 15.383022398009581
microbatch grad before clipping 24.03594921374297
microbatch grad before clipping 19.519753321449777
microbatch grad before clipping 13.31927677930316
microbatch grad before clipping 9.96603220877711
microbatch grad before clipping 24.653815008608376
microbatch grad before clipping 15.602796266104672
microbatch grad before clipping 29.25424180993229
microbatch grad before clipping 25.743869504849236
microbatch grad before clipping 22.362118105538684
microbatch grad before clipping 11.751997002062563
microbatch grad before clipping 29.279618799663055
microbatch grad before clipping 23.451598648278257
cpu
[0.088 0.147 0.052 0.11  0.196 0.098 0.138 0.127 0.134 0.368 0.114 0.216
 0.125 0.115 0.226 0.092 0.046 0.128 0.214 0.127 0.154 0.068 0.152 0.215
 0.087 0.075 0.103 0.461 0.049 0.083 0.24  0.102 0.142 0.146 0.111 0.154
 0.064 0.125 0.219 0.096 0.089 0.182 0.104 0.072 0.202 0.059 0.084 0.082
 0.14  0.157 0.119 0.113 0.151 0.109 0.109 0.09  0.189 0.132 0.08  0.064
 0.092 0.057 0.106 0.079 0.126 0.157 0.127 0.131 0.116 0.122 0.18  0.556
 0.19  0.191 0.232 0.108 0.165 0.143 0.353 0.088 0.37  0.122 0.109 0.084
 0.096 0.255 0.061 0.114 0.054 0.059]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9684499314128945, 0.8875661375661377)
Objective function 8.93 = squared loss an data 6.66 + 0.5*rho*h**2 0.072288 + alpha*h 0.321936 + L2reg 0.59 + L1reg 1.24 + box penalty 0.04 ; SHD = 4 ; DAG True
Proportion of microbatches that were clipped  0.9525596997044965
iteration 1 in inner loop, alpha 8.466824319481887 rho 100.0 h 0.038023248190867776
cpu
[0.088 0.147 0.052 0.11  0.196 0.098 0.138 0.127 0.134 0.368 0.114 0.216
 0.125 0.115 0.226 0.092 0.046 0.128 0.214 0.127 0.154 0.068 0.152 0.215
 0.087 0.075 0.103 0.461 0.049 0.083 0.24  0.102 0.142 0.146 0.111 0.154
 0.064 0.125 0.219 0.096 0.089 0.182 0.104 0.072 0.202 0.059 0.084 0.082
 0.14  0.157 0.119 0.113 0.151 0.109 0.109 0.09  0.189 0.132 0.08  0.064
 0.092 0.057 0.106 0.079 0.126 0.157 0.127 0.131 0.116 0.122 0.18  0.556
 0.19  0.191 0.232 0.108 0.165 0.143 0.353 0.088 0.37  0.122 0.109 0.084
 0.096 0.255 0.061 0.114 0.054 0.059]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9684499314128945, 0.8875661375661377)
Objective function 9.58 = squared loss an data 6.66 + 0.5*rho*h**2 0.722884 + alpha*h 0.321936 + L2reg 0.59 + L1reg 1.24 + box penalty 0.04 ; SHD = 4 ; DAG True
microbatch grad before clipping 25.388429395628055
microbatch grad before clipping 25.599473630477377
microbatch grad before clipping 23.381701655442203
microbatch grad before clipping 35.52850610236471
microbatch grad before clipping 24.41516322654656
microbatch grad before clipping 10.673019959858598
microbatch grad before clipping 22.587263514497643
microbatch grad before clipping 45.505313469776
microbatch grad before clipping 25.95820236372332
microbatch grad before clipping 14.892467345902194
microbatch grad before clipping 18.040159622781662
microbatch grad before clipping 31.33688337655399
microbatch grad before clipping 11.529080623937078
cpu
[0.031 0.173 0.036 0.105 0.133 0.114 0.063 0.065 0.106 0.443 0.106 0.113
 0.124 0.12  0.106 0.054 0.084 0.118 0.302 0.134 0.101 0.105 0.136 0.196
 0.056 0.082 0.139 0.548 0.124 0.151 0.132 0.142 0.155 0.053 0.071 0.158
 0.146 0.092 0.125 0.068 0.097 0.14  0.053 0.115 0.165 0.096 0.059 0.054
 0.101 0.095 0.104 0.135 0.097 0.109 0.129 0.086 0.152 0.087 0.103 0.111
 0.086 0.026 0.085 0.141 0.173 0.178 0.186 0.167 0.103 0.146 0.082 0.753
 0.103 0.121 0.159 0.114 0.1   0.091 0.412 0.115 0.273 0.099 0.079 0.059
 0.071 0.2   0.095 0.143 0.019 0.037]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9602194787379973, 0.8358294930875578)
Objective function 9.15 = squared loss an data 6.99 + 0.5*rho*h**2 0.135905 + alpha*h 0.139589 + L2reg 0.56 + L1reg 1.28 + box penalty 0.04 ; SHD = 5 ; DAG True
Proportion of microbatches that were clipped  0.9683676754668383
iteration 2 in inner loop, alpha 8.466824319481887 rho 1000.0 h 0.016486632833547787
iteration 3 in outer loop, alpha = 24.953457153029674, rho = 1000.0, h = 0.016486632833547787
cpu
cpu
[0.031 0.173 0.036 0.105 0.133 0.114 0.063 0.065 0.106 0.443 0.106 0.113
 0.124 0.12  0.106 0.054 0.084 0.118 0.302 0.134 0.101 0.105 0.136 0.196
 0.056 0.082 0.139 0.548 0.124 0.151 0.132 0.142 0.155 0.053 0.071 0.158
 0.146 0.092 0.125 0.068 0.097 0.14  0.053 0.115 0.165 0.096 0.059 0.054
 0.101 0.095 0.104 0.135 0.097 0.109 0.129 0.086 0.152 0.087 0.103 0.111
 0.086 0.026 0.085 0.141 0.173 0.178 0.186 0.167 0.103 0.146 0.082 0.753
 0.103 0.121 0.159 0.114 0.1   0.091 0.412 0.115 0.273 0.099 0.079 0.059
 0.071 0.2   0.095 0.143 0.019 0.037]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9602194787379973, 0.8358294930875578)
Objective function 9.42 = squared loss an data 6.99 + 0.5*rho*h**2 0.135905 + alpha*h 0.411398 + L2reg 0.56 + L1reg 1.28 + box penalty 0.04 ; SHD = 5 ; DAG True
microbatch grad before clipping 47.29415971119296
microbatch grad before clipping 21.233599884198874
microbatch grad before clipping 32.00872074136749
microbatch grad before clipping 14.037418626755489
microbatch grad before clipping 20.88332552037945
microbatch grad before clipping 15.139677156140781
microbatch grad before clipping 22.86162401471817
cpu
[0.021 0.15  0.021 0.09  0.072 0.048 0.085 0.059 0.162 0.636 0.239 0.114
 0.103 0.077 0.041 0.054 0.097 0.162 0.213 0.027 0.054 0.098 0.112 0.095
 0.049 0.105 0.076 0.66  0.074 0.219 0.175 0.074 0.23  0.088 0.139 0.077
 0.102 0.068 0.084 0.059 0.123 0.064 0.053 0.121 0.076 0.185 0.139 0.097
 0.128 0.06  0.104 0.142 0.054 0.128 0.205 0.147 0.238 0.062 0.154 0.126
 0.06  0.02  0.101 0.078 0.168 0.129 0.135 0.175 0.125 0.117 0.18  0.729
 0.133 0.135 0.09  0.114 0.106 0.186 0.491 0.057 0.438 0.057 0.082 0.101
 0.112 0.316 0.06  0.128 0.016 0.025]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9080932784636488, 0.77454974271012)
Objective function 9.28 = squared loss an data 7.20 + 0.5*rho*h**2 0.054075 + alpha*h 0.259503 + L2reg 0.53 + L1reg 1.18 + box penalty 0.06 ; SHD = 3 ; DAG True
Proportion of microbatches that were clipped  0.9739842620844709
iteration 1 in inner loop, alpha 24.953457153029674 rho 1000.0 h 0.010399474959534416
cpu
[0.021 0.15  0.021 0.09  0.072 0.048 0.085 0.059 0.162 0.636 0.239 0.114
 0.103 0.077 0.041 0.054 0.097 0.162 0.213 0.027 0.054 0.098 0.112 0.095
 0.049 0.105 0.076 0.66  0.074 0.219 0.175 0.074 0.23  0.088 0.139 0.077
 0.102 0.068 0.084 0.059 0.123 0.064 0.053 0.121 0.076 0.185 0.139 0.097
 0.128 0.06  0.104 0.142 0.054 0.128 0.205 0.147 0.238 0.062 0.154 0.126
 0.06  0.02  0.101 0.078 0.168 0.129 0.135 0.175 0.125 0.117 0.18  0.729
 0.133 0.135 0.09  0.114 0.106 0.186 0.491 0.057 0.438 0.057 0.082 0.101
 0.112 0.316 0.06  0.128 0.016 0.025]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.9080932784636488, 0.77454974271012)
Objective function 9.76 = squared loss an data 7.20 + 0.5*rho*h**2 0.540745 + alpha*h 0.259503 + L2reg 0.53 + L1reg 1.18 + box penalty 0.06 ; SHD = 3 ; DAG True
microbatch grad before clipping 26.86556096462105
microbatch grad before clipping 12.236843293722247
microbatch grad before clipping 19.13909428567655
microbatch grad before clipping 16.62743254691878
microbatch grad before clipping 12.18755158731883
microbatch grad before clipping 18.281612115664988
microbatch grad before clipping 43.97222831409809
microbatch grad before clipping 13.479093887796726
microbatch grad before clipping 40.69729892577079
microbatch grad before clipping 52.19485358705273
microbatch grad before clipping 23.58700279811938
microbatch grad before clipping 52.4380474081098
microbatch grad before clipping 10.667098177665098
cpu
[0.014 0.025 0.012 0.092 0.039 0.09  0.062 0.022 0.068 0.695 0.354 0.066
 0.177 0.183 0.135 0.054 0.073 0.065 0.352 0.025 0.045 0.042 0.108 0.278
 0.06  0.058 0.11  0.696 0.119 0.145 0.241 0.107 0.173 0.12  0.096 0.201
 0.089 0.051 0.178 0.025 0.046 0.071 0.027 0.028 0.048 0.145 0.046 0.096
 0.059 0.142 0.126 0.045 0.063 0.119 0.099 0.023 0.068 0.045 0.101 0.081
 0.08  0.013 0.135 0.137 0.12  0.087 0.047 0.27  0.181 0.049 0.13  0.82
 0.206 0.093 0.148 0.051 0.148 0.107 0.536 0.064 0.576 0.114 0.082 0.09
 0.042 0.255 0.064 0.084 0.01  0.009]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8888888888888888, 0.809669869425967)
Objective function 9.28 = squared loss an data 7.38 + 0.5*rho*h**2 0.085354 + alpha*h 0.103099 + L2reg 0.50 + L1reg 1.18 + box penalty 0.03 ; SHD = 5 ; DAG True
Proportion of microbatches that were clipped  0.9833333333333333
iteration 2 in inner loop, alpha 24.953457153029674 rho 10000.0 h 0.0041316718295192345
cpu
[0.014 0.025 0.012 0.092 0.039 0.09  0.062 0.022 0.068 0.695 0.354 0.066
 0.177 0.183 0.135 0.054 0.073 0.065 0.352 0.025 0.045 0.042 0.108 0.278
 0.06  0.058 0.11  0.696 0.119 0.145 0.241 0.107 0.173 0.12  0.096 0.201
 0.089 0.051 0.178 0.025 0.046 0.071 0.027 0.028 0.048 0.145 0.046 0.096
 0.059 0.142 0.126 0.045 0.063 0.119 0.099 0.023 0.068 0.045 0.101 0.081
 0.08  0.013 0.135 0.137 0.12  0.087 0.047 0.27  0.181 0.049 0.13  0.82
 0.206 0.093 0.148 0.051 0.148 0.107 0.536 0.064 0.576 0.114 0.082 0.09
 0.042 0.255 0.064 0.084 0.01  0.009]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8888888888888888, 0.809669869425967)
Objective function 10.04 = squared loss an data 7.38 + 0.5*rho*h**2 0.853536 + alpha*h 0.103099 + L2reg 0.50 + L1reg 1.18 + box penalty 0.03 ; SHD = 5 ; DAG True
microbatch grad before clipping 18.53038964256758
microbatch grad before clipping 32.4877296171664
microbatch grad before clipping 19.249867833001247
microbatch grad before clipping 17.413329210396313
microbatch grad before clipping 31.912463167044574
microbatch grad before clipping 21.208259162500283
microbatch grad before clipping 20.303160895224195
microbatch grad before clipping 29.78248494331178
microbatch grad before clipping 20.33005954001501
microbatch grad before clipping 15.847410007488833
microbatch grad before clipping 15.84396803322159
microbatch grad before clipping 14.823704126452798
microbatch grad before clipping 34.41190586349117
microbatch grad before clipping 20.877300504447113
cpu
[0.006 0.014 0.007 0.087 0.04  0.112 0.027 0.021 0.057 0.68  0.22  0.032
 0.268 0.026 0.063 0.09  0.055 0.138 0.515 0.021 0.028 0.072 0.071 0.257
 0.023 0.025 0.101 0.93  0.21  0.268 0.173 0.107 0.21  0.047 0.095 0.084
 0.059 0.015 0.06  0.034 0.026 0.149 0.009 0.029 0.026 0.11  0.144 0.069
 0.042 0.203 0.108 0.035 0.068 0.121 0.031 0.053 0.027 0.023 0.03  0.029
 0.065 0.014 0.034 0.149 0.048 0.208 0.095 0.223 0.115 0.085 0.051 0.813
 0.237 0.086 0.172 0.066 0.18  0.07  0.487 0.096 0.493 0.081 0.033 0.054
 0.034 0.292 0.023 0.169 0.003 0.007]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8724279835390947, 0.746379019444035)
Objective function 9.27 = squared loss an data 7.43 + 0.5*rho*h**2 0.104229 + alpha*h 0.036028 + L2reg 0.43 + L1reg 1.23 + box penalty 0.02 ; SHD = 4 ; DAG True
Proportion of microbatches that were clipped  0.9990420691306777
iteration 3 in inner loop, alpha 24.953457153029674 rho 100000.0 h 0.0014438105604064333
iteration 4 in outer loop, alpha = 169.334513193673, rho = 100000.0, h = 0.0014438105604064333
cpu
cpu
[0.006 0.014 0.007 0.087 0.04  0.112 0.027 0.021 0.057 0.68  0.22  0.032
 0.268 0.026 0.063 0.09  0.055 0.138 0.515 0.021 0.028 0.072 0.071 0.257
 0.023 0.025 0.101 0.93  0.21  0.268 0.173 0.107 0.21  0.047 0.095 0.084
 0.059 0.015 0.06  0.034 0.026 0.149 0.009 0.029 0.026 0.11  0.144 0.069
 0.042 0.203 0.108 0.035 0.068 0.121 0.031 0.053 0.027 0.023 0.03  0.029
 0.065 0.014 0.034 0.149 0.048 0.208 0.095 0.223 0.115 0.085 0.051 0.813
 0.237 0.086 0.172 0.066 0.18  0.07  0.487 0.096 0.493 0.081 0.033 0.054
 0.034 0.292 0.023 0.169 0.003 0.007]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8724279835390947, 0.746379019444035)
Objective function 9.48 = squared loss an data 7.43 + 0.5*rho*h**2 0.104229 + alpha*h 0.244487 + L2reg 0.43 + L1reg 1.23 + box penalty 0.02 ; SHD = 4 ; DAG True
microbatch grad before clipping 61.9462661319276
microbatch grad before clipping 17.671021592048184
microbatch grad before clipping 23.843250364779138
microbatch grad before clipping 13.735822917393948
microbatch grad before clipping 24.230036951728398
microbatch grad before clipping 26.899804091929198
microbatch grad before clipping 19.222675353165357
microbatch grad before clipping 26.317422637690957
microbatch grad before clipping 15.63509199451738
microbatch grad before clipping 15.235906763151942
microbatch grad before clipping 16.148996416393803
microbatch grad before clipping 31.78690444945281
microbatch grad before clipping 26.374479631123712
microbatch grad before clipping 29.47014227789389
cpu
[0.006 0.008 0.003 0.051 0.026 0.063 0.038 0.042 0.066 0.847 0.199 0.131
 0.091 0.101 0.096 0.123 0.082 0.106 0.546 0.025 0.011 0.144 0.037 0.433
 0.058 0.029 0.077 0.95  0.046 0.332 0.131 0.054 0.1   0.036 0.088 0.094
 0.091 0.063 0.036 0.03  0.053 0.033 0.012 0.017 0.01  0.15  0.035 0.091
 0.042 0.056 0.04  0.075 0.019 0.059 0.065 0.043 0.013 0.021 0.12  0.082
 0.028 0.007 0.035 0.093 0.025 0.1   0.067 0.153 0.055 0.093 0.071 1.032
 0.108 0.046 0.131 0.046 0.166 0.124 0.618 0.053 0.631 0.058 0.032 0.064
 0.04  0.466 0.073 0.121 0.004 0.006]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8806584362139918, 0.8120681605975724)
Objective function 9.36 = squared loss an data 7.45 + 0.5*rho*h**2 0.045133 + alpha*h 0.160882 + L2reg 0.41 + L1reg 1.26 + box penalty 0.04 ; SHD = 3 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 1 in inner loop, alpha 169.334513193673 rho 100000.0 h 0.000950086083438606
iteration 5 in outer loop, alpha = 1119.420596632279, rho = 1000000.0, h = 0.000950086083438606
Threshold 0.3
[[0.004 0.006 0.008 0.003 0.051 0.026 0.063 0.038 0.042 0.066]
 [0.847 0.005 0.199 0.131 0.091 0.101 0.096 0.123 0.082 0.106]
 [0.546 0.025 0.005 0.011 0.144 0.037 0.433 0.058 0.029 0.077]
 [0.95  0.046 0.332 0.002 0.131 0.054 0.1   0.036 0.088 0.094]
 [0.091 0.063 0.036 0.03  0.003 0.053 0.033 0.012 0.017 0.01 ]
 [0.15  0.035 0.091 0.042 0.056 0.006 0.04  0.075 0.019 0.059]
 [0.065 0.043 0.013 0.021 0.12  0.082 0.004 0.028 0.007 0.035]
 [0.093 0.025 0.1   0.067 0.153 0.055 0.093 0.003 0.071 1.032]
 [0.108 0.046 0.131 0.046 0.166 0.124 0.618 0.053 0.004 0.631]
 [0.058 0.032 0.064 0.04  0.466 0.073 0.121 0.004 0.006 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.847 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.546 0.    0.    0.    0.    0.    0.433 0.    0.    0.   ]
 [0.95  0.    0.332 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.032]
 [0.    0.    0.    0.    0.    0.    0.618 0.    0.    0.631]
 [0.    0.    0.    0.    0.466 0.    0.    0.    0.    0.   ]]
{'fdr': 0.2222222222222222, 'tpr': 0.7777777777777778, 'fpr': 0.05555555555555555, 'f1': 0.7777777777777778, 'shd': 3, 'npred': 9, 'ntrue': 9}
[0.006 0.008 0.003 0.051 0.026 0.063 0.038 0.042 0.066 0.847 0.199 0.131
 0.091 0.101 0.096 0.123 0.082 0.106 0.546 0.025 0.011 0.144 0.037 0.433
 0.058 0.029 0.077 0.95  0.046 0.332 0.131 0.054 0.1   0.036 0.088 0.094
 0.091 0.063 0.036 0.03  0.053 0.033 0.012 0.017 0.01  0.15  0.035 0.091
 0.042 0.056 0.04  0.075 0.019 0.059 0.065 0.043 0.013 0.021 0.12  0.082
 0.028 0.007 0.035 0.093 0.025 0.1   0.067 0.153 0.055 0.093 0.071 1.032
 0.108 0.046 0.131 0.046 0.166 0.124 0.618 0.053 0.631 0.058 0.032 0.064
 0.04  0.466 0.073 0.121 0.004 0.006]
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
aucroc, aucpr (0.8806584362139918, 0.8120681605975724)
Iterations 2500
Achieves (13.610683049707138, 1e-05)-DP
