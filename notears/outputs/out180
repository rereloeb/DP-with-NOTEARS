samples  5000  graph  20 80 ER mlp  minibatch size  50  noise  0.6  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7622777646747636
iteration 1 in outer loop, alpha = 1.7622777646747636, rho = 1.0, h = 1.7622777646747636
cuda
iteration 1 in inner loop,alpha 1.7622777646747636 rho 1.0 h 1.1769941079911774
iteration 2 in inner loop,alpha 1.7622777646747636 rho 10.0 h 0.546699080645368
iteration 3 in inner loop,alpha 1.7622777646747636 rho 100.0 h 0.18576399615164618
iteration 2 in outer loop, alpha = 20.338677379839382, rho = 100.0, h = 0.18576399615164618
cuda
iteration 1 in inner loop,alpha 20.338677379839382 rho 100.0 h 0.1095608426621908
iteration 2 in inner loop,alpha 20.338677379839382 rho 1000.0 h 0.04370983218316127
iteration 3 in outer loop, alpha = 64.04850956300065, rho = 1000.0, h = 0.04370983218316127
cuda
iteration 1 in inner loop,alpha 64.04850956300065 rho 1000.0 h 0.023592627798716848
iteration 2 in inner loop,alpha 64.04850956300065 rho 10000.0 h 0.008006307647868027
iteration 4 in outer loop, alpha = 144.11158604168094, rho = 10000.0, h = 0.008006307647868027
cuda
iteration 1 in inner loop,alpha 144.11158604168094 rho 10000.0 h 0.004045666927897429
iteration 2 in inner loop,alpha 144.11158604168094 rho 100000.0 h 0.0014008827351261743
iteration 5 in outer loop, alpha = 284.19985955429837, rho = 100000.0, h = 0.0014008827351261743
cuda
iteration 1 in inner loop,alpha 284.19985955429837 rho 100000.0 h 0.0006641462601493231
iteration 6 in outer loop, alpha = 948.3461197036215, rho = 1000000.0, h = 0.0006641462601493231
Threshold 0.3
[[0.001 3.147 0.077 0.241 0.906 1.119 0.317 0.    1.948 1.112 0.124 1.123
  0.274 0.001 0.367 0.434 0.296 1.21  1.296 0.473]
 [0.    0.001 0.023 0.492 0.486 1.651 1.788 0.    0.571 0.462 0.093 0.677
  0.317 0.001 1.179 0.298 0.361 0.862 0.482 1.89 ]
 [0.002 0.009 0.001 0.058 0.439 0.552 1.519 0.    0.187 0.467 0.094 0.19
  1.302 0.    1.493 0.317 0.605 0.    1.19  0.334]
 [0.    0.    0.    0.02  0.001 0.003 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.205 0.231 0.184 0.    0.001 0.715]
 [0.    0.    0.    0.57  0.004 0.319 0.221 0.    0.    0.404 0.002 0.001
  0.003 0.    0.213 0.341 1.206 0.    0.344 1.059]
 [0.    0.    0.    0.493 0.002 0.003 0.002 0.    0.    0.    0.    0.
  0.    0.    0.348 1.658 1.799 0.    0.296 0.3  ]
 [0.    0.    0.    0.2   0.001 0.224 0.002 0.    0.    0.009 0.    0.
  0.    0.    0.328 0.349 0.297 0.    0.168 0.198]
 [2.493 0.131 0.    0.101 0.704 0.223 0.721 0.001 0.301 0.452 0.133 0.382
  1.953 0.69  0.221 0.186 0.18  1.13  0.493 0.449]
 [0.    0.    0.002 0.349 1.588 0.945 1.296 0.    0.001 0.824 0.44  0.296
  1.769 0.    1.214 1.257 0.813 0.    1.33  0.69 ]
 [0.    0.    0.    0.099 0.016 2.2   0.118 0.    0.    0.008 0.001 0.001
  0.    0.    1.176 1.414 1.74  0.    0.816 0.227]
 [0.    0.    0.    0.944 0.107 0.589 2.321 0.    0.    0.257 0.001 0.001
  0.    0.    1.509 0.472 1.023 0.    0.279 1.304]
 [0.    0.    0.001 0.143 0.439 0.258 1.666 0.    0.    0.247 0.126 0.001
  0.114 0.    0.162 0.823 0.279 0.    0.12  0.215]
 [0.    0.    0.    0.105 0.139 1.384 1.078 0.    0.    0.173 2.142 0.002
  0.001 0.    0.302 0.323 0.294 0.    0.483 1.079]
 [0.156 0.037 0.033 0.739 0.337 1.63  0.323 0.001 0.288 0.351 1.71  0.533
  2.059 0.001 0.435 0.439 1.018 0.355 0.731 1.957]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.254 0.003 0.    0.001 0.002]
 [0.    0.    0.    0.003 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.001 0.006 0.001 0.    0.001 0.   ]
 [0.    0.    0.    0.003 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.1   1.19  0.005 0.    0.001 0.002]
 [0.    0.    0.031 0.044 0.453 0.291 1.386 0.    3.187 0.546 0.095 0.727
  0.082 0.    0.343 0.191 0.184 0.001 0.421 0.244]
 [0.    0.    0.    0.734 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.333 1.057 0.389 0.    0.001 0.235]
 [0.    0.    0.    0.003 0.    0.    0.001 0.    0.    0.001 0.    0.
  0.    0.    0.115 1.753 0.272 0.    0.001 0.002]]
[[0.    3.147 0.    0.    0.906 1.119 0.317 0.    1.948 1.112 0.    1.123
  0.    0.    0.367 0.434 0.    1.21  1.296 0.473]
 [0.    0.    0.    0.492 0.486 1.651 1.788 0.    0.571 0.462 0.    0.677
  0.317 0.    1.179 0.    0.361 0.862 0.482 1.89 ]
 [0.    0.    0.    0.    0.439 0.552 1.519 0.    0.    0.467 0.    0.
  1.302 0.    1.493 0.317 0.605 0.    1.19  0.334]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.715]
 [0.    0.    0.    0.57  0.    0.319 0.    0.    0.    0.404 0.    0.
  0.    0.    0.    0.341 1.206 0.    0.344 1.059]
 [0.    0.    0.    0.493 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.348 1.658 1.799 0.    0.    0.3  ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.328 0.349 0.    0.    0.    0.   ]
 [2.493 0.    0.    0.    0.704 0.    0.721 0.    0.301 0.452 0.    0.382
  1.953 0.69  0.    0.    0.    1.13  0.493 0.449]
 [0.    0.    0.    0.349 1.588 0.945 1.296 0.    0.    0.824 0.44  0.
  1.769 0.    1.214 1.257 0.813 0.    1.33  0.69 ]
 [0.    0.    0.    0.    0.    2.2   0.    0.    0.    0.    0.    0.
  0.    0.    1.176 1.414 1.74  0.    0.816 0.   ]
 [0.    0.    0.    0.944 0.    0.589 2.321 0.    0.    0.    0.    0.
  0.    0.    1.509 0.472 1.023 0.    0.    1.304]
 [0.    0.    0.    0.    0.439 0.    1.666 0.    0.    0.    0.    0.
  0.    0.    0.    0.823 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.384 1.078 0.    0.    0.    2.142 0.
  0.    0.    0.302 0.323 0.    0.    0.483 1.079]
 [0.    0.    0.    0.739 0.337 1.63  0.323 0.    0.    0.351 1.71  0.533
  2.059 0.    0.435 0.439 1.018 0.355 0.731 1.957]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.1   1.19  0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.453 0.    1.386 0.    3.187 0.546 0.    0.727
  0.    0.    0.343 0.    0.    0.    0.421 0.   ]
 [0.    0.    0.    0.734 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.333 1.057 0.389 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.753 0.    0.    0.    0.   ]]
{'fdr': 0.43089430894308944, 'tpr': 0.875, 'fpr': 0.4818181818181818, 'f1': 0.689655172413793, 'shd': 58, 'npred': 123, 'ntrue': 80}
[3.147e+00 7.672e-02 2.412e-01 9.059e-01 1.119e+00 3.171e-01 2.482e-04
 1.948e+00 1.112e+00 1.245e-01 1.123e+00 2.741e-01 1.392e-03 3.675e-01
 4.338e-01 2.958e-01 1.210e+00 1.296e+00 4.729e-01 4.078e-04 2.304e-02
 4.918e-01 4.863e-01 1.651e+00 1.788e+00 1.804e-04 5.707e-01 4.616e-01
 9.305e-02 6.769e-01 3.173e-01 1.143e-03 1.179e+00 2.982e-01 3.615e-01
 8.622e-01 4.818e-01 1.890e+00 2.463e-03 9.035e-03 5.792e-02 4.393e-01
 5.517e-01 1.519e+00 2.698e-04 1.868e-01 4.666e-01 9.447e-02 1.898e-01
 1.302e+00 2.740e-04 1.493e+00 3.172e-01 6.051e-01 1.855e-04 1.190e+00
 3.341e-01 8.247e-05 1.484e-05 9.268e-05 6.824e-04 2.789e-03 7.561e-04
 6.048e-05 1.004e-04 2.069e-03 4.228e-04 2.407e-04 8.838e-05 3.508e-06
 2.050e-01 2.311e-01 1.844e-01 2.039e-05 1.237e-03 7.151e-01 3.708e-06
 1.255e-05 2.176e-05 5.699e-01 3.191e-01 2.209e-01 1.914e-05 1.545e-04
 4.038e-01 1.674e-03 1.212e-03 2.806e-03 6.299e-05 2.129e-01 3.412e-01
 1.206e+00 2.873e-05 3.435e-01 1.059e+00 8.650e-05 2.310e-04 7.711e-05
 4.931e-01 2.156e-03 1.522e-03 5.210e-05 8.565e-05 4.389e-04 5.434e-05
 3.359e-04 2.604e-04 3.773e-05 3.484e-01 1.658e+00 1.799e+00 1.309e-05
 2.956e-01 3.004e-01 5.893e-05 9.175e-05 2.347e-05 2.000e-01 7.544e-04
 2.240e-01 3.376e-05 9.076e-06 9.419e-03 1.368e-04 2.123e-04 6.933e-05
 5.664e-06 3.280e-01 3.486e-01 2.967e-01 1.988e-06 1.681e-01 1.978e-01
 2.493e+00 1.314e-01 2.046e-04 1.007e-01 7.045e-01 2.225e-01 7.207e-01
 3.015e-01 4.522e-01 1.334e-01 3.822e-01 1.953e+00 6.896e-01 2.213e-01
 1.859e-01 1.800e-01 1.130e+00 4.930e-01 4.488e-01 1.281e-05 1.371e-05
 2.081e-03 3.494e-01 1.588e+00 9.452e-01 1.296e+00 1.108e-05 8.241e-01
 4.400e-01 2.964e-01 1.769e+00 2.234e-05 1.214e+00 1.257e+00 8.133e-01
 2.868e-05 1.330e+00 6.901e-01 2.639e-05 3.058e-05 1.068e-04 9.906e-02
 1.632e-02 2.200e+00 1.184e-01 3.459e-05 1.751e-04 6.106e-04 1.328e-03
 3.221e-04 1.363e-04 1.176e+00 1.414e+00 1.740e+00 3.210e-05 8.163e-01
 2.270e-01 7.760e-06 2.675e-05 6.867e-05 9.438e-01 1.069e-01 5.894e-01
 2.321e+00 2.317e-05 1.249e-04 2.570e-01 7.354e-04 2.276e-04 5.475e-05
 1.509e+00 4.718e-01 1.023e+00 1.417e-05 2.788e-01 1.304e+00 2.747e-05
 6.754e-05 5.476e-04 1.431e-01 4.387e-01 2.578e-01 1.666e+00 2.086e-05
 2.247e-04 2.466e-01 1.263e-01 1.136e-01 4.632e-05 1.624e-01 8.229e-01
 2.793e-01 1.724e-04 1.202e-01 2.155e-01 5.441e-06 1.970e-05 1.898e-04
 1.047e-01 1.393e-01 1.384e+00 1.078e+00 3.247e-05 2.591e-04 1.731e-01
 2.142e+00 1.596e-03 8.250e-05 3.025e-01 3.225e-01 2.939e-01 2.730e-05
 4.827e-01 1.079e+00 1.556e-01 3.734e-02 3.278e-02 7.394e-01 3.369e-01
 1.630e+00 3.231e-01 6.689e-04 2.875e-01 3.511e-01 1.710e+00 5.331e-01
 2.059e+00 4.354e-01 4.391e-01 1.018e+00 3.546e-01 7.312e-01 1.957e+00
 4.607e-05 7.660e-05 3.376e-05 3.791e-03 1.174e-03 7.965e-04 3.861e-04
 3.160e-05 5.760e-06 4.359e-05 9.222e-05 2.304e-04 5.702e-05 1.924e-05
 2.544e-01 2.789e-03 2.672e-06 5.842e-04 2.123e-03 2.829e-05 2.303e-05
 3.281e-05 2.580e-03 6.197e-04 1.238e-04 3.598e-04 2.571e-05 2.270e-05
 2.655e-04 9.440e-05 7.273e-05 2.603e-05 1.380e-05 1.175e-03 1.083e-03
 5.664e-06 5.749e-04 2.780e-04 5.376e-05 9.486e-05 3.193e-05 2.978e-03
 9.489e-04 6.401e-04 8.575e-04 4.507e-05 3.091e-05 5.882e-05 7.902e-05
 9.373e-05 2.596e-05 4.517e-05 1.100e+00 1.190e+00 5.584e-06 6.592e-04
 1.602e-03 9.377e-05 6.053e-06 3.111e-02 4.429e-02 4.535e-01 2.906e-01
 1.386e+00 9.397e-05 3.187e+00 5.461e-01 9.491e-02 7.274e-01 8.247e-02
 1.364e-04 3.427e-01 1.910e-01 1.839e-01 4.211e-01 2.439e-01 6.790e-05
 1.596e-05 1.569e-04 7.338e-01 1.004e-03 7.710e-04 9.834e-04 4.886e-05
 1.627e-04 5.249e-04 2.392e-04 3.273e-04 1.550e-04 2.260e-05 3.327e-01
 1.057e+00 3.895e-01 1.841e-05 2.354e-01 3.909e-05 6.079e-05 7.471e-05
 2.706e-03 2.640e-04 3.530e-04 5.755e-04 2.027e-05 5.949e-05 9.690e-04
 3.519e-04 1.378e-04 2.290e-04 2.452e-05 1.147e-01 1.753e+00 2.725e-01
 1.581e-05 6.558e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.9364583333333333, 0.8885089293610698)
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
total norm for a microbatch 48.347575717616095 clip 47.87110448512058
total norm for a microbatch 53.153895091591615 clip 53.13773334235893
total norm for a microbatch 56.35204528483009 clip 51.56798348801208
total norm for a microbatch 72.038979088991 clip 51.058683234167304
total norm for a microbatch 52.692961302705875 clip 56.401003485731216
total norm for a microbatch 89.77965106226962 clip 61.48006517009044
total norm for a microbatch 109.6567886206514 clip 64.68235802646491
total norm for a microbatch 56.22158649835322 clip 69.95759947990808
total norm for a microbatch 128.70134588126072 clip 71.77247392892679
total norm for a microbatch 86.50495833823305 clip 80.4246281323188
total norm for a microbatch 89.50091315529153 clip 82.33726923444868
total norm for a microbatch 161.12838069897882 clip 82.59499090833486
cuda
Objective function 35.79 = squared loss an data 29.02 + 0.5*rho*h**2 5.110241 + alpha*h 0.000000 + L2reg 1.37 + L1reg 0.29 ; SHD = 123 ; DAG False
Proportion of microbatches that were clipped  0.8148925860119528
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 3.1969489955677446
iteration 1 in outer loop, alpha = 3.1969489955677446, rho = 1.0, h = 3.1969489955677446
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 46.01 = squared loss an data 29.02 + 0.5*rho*h**2 5.110241 + alpha*h 10.220483 + L2reg 1.37 + L1reg 0.29 ; SHD = 123 ; DAG False
total norm for a microbatch 99.81017303307961 clip 1.2048967501798744
total norm for a microbatch 112.51061565590443 clip 5.734361006279214
total norm for a microbatch 103.6202196561228 clip 12.067532816689715
total norm for a microbatch 111.29699650629506 clip 102.5641479611188
total norm for a microbatch 136.13529904637272 clip 100.01539867148162
total norm for a microbatch 182.4254854270853 clip 106.30759236693576
total norm for a microbatch 125.08742350370775 clip 106.58661829270945
total norm for a microbatch 223.88930520462682 clip 113.6620391760303
total norm for a microbatch 197.23210931871253 clip 109.23372102439247
total norm for a microbatch 109.88158809823825 clip 109.37486612002849
total norm for a microbatch 142.6302483906574 clip 111.71540603792945
total norm for a microbatch 101.15673966687413 clip 109.56892364215224
cuda
Objective function 34.63 = squared loss an data 23.40 + 0.5*rho*h**2 2.260971 + alpha*h 6.798266 + L2reg 1.90 + L1reg 0.27 ; SHD = 107 ; DAG False
Proportion of microbatches that were clipped  0.8179597818122608
iteration 1 in inner loop, alpha 3.1969489955677446 rho 1.0 h 2.1264856701849517
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 54.98 = squared loss an data 23.40 + 0.5*rho*h**2 22.609707 + alpha*h 6.798266 + L2reg 1.90 + L1reg 0.27 ; SHD = 107 ; DAG False
total norm for a microbatch 212.18965949195857 clip 6.535292006935008
total norm for a microbatch 205.5927312525574 clip 115.50648841305446
total norm for a microbatch 166.96379634295855 clip 137.37923458836832
total norm for a microbatch 185.14024223315292 clip 139.66179991467402
total norm for a microbatch 182.96005940249927 clip 138.76424886320385
total norm for a microbatch 114.86652463465221 clip 135.0036280378856
total norm for a microbatch 118.8020308592492 clip 135.10391238942657
total norm for a microbatch 208.5207159900024 clip 138.17905900604538
total norm for a microbatch 156.95729155325782 clip 134.5076365953644
total norm for a microbatch 225.27247575920717 clip 134.1769733425636
total norm for a microbatch 136.43304549792296 clip 130.65114643889777
cuda
Objective function 36.80 = squared loss an data 26.33 + 0.5*rho*h**2 4.875207 + alpha*h 3.156801 + L2reg 2.19 + L1reg 0.25 ; SHD = 98 ; DAG False
Proportion of microbatches that were clipped  0.8228204098031338
iteration 2 in inner loop, alpha 3.1969489955677446 rho 10.0 h 0.9874418119749038
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 80.67 = squared loss an data 26.33 + 0.5*rho*h**2 48.752067 + alpha*h 3.156801 + L2reg 2.19 + L1reg 0.25 ; SHD = 98 ; DAG False
total norm for a microbatch 161.65833925403058 clip 2.948653418152897
total norm for a microbatch 193.36914391243852 clip 92.30126067140115
total norm for a microbatch 230.58551053617262 clip 175.05216380907214
total norm for a microbatch 337.18359191476156 clip 157.75192279852956
total norm for a microbatch 211.44702269025967 clip 155.65468940578398
total norm for a microbatch 230.00386143202604 clip 145.05676891944304
total norm for a microbatch 197.77442899744517 clip 151.23780378486137
total norm for a microbatch 190.83374154792622 clip 162.51974962085035
cuda
Objective function 40.24 = squared loss an data 29.78 + 0.5*rho*h**2 6.656047 + alpha*h 1.166431 + L2reg 2.42 + L1reg 0.22 ; SHD = 96 ; DAG True
Proportion of microbatches that were clipped  0.8251095601363415
iteration 3 in inner loop, alpha 3.1969489955677446 rho 100.0 h 0.3648574370224331
iteration 2 in outer loop, alpha = 39.682692697811056, rho = 100.0, h = 0.3648574370224331
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 53.55 = squared loss an data 29.78 + 0.5*rho*h**2 6.656047 + alpha*h 14.478526 + L2reg 2.42 + L1reg 0.22 ; SHD = 96 ; DAG True
total norm for a microbatch 170.18194610890245 clip 1.8892737511837898
total norm for a microbatch 165.08074055623612 clip 4.232688379914981
total norm for a microbatch 319.64461224814085 clip 12.870221525249661
total norm for a microbatch 141.18951625026816 clip 22.389119785841025
total norm for a microbatch 196.88870428975852 clip 176.81559423752174
total norm for a microbatch 343.74502702323144 clip 181.71171152936878
total norm for a microbatch 193.53521878642377 clip 185.73715796884017
total norm for a microbatch 316.2727925441254 clip 167.7935443659359
total norm for a microbatch 148.90879697712703 clip 176.73635827927444
total norm for a microbatch 255.05251315989895 clip 183.4203095468716
total norm for a microbatch 335.37063880109815 clip 168.6386152470894
total norm for a microbatch 246.56797996614858 clip 186.30999866778123
total norm for a microbatch 269.6649742170807 clip 172.5928010903143
total norm for a microbatch 177.0389214497474 clip 162.5348406132519
total norm for a microbatch 190.0947865350317 clip 155.46992255180538
cuda
Objective function 45.89 = squared loss an data 30.70 + 0.5*rho*h**2 2.842802 + alpha*h 9.462142 + L2reg 2.66 + L1reg 0.22 ; SHD = 91 ; DAG True
Proportion of microbatches that were clipped  0.8252144359928791
iteration 1 in inner loop, alpha 39.682692697811056 rho 100.0 h 0.23844505720605014
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 71.47 = squared loss an data 30.70 + 0.5*rho*h**2 28.428023 + alpha*h 9.462142 + L2reg 2.66 + L1reg 0.22 ; SHD = 91 ; DAG True
total norm for a microbatch 134.56816640181304 clip 13.748719700092343
total norm for a microbatch 200.51858178367223 clip 31.423795728973765
total norm for a microbatch 180.43434176525142 clip 193.70855497461795
total norm for a microbatch 217.8980536219004 clip 181.87091222403154
total norm for a microbatch 255.68155907800946 clip 192.93339093607636
total norm for a microbatch 176.5737471456728 clip 179.9116355791818
cuda
Objective function 45.87 = squared loss an data 32.71 + 0.5*rho*h**2 5.835150 + alpha*h 4.286888 + L2reg 2.83 + L1reg 0.21 ; SHD = 89 ; DAG True
Proportion of microbatches that were clipped  0.8281237572576156
iteration 2 in inner loop, alpha 39.682692697811056 rho 1000.0 h 0.10802916107193994
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 98.39 = squared loss an data 32.71 + 0.5*rho*h**2 58.351498 + alpha*h 4.286888 + L2reg 2.83 + L1reg 0.21 ; SHD = 89 ; DAG True
total norm for a microbatch 648.0844936537405 clip 1.1023523130427815
total norm for a microbatch 318.8098907570786 clip 1.724667458972577
total norm for a microbatch 291.85933369990545 clip 5.069901930891112
total norm for a microbatch 303.9070241882738 clip 24.459772588059636
total norm for a microbatch 407.2342312446516 clip 249.98562099813086
total norm for a microbatch 258.3932734330717 clip 237.63550854615127
total norm for a microbatch 299.8608096601901 clip 239.4955280346757
total norm for a microbatch 197.01862870885068 clip 199.6639401855308
total norm for a microbatch 258.27886992007075 clip 205.55537322826876
total norm for a microbatch 240.95414958500805 clip 213.09271437222884
total norm for a microbatch 280.5451085262363 clip 216.16255332300412
total norm for a microbatch 318.52723924482524 clip 192.3805974001482
cuda
Objective function 44.68 = squared loss an data 33.92 + 0.5*rho*h**2 6.177586 + alpha*h 1.394844 + L2reg 2.98 + L1reg 0.21 ; SHD = 86 ; DAG True
Proportion of microbatches that were clipped  0.8276577079511923
iteration 3 in inner loop, alpha 39.682692697811056 rho 10000.0 h 0.03514992352636881
iteration 3 in outer loop, alpha = 391.1819279614991, rho = 10000.0, h = 0.03514992352636881
cuda
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 57.03 = squared loss an data 33.92 + 0.5*rho*h**2 6.177586 + alpha*h 13.750015 + L2reg 2.98 + L1reg 0.21 ; SHD = 86 ; DAG True
total norm for a microbatch 360.599426283927 clip 25.04474568717045
total norm for a microbatch 646.478293146021 clip 61.003513761105026
total norm for a microbatch 457.3019497018873 clip 167.70596421678044
total norm for a microbatch 264.08977098983877 clip 276.2358571890094
total norm for a microbatch 187.1440122812921 clip 243.24380401570548
total norm for a microbatch 229.8943193451936 clip 223.04091692403796
total norm for a microbatch 202.52374156861848 clip 226.8477062710955
total norm for a microbatch 176.470196417698 clip 195.81699696390007
total norm for a microbatch 307.38552274894687 clip 195.81699696390007
total norm for a microbatch 253.27246133440588 clip 206.83050075685904
total norm for a microbatch 267.3391633905351 clip 212.0548051093307
total norm for a microbatch 466.3153148602139 clip 222.8160823879275
cuda
Objective function 50.52 = squared loss an data 34.33 + 0.5*rho*h**2 3.114817 + alpha*h 9.763601 + L2reg 3.09 + L1reg 0.22 ; SHD = 92 ; DAG True
Proportion of microbatches that were clipped  0.8286624203821656
iteration 1 in inner loop, alpha 391.1819279614991 rho 10000.0 h 0.024959234468404645
noise_multiplier  0.6  noise_multiplier_b  2.5  noise_multiplier_delta  0.6043672230190352
cuda
Objective function 78.55 = squared loss an data 34.33 + 0.5*rho*h**2 31.148169 + alpha*h 9.763601 + L2reg 3.09 + L1reg 0.22 ; SHD = 92 ; DAG True
total norm for a microbatch 342.83049492993246 clip 7.1683676437658574
total norm for a microbatch 411.4531460117668 clip 73.62502321720677
total norm for a microbatch 475.28203111798877 clip 169.08077693103417
total norm for a microbatch 376.9441070616118 clip 361.31280088966975
total norm for a microbatch 417.8647163040775 clip 275.2265095654154
total norm for a microbatch 239.2401807665102 clip 248.67466649953874
total norm for a microbatch 293.41045693648806 clip 263.77867053108935
total norm for a microbatch 211.74970703328995 clip 248.94619765445614
total norm for a microbatch 321.62881570270025 clip 259.8253831007555
total norm for a microbatch 366.76151809894293 clip 230.1831598060675
total norm for a microbatch 389.736203213725 clip 257.95181808501667
cuda
Objective function 47.02 = squared loss an data 34.30 + 0.5*rho*h**2 5.246766 + alpha*h 4.007187 + L2reg 3.24 + L1reg 0.23 ; SHD = 86 ; DAG True
Proportion of microbatches that were clipped  0.8302114317871212
iteration 2 in inner loop, alpha 391.1819279614991 rho 100000.0 h 0.010243793984606953
iteration 4 in outer loop, alpha = 10634.975912568452, rho = 1000000.0, h = 0.010243793984606953
Threshold 0.3
[[0.005 2.149 0.576 0.92  1.176 0.78  0.91  1.144 0.455 0.805 0.791 1.173
  0.798 0.708 0.848 1.004 0.756 0.948 0.589 0.925]
 [0.003 0.005 0.196 0.971 0.7   0.424 1.104 0.178 0.109 0.53  0.054 0.306
  0.216 0.191 0.415 0.472 0.188 0.459 0.209 0.487]
 [0.013 0.03  0.008 0.41  0.728 0.082 0.72  0.054 0.033 0.125 0.027 0.177
  0.408 0.066 0.373 0.347 0.199 0.161 0.159 0.087]
 [0.002 0.006 0.024 0.007 0.053 0.009 0.041 0.006 0.007 0.029 0.003 0.025
  0.029 0.01  0.04  0.06  0.016 0.026 0.008 0.005]
 [0.003 0.009 0.013 0.122 0.007 0.006 0.043 0.017 0.005 0.014 0.006 0.012
  0.079 0.005 0.071 0.091 0.025 0.024 0.013 0.017]
 [0.009 0.011 0.098 0.675 0.729 0.009 0.497 0.206 0.029 0.397 0.031 0.241
  0.28  0.253 0.425 1.168 1.045 0.657 0.282 0.341]
 [0.004 0.005 0.014 0.202 0.141 0.014 0.007 0.024 0.005 0.064 0.003 0.03
  0.023 0.011 0.207 0.034 0.018 0.096 0.016 0.037]
 [0.004 0.042 0.095 1.002 0.659 0.02  0.261 0.006 0.055 0.322 0.016 0.414
  0.102 0.045 0.268 0.474 0.064 0.234 0.145 0.077]
 [0.015 0.055 0.153 0.697 1.041 0.23  0.877 0.127 0.006 0.456 0.015 0.296
  0.174 0.205 0.592 0.473 0.389 0.876 0.227 0.33 ]
 [0.003 0.009 0.039 0.285 0.416 0.012 0.094 0.013 0.013 0.005 0.004 0.08
  0.075 0.011 0.131 0.08  0.012 0.032 0.007 0.038]
 [0.007 0.101 0.24  1.051 0.677 0.345 1.57  0.363 0.452 1.274 0.006 0.794
  0.519 0.313 0.66  0.742 0.525 0.856 0.403 0.738]
 [0.004 0.028 0.035 0.259 0.567 0.024 0.19  0.02  0.016 0.149 0.006 0.009
  0.119 0.016 0.166 0.304 0.046 0.124 0.035 0.087]
 [0.008 0.017 0.022 0.331 0.072 0.03  0.296 0.045 0.034 0.116 0.014 0.059
  0.007 0.013 0.146 0.191 0.058 0.105 0.026 0.046]
 [0.005 0.04  0.085 0.437 0.557 0.027 0.47  0.174 0.032 0.561 0.024 0.214
  0.429 0.004 0.267 0.36  0.11  0.382 0.066 0.383]
 [0.002 0.01  0.018 0.262 0.084 0.011 0.041 0.027 0.015 0.047 0.012 0.048
  0.045 0.027 0.005 0.107 0.008 0.109 0.01  0.022]
 [0.004 0.013 0.012 0.15  0.104 0.003 0.069 0.01  0.007 0.085 0.007 0.027
  0.03  0.019 0.059 0.006 0.014 0.017 0.006 0.01 ]
 [0.007 0.022 0.033 0.399 0.327 0.008 0.178 0.058 0.014 0.571 0.015 0.175
  0.126 0.115 0.692 0.68  0.006 0.202 0.13  0.086]
 [0.006 0.009 0.022 0.331 0.361 0.009 0.075 0.027 0.008 0.211 0.009 0.059
  0.059 0.016 0.064 0.38  0.036 0.006 0.016 0.076]
 [0.007 0.036 0.04  0.79  0.488 0.023 0.526 0.048 0.039 0.836 0.02  0.218
  0.281 0.102 0.366 1.227 0.043 0.434 0.005 0.18 ]
 [0.004 0.01  0.074 1.001 0.492 0.018 0.211 0.107 0.014 0.198 0.01  0.097
  0.18  0.022 0.293 0.726 0.07  0.104 0.033 0.008]]
[[0.    2.149 0.576 0.92  1.176 0.78  0.91  1.144 0.455 0.805 0.791 1.173
  0.798 0.708 0.848 1.004 0.756 0.948 0.589 0.925]
 [0.    0.    0.    0.971 0.7   0.424 1.104 0.    0.    0.53  0.    0.306
  0.    0.    0.415 0.472 0.    0.459 0.    0.487]
 [0.    0.    0.    0.41  0.728 0.    0.72  0.    0.    0.    0.    0.
  0.408 0.    0.373 0.347 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.675 0.729 0.    0.497 0.    0.    0.397 0.    0.
  0.    0.    0.425 1.168 1.045 0.657 0.    0.341]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.002 0.659 0.    0.    0.    0.    0.322 0.    0.414
  0.    0.    0.    0.474 0.    0.    0.    0.   ]
 [0.    0.    0.    0.697 1.041 0.    0.877 0.    0.    0.456 0.    0.
  0.    0.    0.592 0.473 0.389 0.876 0.    0.33 ]
 [0.    0.    0.    0.    0.416 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.051 0.677 0.345 1.57  0.363 0.452 1.274 0.    0.794
  0.519 0.313 0.66  0.742 0.525 0.856 0.403 0.738]
 [0.    0.    0.    0.    0.567 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.304 0.    0.    0.    0.   ]
 [0.    0.    0.    0.331 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.437 0.557 0.    0.47  0.    0.    0.561 0.    0.
  0.429 0.    0.    0.36  0.    0.382 0.    0.383]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.399 0.327 0.    0.    0.    0.    0.571 0.    0.
  0.    0.    0.692 0.68  0.    0.    0.    0.   ]
 [0.    0.    0.    0.331 0.361 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.38  0.    0.    0.    0.   ]
 [0.    0.    0.    0.79  0.488 0.    0.526 0.    0.    0.836 0.    0.
  0.    0.    0.366 1.227 0.    0.434 0.    0.   ]
 [0.    0.    0.    1.001 0.492 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.726 0.    0.    0.    0.   ]]
{'fdr': 0.5673076923076923, 'tpr': 0.5625, 'fpr': 0.5363636363636364, 'f1': 0.4891304347826087, 'shd': 86, 'npred': 104, 'ntrue': 80}
[2.149 0.576 0.92  1.176 0.78  0.91  1.144 0.455 0.805 0.791 1.173 0.798
 0.708 0.848 1.004 0.756 0.948 0.589 0.925 0.003 0.196 0.971 0.7   0.424
 1.104 0.178 0.109 0.53  0.054 0.306 0.216 0.191 0.415 0.472 0.188 0.459
 0.209 0.487 0.013 0.03  0.41  0.728 0.082 0.72  0.054 0.033 0.125 0.027
 0.177 0.408 0.066 0.373 0.347 0.199 0.161 0.159 0.087 0.002 0.006 0.024
 0.053 0.009 0.041 0.006 0.007 0.029 0.003 0.025 0.029 0.01  0.04  0.06
 0.016 0.026 0.008 0.005 0.003 0.009 0.013 0.122 0.006 0.043 0.017 0.005
 0.014 0.006 0.012 0.079 0.005 0.071 0.091 0.025 0.024 0.013 0.017 0.009
 0.011 0.098 0.675 0.729 0.497 0.206 0.029 0.397 0.031 0.241 0.28  0.253
 0.425 1.168 1.045 0.657 0.282 0.341 0.004 0.005 0.014 0.202 0.141 0.014
 0.024 0.005 0.064 0.003 0.03  0.023 0.011 0.207 0.034 0.018 0.096 0.016
 0.037 0.004 0.042 0.095 1.002 0.659 0.02  0.261 0.055 0.322 0.016 0.414
 0.102 0.045 0.268 0.474 0.064 0.234 0.145 0.077 0.015 0.055 0.153 0.697
 1.041 0.23  0.877 0.127 0.456 0.015 0.296 0.174 0.205 0.592 0.473 0.389
 0.876 0.227 0.33  0.003 0.009 0.039 0.285 0.416 0.012 0.094 0.013 0.013
 0.004 0.08  0.075 0.011 0.131 0.08  0.012 0.032 0.007 0.038 0.007 0.101
 0.24  1.051 0.677 0.345 1.57  0.363 0.452 1.274 0.794 0.519 0.313 0.66
 0.742 0.525 0.856 0.403 0.738 0.004 0.028 0.035 0.259 0.567 0.024 0.19
 0.02  0.016 0.149 0.006 0.119 0.016 0.166 0.304 0.046 0.124 0.035 0.087
 0.008 0.017 0.022 0.331 0.072 0.03  0.296 0.045 0.034 0.116 0.014 0.059
 0.013 0.146 0.191 0.058 0.105 0.026 0.046 0.005 0.04  0.085 0.437 0.557
 0.027 0.47  0.174 0.032 0.561 0.024 0.214 0.429 0.267 0.36  0.11  0.382
 0.066 0.383 0.002 0.01  0.018 0.262 0.084 0.011 0.041 0.027 0.015 0.047
 0.012 0.048 0.045 0.027 0.107 0.008 0.109 0.01  0.022 0.004 0.013 0.012
 0.15  0.104 0.003 0.069 0.01  0.007 0.085 0.007 0.027 0.03  0.019 0.059
 0.014 0.017 0.006 0.01  0.007 0.022 0.033 0.399 0.327 0.008 0.178 0.058
 0.014 0.571 0.015 0.175 0.126 0.115 0.692 0.68  0.202 0.13  0.086 0.006
 0.009 0.022 0.331 0.361 0.009 0.075 0.027 0.008 0.211 0.009 0.059 0.059
 0.016 0.064 0.38  0.036 0.016 0.076 0.007 0.036 0.04  0.79  0.488 0.023
 0.526 0.048 0.039 0.836 0.02  0.218 0.281 0.102 0.366 1.227 0.043 0.434
 0.18  0.004 0.01  0.074 1.001 0.492 0.018 0.211 0.107 0.014 0.198 0.01
 0.097 0.18  0.022 0.293 0.726 0.07  0.104 0.033]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.7485416666666667, 0.48318305107945286)
Iterations 2250
Achieves (13.034436291409676, 1e-05)-DP
