samples  5000  graph  20 40 ER mlp  minibatch size  50  noise  1.0  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617290166446
iteration 1 in outer loop, alpha = 1.5634617290166446, rho = 1.0, h = 1.5634617290166446
cuda
iteration 1 in inner loop,alpha 1.5634617290166446 rho 1.0 h 1.0381693524821998
iteration 2 in inner loop,alpha 1.5634617290166446 rho 10.0 h 0.4486130544339133
iteration 3 in inner loop,alpha 1.5634617290166446 rho 100.0 h 0.13645699227387453
iteration 2 in outer loop, alpha = 15.209160956404098, rho = 100.0, h = 0.13645699227387453
cuda
iteration 1 in inner loop,alpha 15.209160956404098 rho 100.0 h 0.07242523875783746
iteration 2 in inner loop,alpha 15.209160956404098 rho 1000.0 h 0.023501741727898207
iteration 3 in outer loop, alpha = 38.710902684302305, rho = 1000.0, h = 0.023501741727898207
cuda
iteration 1 in inner loop,alpha 38.710902684302305 rho 1000.0 h 0.009403667239261893
iteration 2 in inner loop,alpha 38.710902684302305 rho 10000.0 h 0.003102453628152091
iteration 4 in outer loop, alpha = 69.73543896582322, rho = 10000.0, h = 0.003102453628152091
cuda
iteration 1 in inner loop,alpha 69.73543896582322 rho 10000.0 h 0.0014436481049351357
iteration 2 in inner loop,alpha 69.73543896582322 rho 100000.0 h 0.000545532359350176
iteration 5 in outer loop, alpha = 124.28867490084082, rho = 100000.0, h = 0.000545532359350176
cuda
iteration 1 in inner loop,alpha 124.28867490084082 rho 100000.0 h 0.00023938677032475653
iteration 6 in outer loop, alpha = 363.67544522559734, rho = 1000000.0, h = 0.00023938677032475653
Threshold 0.3
[[0.    0.011 0.05  2.315 0.032 0.031 0.042 0.009 1.995 0.015 1.851 0.006
  0.101 0.009 0.244 0.937 0.009 0.001 0.039 0.21 ]
 [0.032 0.005 0.214 0.058 0.416 0.113 0.029 0.003 3.052 0.048 0.017 0.011
  0.6   0.001 1.893 1.583 0.288 0.    0.227 0.338]
 [0.006 0.002 0.003 0.072 0.138 1.191 0.    0.    2.293 0.077 0.06  0.
  2.297 0.003 1.459 0.889 0.001 0.    3.746 2.305]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.001 0.    0.    0.
  0.    0.    0.099 1.497 0.    0.    0.    0.001]
 [0.    0.    0.    1.679 0.001 0.197 0.001 0.    0.001 0.032 0.018 0.
  0.373 0.002 0.184 0.344 0.001 0.    0.035 0.17 ]
 [0.003 0.    0.    0.11  0.002 0.001 0.    0.    0.003 0.037 0.068 0.
  0.237 0.    1.914 0.453 0.    0.    0.001 0.154]
 [0.001 0.017 0.574 0.057 0.227 0.104 0.005 0.    0.034 0.004 0.017 0.
  0.036 0.    0.051 1.642 0.007 0.003 0.064 0.028]
 [0.01  0.014 0.687 0.05  0.188 0.094 4.459 0.001 0.151 0.009 0.01  0.051
  0.01  0.008 0.131 0.229 0.063 0.007 0.062 0.055]
 [0.    0.    0.    0.063 0.999 0.119 0.001 0.    0.002 0.048 0.045 0.
  0.28  0.002 0.379 0.517 0.    0.    0.018 0.218]
 [0.    0.    0.    0.095 0.    0.    0.    0.    0.    0.001 0.834 0.
  0.    0.    0.191 0.248 0.    0.    0.    0.179]
 [0.    0.    0.    0.095 0.    0.003 0.    0.    0.    0.    0.001 0.
  0.    0.    1.285 0.286 0.    0.    0.    2.335]
 [0.006 0.001 2.762 0.067 0.024 0.22  3.595 0.004 0.218 0.015 0.016 0.001
  0.227 0.004 0.524 0.365 0.008 0.025 0.208 0.187]
 [0.    0.    0.    0.108 0.001 0.002 0.    0.    0.001 3.205 0.126 0.
  0.002 0.    0.254 0.53  0.    0.    0.    0.422]
 [0.01  0.01  0.015 0.054 0.03  0.125 0.008 0.015 0.095 0.029 0.077 0.031
  0.274 0.    0.209 0.179 0.009 0.03  2.63  0.224]
 [0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.004 0.311 0.    0.    0.    0.004]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.004 0.008 0.    0.    0.    0.   ]
 [0.009 0.006 0.333 0.064 0.372 0.204 0.03  0.003 0.268 0.05  0.056 0.013
  0.398 0.004 1.995 1.527 0.003 0.    0.328 0.502]
 [0.008 4.267 0.008 0.052 0.197 0.007 0.011 0.009 0.593 0.011 0.009 0.003
  1.392 0.007 0.226 0.246 2.994 0.001 1.417 1.03 ]
 [0.004 0.001 0.    0.064 0.018 1.232 0.    0.    0.085 0.049 0.019 0.
  1.035 0.    0.372 1.344 0.    0.    0.003 0.464]
 [0.    0.    0.    2.09  0.    0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.241 0.338 0.    0.    0.    0.003]]
[[0.    0.    0.    2.315 0.    0.    0.    0.    1.995 0.    1.851 0.
  0.    0.    0.    0.937 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.416 0.    0.    0.    3.052 0.    0.    0.
  0.6   0.    1.893 1.583 0.    0.    0.    0.338]
 [0.    0.    0.    0.    0.    1.191 0.    0.    2.293 0.    0.    0.
  2.297 0.    1.459 0.889 0.    0.    3.746 2.305]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.497 0.    0.    0.    0.   ]
 [0.    0.    0.    1.679 0.    0.    0.    0.    0.    0.    0.    0.
  0.373 0.    0.    0.344 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.914 0.453 0.    0.    0.    0.   ]
 [0.    0.    0.574 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.642 0.    0.    0.    0.   ]
 [0.    0.    0.687 0.    0.    0.    4.459 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.379 0.517 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.834 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.285 0.    0.    0.    0.    2.335]
 [0.    0.    2.762 0.    0.    0.    3.595 0.    0.    0.    0.    0.
  0.    0.    0.524 0.365 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.205 0.    0.
  0.    0.    0.    0.53  0.    0.    0.    0.422]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.63  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.311 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.333 0.    0.372 0.    0.    0.    0.    0.    0.    0.
  0.398 0.    1.995 1.527 0.    0.    0.328 0.502]
 [0.    4.267 0.    0.    0.    0.    0.    0.    0.593 0.    0.    0.
  1.392 0.    0.    0.    2.994 0.    1.417 1.03 ]
 [0.    0.    0.    0.    0.    1.232 0.    0.    0.    0.    0.    0.
  1.035 0.    0.372 1.344 0.    0.    0.    0.464]
 [0.    0.    0.    2.09  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.338 0.    0.    0.    0.   ]]
{'fdr': 0.3548387096774194, 'tpr': 1.0, 'fpr': 0.14666666666666667, 'f1': 0.7843137254901961, 'shd': 22, 'npred': 62, 'ntrue': 40}
[1.077e-02 5.047e-02 2.315e+00 3.190e-02 3.114e-02 4.157e-02 9.015e-03
 1.995e+00 1.514e-02 1.851e+00 6.057e-03 1.011e-01 8.577e-03 2.436e-01
 9.368e-01 8.840e-03 9.820e-04 3.874e-02 2.101e-01 3.226e-02 2.140e-01
 5.761e-02 4.158e-01 1.132e-01 2.914e-02 2.827e-03 3.052e+00 4.844e-02
 1.714e-02 1.097e-02 5.998e-01 8.716e-04 1.893e+00 1.583e+00 2.884e-01
 3.759e-04 2.268e-01 3.378e-01 5.551e-03 1.515e-03 7.218e-02 1.380e-01
 1.191e+00 3.059e-04 6.151e-05 2.293e+00 7.698e-02 5.984e-02 2.324e-04
 2.297e+00 2.838e-03 1.459e+00 8.892e-01 6.412e-04 7.239e-05 3.746e+00
 2.305e+00 1.106e-05 1.321e-04 1.844e-04 5.237e-04 1.053e-03 1.415e-04
 3.624e-05 5.129e-04 2.774e-05 2.227e-05 8.351e-05 6.227e-05 9.266e-06
 9.863e-02 1.497e+00 2.372e-04 8.580e-05 2.442e-04 9.791e-04 1.057e-05
 1.087e-04 1.465e-04 1.679e+00 1.967e-01 6.252e-04 8.280e-06 7.062e-04
 3.167e-02 1.770e-02 6.078e-05 3.726e-01 2.434e-03 1.839e-01 3.443e-01
 5.672e-04 3.008e-05 3.469e-02 1.701e-01 2.954e-03 1.501e-04 9.625e-05
 1.104e-01 1.917e-03 7.243e-05 1.202e-05 2.594e-03 3.690e-02 6.794e-02
 3.034e-05 2.367e-01 7.297e-06 1.914e+00 4.530e-01 1.845e-04 2.145e-05
 6.973e-04 1.541e-01 5.211e-04 1.677e-02 5.737e-01 5.733e-02 2.273e-01
 1.035e-01 1.728e-04 3.446e-02 3.968e-03 1.652e-02 2.739e-05 3.571e-02
 4.490e-04 5.104e-02 1.642e+00 7.317e-03 3.429e-03 6.449e-02 2.798e-02
 9.650e-03 1.414e-02 6.875e-01 4.981e-02 1.877e-01 9.433e-02 4.459e+00
 1.509e-01 9.094e-03 9.914e-03 5.149e-02 9.728e-03 7.940e-03 1.307e-01
 2.290e-01 6.261e-02 7.286e-03 6.184e-02 5.518e-02 4.105e-05 2.472e-04
 3.609e-04 6.341e-02 9.987e-01 1.187e-01 5.365e-04 2.271e-05 4.826e-02
 4.542e-02 8.197e-05 2.798e-01 1.804e-03 3.793e-01 5.169e-01 4.166e-04
 3.733e-05 1.818e-02 2.181e-01 8.549e-06 1.538e-05 4.599e-05 9.512e-02
 9.183e-05 3.167e-04 2.430e-05 8.855e-06 8.440e-05 8.340e-01 2.101e-05
 1.519e-04 8.361e-06 1.906e-01 2.479e-01 4.105e-05 1.245e-05 2.716e-05
 1.793e-01 5.314e-05 1.453e-04 3.557e-05 9.515e-02 4.159e-04 2.641e-03
 4.854e-04 4.661e-06 4.386e-04 8.136e-05 5.802e-05 2.062e-05 6.649e-06
 1.285e+00 2.861e-01 2.882e-04 4.104e-06 5.119e-05 2.335e+00 5.897e-03
 1.479e-03 2.762e+00 6.747e-02 2.370e-02 2.201e-01 3.595e+00 3.788e-03
 2.177e-01 1.537e-02 1.561e-02 2.271e-01 3.730e-03 5.243e-01 3.645e-01
 7.848e-03 2.537e-02 2.082e-01 1.874e-01 4.295e-05 4.013e-05 1.811e-04
 1.076e-01 9.679e-04 2.060e-03 4.751e-05 1.557e-05 1.186e-03 3.205e+00
 1.260e-01 4.474e-05 4.012e-05 2.544e-01 5.300e-01 3.555e-04 4.720e-05
 2.131e-04 4.217e-01 1.047e-02 9.621e-03 1.470e-02 5.362e-02 2.976e-02
 1.253e-01 7.784e-03 1.540e-02 9.526e-02 2.903e-02 7.662e-02 3.118e-02
 2.738e-01 2.086e-01 1.791e-01 8.777e-03 3.007e-02 2.630e+00 2.241e-01
 1.241e-05 3.833e-04 9.378e-05 9.999e-03 2.570e-04 1.253e-04 1.397e-04
 2.083e-05 3.922e-04 7.183e-05 1.928e-04 3.124e-05 3.855e-05 3.829e-05
 3.107e-01 3.196e-04 6.816e-05 4.687e-05 4.326e-03 7.528e-06 2.157e-04
 4.689e-05 1.398e-04 3.139e-05 1.840e-04 3.837e-04 3.353e-05 2.932e-05
 4.522e-05 5.920e-05 1.275e-05 3.388e-05 1.062e-05 3.552e-03 1.177e-05
 2.960e-05 4.130e-05 8.403e-05 9.003e-03 6.086e-03 3.333e-01 6.375e-02
 3.716e-01 2.043e-01 3.016e-02 2.980e-03 2.677e-01 5.017e-02 5.624e-02
 1.271e-02 3.981e-01 3.699e-03 1.995e+00 1.527e+00 3.991e-04 3.285e-01
 5.017e-01 8.305e-03 4.267e+00 8.214e-03 5.184e-02 1.966e-01 7.432e-03
 1.136e-02 9.125e-03 5.926e-01 1.117e-02 9.118e-03 3.355e-03 1.392e+00
 7.376e-03 2.259e-01 2.464e-01 2.994e+00 1.417e+00 1.030e+00 3.810e-03
 1.151e-03 4.122e-04 6.409e-02 1.753e-02 1.232e+00 1.332e-04 2.226e-05
 8.485e-02 4.891e-02 1.864e-02 9.093e-05 1.035e+00 6.482e-05 3.719e-01
 1.344e+00 1.604e-04 9.657e-05 4.637e-01 1.143e-05 6.215e-05 1.689e-04
 2.090e+00 2.444e-04 1.019e-03 9.036e-05 5.507e-06 2.228e-04 3.179e-05
 5.261e-05 6.311e-05 2.676e-05 1.893e-05 2.407e-01 3.377e-01 3.963e-05
 1.402e-04 1.121e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9991176470588236, 0.9923891402022302)
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
total norm for a microbatch 51.51731495848658 clip 50.6283692213768
total norm for a microbatch 69.20180429430529 clip 50.24380646770833
total norm for a microbatch 69.03510113409173 clip 54.103615029879094
total norm for a microbatch 56.2824613906364 clip 59.44048874580051
total norm for a microbatch 68.24879532646213 clip 64.61760847418284
total norm for a microbatch 139.31787039208282 clip 71.57264720427406
total norm for a microbatch 92.7590514503892 clip 84.90122235968894
total norm for a microbatch 120.91083516556291 clip 80.79291990989641
total norm for a microbatch 91.35504014022499 clip 79.63876972195379
total norm for a microbatch 223.69717421330148 clip 81.07060562212679
total norm for a microbatch 70.05361076814445 clip 81.68119886110452
total norm for a microbatch 76.91353116823097 clip 81.9368669867448
cuda
Objective function 46.18 = squared loss an data 36.73 + 0.5*rho*h**2 8.059498 + alpha*h 0.000000 + L2reg 1.09 + L1reg 0.30 ; SHD = 134 ; DAG False
Proportion of microbatches that were clipped  0.8148118236149249
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 4.0148469530574715
iteration 1 in outer loop, alpha = 4.0148469530574715, rho = 1.0, h = 4.0148469530574715
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 62.30 = squared loss an data 36.73 + 0.5*rho*h**2 8.059498 + alpha*h 16.118996 + L2reg 1.09 + L1reg 0.30 ; SHD = 134 ; DAG False
total norm for a microbatch 90.16018177099387 clip 1.2048967501798744
total norm for a microbatch 216.4274340477068 clip 5.734361006279214
total norm for a microbatch 100.91924932254172 clip 12.067532816689715
total norm for a microbatch 178.77893962681696 clip 117.50588725103827
total norm for a microbatch 153.88918877582094 clip 106.62565628276816
total norm for a microbatch 122.70414382462862 clip 117.95895285133233
total norm for a microbatch 152.67087973421624 clip 116.3913211427922
total norm for a microbatch 135.17309370593136 clip 115.49525844851583
total norm for a microbatch 121.05701015987326 clip 109.23372102439258
total norm for a microbatch 126.64370971735481 clip 112.03161638732824
total norm for a microbatch 135.5220864844168 clip 111.7154060379295
total norm for a microbatch 151.7557545815031 clip 111.33612634186099
cuda
Objective function 42.02 = squared loss an data 26.76 + 0.5*rho*h**2 3.236408 + alpha*h 10.214465 + L2reg 1.54 + L1reg 0.28 ; SHD = 113 ; DAG False
Proportion of microbatches that were clipped  0.8183668484897826
iteration 1 in inner loop, alpha 4.0148469530574715 rho 1.0 h 2.5441728415575753
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 71.15 = squared loss an data 26.76 + 0.5*rho*h**2 32.364077 + alpha*h 10.214465 + L2reg 1.54 + L1reg 0.28 ; SHD = 113 ; DAG False
total norm for a microbatch 217.8251945416948 clip 6.535292006935008
total norm for a microbatch 285.745033381331 clip 117.36945622703136
total norm for a microbatch 137.6278378751585 clip 147.63532861378638
total norm for a microbatch 279.0370666415378 clip 160.00799538911045
total norm for a microbatch 165.5016590628532 clip 150.32151624913834
total norm for a microbatch 335.1773440089268 clip 148.60646723477433
total norm for a microbatch 140.01856575662873 clip 153.55275730232344
total norm for a microbatch 111.672121706396 clip 135.98578702738092
total norm for a microbatch 262.6497843711782 clip 139.99699737961006
total norm for a microbatch 177.42803938658625 clip 141.90525621788464
total norm for a microbatch 134.41913459367674 clip 134.8995957863448
cuda
Objective function 43.73 = squared loss an data 28.92 + 0.5*rho*h**2 7.757045 + alpha*h 5.000716 + L2reg 1.80 + L1reg 0.26 ; SHD = 94 ; DAG False
Proportion of microbatches that were clipped  0.8227400562474889
iteration 2 in inner loop, alpha 4.0148469530574715 rho 10.0 h 1.2455557127638883
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 113.55 = squared loss an data 28.92 + 0.5*rho*h**2 77.570452 + alpha*h 5.000716 + L2reg 1.80 + L1reg 0.26 ; SHD = 94 ; DAG False
total norm for a microbatch 341.43268097242327 clip 2.948653418152897
total norm for a microbatch 263.9052871682962 clip 92.30126067140115
total norm for a microbatch 211.46013268330728 clip 191.1548836148412
total norm for a microbatch 334.39964129067346 clip 173.6468588829599
total norm for a microbatch 330.3582563942442 clip 168.61871205624823
total norm for a microbatch 154.78457053313397 clip 170.2256952379134
total norm for a microbatch 285.82433722830814 clip 180.341713139811
total norm for a microbatch 148.31042671088326 clip 167.80448644133287
cuda
Objective function 47.46 = squared loss an data 31.80 + 0.5*rho*h**2 11.470308 + alpha*h 1.922966 + L2reg 2.03 + L1reg 0.24 ; SHD = 99 ; DAG True
Proportion of microbatches that were clipped  0.8259211167018341
iteration 3 in inner loop, alpha 4.0148469530574715 rho 100.0 h 0.4789636234690242
iteration 2 in outer loop, alpha = 51.91120929995989, rho = 100.0, h = 0.4789636234690242
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 70.41 = squared loss an data 31.80 + 0.5*rho*h**2 11.470308 + alpha*h 24.863581 + L2reg 2.03 + L1reg 0.24 ; SHD = 99 ; DAG True
total norm for a microbatch 192.4309704948723 clip 1.8892737511837898
total norm for a microbatch 225.31694062159065 clip 4.232688379914981
total norm for a microbatch 152.26140731030696 clip 12.870221525249661
total norm for a microbatch 331.42228919301164 clip 22.389119785841025
total norm for a microbatch 165.98432116236202 clip 190.0158232611716
total norm for a microbatch 322.6361979274818 clip 198.42703062348508
total norm for a microbatch 165.44475494574888 clip 187.22901470296662
total norm for a microbatch 175.853432280863 clip 181.76857665096276
total norm for a microbatch 272.1670369613501 clip 185.42660083552678
total norm for a microbatch 126.59802195552523 clip 189.38468043562852
total norm for a microbatch 250.5101737690997 clip 179.78434585258816
total norm for a microbatch 175.70257073372434 clip 189.31493402430385
total norm for a microbatch 319.3550834978255 clip 186.96754939493405
total norm for a microbatch 190.19065591457587 clip 171.8964709590745
total norm for a microbatch 241.36264597988244 clip 165.74530266851983
cuda
Objective function 53.43 = squared loss an data 30.63 + 0.5*rho*h**2 4.604114 + alpha*h 15.752488 + L2reg 2.21 + L1reg 0.24 ; SHD = 89 ; DAG True
Proportion of microbatches that were clipped  0.8258617899336462
iteration 1 in inner loop, alpha 51.91120929995989 rho 100.0 h 0.3034506128533465
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 94.87 = squared loss an data 30.63 + 0.5*rho*h**2 46.041137 + alpha*h 15.752488 + L2reg 2.21 + L1reg 0.24 ; SHD = 89 ; DAG True
total norm for a microbatch 405.90746622189204 clip 13.748719700092343
total norm for a microbatch 214.27263805149528 clip 31.423795728973765
total norm for a microbatch 203.34389428642632 clip 204.86571906112187
total norm for a microbatch 204.33360374697705 clip 203.42495429982532
total norm for a microbatch 228.4779878131727 clip 209.0022473237284
total norm for a microbatch 201.6964759479664 clip 206.1215033301055
cuda
Objective function 50.50 = squared loss an data 31.33 + 0.5*rho*h**2 9.373564 + alpha*h 7.107691 + L2reg 2.45 + L1reg 0.24 ; SHD = 91 ; DAG True
Proportion of microbatches that were clipped  0.8288395768710729
iteration 2 in inner loop, alpha 51.91120929995989 rho 1000.0 h 0.13692015157957727
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 134.86 = squared loss an data 31.33 + 0.5*rho*h**2 93.735640 + alpha*h 7.107691 + L2reg 2.45 + L1reg 0.24 ; SHD = 91 ; DAG True
total norm for a microbatch 909.4806860410997 clip 1.1023523130427815
total norm for a microbatch 324.2396743406732 clip 1.724667458972577
total norm for a microbatch 238.021928911503 clip 5.069901930891112
total norm for a microbatch 330.6109269902787 clip 24.459772588059636
total norm for a microbatch 368.0366066021695 clip 338.79777660612734
total norm for a microbatch 365.84381150796764 clip 351.6857506657101
total norm for a microbatch 379.4482296472741 clip 332.4650509303666
total norm for a microbatch 370.50120653631285 clip 236.1897846716588
total norm for a microbatch 254.01163653505697 clip 237.39263396323483
total norm for a microbatch 319.20773805839224 clip 240.2613645491911
total norm for a microbatch 282.95360411371894 clip 243.72259830104593
total norm for a microbatch 317.19436162502694 clip 231.24452213169718
cuda
Objective function 48.89 = squared loss an data 31.96 + 0.5*rho*h**2 11.559074 + alpha*h 2.495960 + L2reg 2.64 + L1reg 0.24 ; SHD = 84 ; DAG True
Proportion of microbatches that were clipped  0.8295717361831088
iteration 3 in inner loop, alpha 51.91120929995989 rho 10000.0 h 0.048081335879786025
iteration 3 in outer loop, alpha = 532.7245680978201, rho = 10000.0, h = 0.048081335879786025
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 72.01 = squared loss an data 31.96 + 0.5*rho*h**2 11.559074 + alpha*h 25.614109 + L2reg 2.64 + L1reg 0.24 ; SHD = 84 ; DAG True
total norm for a microbatch 420.67004728004997 clip 25.04474568717045
total norm for a microbatch 535.3887795213853 clip 61.003513761105026
total norm for a microbatch 456.17210967406635 clip 167.70596421678044
total norm for a microbatch 578.9148863138405 clip 439.33185253259484
total norm for a microbatch 438.7036766591173 clip 415.74173954379137
total norm for a microbatch 387.98757849744493 clip 384.2737689702059
total norm for a microbatch 357.10134702016575 clip 343.87523232205535
total norm for a microbatch 261.26719902016504 clip 244.98097001302705
total norm for a microbatch 279.8686663471302 clip 244.98097001302705
total norm for a microbatch 352.75459600101294 clip 250.61041977617276
total norm for a microbatch 273.72896304162776 clip 252.8622201321166
total norm for a microbatch 232.1483597821854 clip 253.24228748719509
cuda
Objective function 56.09 = squared loss an data 32.72 + 0.5*rho*h**2 4.463480 + alpha*h 15.916755 + L2reg 2.75 + L1reg 0.23 ; SHD = 79 ; DAG True
Proportion of microbatches that were clipped  0.8298566878980892
iteration 1 in inner loop, alpha 532.7245680978201 rho 10000.0 h 0.02987801980893323
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 96.26 = squared loss an data 32.72 + 0.5*rho*h**2 44.634803 + alpha*h 15.916755 + L2reg 2.75 + L1reg 0.23 ; SHD = 79 ; DAG True
total norm for a microbatch 994.6670499738208 clip 7.1683676437658574
total norm for a microbatch 1279.1006713073482 clip 73.62502321720677
total norm for a microbatch 1699.973981798061 clip 169.08077693103417
total norm for a microbatch 3858.472341844613 clip 3083.2640385771892
total norm for a microbatch 1811.951041383716 clip 1612.586079341688
total norm for a microbatch 1163.874496752887 clip 1016.5252591794617
total norm for a microbatch 251.13640527320393 clip 237.7240108027514
total norm for a microbatch 391.9558221125577 clip 227.97518822098348
total norm for a microbatch 336.83714628846934 clip 228.6082413087519
total norm for a microbatch 231.24697658631044 clip 215.91295470587272
total norm for a microbatch 318.6977263688583 clip 218.060558582049
cuda
Objective function 46.63 = squared loss an data 39.80 + 0.5*rho*h**2 1.159568 + alpha*h 2.565464 + L2reg 2.87 + L1reg 0.24 ; SHD = 94 ; DAG True
Proportion of microbatches that were clipped  0.8277996623522791
iteration 2 in inner loop, alpha 532.7245680978201 rho 100000.0 h 0.0048157415844727325
iteration 4 in outer loop, alpha = 1014.2987265450934, rho = 100000.0, h = 0.0048157415844727325
cuda
noise_multiplier  1.0  noise_multiplier_b  2.5  noise_multiplier_delta  1.0206207261596576
cuda
Objective function 48.95 = squared loss an data 39.80 + 0.5*rho*h**2 1.159568 + alpha*h 4.884601 + L2reg 2.87 + L1reg 0.24 ; SHD = 94 ; DAG True
total norm for a microbatch 1171.543490847644 clip 14.910992334882119
total norm for a microbatch 1062.2997433728672 clip 16.170435013909188
total norm for a microbatch 1018.1172948612895 clip 23.28079197829579
total norm for a microbatch 1255.5802631661202 clip 28.18538658198441
total norm for a microbatch 2741.0021653620092 clip 678.2505316234354
total norm for a microbatch 3046.8858181839123 clip 1263.4750714756124
total norm for a microbatch 4110.883573481607 clip 4423.109090354151
total norm for a microbatch 1402.838511279899 clip 1570.508281170122
total norm for a microbatch 648.3590325318278 clip 580.1101649539359
total norm for a microbatch 479.3396893635025 clip 264.83106475273195
total norm for a microbatch 313.915999710174 clip 242.93999990613966
cuda
Objective function 51.69 = squared loss an data 44.16 + 0.5*rho*h**2 0.653796 + alpha*h 3.667769 + L2reg 2.97 + L1reg 0.25 ; SHD = 98 ; DAG True
Proportion of microbatches that were clipped  0.8284639398062915
iteration 1 in inner loop, alpha 1014.2987265450934 rho 100000.0 h 0.0036160637635660464
iteration 5 in outer loop, alpha = 4630.36249011114, rho = 1000000.0, h = 0.0036160637635660464
Threshold 0.3
[[0.003 0.016 0.003 0.303 0.019 0.009 0.062 0.004 0.226 0.336 0.009 0.028
  0.005 0.015 0.427 0.082 0.011 0.006 0.379 0.013]
 [0.146 0.006 0.003 0.732 0.248 0.139 0.14  0.01  1.524 0.406 0.018 0.124
  0.019 0.169 0.753 0.203 0.016 0.014 0.441 0.043]
 [0.989 0.663 0.004 1.235 1.43  0.715 1.224 0.271 2.018 1.319 0.306 0.596
  0.232 1.1   2.107 0.922 0.252 0.239 2.206 0.34 ]
 [0.012 0.005 0.001 0.004 0.006 0.006 0.051 0.002 0.085 0.016 0.003 0.009
  0.002 0.006 0.009 0.017 0.008 0.003 0.023 0.003]
 [0.166 0.018 0.002 0.554 0.004 0.01  0.123 0.003 0.203 0.195 0.01  0.021
  0.011 0.055 0.205 0.098 0.014 0.008 0.205 0.02 ]
 [0.383 0.038 0.008 0.602 0.344 0.003 0.202 0.007 0.685 0.336 0.035 0.081
  0.027 0.39  0.643 0.286 0.009 0.013 0.744 0.027]
 [0.067 0.018 0.002 0.115 0.038 0.02  0.004 0.001 0.17  0.097 0.008 0.015
  0.005 0.021 0.077 0.06  0.005 0.004 0.093 0.007]
 [0.852 0.488 0.013 1.582 0.89  0.636 3.699 0.004 1.117 1.134 0.231 0.286
  0.271 0.443 1.087 1.366 0.269 0.27  0.948 0.412]
 [0.021 0.002 0.001 0.062 0.02  0.005 0.029 0.003 0.004 0.019 0.003 0.004
  0.003 0.008 0.026 0.011 0.004 0.003 0.078 0.003]
 [0.01  0.005 0.002 0.258 0.02  0.011 0.049 0.002 0.156 0.003 0.003 0.007
  0.001 0.009 0.091 0.017 0.003 0.003 0.037 0.006]
 [0.476 0.192 0.013 0.923 0.38  0.121 0.502 0.024 1.105 1.059 0.004 0.336
  0.24  0.305 0.643 0.157 0.014 0.152 0.583 0.168]
 [0.149 0.034 0.006 0.362 0.169 0.05  0.264 0.009 0.673 0.434 0.01  0.003
  0.028 0.095 0.516 0.319 0.013 0.024 0.468 0.028]
 [0.663 0.261 0.029 1.143 0.233 0.113 0.586 0.014 0.566 1.533 0.021 0.152
  0.002 0.166 0.414 0.218 0.038 0.153 0.424 0.156]
 [0.244 0.027 0.003 0.556 0.083 0.01  0.24  0.008 0.365 0.519 0.012 0.05
  0.016 0.002 0.28  0.096 0.008 0.014 0.282 0.026]
 [0.012 0.005 0.001 0.348 0.015 0.006 0.082 0.002 0.177 0.074 0.005 0.005
  0.007 0.01  0.006 0.021 0.007 0.004 0.106 0.006]
 [0.073 0.027 0.004 0.29  0.046 0.013 0.169 0.002 0.424 0.22  0.02  0.012
  0.014 0.061 0.31  0.004 0.009 0.007 0.246 0.01 ]
 [0.332 0.654 0.017 0.619 0.345 0.274 0.518 0.017 0.646 1.315 0.201 0.221
  0.102 0.375 0.856 0.64  0.007 0.03  0.557 0.068]
 [0.549 0.353 0.019 1.059 0.435 0.28  0.623 0.015 0.861 0.736 0.029 0.13
  0.038 0.35  0.713 0.557 0.163 0.002 0.736 0.173]
 [0.01  0.008 0.002 0.181 0.019 0.005 0.066 0.003 0.092 0.111 0.005 0.01
  0.011 0.014 0.072 0.009 0.006 0.004 0.004 0.003]
 [0.351 0.116 0.011 1.471 0.196 0.173 0.35  0.008 1.134 0.542 0.026 0.141
  0.047 0.155 0.48  0.283 0.094 0.029 0.787 0.004]]
[[0.    0.    0.    0.303 0.    0.    0.    0.    0.    0.336 0.    0.
  0.    0.    0.427 0.    0.    0.    0.379 0.   ]
 [0.    0.    0.    0.732 0.    0.    0.    0.    1.524 0.406 0.    0.
  0.    0.    0.753 0.    0.    0.    0.441 0.   ]
 [0.989 0.663 0.    1.235 1.43  0.715 1.224 0.    2.018 1.319 0.306 0.596
  0.    1.1   2.107 0.922 0.    0.    2.206 0.34 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.554 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.383 0.    0.    0.602 0.344 0.    0.    0.    0.685 0.336 0.    0.
  0.    0.39  0.643 0.    0.    0.    0.744 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.852 0.488 0.    1.582 0.89  0.636 3.699 0.    1.117 1.134 0.    0.
  0.    0.443 1.087 1.366 0.    0.    0.948 0.412]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.476 0.    0.    0.923 0.38  0.    0.502 0.    1.105 1.059 0.    0.336
  0.    0.305 0.643 0.    0.    0.    0.583 0.   ]
 [0.    0.    0.    0.362 0.    0.    0.    0.    0.673 0.434 0.    0.
  0.    0.    0.516 0.319 0.    0.    0.468 0.   ]
 [0.663 0.    0.    1.143 0.    0.    0.586 0.    0.566 1.533 0.    0.
  0.    0.    0.414 0.    0.    0.    0.424 0.   ]
 [0.    0.    0.    0.556 0.    0.    0.    0.    0.365 0.519 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.348 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.424 0.    0.    0.
  0.    0.    0.31  0.    0.    0.    0.    0.   ]
 [0.332 0.654 0.    0.619 0.345 0.    0.518 0.    0.646 1.315 0.    0.
  0.    0.375 0.856 0.64  0.    0.    0.557 0.   ]
 [0.549 0.353 0.    1.059 0.435 0.    0.623 0.    0.861 0.736 0.    0.
  0.    0.35  0.713 0.557 0.    0.    0.736 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.351 0.    0.    1.471 0.    0.    0.35  0.    1.134 0.542 0.    0.
  0.    0.    0.48  0.    0.    0.    0.787 0.   ]]
{'fdr': 0.7980769230769231, 'tpr': 0.525, 'fpr': 0.5533333333333333, 'f1': 0.29166666666666663, 'shd': 98, 'npred': 104, 'ntrue': 40}
[1.641e-02 3.241e-03 3.025e-01 1.852e-02 8.983e-03 6.213e-02 3.777e-03
 2.261e-01 3.364e-01 8.781e-03 2.755e-02 4.594e-03 1.540e-02 4.268e-01
 8.250e-02 1.123e-02 5.964e-03 3.795e-01 1.260e-02 1.457e-01 3.022e-03
 7.319e-01 2.481e-01 1.389e-01 1.405e-01 1.027e-02 1.524e+00 4.059e-01
 1.792e-02 1.239e-01 1.876e-02 1.694e-01 7.529e-01 2.027e-01 1.625e-02
 1.424e-02 4.408e-01 4.333e-02 9.887e-01 6.630e-01 1.235e+00 1.430e+00
 7.154e-01 1.224e+00 2.708e-01 2.018e+00 1.319e+00 3.058e-01 5.965e-01
 2.322e-01 1.100e+00 2.107e+00 9.220e-01 2.520e-01 2.387e-01 2.206e+00
 3.405e-01 1.249e-02 4.950e-03 1.117e-03 5.996e-03 5.703e-03 5.143e-02
 1.625e-03 8.497e-02 1.646e-02 3.136e-03 8.565e-03 2.282e-03 6.410e-03
 9.489e-03 1.668e-02 7.515e-03 2.541e-03 2.334e-02 2.872e-03 1.657e-01
 1.816e-02 1.827e-03 5.536e-01 9.623e-03 1.227e-01 3.348e-03 2.032e-01
 1.950e-01 1.048e-02 2.065e-02 1.135e-02 5.517e-02 2.053e-01 9.821e-02
 1.370e-02 7.822e-03 2.045e-01 1.960e-02 3.830e-01 3.772e-02 7.999e-03
 6.020e-01 3.436e-01 2.016e-01 6.784e-03 6.850e-01 3.363e-01 3.471e-02
 8.098e-02 2.720e-02 3.904e-01 6.431e-01 2.856e-01 8.829e-03 1.264e-02
 7.438e-01 2.722e-02 6.731e-02 1.761e-02 1.925e-03 1.149e-01 3.836e-02
 2.008e-02 5.581e-04 1.705e-01 9.709e-02 7.537e-03 1.457e-02 5.123e-03
 2.089e-02 7.713e-02 5.996e-02 4.799e-03 3.822e-03 9.251e-02 7.312e-03
 8.518e-01 4.878e-01 1.308e-02 1.582e+00 8.899e-01 6.359e-01 3.699e+00
 1.117e+00 1.134e+00 2.314e-01 2.862e-01 2.705e-01 4.425e-01 1.087e+00
 1.366e+00 2.687e-01 2.701e-01 9.478e-01 4.123e-01 2.129e-02 2.325e-03
 1.045e-03 6.158e-02 2.001e-02 4.865e-03 2.938e-02 2.619e-03 1.939e-02
 2.530e-03 3.520e-03 3.125e-03 7.690e-03 2.594e-02 1.054e-02 3.670e-03
 3.117e-03 7.832e-02 3.255e-03 9.921e-03 4.992e-03 1.736e-03 2.577e-01
 2.027e-02 1.085e-02 4.855e-02 2.024e-03 1.558e-01 3.103e-03 7.053e-03
 1.475e-03 9.398e-03 9.115e-02 1.660e-02 2.751e-03 3.067e-03 3.695e-02
 5.706e-03 4.756e-01 1.922e-01 1.334e-02 9.230e-01 3.802e-01 1.211e-01
 5.017e-01 2.416e-02 1.105e+00 1.059e+00 3.360e-01 2.399e-01 3.051e-01
 6.427e-01 1.566e-01 1.446e-02 1.520e-01 5.834e-01 1.683e-01 1.490e-01
 3.420e-02 5.731e-03 3.623e-01 1.694e-01 5.039e-02 2.644e-01 9.327e-03
 6.731e-01 4.342e-01 1.045e-02 2.832e-02 9.465e-02 5.165e-01 3.187e-01
 1.294e-02 2.395e-02 4.677e-01 2.752e-02 6.633e-01 2.609e-01 2.933e-02
 1.143e+00 2.327e-01 1.134e-01 5.860e-01 1.389e-02 5.656e-01 1.533e+00
 2.134e-02 1.517e-01 1.661e-01 4.141e-01 2.183e-01 3.790e-02 1.529e-01
 4.236e-01 1.556e-01 2.440e-01 2.668e-02 2.657e-03 5.563e-01 8.262e-02
 9.862e-03 2.399e-01 8.347e-03 3.646e-01 5.189e-01 1.189e-02 5.016e-02
 1.570e-02 2.800e-01 9.638e-02 8.447e-03 1.383e-02 2.815e-01 2.622e-02
 1.193e-02 4.563e-03 7.739e-04 3.480e-01 1.536e-02 5.901e-03 8.215e-02
 2.124e-03 1.767e-01 7.383e-02 5.311e-03 5.288e-03 6.717e-03 1.036e-02
 2.050e-02 7.487e-03 3.960e-03 1.057e-01 6.354e-03 7.304e-02 2.708e-02
 3.664e-03 2.899e-01 4.644e-02 1.297e-02 1.688e-01 1.757e-03 4.238e-01
 2.201e-01 2.015e-02 1.199e-02 1.438e-02 6.087e-02 3.101e-01 8.548e-03
 6.656e-03 2.459e-01 9.828e-03 3.323e-01 6.544e-01 1.722e-02 6.190e-01
 3.450e-01 2.745e-01 5.178e-01 1.722e-02 6.455e-01 1.315e+00 2.010e-01
 2.214e-01 1.020e-01 3.754e-01 8.560e-01 6.400e-01 3.020e-02 5.571e-01
 6.850e-02 5.486e-01 3.533e-01 1.919e-02 1.059e+00 4.349e-01 2.801e-01
 6.227e-01 1.484e-02 8.611e-01 7.360e-01 2.911e-02 1.295e-01 3.767e-02
 3.498e-01 7.127e-01 5.573e-01 1.627e-01 7.362e-01 1.726e-01 1.030e-02
 8.085e-03 1.553e-03 1.807e-01 1.932e-02 4.955e-03 6.633e-02 2.569e-03
 9.151e-02 1.110e-01 4.986e-03 9.894e-03 1.070e-02 1.395e-02 7.242e-02
 9.334e-03 6.184e-03 4.089e-03 3.058e-03 3.513e-01 1.159e-01 1.134e-02
 1.471e+00 1.958e-01 1.735e-01 3.505e-01 7.894e-03 1.134e+00 5.417e-01
 2.580e-02 1.411e-01 4.689e-02 1.552e-01 4.797e-01 2.834e-01 9.412e-02
 2.946e-02 7.865e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7041911764705883, 0.34111034534615736)
Iterations 2500
Achieves (3.6993101023104464, 1e-05)-DP
