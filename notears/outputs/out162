samples  5000  graph  20 40 ER mlp  minibatch size  100  noise  0.8  minibatches per NN training  63 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617290166446
iteration 1 in outer loop, alpha = 1.5634617290166446, rho = 1.0, h = 1.5634617290166446
cuda
iteration 1 in inner loop,alpha 1.5634617290166446 rho 1.0 h 1.0381693524821998
iteration 2 in inner loop,alpha 1.5634617290166446 rho 10.0 h 0.4486130544339133
iteration 3 in inner loop,alpha 1.5634617290166446 rho 100.0 h 0.13645699227387453
iteration 2 in outer loop, alpha = 15.209160956404098, rho = 100.0, h = 0.13645699227387453
cuda
iteration 1 in inner loop,alpha 15.209160956404098 rho 100.0 h 0.07242523875783746
iteration 2 in inner loop,alpha 15.209160956404098 rho 1000.0 h 0.02272817333826893
iteration 3 in outer loop, alpha = 37.93733429467303, rho = 1000.0, h = 0.02272817333826893
cuda
iteration 1 in inner loop,alpha 37.93733429467303 rho 1000.0 h 0.010195562783067658
iteration 2 in inner loop,alpha 37.93733429467303 rho 10000.0 h 0.0032923257204373613
iteration 4 in outer loop, alpha = 70.86059149904665, rho = 10000.0, h = 0.0032923257204373613
cuda
iteration 1 in inner loop,alpha 70.86059149904665 rho 10000.0 h 0.0015081687033671187
iteration 2 in inner loop,alpha 70.86059149904665 rho 100000.0 h 0.0005712122161831701
iteration 5 in outer loop, alpha = 127.98181311736366, rho = 100000.0, h = 0.0005712122161831701
cuda
iteration 1 in inner loop,alpha 127.98181311736366 rho 100000.0 h 0.00026301261159389355
iteration 6 in outer loop, alpha = 390.99442471125724, rho = 1000000.0, h = 0.00026301261159389355
Threshold 0.3
[[0.    0.011 0.044 2.301 0.033 0.032 0.041 0.005 1.994 0.014 1.847 0.006
  0.101 0.003 0.208 0.932 0.012 0.001 0.038 0.218]
 [0.033 0.005 0.211 0.058 0.382 0.111 0.03  0.002 3.05  0.049 0.018 0.009
  0.601 0.016 1.806 1.569 0.287 0.    0.227 0.301]
 [0.02  0.    0.003 0.069 0.127 1.162 0.    0.    2.294 0.078 0.06  0.
  2.292 0.018 1.348 0.879 0.    0.    3.746 2.216]
 [0.    0.    0.    0.003 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.152 1.495 0.    0.    0.    0.001]
 [0.    0.    0.    1.666 0.001 0.201 0.    0.    0.001 0.032 0.018 0.
  0.375 0.005 0.089 0.34  0.001 0.    0.036 0.177]
 [0.    0.    0.    0.106 0.002 0.001 0.    0.    0.001 0.037 0.065 0.
  0.233 0.    1.904 0.448 0.    0.    0.001 0.103]
 [0.012 0.021 0.566 0.056 0.223 0.104 0.005 0.    0.034 0.008 0.016 0.
  0.036 0.027 0.067 1.634 0.011 0.008 0.063 0.029]
 [0.004 0.017 0.653 0.049 0.184 0.095 4.456 0.001 0.148 0.011 0.005 0.049
  0.008 0.004 0.125 0.235 0.064 0.009 0.061 0.05 ]
 [0.    0.    0.    0.062 0.983 0.125 0.001 0.    0.002 0.048 0.046 0.
  0.279 0.    0.298 0.496 0.    0.    0.017 0.261]
 [0.    0.    0.    0.095 0.    0.    0.    0.    0.    0.001 0.828 0.
  0.    0.    0.177 0.255 0.    0.    0.    0.19 ]
 [0.    0.    0.    0.099 0.001 0.    0.    0.    0.    0.    0.001 0.
  0.    0.    1.335 0.281 0.    0.    0.    2.286]
 [0.004 0.001 2.725 0.067 0.022 0.213 3.595 0.005 0.218 0.016 0.015 0.001
  0.227 0.007 0.579 0.356 0.009 0.029 0.209 0.152]
 [0.001 0.    0.    0.105 0.    0.001 0.    0.    0.001 3.203 0.129 0.
  0.002 0.    0.371 0.523 0.    0.    0.    0.415]
 [0.005 0.01  0.016 0.053 0.03  0.12  0.008 0.009 0.096 0.028 0.076 0.015
  0.275 0.    0.172 0.167 0.013 0.023 2.629 0.242]
 [0.    0.    0.    0.004 0.    0.    0.    0.    0.001 0.    0.    0.
  0.    0.    0.005 0.273 0.    0.    0.    0.004]
 [0.    0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.006 0.009 0.    0.    0.    0.   ]
 [0.012 0.007 0.33  0.063 0.32  0.198 0.03  0.002 0.268 0.048 0.053 0.009
  0.4   0.011 1.843 1.505 0.003 0.    0.327 0.443]
 [0.006 4.263 0.005 0.052 0.213 0.007 0.011 0.006 0.59  0.004 0.006 0.003
  1.39  0.008 0.151 0.251 2.995 0.001 1.412 0.991]
 [0.006 0.001 0.    0.061 0.017 1.218 0.    0.    0.085 0.049 0.017 0.
  1.034 0.    0.358 1.333 0.    0.    0.003 0.452]
 [0.    0.    0.    2.077 0.    0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.222 0.332 0.    0.    0.    0.003]]
[[0.    0.    0.    2.301 0.    0.    0.    0.    1.994 0.    1.847 0.
  0.    0.    0.    0.932 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.382 0.    0.    0.    3.05  0.    0.    0.
  0.601 0.    1.806 1.569 0.    0.    0.    0.301]
 [0.    0.    0.    0.    0.    1.162 0.    0.    2.294 0.    0.    0.
  2.292 0.    1.348 0.879 0.    0.    3.746 2.216]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.495 0.    0.    0.    0.   ]
 [0.    0.    0.    1.666 0.    0.    0.    0.    0.    0.    0.    0.
  0.375 0.    0.    0.34  0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.904 0.448 0.    0.    0.    0.   ]
 [0.    0.    0.566 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.634 0.    0.    0.    0.   ]
 [0.    0.    0.653 0.    0.    0.    4.456 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.983 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.496 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.828 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.335 0.    0.    0.    0.    2.286]
 [0.    0.    2.725 0.    0.    0.    3.595 0.    0.    0.    0.    0.
  0.    0.    0.579 0.356 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.203 0.    0.
  0.    0.    0.371 0.523 0.    0.    0.    0.415]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.629 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.33  0.    0.32  0.    0.    0.    0.    0.    0.    0.
  0.4   0.    1.843 1.505 0.    0.    0.327 0.443]
 [0.    4.263 0.    0.    0.    0.    0.    0.    0.59  0.    0.    0.
  1.39  0.    0.    0.    2.995 0.    1.412 0.991]
 [0.    0.    0.    0.    0.    1.218 0.    0.    0.    0.    0.    0.
  1.034 0.    0.358 1.333 0.    0.    0.    0.452]
 [0.    0.    0.    2.077 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.332 0.    0.    0.    0.   ]]
{'fdr': 0.3442622950819672, 'tpr': 1.0, 'fpr': 0.14, 'f1': 0.7920792079207921, 'shd': 21, 'npred': 61, 'ntrue': 40}
[1.087e-02 4.369e-02 2.301e+00 3.254e-02 3.198e-02 4.060e-02 5.295e-03
 1.994e+00 1.361e-02 1.847e+00 5.703e-03 1.007e-01 2.668e-03 2.080e-01
 9.317e-01 1.168e-02 1.056e-03 3.752e-02 2.180e-01 3.263e-02 2.107e-01
 5.837e-02 3.816e-01 1.109e-01 2.984e-02 1.990e-03 3.050e+00 4.935e-02
 1.763e-02 8.918e-03 6.007e-01 1.643e-02 1.806e+00 1.569e+00 2.868e-01
 3.750e-04 2.268e-01 3.005e-01 2.050e-02 3.555e-04 6.909e-02 1.272e-01
 1.162e+00 4.026e-04 6.137e-05 2.294e+00 7.843e-02 6.010e-02 2.084e-04
 2.292e+00 1.759e-02 1.348e+00 8.788e-01 3.677e-04 6.767e-05 3.746e+00
 2.216e+00 1.025e-05 1.163e-04 1.884e-04 5.827e-04 1.321e-03 1.456e-04
 3.745e-05 4.610e-04 6.979e-05 4.653e-05 9.601e-05 6.456e-05 2.744e-04
 1.515e-01 1.495e+00 2.882e-04 9.303e-05 2.863e-04 1.019e-03 1.058e-04
 1.038e-04 1.614e-04 1.666e+00 2.008e-01 3.987e-04 3.208e-05 6.074e-04
 3.223e-02 1.755e-02 6.131e-05 3.747e-01 4.761e-03 8.918e-02 3.401e-01
 5.514e-04 3.213e-05 3.573e-02 1.766e-01 1.140e-04 1.916e-04 1.314e-04
 1.058e-01 1.806e-03 1.290e-04 1.251e-05 1.191e-03 3.666e-02 6.523e-02
 3.328e-05 2.333e-01 8.679e-06 1.904e+00 4.475e-01 1.524e-04 1.837e-05
 7.370e-04 1.035e-01 1.219e-02 2.137e-02 5.662e-01 5.569e-02 2.230e-01
 1.036e-01 1.713e-04 3.428e-02 8.066e-03 1.612e-02 2.638e-05 3.631e-02
 2.721e-02 6.661e-02 1.634e+00 1.070e-02 8.254e-03 6.311e-02 2.921e-02
 4.177e-03 1.722e-02 6.530e-01 4.881e-02 1.836e-01 9.495e-02 4.456e+00
 1.478e-01 1.083e-02 5.363e-03 4.946e-02 7.774e-03 3.520e-03 1.247e-01
 2.349e-01 6.352e-02 9.426e-03 6.146e-02 4.962e-02 4.304e-05 2.531e-04
 3.460e-04 6.188e-02 9.826e-01 1.250e-01 5.935e-04 2.191e-05 4.756e-02
 4.630e-02 8.546e-05 2.791e-01 1.621e-04 2.984e-01 4.963e-01 4.439e-04
 4.760e-05 1.713e-02 2.607e-01 6.114e-05 1.851e-06 4.519e-05 9.546e-02
 9.836e-05 3.896e-04 9.820e-06 9.330e-06 1.995e-04 8.282e-01 2.326e-05
 1.396e-04 4.738e-05 1.771e-01 2.553e-01 5.904e-05 1.256e-05 2.609e-05
 1.899e-01 5.381e-05 1.939e-04 3.781e-05 9.942e-02 6.032e-04 3.192e-04
 3.680e-04 2.722e-05 4.481e-04 9.617e-05 3.081e-05 1.205e-05 4.635e-05
 1.335e+00 2.813e-01 3.019e-04 7.528e-06 5.781e-05 2.286e+00 4.158e-03
 1.073e-03 2.725e+00 6.672e-02 2.159e-02 2.134e-01 3.595e+00 5.342e-03
 2.180e-01 1.557e-02 1.497e-02 2.273e-01 6.983e-03 5.790e-01 3.563e-01
 9.462e-03 2.942e-02 2.092e-01 1.516e-01 7.131e-04 5.006e-05 1.288e-04
 1.054e-01 1.822e-04 1.289e-03 3.985e-05 1.422e-05 5.607e-04 3.203e+00
 1.292e-01 4.744e-05 5.062e-06 3.709e-01 5.231e-01 4.039e-04 3.978e-05
 2.247e-04 4.145e-01 4.926e-03 9.565e-03 1.639e-02 5.290e-02 3.015e-02
 1.197e-01 7.811e-03 9.359e-03 9.587e-02 2.841e-02 7.619e-02 1.516e-02
 2.751e-01 1.718e-01 1.668e-01 1.327e-02 2.338e-02 2.629e+00 2.421e-01
 7.896e-06 4.380e-04 9.301e-05 4.058e-03 3.544e-04 1.184e-04 1.256e-04
 1.339e-05 8.044e-04 1.027e-04 2.101e-04 3.292e-05 8.675e-05 4.275e-05
 2.731e-01 3.556e-04 7.827e-05 2.780e-05 3.934e-03 7.047e-06 2.020e-04
 4.957e-05 1.330e-04 3.819e-05 7.999e-04 3.901e-04 3.422e-05 2.600e-05
 5.041e-05 5.458e-05 1.450e-05 2.963e-05 1.138e-05 5.815e-03 9.149e-06
 3.136e-05 5.091e-05 1.008e-04 1.230e-02 6.888e-03 3.295e-01 6.323e-02
 3.204e-01 1.983e-01 3.013e-02 1.905e-03 2.678e-01 4.758e-02 5.261e-02
 8.649e-03 3.999e-01 1.111e-02 1.843e+00 1.505e+00 4.216e-04 3.272e-01
 4.430e-01 5.776e-03 4.263e+00 5.297e-03 5.217e-02 2.125e-01 7.428e-03
 1.138e-02 6.203e-03 5.904e-01 4.143e-03 6.470e-03 3.323e-03 1.390e+00
 8.314e-03 1.506e-01 2.512e-01 2.995e+00 1.412e+00 9.909e-01 6.095e-03
 8.145e-04 4.025e-04 6.117e-02 1.691e-02 1.218e+00 1.165e-04 2.654e-05
 8.492e-02 4.859e-02 1.728e-02 9.517e-05 1.034e+00 1.131e-05 3.575e-01
 1.333e+00 1.579e-04 9.139e-05 4.516e-01 2.689e-05 2.490e-05 1.741e-04
 2.077e+00 2.677e-04 1.388e-03 1.057e-04 2.964e-05 4.563e-04 2.943e-05
 4.930e-05 7.064e-05 2.782e-05 7.182e-05 2.218e-01 3.318e-01 6.245e-05
 4.170e-05 1.332e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9992647058823529, 0.9934713912844813)
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
total norm for a microbatch 61.345507687364424 clip 9.735638618462225
total norm for a microbatch 69.71453236948156 clip 24.671505942188162
total norm for a microbatch 53.98443306223787 clip 26.799095292333973
total norm for a microbatch 61.26820051065939 clip 53.560261584842536
total norm for a microbatch 34.72271900623902 clip 47.85605430875439
cuda
Objective function 133.69 = squared loss an data 130.72 + 0.5*rho*h**2 2.204029 + alpha*h 0.000000 + L2reg 0.49 + L1reg 0.27 ; SHD = 118 ; DAG False
Proportion of microbatches that were clipped  0.9307765754296626
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 2.0995377399980732
iteration 1 in outer loop, alpha = 2.0995377399980732, rho = 1.0, h = 2.0995377399980732
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 138.10 = squared loss an data 130.72 + 0.5*rho*h**2 2.204029 + alpha*h 4.408059 + L2reg 0.49 + L1reg 0.27 ; SHD = 118 ; DAG False
total norm for a microbatch 72.30573671672158 clip 5.796128526196119
total norm for a microbatch 92.09828211182584 clip 7.00071832024005
total norm for a microbatch 75.48851534084412 clip 38.02246464080932
total norm for a microbatch 137.78456639100753 clip 58.46987168892802
total norm for a microbatch 112.6687243607358 clip 68.87358411556468
cuda
Objective function 60.64 = squared loss an data 52.03 + 0.5*rho*h**2 2.602399 + alpha*h 4.789891 + L2reg 0.95 + L1reg 0.26 ; SHD = 87 ; DAG False
Proportion of microbatches that were clipped  0.9487512163477133
iteration 1 in inner loop, alpha 2.0995377399980732 rho 1.0 h 2.281402521109488
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 84.06 = squared loss an data 52.03 + 0.5*rho*h**2 26.023987 + alpha*h 4.789891 + L2reg 0.95 + L1reg 0.26 ; SHD = 87 ; DAG False
total norm for a microbatch 154.25901823033755 clip 3.856639643488005
total norm for a microbatch 101.0548676059229 clip 20.829888840481726
total norm for a microbatch 76.12414562176247 clip 39.41742534075676
total norm for a microbatch 80.72621160141362 clip 85.43696353737252
total norm for a microbatch 178.7505892566933 clip 114.09293749139097
cuda
Objective function 42.87 = squared loss an data 33.50 + 0.5*rho*h**2 5.665096 + alpha*h 2.234819 + L2reg 1.23 + L1reg 0.23 ; SHD = 65 ; DAG False
Proportion of microbatches that were clipped  0.9630909376472436
iteration 2 in inner loop, alpha 2.0995377399980732 rho 10.0 h 1.0644337241236101
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 93.85 = squared loss an data 33.50 + 0.5*rho*h**2 56.650958 + alpha*h 2.234819 + L2reg 1.23 + L1reg 0.23 ; SHD = 65 ; DAG False
total norm for a microbatch 179.46550675474043 clip 1.7191142432582052
total norm for a microbatch 207.47990020216258 clip 23.116110703380883
total norm for a microbatch 194.10386161858275 clip 32.907788644336755
total norm for a microbatch 158.74887694977002 clip 135.05404242286747
cuda
Objective function 43.98 = squared loss an data 34.02 + 0.5*rho*h**2 7.503899 + alpha*h 0.813359 + L2reg 1.43 + L1reg 0.21 ; SHD = 54 ; DAG False
Proportion of microbatches that were clipped  0.9735978753319794
iteration 3 in inner loop, alpha 2.0995377399980732 rho 100.0 h 0.38739899968724956
iteration 2 in outer loop, alpha = 40.839437708723025, rho = 100.0, h = 0.38739899968724956
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 58.99 = squared loss an data 34.02 + 0.5*rho*h**2 7.503899 + alpha*h 15.821157 + L2reg 1.43 + L1reg 0.21 ; SHD = 54 ; DAG False
total norm for a microbatch 226.53236372589353 clip 1.7234391714497566
total norm for a microbatch 249.1579390897739 clip 1.7234391714497566
total norm for a microbatch 138.0510544573646 clip 2.4961687087424425
total norm for a microbatch 135.3192048997246 clip 6.133355975213887
total norm for a microbatch 200.69584842267025 clip 7.901057120076323
total norm for a microbatch 155.143513303044 clip 13.659422633934094
total norm for a microbatch 202.36187708249997 clip 16.47420604951095
total norm for a microbatch 266.2917450181748 clip 91.14236758354487
total norm for a microbatch 161.67683192608732 clip 150.19767903733097
total norm for a microbatch 179.39410529964525 clip 162.02268157308163
cuda
Objective function 47.73 = squared loss an data 32.87 + 0.5*rho*h**2 3.016142 + alpha*h 10.030455 + L2reg 1.61 + L1reg 0.20 ; SHD = 48 ; DAG False
Proportion of microbatches that were clipped  0.9745389260649584
iteration 1 in inner loop, alpha 40.839437708723025 rho 100.0 h 0.24560707937776272
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 74.88 = squared loss an data 32.87 + 0.5*rho*h**2 30.161419 + alpha*h 10.030455 + L2reg 1.61 + L1reg 0.20 ; SHD = 48 ; DAG False
total norm for a microbatch 256.822032849434 clip 2.08949831268089
total norm for a microbatch 190.64228015091797 clip 6.143904674117205
total norm for a microbatch 244.43885335704778 clip 13.769505993346549
cuda
Objective function 44.73 = squared loss an data 32.34 + 0.5*rho*h**2 5.965620 + alpha*h 4.460901 + L2reg 1.76 + L1reg 0.19 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.9751315160210425
iteration 2 in inner loop, alpha 40.839437708723025 rho 1000.0 h 0.10923021978558012
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 98.42 = squared loss an data 32.34 + 0.5*rho*h**2 59.656205 + alpha*h 4.460901 + L2reg 1.76 + L1reg 0.19 ; SHD = 51 ; DAG True
total norm for a microbatch 286.88902846059085 clip 5.06717997469573
total norm for a microbatch 224.06651556770282 clip 47.46975445010847
total norm for a microbatch 257.99929803963386 clip 52.02686497924699
total norm for a microbatch 225.94093509933398 clip 108.16866203959832
total norm for a microbatch 233.73366400441492 clip 156.50102333012663
cuda
Objective function 44.18 = squared loss an data 32.02 + 0.5*rho*h**2 8.430917 + alpha*h 1.676997 + L2reg 1.87 + L1reg 0.19 ; SHD = 55 ; DAG True
Proportion of microbatches that were clipped  0.9819150653964153
iteration 3 in inner loop, alpha 40.839437708723025 rho 10000.0 h 0.041063164215707104
iteration 3 in outer loop, alpha = 451.4710798657941, rho = 10000.0, h = 0.041063164215707104
cuda
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 61.04 = squared loss an data 32.02 + 0.5*rho*h**2 8.430917 + alpha*h 18.538831 + L2reg 1.87 + L1reg 0.19 ; SHD = 55 ; DAG True
total norm for a microbatch 277.1325164426894 clip 1.0
total norm for a microbatch 350.16423960381695 clip 1.401524849263198
total norm for a microbatch 141.24313500531855 clip 24.48492849976962
total norm for a microbatch 166.19289180144742 clip 31.635876672410046
total norm for a microbatch 331.9961310793579 clip 60.189203515601456
cuda
Objective function 45.27 = squared loss an data 29.22 + 0.5*rho*h**2 2.916491 + alpha*h 10.903735 + L2reg 2.03 + L1reg 0.19 ; SHD = 68 ; DAG True
Proportion of microbatches that were clipped  0.9871039643368891
iteration 1 in inner loop, alpha 451.4710798657941 rho 10000.0 h 0.024151568054229244
noise_multiplier  0.8  noise_multiplier_b  5.0  noise_multiplier_delta  0.802572353905128
cuda
Objective function 71.51 = squared loss an data 29.22 + 0.5*rho*h**2 29.164912 + alpha*h 10.903735 + L2reg 2.03 + L1reg 0.19 ; SHD = 68 ; DAG True
total norm for a microbatch 591.4227135306302 clip 1.0
total norm for a microbatch 922.0566527296445 clip 1.7444983489094892
total norm for a microbatch 198.6750641202384 clip 10.755457944377467
total norm for a microbatch 236.03252005828168 clip 20.31047702811882
total norm for a microbatch 322.61420237429644 clip 54.65658902491213
cuda
Objective function 47.94 = squared loss an data 29.38 + 0.5*rho*h**2 9.896897 + alpha*h 6.351766 + L2reg 2.11 + L1reg 0.20 ; SHD = 68 ; DAG True
Proportion of microbatches that were clipped  1.0
iteration 2 in inner loop, alpha 451.4710798657941 rho 100000.0 h 0.014069041863923104
iteration 4 in outer loop, alpha = 14520.512943788899, rho = 1000000.0, h = 0.014069041863923104
Threshold 0.3
[[0.006 0.079 0.024 0.223 0.216 0.058 0.291 0.039 0.488 0.324 0.354 0.134
  0.029 0.214 0.259 0.34  0.185 0.036 0.534 0.043]
 [0.06  0.011 0.028 0.104 0.07  0.074 0.127 0.022 1.253 0.249 0.142 0.047
  0.027 0.08  0.61  0.263 0.362 0.005 0.164 0.05 ]
 [0.267 0.286 0.008 0.363 0.289 0.252 0.353 0.026 1.031 0.305 0.172 0.628
  0.019 0.176 0.503 0.665 0.203 0.076 2.405 0.685]
 [0.032 0.05  0.013 0.007 0.005 0.028 0.042 0.009 0.07  0.035 0.032 0.023
  0.009 0.023 0.11  0.066 0.031 0.012 0.152 0.004]
 [0.038 0.074 0.032 1.109 0.007 0.026 0.079 0.008 0.798 0.127 0.211 0.094
  0.025 0.062 0.238 0.153 0.073 0.015 0.38  0.034]
 [0.133 0.076 0.027 0.183 0.275 0.007 0.109 0.02  0.67  0.185 0.273 0.053
  0.033 0.116 0.509 0.302 0.186 0.019 0.941 0.043]
 [0.024 0.041 0.023 0.113 0.079 0.032 0.01  0.004 0.113 0.058 0.068 0.049
  0.017 0.045 0.072 0.411 0.051 0.011 0.172 0.027]
 [0.226 0.295 0.211 0.38  0.585 0.35  3.505 0.007 0.338 0.448 0.353 0.607
  0.071 0.38  0.541 0.724 0.428 0.056 0.768 0.194]
 [0.017 0.007 0.006 0.089 0.012 0.013 0.037 0.011 0.006 0.023 0.019 0.018
  0.008 0.018 0.066 0.108 0.03  0.004 0.043 0.016]
 [0.024 0.036 0.032 0.232 0.073 0.042 0.139 0.017 0.265 0.01  0.053 0.086
  0.007 0.041 0.094 0.19  0.033 0.012 0.289 0.057]
 [0.033 0.026 0.053 0.324 0.041 0.027 0.123 0.024 0.403 0.124 0.01  0.091
  0.065 0.091 0.077 0.169 0.069 0.017 0.209 0.097]
 [0.064 0.178 0.011 0.228 0.131 0.081 0.158 0.011 0.316 0.147 0.084 0.007
  0.02  0.065 0.183 0.193 0.067 0.015 0.369 0.036]
 [0.219 0.249 0.458 0.351 0.231 0.31  0.426 0.076 0.512 1.251 0.129 0.421
  0.008 0.221 0.614 0.458 0.252 0.033 0.335 0.343]
 [0.035 0.07  0.025 0.392 0.115 0.085 0.139 0.014 0.409 0.171 0.07  0.145
  0.034 0.009 0.48  0.17  0.156 0.015 0.454 0.105]
 [0.034 0.016 0.014 0.093 0.03  0.015 0.1   0.007 0.095 0.078 0.081 0.031
  0.006 0.017 0.008 0.256 0.008 0.005 0.069 0.031]
 [0.021 0.03  0.013 0.109 0.036 0.022 0.027 0.005 0.072 0.062 0.054 0.041
  0.019 0.045 0.028 0.007 0.034 0.011 0.058 0.013]
 [0.06  0.034 0.022 0.211 0.093 0.049 0.185 0.023 0.312 0.216 0.064 0.112
  0.03  0.046 1.02  0.258 0.007 0.006 0.237 0.061]
 [0.213 2.858 0.153 0.516 0.528 0.242 0.424 0.092 0.65  0.335 0.502 0.344
  0.214 0.6   0.582 0.584 1.199 0.003 0.551 0.11 ]
 [0.016 0.059 0.004 0.058 0.027 0.008 0.037 0.009 0.198 0.022 0.035 0.018
  0.009 0.025 0.065 0.09  0.021 0.007 0.009 0.017]
 [0.134 0.111 0.008 1.46  0.221 0.135 0.228 0.025 0.417 0.244 0.083 0.279
  0.018 0.093 0.265 0.562 0.121 0.039 0.526 0.009]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.488 0.324 0.354 0.
  0.    0.    0.    0.34  0.    0.    0.534 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.253 0.    0.    0.
  0.    0.    0.61  0.    0.362 0.    0.    0.   ]
 [0.    0.    0.    0.363 0.    0.    0.353 0.    1.031 0.305 0.    0.628
  0.    0.    0.503 0.665 0.    0.    2.405 0.685]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.109 0.    0.    0.    0.    0.798 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.38  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.67  0.    0.    0.
  0.    0.    0.509 0.302 0.    0.    0.941 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.411 0.    0.    0.    0.   ]
 [0.    0.    0.    0.38  0.585 0.35  3.505 0.    0.338 0.448 0.353 0.607
  0.    0.38  0.541 0.724 0.428 0.    0.768 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.324 0.    0.    0.    0.    0.403 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.316 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.369 0.   ]
 [0.    0.    0.458 0.351 0.    0.31  0.426 0.    0.512 1.251 0.    0.421
  0.    0.    0.614 0.458 0.    0.    0.335 0.343]
 [0.    0.    0.    0.392 0.    0.    0.    0.    0.409 0.    0.    0.
  0.    0.    0.48  0.    0.    0.    0.454 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.312 0.    0.    0.
  0.    0.    1.02  0.    0.    0.    0.    0.   ]
 [0.    2.858 0.    0.516 0.528 0.    0.424 0.    0.65  0.335 0.502 0.344
  0.    0.6   0.582 0.584 1.199 0.    0.551 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.46  0.    0.    0.    0.    0.417 0.    0.    0.
  0.    0.    0.    0.562 0.    0.    0.526 0.   ]]
{'fdr': 0.7105263157894737, 'tpr': 0.55, 'fpr': 0.36, 'f1': 0.37931034482758624, 'shd': 68, 'npred': 76, 'ntrue': 40}
[0.079 0.024 0.223 0.216 0.058 0.291 0.039 0.488 0.324 0.354 0.134 0.029
 0.214 0.259 0.34  0.185 0.036 0.534 0.043 0.06  0.028 0.104 0.07  0.074
 0.127 0.022 1.253 0.249 0.142 0.047 0.027 0.08  0.61  0.263 0.362 0.005
 0.164 0.05  0.267 0.286 0.363 0.289 0.252 0.353 0.026 1.031 0.305 0.172
 0.628 0.019 0.176 0.503 0.665 0.203 0.076 2.405 0.685 0.032 0.05  0.013
 0.005 0.028 0.042 0.009 0.07  0.035 0.032 0.023 0.009 0.023 0.11  0.066
 0.031 0.012 0.152 0.004 0.038 0.074 0.032 1.109 0.026 0.079 0.008 0.798
 0.127 0.211 0.094 0.025 0.062 0.238 0.153 0.073 0.015 0.38  0.034 0.133
 0.076 0.027 0.183 0.275 0.109 0.02  0.67  0.185 0.273 0.053 0.033 0.116
 0.509 0.302 0.186 0.019 0.941 0.043 0.024 0.041 0.023 0.113 0.079 0.032
 0.004 0.113 0.058 0.068 0.049 0.017 0.045 0.072 0.411 0.051 0.011 0.172
 0.027 0.226 0.295 0.211 0.38  0.585 0.35  3.505 0.338 0.448 0.353 0.607
 0.071 0.38  0.541 0.724 0.428 0.056 0.768 0.194 0.017 0.007 0.006 0.089
 0.012 0.013 0.037 0.011 0.023 0.019 0.018 0.008 0.018 0.066 0.108 0.03
 0.004 0.043 0.016 0.024 0.036 0.032 0.232 0.073 0.042 0.139 0.017 0.265
 0.053 0.086 0.007 0.041 0.094 0.19  0.033 0.012 0.289 0.057 0.033 0.026
 0.053 0.324 0.041 0.027 0.123 0.024 0.403 0.124 0.091 0.065 0.091 0.077
 0.169 0.069 0.017 0.209 0.097 0.064 0.178 0.011 0.228 0.131 0.081 0.158
 0.011 0.316 0.147 0.084 0.02  0.065 0.183 0.193 0.067 0.015 0.369 0.036
 0.219 0.249 0.458 0.351 0.231 0.31  0.426 0.076 0.512 1.251 0.129 0.421
 0.221 0.614 0.458 0.252 0.033 0.335 0.343 0.035 0.07  0.025 0.392 0.115
 0.085 0.139 0.014 0.409 0.171 0.07  0.145 0.034 0.48  0.17  0.156 0.015
 0.454 0.105 0.034 0.016 0.014 0.093 0.03  0.015 0.1   0.007 0.095 0.078
 0.081 0.031 0.006 0.017 0.256 0.008 0.005 0.069 0.031 0.021 0.03  0.013
 0.109 0.036 0.022 0.027 0.005 0.072 0.062 0.054 0.041 0.019 0.045 0.028
 0.034 0.011 0.058 0.013 0.06  0.034 0.022 0.211 0.093 0.049 0.185 0.023
 0.312 0.216 0.064 0.112 0.03  0.046 1.02  0.258 0.006 0.237 0.061 0.213
 2.858 0.153 0.516 0.528 0.242 0.424 0.092 0.65  0.335 0.502 0.344 0.214
 0.6   0.582 0.584 1.199 0.551 0.11  0.016 0.059 0.004 0.058 0.027 0.008
 0.037 0.009 0.198 0.022 0.035 0.018 0.009 0.025 0.065 0.09  0.021 0.007
 0.017 0.134 0.111 0.008 1.46  0.221 0.135 0.228 0.025 0.417 0.244 0.083
 0.279 0.018 0.093 0.265 0.562 0.121 0.039 0.526]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.7733823529411765, 0.4945936935574569)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
