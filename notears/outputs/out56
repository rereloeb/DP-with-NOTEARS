samples  5000  graph  13 26 ER mlp  minibatch size  50  noise  0.5  minibatches per NN training  250 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3261408551088962
iteration 1 in outer loop, alpha = 1.3261408551088962, rho = 1.0, h = 1.3261408551088962
cuda
iteration 1 in inner loop,alpha 1.3261408551088962 rho 1.0 h 0.8786766100353898
iteration 2 in inner loop,alpha 1.3261408551088962 rho 10.0 h 0.3978247213259305
iteration 3 in inner loop,alpha 1.3261408551088962 rho 100.0 h 0.14417869051992582
iteration 2 in outer loop, alpha = 15.744009907101479, rho = 100.0, h = 0.14417869051992582
cuda
iteration 1 in inner loop,alpha 15.744009907101479 rho 100.0 h 0.0924576456412467
iteration 2 in inner loop,alpha 15.744009907101479 rho 1000.0 h 0.03625989184186729
iteration 3 in inner loop,alpha 15.744009907101479 rho 10000.0 h 0.00983148589760674
iteration 3 in outer loop, alpha = 114.05886888316888, rho = 10000.0, h = 0.00983148589760674
cuda
iteration 1 in inner loop,alpha 114.05886888316888 rho 10000.0 h 0.004247812498725523
iteration 2 in inner loop,alpha 114.05886888316888 rho 100000.0 h 0.0014845585668261663
iteration 4 in outer loop, alpha = 262.5147255657855, rho = 100000.0, h = 0.0014845585668261663
cuda
iteration 1 in inner loop,alpha 262.5147255657855 rho 100000.0 h 0.0007731191004367588
iteration 5 in outer loop, alpha = 1035.6338260025443, rho = 1000000.0, h = 0.0007731191004367588
Threshold 0.3
[[0.001 0.    0.179 2.533 0.589 0.    0.017 0.001 0.    0.337 0.    1.897
  0.   ]
 [0.65  0.001 0.324 0.054 0.166 0.002 0.041 0.    0.    2.069 0.005 0.221
  0.   ]
 [0.    0.    0.002 0.    0.    0.    0.001 0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.676 0.001 0.103 0.001 0.088 0.    0.    1.556 0.    0.296
  0.   ]
 [0.    0.    0.253 0.    0.001 0.    0.002 0.    0.    0.    0.    0.
  0.   ]
 [0.085 0.039 0.331 0.067 0.555 0.001 0.011 0.    0.091 2.284 0.006 0.127
  0.008]
 [0.01  0.012 1.086 0.025 0.094 0.054 0.027 0.001 0.002 1.034 0.001 0.324
  0.003]
 [0.018 0.011 0.014 0.019 1.469 3.72  0.088 0.001 0.679 0.409 0.004 0.08
  0.006]
 [0.023 1.531 2.225 0.027 0.088 0.006 0.36  0.    0.    1.457 0.002 0.201
  0.038]
 [0.    0.    1.115 0.    0.248 0.    0.001 0.    0.    0.004 0.    2.423
  0.   ]
 [1.734 0.005 2.543 0.074 0.376 0.011 0.012 0.009 0.003 0.956 0.    0.118
  0.003]
 [0.    0.    0.948 0.    0.892 0.    0.001 0.    0.    0.    0.    0.001
  0.   ]
 [0.015 1.001 2.316 0.017 0.274 0.01  0.092 0.001 0.003 2.167 0.012 0.39
  0.   ]]
[[0.    0.    0.    2.533 0.589 0.    0.    0.    0.    0.337 0.    1.897
  0.   ]
 [0.65  0.    0.324 0.    0.    0.    0.    0.    0.    2.069 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    2.676 0.    0.    0.    0.    0.    0.    1.556 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.331 0.    0.555 0.    0.    0.    0.    2.284 0.    0.
  0.   ]
 [0.    0.    1.086 0.    0.    0.    0.    0.    0.    1.034 0.    0.324
  0.   ]
 [0.    0.    0.    0.    1.469 3.72  0.    0.    0.679 0.409 0.    0.
  0.   ]
 [0.    1.531 2.225 0.    0.    0.    0.36  0.    0.    1.457 0.    0.
  0.   ]
 [0.    0.    1.115 0.    0.    0.    0.    0.    0.    0.    0.    2.423
  0.   ]
 [1.734 0.    2.543 0.    0.376 0.    0.    0.    0.    0.956 0.    0.
  0.   ]
 [0.    0.    0.948 0.    0.892 0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    1.001 2.316 0.    0.    0.    0.    0.    0.    2.167 0.    0.39
  0.   ]]
{'fdr': 0.42857142857142855, 'tpr': 0.7692307692307693, 'fpr': 0.28846153846153844, 'f1': 0.6557377049180327, 'shd': 16, 'npred': 35, 'ntrue': 26}
[1.552e-04 1.791e-01 2.533e+00 5.892e-01 4.957e-04 1.719e-02 5.707e-04
 2.005e-04 3.373e-01 3.128e-05 1.897e+00 3.273e-04 6.501e-01 3.238e-01
 5.393e-02 1.664e-01 2.475e-03 4.143e-02 2.387e-04 1.679e-04 2.069e+00
 4.969e-03 2.210e-01 1.360e-04 3.754e-05 1.530e-05 1.066e-04 3.984e-04
 2.390e-05 9.281e-04 4.999e-06 2.206e-05 1.786e-04 1.551e-06 2.008e-05
 1.816e-05 1.311e-04 6.833e-05 2.676e+00 1.029e-01 6.711e-04 8.847e-02
 1.590e-04 5.381e-05 1.556e+00 1.050e-05 2.961e-01 3.449e-05 6.544e-05
 4.702e-05 2.530e-01 2.911e-04 5.026e-05 1.756e-03 8.111e-06 1.128e-04
 2.187e-04 1.287e-05 3.783e-04 2.025e-05 8.526e-02 3.948e-02 3.313e-01
 6.698e-02 5.550e-01 1.146e-02 1.093e-04 9.097e-02 2.284e+00 6.128e-03
 1.268e-01 8.167e-03 1.029e-02 1.177e-02 1.086e+00 2.503e-02 9.413e-02
 5.376e-02 9.007e-04 2.371e-03 1.034e+00 1.343e-03 3.241e-01 3.370e-03
 1.773e-02 1.078e-02 1.405e-02 1.944e-02 1.469e+00 3.720e+00 8.812e-02
 6.787e-01 4.086e-01 4.421e-03 8.010e-02 6.238e-03 2.337e-02 1.531e+00
 2.225e+00 2.726e-02 8.791e-02 6.410e-03 3.603e-01 3.842e-04 1.457e+00
 1.948e-03 2.006e-01 3.821e-02 4.806e-05 9.382e-05 1.115e+00 1.108e-04
 2.484e-01 2.088e-04 8.882e-04 4.111e-05 1.432e-05 1.391e-05 2.423e+00
 5.244e-05 1.734e+00 5.115e-03 2.543e+00 7.443e-02 3.759e-01 1.069e-02
 1.176e-02 8.912e-03 3.264e-03 9.559e-01 1.184e-01 3.003e-03 2.145e-06
 6.448e-06 9.484e-01 2.872e-05 8.917e-01 6.543e-06 9.032e-04 1.593e-05
 3.051e-06 7.237e-05 2.788e-06 1.496e-06 1.499e-02 1.001e+00 2.316e+00
 1.692e-02 2.738e-01 1.018e-02 9.231e-02 1.309e-03 2.516e-03 2.167e+00
 1.226e-02 3.898e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.8618343195266271, 0.7584083198915718)
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 480.43 = squared loss an data 285.64 + 0.5*rho*h**2 194.315677 + alpha*h 0.000000 + L2reg 0.24 + L1reg 0.24 ; SHD = 91 ; DAG False
total norm for a microbatch 134.0826132127556 clip 1.1789707871106676
total norm for a microbatch 62.43867669275504 clip 3.5720512745825572
total norm for a microbatch 57.52974755654889 clip 4.221167807303719
total norm for a microbatch 52.65939842382471 clip 15.779252318089316
total norm for a microbatch 40.2629351614763 clip 45.68109553245822
total norm for a microbatch 28.44621713191203 clip 30.98637518018561
total norm for a microbatch 32.55904593229984 clip 37.267844987288484
total norm for a microbatch 54.338120008267616 clip 38.19551477270091
cuda
Objective function 13.01 = squared loss an data 10.72 + 0.5*rho*h**2 1.599089 + alpha*h 0.000000 + L2reg 0.58 + L1reg 0.12 ; SHD = 43 ; DAG False
Proportion of microbatches that were clipped  0.8096852300242131
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.788344969937821
iteration 1 in outer loop, alpha = 1.788344969937821, rho = 1.0, h = 1.788344969937821
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 16.21 = squared loss an data 10.72 + 0.5*rho*h**2 1.599089 + alpha*h 3.198178 + L2reg 0.58 + L1reg 0.12 ; SHD = 43 ; DAG False
total norm for a microbatch 73.11597254217254 clip 1.5546913834849168
total norm for a microbatch 101.57463252091145 clip 2.4265332666518735
total norm for a microbatch 52.23286758300018 clip 6.593161475828652
total norm for a microbatch 37.28536610804068 clip 6.593161475828652
total norm for a microbatch 51.18262107831653 clip 21.03552661555451
total norm for a microbatch 34.10462388219683 clip 41.29507558575954
total norm for a microbatch 39.84120621925298 clip 49.08377188822579
total norm for a microbatch 89.0668962265986 clip 50.091716146015216
total norm for a microbatch 56.75973024233384 clip 47.74211352785523
total norm for a microbatch 117.48509025315275 clip 51.826844712141
total norm for a microbatch 71.58857828193148 clip 46.969503149505556
total norm for a microbatch 75.12897844475445 clip 48.9361809857781
total norm for a microbatch 47.30130562157208 clip 54.50228362850388
total norm for a microbatch 79.86143976471293 clip 49.8966300031167
total norm for a microbatch 88.9668997122256 clip 55.27852939608142
total norm for a microbatch 54.492242225441444 clip 49.64262818606013
cuda
Objective function 12.59 = squared loss an data 8.67 + 0.5*rho*h**2 0.733599 + alpha*h 2.166185 + L2reg 0.90 + L1reg 0.12 ; SHD = 50 ; DAG False
Proportion of microbatches that were clipped  0.8113407094044908
iteration 1 in inner loop, alpha 1.788344969937821 rho 1.0 h 1.211279272820505
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 19.19 = squared loss an data 8.67 + 0.5*rho*h**2 7.335987 + alpha*h 2.166185 + L2reg 0.90 + L1reg 0.12 ; SHD = 50 ; DAG False
total norm for a microbatch 40.72617586389421 clip 1.0
total norm for a microbatch 73.66426847115584 clip 1.0
total norm for a microbatch 41.407435898853194 clip 27.65971145013648
total norm for a microbatch 69.7744181558119 clip 72.33876610002667
total norm for a microbatch 65.05101716672559 clip 67.56889972311413
total norm for a microbatch 71.78783486413653 clip 66.69159842961409
total norm for a microbatch 62.34704935565448 clip 65.958876929373
total norm for a microbatch 81.16815077895748 clip 66.67408848567493
total norm for a microbatch 48.37375183043211 clip 66.3710637105522
total norm for a microbatch 63.481926033554686 clip 66.94401279653054
total norm for a microbatch 121.4863167668941 clip 64.56364840185924
total norm for a microbatch 60.443460741706204 clip 65.0780114186365
cuda
Objective function 13.41 = squared loss an data 10.00 + 0.5*rho*h**2 1.312160 + alpha*h 0.916135 + L2reg 1.07 + L1reg 0.12 ; SHD = 38 ; DAG False
Proportion of microbatches that were clipped  0.8158804146909909
iteration 2 in inner loop, alpha 1.788344969937821 rho 10.0 h 0.5122811399441662
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 25.22 = squared loss an data 10.00 + 0.5*rho*h**2 13.121598 + alpha*h 0.916135 + L2reg 1.07 + L1reg 0.12 ; SHD = 38 ; DAG False
total norm for a microbatch 99.72393591997312 clip 9.321698031350559
total norm for a microbatch 117.33883648450607 clip 13.468027575010115
total norm for a microbatch 75.48223486096138 clip 59.75415236869963
total norm for a microbatch 182.9166094537429 clip 85.23424722356397
total norm for a microbatch 50.93093701760735 clip 83.88931657563437
total norm for a microbatch 74.38062981931888 clip 86.80905317292024
total norm for a microbatch 106.39930804513095 clip 85.94014738815585
total norm for a microbatch 118.7391170059908 clip 87.14805923164657
total norm for a microbatch 125.77906957116247 clip 88.0660724170311
total norm for a microbatch 138.71431211584138 clip 87.49344404667391
total norm for a microbatch 119.61338091341037 clip 82.62210982232021
cuda
Objective function 15.89 = squared loss an data 12.38 + 0.5*rho*h**2 1.843164 + alpha*h 0.343359 + L2reg 1.20 + L1reg 0.11 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.8180563448891776
iteration 3 in inner loop, alpha 1.788344969937821 rho 100.0 h 0.19199814620652944
iteration 2 in outer loop, alpha = 20.988159590590765, rho = 100.0, h = 0.19199814620652944
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 19.57 = squared loss an data 12.38 + 0.5*rho*h**2 1.843164 + alpha*h 4.029688 + L2reg 1.20 + L1reg 0.11 ; SHD = 37 ; DAG True
total norm for a microbatch 87.33090146811867 clip 1.3006785832145906
total norm for a microbatch 100.70886443651474 clip 1.416965171037647
total norm for a microbatch 106.72778344250587 clip 4.162256233065989
total norm for a microbatch 79.57102328858144 clip 51.67951768088758
total norm for a microbatch 157.91739128995357 clip 92.4561832209151
total norm for a microbatch 85.46189633971676 clip 89.3038855169552
total norm for a microbatch 105.23930687152414 clip 88.69654051351057
total norm for a microbatch 147.9303999061937 clip 86.82436565720465
total norm for a microbatch 51.294620421787336 clip 86.28938152076236
total norm for a microbatch 74.35819429308484 clip 89.96928074810846
total norm for a microbatch 135.9342564652482 clip 88.52635029262393
total norm for a microbatch 103.23701029289603 clip 90.04444325758497
total norm for a microbatch 165.81503902518062 clip 90.45801275215543
total norm for a microbatch 119.30428017782853 clip 89.79329041370889
total norm for a microbatch 107.99888741812312 clip 87.78381348638656
total norm for a microbatch 85.2431950577563 clip 87.82962701286016
total norm for a microbatch 71.13282464432848 clip 90.42702146222783
cuda
Objective function 17.23 = squared loss an data 13.26 + 0.5*rho*h**2 0.499944 + alpha*h 2.098698 + L2reg 1.27 + L1reg 0.11 ; SHD = 32 ; DAG True
Proportion of microbatches that were clipped  0.818697342838626
iteration 1 in inner loop, alpha 20.988159590590765 rho 100.0 h 0.09999435709298332
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 21.73 = squared loss an data 13.26 + 0.5*rho*h**2 4.999436 + alpha*h 2.098698 + L2reg 1.27 + L1reg 0.11 ; SHD = 32 ; DAG True
total norm for a microbatch 65.78186635266117 clip 26.92220891249096
total norm for a microbatch 92.49474264926764 clip 91.70885014840655
total norm for a microbatch 157.77926242378837 clip 95.6243651440813
total norm for a microbatch 109.41060168962994 clip 94.23300177056701
total norm for a microbatch 216.86834150211382 clip 87.00534347444811
total norm for a microbatch 116.39371654916108 clip 86.95542869038998
total norm for a microbatch 190.52582439562264 clip 91.28942017956906
cuda
Objective function 19.15 = squared loss an data 15.67 + 0.5*rho*h**2 1.090630 + alpha*h 0.980230 + L2reg 1.30 + L1reg 0.11 ; SHD = 34 ; DAG True
Proportion of microbatches that were clipped  0.8212099530964305
iteration 2 in inner loop, alpha 20.988159590590765 rho 1000.0 h 0.04670395193504362
iteration 3 in outer loop, alpha = 67.69211152563439, rho = 1000.0, h = 0.04670395193504362
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 21.33 = squared loss an data 15.67 + 0.5*rho*h**2 1.090630 + alpha*h 3.161489 + L2reg 1.30 + L1reg 0.11 ; SHD = 34 ; DAG True
total norm for a microbatch 85.99721305903495 clip 1.4276394002645358
total norm for a microbatch 129.36759979783844 clip 85.8438103199974
total norm for a microbatch 88.3450918245386 clip 93.37376860338631
total norm for a microbatch 90.2998080888122 clip 99.60687027340273
total norm for a microbatch 146.84006824363647 clip 92.4541829663123
total norm for a microbatch 101.2761405826277 clip 95.78973047496507
total norm for a microbatch 118.56007314034241 clip 89.9610645001457
total norm for a microbatch 87.30994181022123 clip 96.0606657428386
total norm for a microbatch 111.06002527661047 clip 89.6734047563418
total norm for a microbatch 116.42202351181984 clip 92.32869857524754
total norm for a microbatch 104.12836514357186 clip 92.75135319840521
total norm for a microbatch 131.0930934281888 clip 89.81674492147404
cuda
Objective function 17.74 = squared loss an data 14.16 + 0.5*rho*h**2 0.357368 + alpha*h 1.809719 + L2reg 1.30 + L1reg 0.11 ; SHD = 35 ; DAG True
Proportion of microbatches that were clipped  0.8210467528322961
iteration 1 in inner loop, alpha 67.69211152563439 rho 1000.0 h 0.026734557884470433
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 20.96 = squared loss an data 14.16 + 0.5*rho*h**2 3.573683 + alpha*h 1.809719 + L2reg 1.30 + L1reg 0.11 ; SHD = 35 ; DAG True
total norm for a microbatch 116.2174249274553 clip 45.69656921362697
total norm for a microbatch 102.28091325838031 clip 96.80294927116984
total norm for a microbatch 108.17455250109117 clip 98.7342107481503
total norm for a microbatch 196.2898994056815 clip 95.29158522719551
total norm for a microbatch 89.48477447913622 clip 94.23615518906901
total norm for a microbatch 177.4124415334171 clip 90.13185180180561
total norm for a microbatch 132.52136270255602 clip 93.96822827418279
total norm for a microbatch 143.64914857950734 clip 84.04598502299186
total norm for a microbatch 182.93694567851716 clip 90.04524025171827
total norm for a microbatch 152.01120966571392 clip 87.10396575786037
total norm for a microbatch 150.88125527865637 clip 85.92393913451522
total norm for a microbatch 145.61462503072576 clip 93.10230394223667
total norm for a microbatch 86.0579025078619 clip 87.55956128023351
cuda
Objective function 17.14 = squared loss an data 13.97 + 0.5*rho*h**2 0.824643 + alpha*h 0.869333 + L2reg 1.35 + L1reg 0.12 ; SHD = 34 ; DAG True
Proportion of microbatches that were clipped  0.8225524475524476
iteration 2 in inner loop, alpha 67.69211152563439 rho 10000.0 h 0.01284245368364445
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 24.56 = squared loss an data 13.97 + 0.5*rho*h**2 8.246431 + alpha*h 0.869333 + L2reg 1.35 + L1reg 0.12 ; SHD = 34 ; DAG True
total norm for a microbatch 1168.6838602552316 clip 1.3148427997632541
total norm for a microbatch 171.35822864816365 clip 57.05778833818974
total norm for a microbatch 161.57518115227722 clip 81.64185671205283
total norm for a microbatch 260.920733202941 clip 150.39004941734396
total norm for a microbatch 190.74239166791082 clip 106.57801263063614
total norm for a microbatch 112.11884244309833 clip 95.58023937025162
total norm for a microbatch 123.71158161353858 clip 91.15990790189441
total norm for a microbatch 87.75454830578128 clip 92.54295171068544
total norm for a microbatch 123.28324536344869 clip 92.38035445773755
total norm for a microbatch 136.74240990665672 clip 91.1257054249507
total norm for a microbatch 180.73612047965833 clip 97.45327140582967
total norm for a microbatch 120.61535627755646 clip 103.10014272620711
cuda
Objective function 17.03 = squared loss an data 14.17 + 0.5*rho*h**2 1.052283 + alpha*h 0.310541 + L2reg 1.38 + L1reg 0.12 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.8194835491915373
iteration 3 in inner loop, alpha 67.69211152563439 rho 100000.0 h 0.004587555374598651
iteration 4 in outer loop, alpha = 526.4476489854994, rho = 100000.0, h = 0.004587555374598651
cuda
noise_multiplier  0.5  noise_multiplier_b  2.5  noise_multiplier_delta  0.502518907629606
cuda
Objective function 19.13 = squared loss an data 14.17 + 0.5*rho*h**2 1.052283 + alpha*h 2.415108 + L2reg 1.38 + L1reg 0.12 ; SHD = 37 ; DAG True
total norm for a microbatch 2349.5668397119252 clip 1.1866916618985432
total norm for a microbatch 235.5219681393562 clip 3.9315469617992718
total norm for a microbatch 141.63745066907683 clip 21.99388329450676
total norm for a microbatch 190.083210482874 clip 26.22529878980078
total norm for a microbatch 158.59311848746495 clip 31.170828634594955
total norm for a microbatch 227.0555620684385 clip 184.29512732347337
total norm for a microbatch 217.4224507217915 clip 178.34377584640825
total norm for a microbatch 164.19515789447118 clip 167.05185940324145
total norm for a microbatch 151.80088749959086 clip 135.44536359997565
total norm for a microbatch 173.36783536858576 clip 112.07811791473628
total norm for a microbatch 118.52562125644724 clip 108.0052935065556
total norm for a microbatch 90.49755676166866 clip 100.9933177484623
total norm for a microbatch 188.63593209966825 clip 107.99954425675817
cuda
Objective function 17.51 = squared loss an data 14.19 + 0.5*rho*h**2 0.354878 + alpha*h 1.402522 + L2reg 1.44 + L1reg 0.13 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.8200769724182168
iteration 1 in inner loop, alpha 526.4476489854994 rho 100000.0 h 0.0026641235945543684
iteration 5 in outer loop, alpha = 3190.571243539868, rho = 1000000.0, h = 0.0026641235945543684
Threshold 0.3
[[0.004 0.36  1.094 1.427 0.649 0.592 0.387 0.021 0.043 0.82  0.011 0.736
  0.138]
 [0.015 0.004 0.209 0.036 0.326 0.056 0.041 0.005 0.003 0.849 0.008 0.057
  0.004]
 [0.002 0.023 0.004 0.003 0.159 0.021 0.065 0.004 0.002 0.757 0.003 0.052
  0.003]
 [0.004 0.08  1.412 0.006 0.464 0.072 0.078 0.006 0.014 0.888 0.005 0.046
  0.021]
 [0.007 0.024 0.048 0.016 0.006 0.017 0.023 0.003 0.005 0.186 0.006 0.008
  0.006]
 [0.009 0.109 0.288 0.049 0.488 0.006 0.048 0.001 0.014 1.401 0.01  0.027
  0.018]
 [0.012 0.119 0.067 0.05  0.222 0.11  0.005 0.009 0.003 0.675 0.006 0.104
  0.013]
 [0.215 0.807 0.753 0.699 1.219 3.353 0.533 0.005 0.058 1.256 0.128 0.885
  0.649]
 [0.165 1.627 1.222 0.324 0.643 0.302 1.199 0.145 0.006 0.852 0.057 0.909
  0.095]
 [0.003 0.005 0.008 0.007 0.045 0.003 0.011 0.001 0.003 0.007 0.003 0.023
  0.003]
 [0.483 0.461 1.369 0.771 0.745 0.481 0.791 0.061 0.108 1.138 0.004 0.335
  0.423]
 [0.006 0.082 0.108 0.1   0.69  0.162 0.043 0.005 0.006 0.262 0.02  0.003
  0.013]
 [0.037 1.034 1.019 0.312 0.769 0.306 0.435 0.012 0.072 1.986 0.015 0.478
  0.003]]
[[0.    0.36  1.094 1.427 0.649 0.592 0.387 0.    0.    0.82  0.    0.736
  0.   ]
 [0.    0.    0.    0.    0.326 0.    0.    0.    0.    0.849 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.757 0.    0.
  0.   ]
 [0.    0.    1.412 0.    0.464 0.    0.    0.    0.    0.888 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    0.    0.    0.    0.488 0.    0.    0.    0.    1.401 0.    0.
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.675 0.    0.
  0.   ]
 [0.    0.807 0.753 0.699 1.219 3.353 0.533 0.    0.    1.256 0.    0.885
  0.649]
 [0.    1.627 1.222 0.324 0.643 0.302 1.199 0.    0.    0.852 0.    0.909
  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.483 0.461 1.369 0.771 0.745 0.481 0.791 0.    0.    1.138 0.    0.335
  0.423]
 [0.    0.    0.    0.    0.69  0.    0.    0.    0.    0.    0.    0.
  0.   ]
 [0.    1.034 1.019 0.312 0.769 0.306 0.435 0.    0.    1.986 0.    0.478
  0.   ]]
{'fdr': 0.6226415094339622, 'tpr': 0.7692307692307693, 'fpr': 0.6346153846153846, 'f1': 0.5063291139240507, 'shd': 37, 'npred': 53, 'ntrue': 26}
[3.603e-01 1.094e+00 1.427e+00 6.492e-01 5.920e-01 3.875e-01 2.062e-02
 4.291e-02 8.197e-01 1.088e-02 7.358e-01 1.385e-01 1.458e-02 2.089e-01
 3.561e-02 3.258e-01 5.563e-02 4.071e-02 4.928e-03 2.668e-03 8.491e-01
 7.717e-03 5.737e-02 3.807e-03 2.172e-03 2.277e-02 3.112e-03 1.593e-01
 2.065e-02 6.536e-02 3.858e-03 1.927e-03 7.569e-01 3.178e-03 5.164e-02
 2.959e-03 3.952e-03 8.002e-02 1.412e+00 4.643e-01 7.158e-02 7.821e-02
 5.838e-03 1.384e-02 8.879e-01 5.179e-03 4.550e-02 2.123e-02 6.934e-03
 2.350e-02 4.788e-02 1.573e-02 1.652e-02 2.325e-02 3.441e-03 4.732e-03
 1.863e-01 5.685e-03 7.748e-03 6.001e-03 8.700e-03 1.092e-01 2.885e-01
 4.935e-02 4.885e-01 4.777e-02 1.272e-03 1.352e-02 1.401e+00 9.731e-03
 2.711e-02 1.795e-02 1.222e-02 1.191e-01 6.749e-02 4.996e-02 2.224e-01
 1.096e-01 9.040e-03 3.432e-03 6.747e-01 6.200e-03 1.038e-01 1.276e-02
 2.146e-01 8.067e-01 7.530e-01 6.986e-01 1.219e+00 3.353e+00 5.331e-01
 5.750e-02 1.256e+00 1.280e-01 8.850e-01 6.490e-01 1.649e-01 1.627e+00
 1.222e+00 3.241e-01 6.431e-01 3.023e-01 1.199e+00 1.449e-01 8.519e-01
 5.744e-02 9.088e-01 9.471e-02 3.018e-03 4.658e-03 8.149e-03 6.656e-03
 4.503e-02 2.856e-03 1.055e-02 1.485e-03 2.847e-03 2.678e-03 2.349e-02
 3.313e-03 4.830e-01 4.608e-01 1.369e+00 7.705e-01 7.453e-01 4.812e-01
 7.909e-01 6.071e-02 1.076e-01 1.138e+00 3.354e-01 4.228e-01 5.917e-03
 8.192e-02 1.084e-01 1.005e-01 6.904e-01 1.623e-01 4.340e-02 4.939e-03
 6.172e-03 2.619e-01 2.025e-02 1.298e-02 3.733e-02 1.034e+00 1.019e+00
 3.123e-01 7.686e-01 3.062e-01 4.354e-01 1.174e-02 7.208e-02 1.986e+00
 1.494e-02 4.785e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
aucroc, aucpr (0.8428994082840237, 0.6859516314556982)
Iterations 2500
Achieves (24.160548736236485, 1e-05)-DP
