samples  5000  graph  20 60 ER mim  minibatch size  100  noise  0.6  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.09233067380627702
iteration 3 in outer loop, alpha = 15.719118134786328, rho = 100.0, h = 0.09233067380627702
cuda
iteration 1 in inner loop,alpha 15.719118134786328 rho 100.0 h 0.05376479367141229
iteration 2 in inner loop,alpha 15.719118134786328 rho 1000.0 h 0.020293393780494995
iteration 4 in outer loop, alpha = 36.01251191528132, rho = 1000.0, h = 0.020293393780494995
cuda
iteration 1 in inner loop,alpha 36.01251191528132 rho 1000.0 h 0.011470833264617397
iteration 2 in inner loop,alpha 36.01251191528132 rho 10000.0 h 0.0033665786062542224
iteration 5 in outer loop, alpha = 69.67829797782355, rho = 10000.0, h = 0.0033665786062542224
cuda
iteration 1 in inner loop,alpha 69.67829797782355 rho 10000.0 h 0.0015374912343588676
iteration 2 in inner loop,alpha 69.67829797782355 rho 100000.0 h 0.0005890039795950486
iteration 6 in outer loop, alpha = 128.5786959373284, rho = 100000.0, h = 0.0005890039795950486
cuda
iteration 1 in inner loop,alpha 128.5786959373284 rho 100000.0 h 0.00034854132673700633
iteration 7 in outer loop, alpha = 477.12002267433473, rho = 1000000.0, h = 0.00034854132673700633
Threshold 0.3
[[0.    0.035 0.012 0.04  0.149 0.181 0.225 0.001 0.043 0.051 0.471 0.901
  1.017 0.    0.095 0.004 0.208 2.558 0.524 0.014]
 [0.    0.001 0.    0.04  0.085 0.47  0.137 0.    0.006 0.061 0.11  0.
  0.042 0.    0.073 0.139 0.004 0.    0.088 0.804]
 [0.001 0.288 0.002 0.125 0.544 0.102 0.076 0.    1.49  0.79  0.088 0.011
  0.024 0.    0.035 0.214 0.119 0.026 0.138 0.114]
 [0.    0.007 0.    0.002 0.001 0.005 0.    0.    0.    0.01  0.129 0.
  0.    0.    0.053 0.001 0.    0.    0.002 0.03 ]
 [0.    0.003 0.    0.175 0.002 0.008 0.004 0.    0.    0.058 0.137 0.
  0.024 0.    0.051 0.122 0.003 0.    0.001 0.008]
 [0.    0.002 0.005 0.089 0.033 0.005 0.009 0.    0.028 0.027 0.05  0.
  0.019 0.    1.199 0.077 0.005 0.001 0.304 0.045]
 [0.    0.004 0.004 0.22  0.301 0.247 0.003 0.    0.012 0.179 0.429 0.001
  0.422 0.    0.094 0.077 0.001 0.    0.009 0.154]
 [0.    0.098 0.005 1.216 0.041 0.056 0.071 0.    0.376 0.085 0.063 2.32
  1.247 0.014 0.109 0.016 0.001 2.148 0.047 0.033]
 [0.    0.024 0.    1.453 1.4   0.015 0.104 0.    0.003 0.392 1.182 0.
  0.066 0.    0.416 0.139 0.023 0.    0.005 0.016]
 [0.    0.002 0.001 0.035 0.003 0.176 0.004 0.    0.004 0.004 1.108 0.
  0.002 0.    0.098 0.007 0.002 0.    0.006 0.667]
 [0.    0.002 0.    0.001 0.001 0.014 0.003 0.    0.    0.001 0.002 0.
  0.001 0.    0.014 0.004 0.002 0.    0.003 0.751]
 [0.    2.089 0.003 1.594 1.307 0.063 0.653 0.    1.052 0.079 0.988 0.001
  0.259 0.    0.006 0.9   0.51  0.    0.069 0.145]
 [0.    0.003 0.002 0.18  0.033 0.103 0.003 0.    0.    0.572 0.054 0.003
  0.003 0.    1.6   0.15  0.002 0.    0.003 0.036]
 [0.003 1.038 4.603 0.1   0.044 0.175 0.085 0.002 0.85  0.179 0.035 0.02
  0.043 0.    0.055 0.67  0.005 0.073 0.72  1.199]
 [0.    0.001 0.    0.013 0.009 0.001 0.    0.    0.002 0.005 0.009 0.
  0.    0.    0.002 0.012 0.001 0.    0.001 0.003]
 [0.    0.001 0.002 0.08  0.004 0.014 0.002 0.    0.004 0.299 0.041 0.
  0.007 0.    0.037 0.003 0.003 0.    0.001 0.106]
 [0.    0.017 0.005 1.002 0.1   0.006 0.993 0.    0.027 0.016 0.113 0.001
  0.058 0.    0.559 0.011 0.002 0.    0.01  0.053]
 [0.    0.035 0.005 0.293 0.045 0.051 0.02  0.    0.104 0.046 0.118 0.774
  1.718 0.    0.006 0.069 0.002 0.002 0.024 1.25 ]
 [0.    0.001 0.01  0.094 0.779 0.015 0.025 0.    0.429 0.213 0.141 0.001
  0.015 0.    0.704 1.034 0.011 0.011 0.004 0.066]
 [0.    0.    0.    0.003 0.001 0.018 0.001 0.    0.001 0.001 0.001 0.
  0.001 0.    0.054 0.001 0.001 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.471 0.901
  1.017 0.    0.    0.    0.    2.558 0.524 0.   ]
 [0.    0.    0.    0.    0.    0.47  0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.804]
 [0.    0.    0.    0.    0.544 0.    0.    0.    1.49  0.79  0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.199 0.    0.    0.    0.304 0.   ]
 [0.    0.    0.    0.    0.301 0.    0.    0.    0.    0.    0.429 0.
  0.422 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.216 0.    0.    0.    0.    0.376 0.    0.    2.32
  1.247 0.    0.    0.    0.    2.148 0.    0.   ]
 [0.    0.    0.    1.453 1.4   0.    0.    0.    0.    0.392 1.182 0.
  0.    0.    0.416 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.108 0.
  0.    0.    0.    0.    0.    0.    0.    0.667]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.751]
 [0.    2.089 0.    1.594 1.307 0.    0.653 0.    1.052 0.    0.988 0.
  0.    0.    0.    0.9   0.51  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.572 0.    0.
  0.    0.    1.6   0.    0.    0.    0.    0.   ]
 [0.    1.038 4.603 0.    0.    0.    0.    0.    0.85  0.    0.    0.
  0.    0.    0.    0.67  0.    0.    0.72  1.199]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.002 0.    0.    0.993 0.    0.    0.    0.    0.
  0.    0.    0.559 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.774
  1.718 0.    0.    0.    0.    0.    0.    1.25 ]
 [0.    0.    0.    0.    0.779 0.    0.    0.    0.429 0.    0.    0.
  0.    0.    0.704 1.034 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.14814814814814814, 'tpr': 0.7666666666666667, 'fpr': 0.06153846153846154, 'f1': 0.8070175438596491, 'shd': 16, 'npred': 54, 'ntrue': 60}
[3.468e-02 1.175e-02 4.017e-02 1.490e-01 1.813e-01 2.249e-01 1.305e-03
 4.321e-02 5.092e-02 4.714e-01 9.013e-01 1.017e+00 3.832e-04 9.480e-02
 3.827e-03 2.083e-01 2.558e+00 5.241e-01 1.359e-02 5.503e-05 2.028e-04
 3.970e-02 8.500e-02 4.704e-01 1.369e-01 3.360e-06 5.961e-03 6.105e-02
 1.104e-01 9.746e-05 4.211e-02 4.132e-05 7.289e-02 1.387e-01 3.917e-03
 1.076e-04 8.848e-02 8.037e-01 7.645e-04 2.881e-01 1.248e-01 5.441e-01
 1.019e-01 7.586e-02 3.509e-04 1.490e+00 7.899e-01 8.787e-02 1.120e-02
 2.408e-02 1.511e-05 3.467e-02 2.141e-01 1.191e-01 2.594e-02 1.378e-01
 1.141e-01 2.814e-05 6.791e-03 4.952e-05 1.389e-03 4.991e-03 3.215e-04
 6.102e-06 3.600e-04 9.833e-03 1.290e-01 1.252e-04 2.135e-04 5.390e-06
 5.254e-02 5.286e-04 3.596e-04 5.800e-05 1.869e-03 2.980e-02 4.538e-05
 3.219e-03 1.492e-04 1.747e-01 8.104e-03 3.885e-03 3.060e-06 4.709e-04
 5.848e-02 1.367e-01 1.420e-04 2.390e-02 3.972e-06 5.133e-02 1.216e-01
 3.016e-03 1.673e-04 6.395e-04 8.252e-03 7.587e-05 1.815e-03 5.487e-03
 8.856e-02 3.294e-02 8.511e-03 2.437e-05 2.756e-02 2.668e-02 5.035e-02
 3.568e-04 1.921e-02 5.927e-05 1.199e+00 7.665e-02 5.251e-03 7.082e-04
 3.041e-01 4.464e-02 5.842e-05 4.283e-03 4.214e-03 2.203e-01 3.005e-01
 2.470e-01 3.473e-05 1.206e-02 1.788e-01 4.288e-01 9.251e-04 4.220e-01
 1.310e-04 9.408e-02 7.682e-02 9.855e-04 2.803e-04 9.229e-03 1.539e-01
 4.472e-04 9.787e-02 4.627e-03 1.216e+00 4.076e-02 5.604e-02 7.086e-02
 3.755e-01 8.508e-02 6.299e-02 2.320e+00 1.247e+00 1.380e-02 1.091e-01
 1.580e-02 7.941e-04 2.148e+00 4.666e-02 3.291e-02 3.810e-05 2.369e-02
 1.977e-04 1.453e+00 1.400e+00 1.528e-02 1.035e-01 3.983e-05 3.916e-01
 1.182e+00 2.901e-04 6.616e-02 1.524e-05 4.163e-01 1.385e-01 2.305e-02
 2.627e-04 5.444e-03 1.598e-02 2.513e-05 1.983e-03 6.803e-04 3.459e-02
 3.467e-03 1.764e-01 3.502e-03 3.077e-05 4.162e-03 1.108e+00 3.133e-04
 2.128e-03 6.278e-06 9.767e-02 7.076e-03 1.942e-03 2.123e-04 6.119e-03
 6.674e-01 2.327e-05 2.133e-03 1.827e-04 1.438e-03 6.398e-04 1.363e-02
 2.657e-03 3.634e-06 4.405e-04 7.530e-04 1.563e-04 1.150e-03 4.863e-06
 1.393e-02 4.096e-03 2.094e-03 1.946e-04 2.693e-03 7.509e-01 6.542e-05
 2.089e+00 2.818e-03 1.594e+00 1.307e+00 6.288e-02 6.532e-01 9.864e-06
 1.052e+00 7.924e-02 9.879e-01 2.590e-01 4.772e-04 6.298e-03 8.997e-01
 5.098e-01 3.985e-04 6.907e-02 1.445e-01 4.083e-06 3.495e-03 1.586e-03
 1.799e-01 3.311e-02 1.032e-01 3.145e-03 7.246e-06 4.019e-04 5.717e-01
 5.366e-02 3.375e-03 2.489e-04 1.600e+00 1.496e-01 1.577e-03 1.433e-04
 2.705e-03 3.573e-02 2.863e-03 1.038e+00 4.603e+00 1.001e-01 4.362e-02
 1.747e-01 8.464e-02 1.501e-03 8.501e-01 1.794e-01 3.488e-02 2.010e-02
 4.320e-02 5.549e-02 6.700e-01 5.132e-03 7.257e-02 7.195e-01 1.199e+00
 3.054e-06 7.954e-04 2.431e-04 1.256e-02 9.417e-03 8.163e-04 4.426e-04
 2.403e-06 1.577e-03 4.721e-03 9.071e-03 8.139e-05 4.191e-04 1.986e-05
 1.168e-02 5.680e-04 5.346e-05 1.218e-03 2.653e-03 2.464e-05 7.965e-04
 1.949e-03 7.980e-02 4.428e-03 1.375e-02 2.475e-03 3.520e-05 3.882e-03
 2.990e-01 4.133e-02 4.682e-04 7.244e-03 2.123e-05 3.701e-02 3.217e-03
 2.071e-04 1.453e-03 1.057e-01 1.747e-04 1.659e-02 4.604e-03 1.002e+00
 1.004e-01 5.739e-03 9.925e-01 1.545e-05 2.664e-02 1.554e-02 1.125e-01
 9.064e-04 5.804e-02 7.772e-05 5.587e-01 1.098e-02 1.921e-04 1.038e-02
 5.275e-02 2.753e-06 3.517e-02 5.142e-03 2.930e-01 4.502e-02 5.134e-02
 2.005e-02 1.891e-05 1.035e-01 4.557e-02 1.184e-01 7.743e-01 1.718e+00
 4.757e-04 5.596e-03 6.869e-02 1.540e-03 2.355e-02 1.250e+00 1.174e-04
 1.069e-03 9.648e-03 9.432e-02 7.785e-01 1.504e-02 2.503e-02 1.584e-04
 4.295e-01 2.126e-01 1.412e-01 7.694e-04 1.464e-02 1.197e-04 7.042e-01
 1.034e+00 1.145e-02 1.080e-02 6.600e-02 4.783e-06 3.264e-04 9.184e-05
 2.958e-03 1.182e-03 1.821e-02 8.004e-04 3.894e-06 5.963e-04 5.587e-04
 6.821e-04 2.033e-05 1.170e-03 1.170e-05 5.368e-02 6.833e-04 1.153e-03
 3.830e-04 1.869e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.91046875, 0.8392702670141864)
cuda
4420
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
||w||^2 0.054334502491391615
exp ma of ||w||^2 50.653536578454975
||w|| 0.23309762437955392
exp ma of ||w|| 0.20881761645583982
||w||^2 0.0412961129204127
exp ma of ||w||^2 0.053363477253911
||w|| 0.20321445056986645
exp ma of ||w|| 0.22466886459913038
||w||^2 0.1000784007857029
exp ma of ||w||^2 0.058860681246014075
||w|| 0.31635170425604303
exp ma of ||w|| 0.23680512506999757
||w||^2 0.08533450363452963
exp ma of ||w||^2 0.10324516814821794
||w|| 0.2921207004553591
exp ma of ||w|| 0.31510264343333333
||w||^2 0.08772947650198329
exp ma of ||w||^2 0.12862980953226902
||w|| 0.29619162125553666
exp ma of ||w|| 0.3481100732256987
cuda
Objective function 20.90 = squared loss an data 19.84 + 0.5*rho*h**2 0.662638 + alpha*h 0.000000 + L2reg 0.17 + L1reg 0.22 ; SHD = 59 ; DAG False
Proportion of microbatches that were clipped  0.7267663908338637
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.151206656901465
iteration 1 in outer loop, alpha = 1.151206656901465, rho = 1.0, h = 1.151206656901465
cuda
4420
cuda
Objective function 22.22 = squared loss an data 19.84 + 0.5*rho*h**2 0.662638 + alpha*h 1.325277 + L2reg 0.17 + L1reg 0.22 ; SHD = 59 ; DAG False
||w||^2 847.8130047453055
exp ma of ||w||^2 33299.184826802964
||w|| 29.1172286583958
exp ma of ||w|| 91.40856474735618
||w||^2 50.16302109419417
exp ma of ||w||^2 5512.007200843786
||w|| 7.082585763278421
exp ma of ||w|| 28.400087765206308
||w||^2 0.029054676030597457
exp ma of ||w||^2 0.0407879756554419
||w|| 0.17045432241687936
exp ma of ||w|| 0.19824653943278286
||w||^2 0.07812956965121927
exp ma of ||w||^2 0.05585043584825048
||w|| 0.2795166715085511
exp ma of ||w|| 0.23100843391695575
||w||^2 0.038162341606857164
exp ma of ||w||^2 0.070067543366685
||w|| 0.19535184055149613
exp ma of ||w|| 0.2588656258824782
cuda
Objective function 20.80 = squared loss an data 18.93 + 0.5*rho*h**2 0.386091 + alpha*h 1.011609 + L2reg 0.25 + L1reg 0.22 ; SHD = 56 ; DAG False
Proportion of microbatches that were clipped  0.7155368147907882
iteration 1 in inner loop, alpha 1.151206656901465 rho 1.0 h 0.8787383848045245
4420
cuda
Objective function 24.27 = squared loss an data 18.93 + 0.5*rho*h**2 3.860906 + alpha*h 1.011609 + L2reg 0.25 + L1reg 0.22 ; SHD = 56 ; DAG False
||w||^2 387713.2680741278
exp ma of ||w||^2 2039790.0628157738
||w|| 622.6662573755926
exp ma of ||w|| 1035.9042162125315
||w||^2 0.02090098444225258
exp ma of ||w||^2 0.06261233407744413
||w|| 0.14457172767264206
exp ma of ||w|| 0.1969591023235681
||w||^2 0.10772725514803136
exp ma of ||w||^2 0.05973620277734552
||w|| 0.32821830410266783
exp ma of ||w|| 0.2384221903674948
||w||^2 0.046990768586857634
exp ma of ||w||^2 0.09064619665314648
||w|| 0.21677354217444902
exp ma of ||w|| 0.2929840740024169
||w||^2 0.12782833819032444
exp ma of ||w||^2 0.13396146577590098
||w|| 0.35753089123923887
exp ma of ||w|| 0.355852033570965
cuda
Objective function 20.42 = squared loss an data 18.95 + 0.5*rho*h**2 0.601229 + alpha*h 0.399198 + L2reg 0.28 + L1reg 0.18 ; SHD = 49 ; DAG False
Proportion of microbatches that were clipped  0.7282864771477933
iteration 2 in inner loop, alpha 1.151206656901465 rho 10.0 h 0.3467647351726981
4420
cuda
Objective function 25.83 = squared loss an data 18.95 + 0.5*rho*h**2 6.012289 + alpha*h 0.399198 + L2reg 0.28 + L1reg 0.18 ; SHD = 49 ; DAG False
||w||^2 393803782.0851352
exp ma of ||w||^2 762439184.732108
||w|| 19844.4899678761
exp ma of ||w|| 25299.383368179784
||w||^2 0.039944280579322755
exp ma of ||w||^2 0.052555302432909956
||w|| 0.19986065290427418
exp ma of ||w|| 0.22071884717998627
||w||^2 0.0759035114523301
exp ma of ||w||^2 0.059699181547654245
||w|| 0.2755059190876488
exp ma of ||w|| 0.23881256911715074
||w||^2 0.1250829736938709
exp ma of ||w||^2 0.1330632825611843
||w|| 0.35367071365024116
exp ma of ||w|| 0.3539685597095464
cuda
Objective function 20.46 = squared loss an data 19.41 + 0.5*rho*h**2 0.467233 + alpha*h 0.111285 + L2reg 0.32 + L1reg 0.15 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.7309795344477426
iteration 3 in inner loop, alpha 1.151206656901465 rho 100.0 h 0.0966677437335477
iteration 2 in outer loop, alpha = 10.817981030256234, rho = 100.0, h = 0.0966677437335477
cuda
4420
cuda
Objective function 21.40 = squared loss an data 19.41 + 0.5*rho*h**2 0.467233 + alpha*h 1.045750 + L2reg 0.32 + L1reg 0.15 ; SHD = 46 ; DAG True
||w||^2 1288522885.3505435
exp ma of ||w||^2 1336936997.7411058
||w|| 35896.00096599263
exp ma of ||w|| 33994.741612569334
||w||^2 1062723746.0310175
exp ma of ||w||^2 1298151697.4450798
||w|| 32599.443952788788
exp ma of ||w|| 33550.828237648326
||w||^2 17727150.515248097
exp ma of ||w||^2 134080104.13546146
||w|| 4210.3622783850915
exp ma of ||w|| 9922.938677757038
||w||^2 398.45774501969436
exp ma of ||w||^2 18411.370420488594
||w|| 19.961406388821764
exp ma of ||w|| 50.94790911599018
||w||^2 15.635132632040476
exp ma of ||w||^2 1491.1347794914002
||w|| 3.9541285553254935
exp ma of ||w|| 9.180226767556261
||w||^2 0.27105452419649184
exp ma of ||w||^2 2.554995761972713
||w|| 0.5206289698014238
exp ma of ||w|| 0.3776737525027967
||w||^2 0.06695066315476478
exp ma of ||w||^2 0.440951614890998
||w|| 0.25874826212897506
exp ma of ||w|| 0.2818060153516764
||w||^2 0.04593367245082456
exp ma of ||w||^2 0.09759353161960123
||w|| 0.21432142321948255
exp ma of ||w|| 0.30333263597999294
||w||^2 0.1476026405052379
exp ma of ||w||^2 0.13708676074518789
||w|| 0.3841908907109041
exp ma of ||w|| 0.3591131594593366
||w||^2 0.18737483525550938
exp ma of ||w||^2 0.1419187211749318
||w|| 0.43286814996660283
exp ma of ||w|| 0.36543235549451253
cuda
Objective function 20.81 = squared loss an data 19.54 + 0.5*rho*h**2 0.157193 + alpha*h 0.606565 + L2reg 0.36 + L1reg 0.14 ; SHD = 50 ; DAG True
Proportion of microbatches that were clipped  0.7207442467765628
iteration 1 in inner loop, alpha 10.817981030256234 rho 100.0 h 0.056070120018006264
4420
cuda
Objective function 22.23 = squared loss an data 19.54 + 0.5*rho*h**2 1.571929 + alpha*h 0.606565 + L2reg 0.36 + L1reg 0.14 ; SHD = 50 ; DAG True
||w||^2 62088538.939532705
exp ma of ||w||^2 232247819.0038335
||w|| 7879.628096524144
exp ma of ||w|| 13330.981606179372
||w||^2 136.72147554087493
exp ma of ||w||^2 20572.081357677394
||w|| 11.692795882117968
exp ma of ||w|| 56.09778429478014
||w||^2 0.0900955351949805
exp ma of ||w||^2 2.4753555384166694
||w|| 0.3001591830928724
exp ma of ||w|| 0.42897098942179956
cuda
Objective function 20.71 = squared loss an data 19.69 + 0.5*rho*h**2 0.250806 + alpha*h 0.242287 + L2reg 0.39 + L1reg 0.13 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.7280408098198629
iteration 2 in inner loop, alpha 10.817981030256234 rho 1000.0 h 0.022396708580536995
iteration 3 in outer loop, alpha = 33.21468961079323, rho = 1000.0, h = 0.022396708580536995
cuda
4420
cuda
Objective function 21.21 = squared loss an data 19.69 + 0.5*rho*h**2 0.250806 + alpha*h 0.743900 + L2reg 0.39 + L1reg 0.13 ; SHD = 51 ; DAG True
||w||^2 6732.605364403517
exp ma of ||w||^2 160261.80722997914
||w|| 82.05245495659175
exp ma of ||w|| 203.3431926378902
||w||^2 0.06820147438424459
exp ma of ||w||^2 0.07019564780725218
||w|| 0.2611541199832861
exp ma of ||w|| 0.2580435901193865
||w||^2 0.2013722203579243
exp ma of ||w||^2 0.07725599047284085
||w|| 0.44874516193260994
exp ma of ||w|| 0.2692653471267062
||w||^2 0.05405624777566119
exp ma of ||w||^2 0.09652968788256848
||w|| 0.23249999521647563
exp ma of ||w|| 0.3016915985482914
||w||^2 0.07655956577973091
exp ma of ||w||^2 0.10709868123961294
||w|| 0.27669399303152736
exp ma of ||w|| 0.31644592169982816
cuda
Objective function 20.89 = squared loss an data 19.72 + 0.5*rho*h**2 0.115111 + alpha*h 0.503969 + L2reg 0.42 + L1reg 0.13 ; SHD = 57 ; DAG True
Proportion of microbatches that were clipped  0.7235588567737768
iteration 1 in inner loop, alpha 33.21468961079323 rho 1000.0 h 0.015173078168398746
4420
cuda
Objective function 21.92 = squared loss an data 19.72 + 0.5*rho*h**2 1.151112 + alpha*h 0.503969 + L2reg 0.42 + L1reg 0.13 ; SHD = 57 ; DAG True
||w||^2 32343116704.824577
exp ma of ||w||^2 5430709020.716344
||w|| 179841.92143330924
exp ma of ||w|| 36690.08192802486
||w||^2 1113661866.558926
exp ma of ||w||^2 3633133366.176423
||w|| 33371.57273127723
exp ma of ||w|| 48792.800909168975
||w||^2 0.03697007278417695
exp ma of ||w||^2 0.08003145120878633
||w|| 0.19227603278666053
exp ma of ||w|| 0.2775310010403969
||w||^2 0.07509126490122152
exp ma of ||w||^2 0.07734861526669227
||w|| 0.2740278542433625
exp ma of ||w|| 0.2716254648498509
||w||^2 0.08301096741007537
exp ma of ||w||^2 0.08802973771667628
||w|| 0.2881162394070757
exp ma of ||w|| 0.2890780855763154
cuda
Objective function 20.67 = squared loss an data 19.71 + 0.5*rho*h**2 0.179375 + alpha*h 0.198942 + L2reg 0.46 + L1reg 0.12 ; SHD = 49 ; DAG True
Proportion of microbatches that were clipped  0.7239293106193281
iteration 2 in inner loop, alpha 33.21468961079323 rho 10000.0 h 0.005989580536976291
4420
cuda
Objective function 22.28 = squared loss an data 19.71 + 0.5*rho*h**2 1.793754 + alpha*h 0.198942 + L2reg 0.46 + L1reg 0.12 ; SHD = 49 ; DAG True
||w||^2 56112242458.0295
exp ma of ||w||^2 9210878334.405497
||w|| 236880.22808590316
exp ma of ||w|| 39025.02167544039
||w||^2 94402751.44819252
exp ma of ||w||^2 12981264843.978788
||w|| 9716.107834322987
exp ma of ||w|| 22433.424226139683
v before min max tensor([[ 4.549e-02, -2.745e-05, -2.713e-04,  ...,  8.111e-04,  2.932e-03,
         -2.218e-04],
        [-4.309e-03, -2.430e-05, -1.359e-04,  ...,  2.519e-04, -2.491e-05,
          1.387e-03],
        [ 7.450e-02, -5.581e-04, -5.694e-05,  ..., -7.145e-05,  2.078e-04,
         -1.310e-04],
        ...,
        [ 8.075e-05, -5.196e-07, -1.148e-03,  ...,  7.502e-04, -2.252e-04,
         -1.299e-02],
        [ 1.855e-06,  5.944e-07, -3.799e-04,  ...,  8.326e-04,  3.358e-05,
         -6.180e-03],
        [ 2.289e-04, -6.319e-05, -3.004e-04,  ..., -1.357e-03, -1.494e-03,
          7.389e-02]], device='cuda:0')
v tensor([[4.549e-02, 1.000e-12, 1.000e-12,  ..., 8.111e-04, 2.932e-03,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 2.519e-04, 1.000e-12,
         1.387e-03],
        [7.450e-02, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 2.078e-04,
         1.000e-12],
        ...,
        [8.075e-05, 1.000e-12, 1.000e-12,  ..., 7.502e-04, 1.000e-12,
         1.000e-12],
        [1.855e-06, 5.944e-07, 1.000e-12,  ..., 8.326e-04, 3.358e-05,
         1.000e-12],
        [2.289e-04, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         7.389e-02]], device='cuda:0')
v before min max tensor([-7.753e-07,  1.043e-06, -4.596e-05, -2.650e-04, -1.935e-05, -2.146e-06,
        -3.129e-05,  2.616e-05,  2.014e-04,  2.385e-03, -4.301e-06, -1.612e-06,
         4.180e-05, -2.381e-04, -3.546e-05, -3.678e-05, -4.586e-06, -7.664e-05,
         4.503e-05,  1.900e-06, -1.446e-03, -3.609e-05,  4.851e-04,  3.239e-05,
        -2.620e-06, -4.339e-05, -5.298e-04, -7.345e-05, -2.197e-04, -5.146e-05,
        -2.471e-04,  2.268e-05,  3.769e-07, -5.968e-07,  2.404e-05, -8.763e-06,
        -6.215e-07, -1.041e-06,  2.747e-06, -2.596e-05,  9.787e-06, -2.452e-03,
        -6.290e-05, -5.692e-07, -5.364e-05,  1.658e-04, -2.800e-06,  1.110e-05,
        -6.223e-05, -6.868e-05, -5.347e-05, -2.746e-04, -2.455e-06,  4.539e-05,
        -7.400e-06, -3.211e-05,  1.352e-04, -6.368e-07, -4.813e-05, -4.388e-04,
         1.356e-04, -2.015e-04,  3.925e-04, -5.542e-04,  1.680e-06, -1.405e-04,
        -1.676e-04,  9.045e-05,  4.478e-05,  7.429e-04, -4.496e-07, -1.645e-05,
         1.907e-04,  8.865e-05,  1.221e-03, -6.587e-04, -9.120e-07, -1.748e-06,
        -1.046e-04, -2.403e-05, -1.634e-03,  2.097e-04, -2.922e-06, -3.328e-05,
        -3.599e-05, -1.339e-04, -4.115e-05, -4.368e-07,  4.102e-04,  2.166e-05,
        -1.145e-05,  7.311e-06, -5.399e-05, -1.520e-05, -8.439e-05,  2.664e-04,
         2.814e-04, -1.503e-05, -9.621e-06, -2.682e-04, -5.869e-05, -1.687e-05,
        -4.615e-05,  4.899e-04, -6.046e-07, -1.708e-05, -1.051e-04,  9.599e-05,
        -5.871e-04,  1.281e-06,  1.011e-04,  2.721e-04,  3.849e-06,  1.078e-05,
        -1.142e-05, -3.938e-06, -2.029e-06, -9.227e-06, -4.206e-05,  7.571e-04,
         2.343e-04, -2.250e-04, -3.226e-04, -6.868e-06, -2.449e-06, -1.205e-04,
        -2.854e-04, -1.419e-04, -2.803e-03,  1.376e-04, -6.059e-04, -3.164e-05,
        -3.665e-05,  1.838e-05,  2.943e-05,  3.728e-06, -4.408e-06, -2.876e-04,
        -8.183e-05, -8.411e-06,  2.571e-05, -5.158e-06, -2.173e-04,  2.915e-06,
        -4.191e-04, -7.334e-06, -3.215e-05, -4.283e-04, -1.386e-04, -8.151e-05,
        -2.527e-03, -1.303e-04, -6.947e-05, -4.000e-06, -2.189e-05, -6.409e-06,
         1.896e-03,  6.263e-05, -4.060e-05,  1.036e-03, -2.977e-06,  7.133e-05,
        -2.098e-05, -2.328e-04, -7.339e-05, -6.471e-06, -4.304e-04,  5.963e-04,
        -1.625e-04,  3.318e-06, -1.679e-05,  1.273e-04, -4.686e-04, -7.328e-05,
        -9.931e-05, -6.108e-07, -3.076e-04, -1.480e-04, -5.629e-07, -5.235e-05,
        -8.073e-05,  1.326e-05,  1.941e-04,  1.743e-06, -6.244e-06,  2.155e-05,
        -3.750e-05,  3.524e-03, -7.341e-05, -1.543e-05,  1.805e-03, -2.382e-04,
         1.221e-03,  2.082e-06,  4.901e-04,  5.876e-06, -1.284e-05,  3.351e-04,
        -2.017e-04, -5.957e-04], device='cuda:0')
v tensor([1.000e-12, 1.043e-06, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.616e-05, 2.014e-04, 2.385e-03, 1.000e-12, 1.000e-12,
        4.180e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        4.503e-05, 1.900e-06, 1.000e-12, 1.000e-12, 4.851e-04, 3.239e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.268e-05, 3.769e-07, 1.000e-12, 2.404e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 2.747e-06, 1.000e-12, 9.787e-06, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.658e-04, 1.000e-12, 1.110e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.539e-05,
        1.000e-12, 1.000e-12, 1.352e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.356e-04, 1.000e-12, 3.925e-04, 1.000e-12, 1.680e-06, 1.000e-12,
        1.000e-12, 9.045e-05, 4.478e-05, 7.429e-04, 1.000e-12, 1.000e-12,
        1.907e-04, 8.865e-05, 1.221e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 2.097e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.102e-04, 2.166e-05,
        1.000e-12, 7.311e-06, 1.000e-12, 1.000e-12, 1.000e-12, 2.664e-04,
        2.814e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 4.899e-04, 1.000e-12, 1.000e-12, 1.000e-12, 9.599e-05,
        1.000e-12, 1.281e-06, 1.011e-04, 2.721e-04, 3.849e-06, 1.078e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 7.571e-04,
        2.343e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.376e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.838e-05, 2.943e-05, 3.728e-06, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 2.571e-05, 1.000e-12, 1.000e-12, 2.915e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.896e-03, 6.263e-05, 1.000e-12, 1.036e-03, 1.000e-12, 7.133e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 5.963e-04,
        1.000e-12, 3.318e-06, 1.000e-12, 1.273e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.326e-05, 1.941e-04, 1.743e-06, 1.000e-12, 2.155e-05,
        1.000e-12, 3.524e-03, 1.000e-12, 1.000e-12, 1.805e-03, 1.000e-12,
        1.221e-03, 2.082e-06, 4.901e-04, 5.876e-06, 1.000e-12, 3.351e-04,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 5.874e-05],
         [-7.904e-07],
         [ 8.577e-06],
         [-1.526e-04],
         [-3.023e-04],
         [ 5.164e-06],
         [ 2.264e-05],
         [ 1.571e-04],
         [-4.570e-04],
         [ 4.132e-05]],

        [[-2.792e-05],
         [-6.417e-05],
         [-2.369e-05],
         [-6.504e-06],
         [-8.785e-05],
         [-9.159e-04],
         [-4.402e-05],
         [-2.371e-05],
         [-1.154e-04],
         [-4.094e-05]],

        [[-7.437e-04],
         [ 5.288e-05],
         [ 7.639e-09],
         [-1.211e-06],
         [-1.040e-03],
         [ 2.180e-04],
         [ 2.236e-04],
         [ 7.212e-06],
         [-1.605e-04],
         [-9.834e-05]],

        [[-1.091e-05],
         [ 1.544e-05],
         [ 1.546e-03],
         [-1.347e-05],
         [-7.826e-05],
         [-4.722e-04],
         [-2.651e-05],
         [-1.164e-04],
         [-1.584e-04],
         [-1.020e-05]],

        [[-7.195e-06],
         [ 3.931e-03],
         [ 1.123e-03],
         [-1.922e-04],
         [-8.195e-05],
         [-4.879e-05],
         [-1.889e-04],
         [-1.094e-05],
         [ 1.537e-04],
         [-1.061e-04]],

        [[-5.276e-04],
         [ 2.204e-03],
         [ 7.964e-06],
         [-4.311e-05],
         [-5.496e-04],
         [-1.077e-04],
         [-3.104e-05],
         [-3.772e-05],
         [-6.242e-07],
         [ 4.170e-04]],

        [[-2.129e-04],
         [ 6.438e-04],
         [-1.128e-04],
         [-1.890e-05],
         [-3.406e-04],
         [-4.585e-04],
         [-1.361e-05],
         [-3.009e-04],
         [-4.430e-04],
         [-2.068e-04]],

        [[-1.946e-04],
         [-6.474e-04],
         [ 3.705e-04],
         [-9.711e-05],
         [-1.941e-06],
         [ 3.125e-04],
         [-1.942e-03],
         [-5.242e-06],
         [-5.392e-05],
         [-2.240e-05]],

        [[-7.036e-05],
         [-3.990e-05],
         [-7.333e-05],
         [-1.086e-05],
         [-2.330e-04],
         [-2.206e-04],
         [-2.710e-05],
         [-1.743e-04],
         [-1.782e-04],
         [ 1.388e-04]],

        [[-7.282e-04],
         [-1.794e-04],
         [-8.758e-05],
         [-8.983e-05],
         [-2.208e-04],
         [-1.285e-04],
         [ 1.131e-03],
         [-1.661e-04],
         [-6.963e-05],
         [-4.109e-04]],

        [[ 3.082e-04],
         [ 6.090e-06],
         [ 1.917e-04],
         [ 3.387e-05],
         [-1.121e-04],
         [-1.451e-03],
         [ 1.086e-05],
         [ 5.468e-08],
         [ 1.907e-03],
         [-2.626e-05]],

        [[-3.128e-04],
         [-6.532e-05],
         [-1.085e-03],
         [ 8.198e-05],
         [-1.615e-04],
         [-1.483e-03],
         [-2.987e-04],
         [ 3.177e-05],
         [-7.804e-04],
         [-3.764e-05]],

        [[ 2.895e-05],
         [-5.588e-04],
         [-4.050e-06],
         [ 3.720e-03],
         [ 1.549e-05],
         [-1.514e-05],
         [-1.794e-04],
         [-1.028e-05],
         [ 4.126e-05],
         [-1.923e-04]],

        [[ 3.768e-05],
         [-3.732e-04],
         [ 1.080e-04],
         [-1.158e-04],
         [-2.676e-03],
         [ 4.300e-04],
         [-5.107e-05],
         [-1.214e-05],
         [ 2.289e-06],
         [-3.171e-07]],

        [[-4.523e-04],
         [-1.730e-05],
         [-2.887e-04],
         [-1.475e-04],
         [ 3.373e-05],
         [ 1.684e-05],
         [-3.474e-06],
         [ 3.489e-04],
         [ 2.483e-05],
         [ 2.592e-04]],

        [[-3.703e-06],
         [-7.804e-05],
         [ 1.807e-05],
         [ 3.235e-04],
         [ 6.076e-06],
         [-1.894e-05],
         [-3.713e-04],
         [-3.409e-06],
         [-8.995e-04],
         [-7.748e-05]],

        [[-8.150e-06],
         [ 1.408e-04],
         [-4.469e-04],
         [-1.332e-04],
         [-4.525e-05],
         [ 2.221e-05],
         [-4.110e-04],
         [ 4.447e-04],
         [ 1.691e-04],
         [ 1.890e-04]],

        [[ 6.352e-05],
         [ 7.562e-04],
         [-1.999e-05],
         [-5.282e-05],
         [ 8.779e-04],
         [-2.538e-04],
         [ 1.659e-04],
         [-9.533e-05],
         [-4.634e-05],
         [-2.984e-04]],

        [[-2.993e-05],
         [-1.128e-03],
         [-4.952e-06],
         [-1.780e-04],
         [ 7.306e-06],
         [-1.100e-05],
         [-4.596e-04],
         [ 5.493e-05],
         [ 4.179e-04],
         [ 2.882e-04]],

        [[-6.802e-04],
         [-4.538e-04],
         [-4.048e-04],
         [-1.190e-05],
         [-1.865e-04],
         [ 1.122e-06],
         [ 3.356e-05],
         [-3.964e-05],
         [-1.037e-03],
         [-5.294e-05]]], device='cuda:0')
v tensor([[[5.874e-05],
         [1.000e-12],
         [8.577e-06],
         [1.000e-12],
         [1.000e-12],
         [5.164e-06],
         [2.264e-05],
         [1.571e-04],
         [1.000e-12],
         [4.132e-05]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [5.288e-05],
         [7.639e-09],
         [1.000e-12],
         [1.000e-12],
         [2.180e-04],
         [2.236e-04],
         [7.212e-06],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.544e-05],
         [1.546e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [3.931e-03],
         [1.123e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.537e-04],
         [1.000e-12]],

        [[1.000e-12],
         [2.204e-03],
         [7.964e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.170e-04]],

        [[1.000e-12],
         [6.438e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [3.705e-04],
         [1.000e-12],
         [1.000e-12],
         [3.125e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.388e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.131e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[3.082e-04],
         [6.090e-06],
         [1.917e-04],
         [3.387e-05],
         [1.000e-12],
         [1.000e-12],
         [1.086e-05],
         [5.468e-08],
         [1.907e-03],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.198e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.177e-05],
         [1.000e-12],
         [1.000e-12]],

        [[2.895e-05],
         [1.000e-12],
         [1.000e-12],
         [3.720e-03],
         [1.549e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.126e-05],
         [1.000e-12]],

        [[3.768e-05],
         [1.000e-12],
         [1.080e-04],
         [1.000e-12],
         [1.000e-12],
         [4.300e-04],
         [1.000e-12],
         [1.000e-12],
         [2.289e-06],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.373e-05],
         [1.684e-05],
         [1.000e-12],
         [3.489e-04],
         [2.483e-05],
         [2.592e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.807e-05],
         [3.235e-04],
         [6.076e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.408e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.221e-05],
         [1.000e-12],
         [4.447e-04],
         [1.691e-04],
         [1.890e-04]],

        [[6.352e-05],
         [7.562e-04],
         [1.000e-12],
         [1.000e-12],
         [8.779e-04],
         [1.000e-12],
         [1.659e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.306e-06],
         [1.000e-12],
         [1.000e-12],
         [5.493e-05],
         [4.179e-04],
         [2.882e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.122e-06],
         [3.356e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-6.908e-05],
        [-7.265e-05],
        [-1.466e-04],
        [-6.939e-05],
        [-8.850e-05],
        [-2.270e-04],
        [-6.773e-05],
        [-4.039e-05],
        [ 1.325e-04],
        [-2.258e-04],
        [ 1.810e-03],
        [-1.244e-03],
        [-2.321e-04],
        [-1.570e-03],
        [-7.016e-05],
        [-1.656e-04],
        [ 3.283e-04],
        [-7.436e-05],
        [-1.355e-03],
        [ 1.108e-05]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.325e-04],
        [1.000e-12],
        [1.810e-03],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [3.283e-04],
        [1.000e-12],
        [1.000e-12],
        [1.108e-05]], device='cuda:0')
a after update for 1 param tensor([[ 1.531e-04,  3.427e-06,  1.519e-06,  ...,  2.836e-05, -4.835e-05,
          1.761e-06],
        [-2.089e-05,  6.063e-06,  2.417e-06,  ...,  2.702e-05, -2.407e-07,
         -5.497e-05],
        [-1.997e-04, -1.876e-05, -7.118e-06,  ..., -3.990e-05,  1.611e-05,
          8.792e-06],
        ...,
        [-1.280e-05, -9.623e-07, -2.872e-05,  ...,  4.447e-05, -2.118e-06,
         -1.640e-04],
        [-1.049e-06, -1.550e-06,  4.337e-06,  ..., -3.323e-05,  3.581e-06,
          3.271e-05],
        [-2.212e-05,  1.130e-05, -2.119e-05,  ..., -8.909e-06, -1.765e-05,
         -2.578e-04]], device='cuda:0')
s after update for 1 param tensor([[6.883e-02, 7.881e-05, 8.283e-04,  ..., 9.490e-03, 1.727e-02,
         6.783e-04],
        [1.490e-02, 7.371e-05, 3.959e-04,  ..., 5.134e-03, 7.745e-05,
         1.200e-02],
        [8.829e-02, 2.030e-03, 3.576e-04,  ..., 2.863e-03, 4.599e-03,
         3.760e-04],
        ...,
        [2.873e-03, 1.882e-06, 3.719e-03,  ..., 8.902e-03, 8.630e-04,
         4.248e-02],
        [4.307e-04, 2.438e-04, 1.091e-03,  ..., 9.136e-03, 1.834e-03,
         2.043e-02],
        [5.139e-03, 6.920e-04, 1.017e-03,  ..., 3.947e-03, 4.309e-03,
         8.615e-02]], device='cuda:0')
b after update for 1 param tensor([[1.260, 0.043, 0.138,  ..., 0.468, 0.631, 0.125],
        [0.586, 0.041, 0.096,  ..., 0.344, 0.042, 0.526],
        [1.427, 0.216, 0.091,  ..., 0.257, 0.326, 0.093],
        ...,
        [0.257, 0.007, 0.293,  ..., 0.453, 0.141, 0.990],
        [0.100, 0.075, 0.159,  ..., 0.459, 0.206, 0.686],
        [0.344, 0.126, 0.153,  ..., 0.302, 0.315, 1.409]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 3.904e-07,  2.659e-06,  3.189e-06, -3.094e-07,  8.165e-06,  3.396e-08,
        -7.946e-07,  1.004e-05, -9.132e-06,  7.586e-05,  1.913e-06,  1.266e-06,
        -1.103e-05,  1.684e-06,  1.418e-06,  6.463e-07,  1.713e-06, -2.410e-06,
        -7.703e-06, -3.729e-06, -9.935e-06, -7.693e-06, -3.460e-05, -1.012e-05,
         3.126e-06, -1.064e-05,  1.566e-05, -1.175e-05,  5.418e-06, -3.138e-06,
        -1.807e-06, -1.801e-05,  1.187e-06, -1.451e-07, -4.004e-06, -1.202e-06,
        -1.503e-07, -3.324e-08, -3.892e-06, -1.427e-06, -8.957e-06, -3.661e-05,
         1.909e-06, -1.030e-06,  5.730e-06,  2.013e-05, -2.718e-06, -6.925e-06,
         6.491e-06,  5.269e-06, -1.339e-05,  2.827e-06,  1.230e-06, -9.692e-06,
        -1.924e-06, -1.551e-06,  1.412e-05,  1.297e-06, -1.801e-06,  2.138e-06,
         1.139e-05,  1.041e-07, -3.552e-05,  1.379e-06, -3.773e-06,  9.405e-06,
         2.404e-05, -1.761e-05,  2.248e-05,  3.368e-05,  3.598e-06, -2.620e-06,
         2.420e-05,  1.044e-05, -4.562e-05, -1.082e-05, -1.748e-07,  8.328e-07,
         2.269e-06, -3.176e-06,  2.934e-05, -2.950e-05, -1.575e-06, -4.850e-06,
        -1.300e-06,  6.419e-06,  3.696e-06, -9.589e-07,  2.044e-05, -5.430e-05,
        -6.825e-06,  3.508e-06, -3.284e-07, -2.839e-06,  9.313e-06, -1.942e-05,
         2.104e-05,  5.743e-06,  9.476e-06, -5.219e-07,  1.503e-05, -2.874e-06,
        -4.085e-06, -3.518e-05,  7.649e-07,  4.123e-06, -8.031e-06, -1.113e-05,
        -2.694e-08, -1.332e-06, -1.318e-05, -1.508e-05, -2.510e-06, -4.558e-06,
        -1.528e-06,  3.268e-06,  5.247e-06,  9.623e-06, -1.266e-05, -3.191e-05,
         1.736e-05, -9.128e-06,  1.723e-05,  9.836e-08, -4.568e-08,  1.829e-05,
        -1.874e-06,  5.508e-06, -3.048e-05,  2.121e-05,  1.058e-05,  4.269e-06,
        -2.447e-05, -4.014e-06, -8.591e-06,  1.070e-06,  4.613e-07, -1.941e-06,
        -5.233e-07,  1.689e-06, -1.061e-05, -4.354e-07,  9.961e-06,  3.551e-06,
        -1.315e-05,  5.997e-06, -5.223e-06, -1.623e-05, -1.874e-06,  2.454e-06,
         2.697e-05,  7.373e-06,  2.356e-05, -2.152e-07,  3.110e-06, -3.861e-06,
        -3.607e-05, -1.088e-05,  3.209e-05, -3.316e-05, -1.629e-07,  7.450e-06,
        -8.867e-06,  2.894e-06, -1.630e-05,  1.452e-06, -2.149e-05,  4.081e-05,
        -1.701e-05, -2.591e-06,  8.094e-06,  2.501e-05,  5.959e-06,  7.502e-06,
        -7.264e-06,  1.073e-07,  3.275e-07,  6.944e-06, -8.939e-07, -1.233e-05,
         4.880e-06,  6.368e-06,  1.636e-05,  4.199e-06, -2.171e-06, -2.214e-05,
         3.075e-06, -9.065e-05, -1.239e-06,  8.311e-07,  4.808e-05, -2.830e-05,
         4.628e-05,  3.475e-06,  4.631e-05,  2.106e-06,  3.409e-06, -3.291e-05,
        -4.760e-05, -3.360e-05], device='cuda:0')
s after update for 1 param tensor([2.383e-06, 3.262e-04, 1.321e-04, 8.235e-04, 1.135e-04, 7.273e-06,
        1.350e-04, 1.632e-03, 4.601e-03, 1.595e-02, 2.693e-05, 5.901e-06,
        2.061e-03, 6.850e-04, 1.095e-04, 1.179e-04, 1.534e-05, 3.170e-04,
        2.122e-03, 4.375e-04, 6.110e-03, 1.620e-04, 7.118e-03, 1.812e-03,
        5.116e-05, 2.935e-04, 1.731e-03, 4.848e-04, 7.201e-04, 1.749e-04,
        8.008e-04, 1.591e-03, 1.941e-04, 1.882e-06, 1.551e-03, 5.104e-05,
        1.882e-06, 8.037e-06, 5.263e-04, 7.450e-05, 1.022e-03, 7.108e-03,
        1.809e-04, 8.937e-06, 1.541e-04, 4.309e-03, 1.206e-05, 1.054e-03,
        1.865e-04, 3.538e-04, 9.282e-04, 9.662e-04, 7.077e-06, 2.134e-03,
        2.297e-05, 9.219e-05, 4.587e-03, 7.141e-06, 1.521e-04, 1.266e-03,
        3.817e-03, 5.848e-04, 6.628e-03, 1.853e-03, 4.100e-04, 1.031e-03,
        8.300e-04, 3.154e-03, 2.269e-03, 8.652e-03, 1.403e-05, 8.346e-05,
        4.378e-03, 2.983e-03, 1.111e-02, 1.938e-03, 3.194e-06, 7.008e-06,
        3.595e-04, 1.212e-04, 4.701e-03, 4.796e-03, 1.288e-05, 1.057e-04,
        1.225e-04, 4.320e-04, 1.670e-04, 2.966e-06, 6.411e-03, 4.295e-03,
        1.561e-04, 8.554e-04, 1.731e-04, 9.250e-05, 3.634e-04, 5.210e-03,
        5.321e-03, 1.133e-04, 1.003e-04, 8.369e-04, 3.377e-04, 5.494e-05,
        1.424e-04, 7.111e-03, 6.543e-06, 7.769e-05, 4.027e-04, 3.100e-03,
        1.847e-03, 3.579e-04, 3.275e-03, 5.258e-03, 6.204e-04, 1.038e-03,
        3.316e-05, 1.481e-05, 4.185e-05, 2.175e-04, 3.181e-04, 8.847e-03,
        4.851e-03, 6.933e-04, 1.286e-03, 2.118e-05, 7.167e-06, 4.986e-04,
        9.122e-04, 4.100e-04, 9.344e-03, 4.139e-03, 3.192e-03, 1.167e-04,
        9.913e-04, 1.356e-03, 1.717e-03, 6.146e-04, 1.342e-05, 8.295e-04,
        2.714e-04, 2.542e-05, 1.607e-03, 2.058e-05, 9.038e-04, 5.404e-04,
        1.318e-03, 1.235e-04, 1.409e-04, 1.534e-03, 5.976e-04, 2.508e-04,
        7.250e-03, 3.874e-04, 7.081e-04, 1.195e-05, 8.618e-05, 4.180e-05,
        1.392e-02, 2.505e-03, 1.024e-03, 1.024e-02, 8.587e-06, 2.672e-03,
        7.666e-04, 6.746e-04, 4.198e-04, 2.177e-05, 1.916e-03, 7.794e-03,
        1.011e-03, 5.760e-04, 1.354e-04, 3.665e-03, 1.414e-03, 2.551e-04,
        2.878e-04, 1.882e-06, 9.001e-04, 5.400e-04, 1.882e-06, 8.118e-04,
        2.518e-04, 1.153e-03, 4.410e-03, 4.188e-04, 2.431e-05, 2.178e-03,
        1.077e-04, 1.918e-02, 2.666e-04, 4.449e-05, 1.463e-02, 1.133e-03,
        1.109e-02, 4.567e-04, 7.119e-03, 7.666e-04, 9.265e-05, 5.935e-03,
        4.541e-03, 2.281e-03], device='cuda:0')
b after update for 1 param tensor([0.007, 0.087, 0.055, 0.138, 0.051, 0.013, 0.056, 0.194, 0.326, 0.606,
        0.025, 0.012, 0.218, 0.126, 0.050, 0.052, 0.019, 0.085, 0.221, 0.100,
        0.375, 0.061, 0.405, 0.204, 0.034, 0.082, 0.200, 0.106, 0.129, 0.063,
        0.136, 0.192, 0.067, 0.007, 0.189, 0.034, 0.007, 0.014, 0.110, 0.041,
        0.153, 0.405, 0.065, 0.014, 0.060, 0.315, 0.017, 0.156, 0.066, 0.090,
        0.146, 0.149, 0.013, 0.222, 0.023, 0.046, 0.325, 0.013, 0.059, 0.171,
        0.297, 0.116, 0.391, 0.207, 0.097, 0.154, 0.138, 0.270, 0.229, 0.447,
        0.018, 0.044, 0.318, 0.262, 0.506, 0.211, 0.009, 0.013, 0.091, 0.053,
        0.329, 0.333, 0.017, 0.049, 0.053, 0.100, 0.062, 0.008, 0.384, 0.315,
        0.060, 0.140, 0.063, 0.046, 0.092, 0.347, 0.350, 0.051, 0.048, 0.139,
        0.088, 0.036, 0.057, 0.405, 0.012, 0.042, 0.096, 0.267, 0.206, 0.091,
        0.275, 0.348, 0.120, 0.155, 0.028, 0.018, 0.031, 0.071, 0.086, 0.452,
        0.334, 0.126, 0.172, 0.022, 0.013, 0.107, 0.145, 0.097, 0.464, 0.309,
        0.271, 0.052, 0.151, 0.177, 0.199, 0.119, 0.018, 0.138, 0.079, 0.024,
        0.192, 0.022, 0.144, 0.112, 0.174, 0.053, 0.057, 0.188, 0.117, 0.076,
        0.409, 0.095, 0.128, 0.017, 0.045, 0.031, 0.567, 0.240, 0.154, 0.486,
        0.014, 0.248, 0.133, 0.125, 0.098, 0.022, 0.210, 0.424, 0.153, 0.115,
        0.056, 0.291, 0.181, 0.077, 0.081, 0.007, 0.144, 0.112, 0.007, 0.137,
        0.076, 0.163, 0.319, 0.098, 0.024, 0.224, 0.050, 0.665, 0.078, 0.032,
        0.581, 0.162, 0.506, 0.103, 0.405, 0.133, 0.046, 0.370, 0.324, 0.229],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 2.084e-05],
         [ 6.505e-08],
         [ 4.564e-06],
         [ 8.302e-06],
         [-3.119e-06],
         [ 3.368e-06],
         [ 9.501e-06],
         [ 1.420e-05],
         [-9.098e-06],
         [-6.849e-06]],

        [[ 5.038e-06],
         [ 3.277e-06],
         [ 2.212e-06],
         [-7.204e-06],
         [ 1.080e-05],
         [ 2.690e-05],
         [ 3.847e-07],
         [-2.249e-06],
         [ 5.902e-06],
         [ 2.292e-06]],

        [[-1.848e-07],
         [-8.683e-06],
         [-3.087e-06],
         [-9.180e-06],
         [ 1.460e-05],
         [ 2.142e-05],
         [ 1.403e-05],
         [-8.546e-06],
         [ 2.845e-06],
         [-7.820e-06]],

        [[-1.634e-06],
         [ 1.111e-05],
         [ 3.998e-05],
         [-3.360e-06],
         [ 1.737e-06],
         [ 1.411e-05],
         [ 1.734e-06],
         [ 9.397e-06],
         [ 1.704e-06],
         [ 3.095e-06]],

        [[-8.279e-07],
         [ 5.985e-05],
         [ 3.537e-05],
         [-1.306e-05],
         [-1.237e-06],
         [-4.218e-06],
         [-5.789e-06],
         [ 1.758e-06],
         [-2.022e-05],
         [ 6.900e-06]],

        [[ 1.250e-05],
         [-6.928e-05],
         [ 2.341e-06],
         [ 1.290e-06],
         [-1.335e-05],
         [ 8.290e-06],
         [-1.488e-05],
         [-3.315e-06],
         [ 3.938e-06],
         [ 2.906e-05]],

        [[ 3.495e-05],
         [ 3.414e-05],
         [-1.713e-05],
         [ 3.168e-06],
         [-5.291e-06],
         [ 1.085e-05],
         [ 7.393e-06],
         [ 1.620e-05],
         [-5.308e-06],
         [-5.230e-06]],

        [[-1.908e-06],
         [-1.531e-06],
         [ 2.335e-05],
         [ 1.592e-05],
         [-3.179e-06],
         [-1.952e-05],
         [-2.988e-05],
         [ 1.307e-06],
         [-1.756e-06],
         [-1.063e-06]],

        [[-6.805e-06],
         [ 8.204e-07],
         [-9.782e-06],
         [ 1.389e-06],
         [-1.733e-06],
         [ 3.976e-06],
         [-1.420e-05],
         [ 2.657e-06],
         [ 4.530e-06],
         [ 1.161e-05]],

        [[-3.964e-06],
         [-2.930e-06],
         [-1.501e-06],
         [ 1.433e-06],
         [-1.000e-05],
         [ 7.689e-06],
         [-6.663e-05],
         [-6.746e-06],
         [-4.366e-06],
         [-4.536e-06]],

        [[-2.189e-05],
         [-5.397e-06],
         [-1.803e-05],
         [-1.444e-05],
         [-1.320e-05],
         [ 2.627e-06],
         [-5.111e-06],
         [-1.352e-05],
         [-6.003e-05],
         [ 3.054e-06]],

        [[ 2.912e-05],
         [ 1.480e-05],
         [-5.754e-06],
         [-1.280e-05],
         [ 6.709e-06],
         [ 2.578e-05],
         [-2.328e-06],
         [-1.351e-05],
         [-3.005e-05],
         [-9.407e-08]],

        [[ 7.688e-06],
         [ 1.829e-05],
         [-2.844e-06],
         [ 4.462e-05],
         [ 1.260e-06],
         [-1.054e-06],
         [-8.022e-06],
         [-9.858e-06],
         [ 5.649e-06],
         [-1.061e-05]],

        [[-9.667e-06],
         [ 2.765e-05],
         [ 1.642e-05],
         [-2.096e-05],
         [-1.732e-05],
         [ 4.421e-05],
         [ 6.180e-06],
         [-4.399e-06],
         [-9.577e-07],
         [-1.985e-06]],

        [[ 9.273e-06],
         [ 9.510e-06],
         [ 4.406e-06],
         [-1.415e-06],
         [ 5.780e-06],
         [-1.324e-05],
         [-2.298e-07],
         [-2.363e-05],
         [-9.674e-06],
         [-3.296e-05]],

        [[-4.145e-07],
         [-7.383e-06],
         [ 5.846e-06],
         [ 1.553e-05],
         [-6.556e-06],
         [ 2.765e-06],
         [ 5.098e-06],
         [-4.210e-07],
         [-4.161e-05],
         [ 7.254e-07]],

        [[-3.202e-06],
         [-1.254e-05],
         [-5.942e-05],
         [-2.122e-05],
         [-7.142e-06],
         [-5.018e-06],
         [ 1.800e-05],
         [-2.639e-05],
         [-1.589e-05],
         [-1.461e-05]],

        [[-8.119e-06],
         [-1.400e-05],
         [ 5.814e-06],
         [ 2.515e-06],
         [ 3.075e-05],
         [-3.236e-06],
         [ 2.701e-05],
         [-1.888e-06],
         [ 2.186e-07],
         [ 2.069e-05]],

        [[ 9.731e-06],
         [-4.235e-06],
         [ 4.682e-06],
         [ 8.112e-06],
         [ 6.196e-06],
         [ 7.331e-07],
         [-4.487e-06],
         [-2.055e-05],
         [-2.803e-05],
         [ 2.776e-05]],

        [[ 1.086e-05],
         [ 1.726e-05],
         [ 6.501e-06],
         [ 2.391e-08],
         [-3.179e-06],
         [ 1.448e-05],
         [ 8.819e-06],
         [ 7.499e-07],
         [-2.951e-06],
         [ 2.064e-05]]], device='cuda:0')
s after update for 1 param tensor([[[2.465e-03],
         [7.703e-06],
         [9.269e-04],
         [5.695e-04],
         [8.750e-04],
         [7.187e-04],
         [1.515e-03],
         [3.968e-03],
         [1.344e-03],
         [2.035e-03]],

        [[1.177e-04],
         [2.309e-04],
         [6.931e-05],
         [1.315e-04],
         [3.540e-04],
         [2.812e-03],
         [3.297e-04],
         [7.444e-05],
         [3.543e-04],
         [1.828e-04]],

        [[2.437e-03],
         [2.310e-03],
         [1.092e-04],
         [1.083e-04],
         [3.041e-03],
         [4.686e-03],
         [4.734e-03],
         [8.509e-04],
         [4.633e-04],
         [2.923e-04]],

        [[2.052e-04],
         [1.267e-03],
         [1.264e-02],
         [4.552e-05],
         [2.285e-04],
         [1.374e-03],
         [7.839e-05],
         [3.473e-04],
         [4.559e-04],
         [5.443e-05]],

        [[5.234e-05],
         [2.013e-02],
         [1.066e-02],
         [6.694e-04],
         [2.368e-04],
         [2.364e-04],
         [5.863e-04],
         [4.297e-05],
         [4.078e-03],
         [3.627e-04]],

        [[2.981e-03],
         [1.524e-02],
         [8.925e-04],
         [1.263e-04],
         [1.782e-03],
         [8.247e-04],
         [2.309e-04],
         [1.379e-04],
         [4.358e-05],
         [6.485e-03]],

        [[2.370e-03],
         [8.044e-03],
         [8.865e-04],
         [1.338e-04],
         [1.035e-03],
         [1.430e-03],
         [8.514e-05],
         [1.005e-03],
         [1.880e-03],
         [1.003e-03]],

        [[5.638e-04],
         [1.936e-03],
         [6.200e-03],
         [7.519e-04],
         [1.239e-05],
         [5.609e-03],
         [5.605e-03],
         [1.390e-04],
         [1.547e-04],
         [6.658e-05]],

        [[2.117e-04],
         [1.153e-04],
         [4.329e-04],
         [3.442e-05],
         [6.974e-04],
         [6.336e-04],
         [7.903e-04],
         [5.051e-04],
         [5.535e-04],
         [3.730e-03]],

        [[2.280e-03],
         [5.147e-04],
         [2.930e-04],
         [2.722e-04],
         [6.676e-04],
         [3.825e-04],
         [1.164e-02],
         [1.035e-03],
         [2.010e-04],
         [1.180e-03]],

        [[5.556e-03],
         [7.819e-04],
         [4.401e-03],
         [1.930e-03],
         [3.032e-03],
         [5.251e-03],
         [1.042e-03],
         [1.440e-04],
         [1.391e-02],
         [9.919e-04]],

        [[1.006e-03],
         [2.990e-04],
         [3.779e-03],
         [2.925e-03],
         [8.659e-04],
         [4.257e-03],
         [9.462e-04],
         [1.815e-03],
         [3.331e-03],
         [1.265e-04]],

        [[1.708e-03],
         [1.676e-03],
         [5.164e-05],
         [1.958e-02],
         [1.245e-03],
         [5.062e-05],
         [6.521e-04],
         [6.384e-04],
         [2.049e-03],
         [2.447e-03]],

        [[1.944e-03],
         [1.799e-03],
         [3.292e-03],
         [1.119e-03],
         [7.826e-03],
         [6.968e-03],
         [4.820e-04],
         [1.011e-04],
         [4.785e-04],
         [1.054e-05]],

        [[1.299e-03],
         [4.467e-04],
         [8.851e-04],
         [7.016e-04],
         [1.839e-03],
         [1.317e-03],
         [1.026e-05],
         [5.916e-03],
         [1.579e-03],
         [5.388e-03]],

        [[1.226e-05],
         [5.187e-04],
         [1.345e-03],
         [5.713e-03],
         [7.814e-04],
         [1.923e-04],
         [1.066e-03],
         [1.129e-05],
         [4.529e-03],
         [2.299e-04]],

        [[2.956e-05],
         [3.752e-03],
         [5.167e-03],
         [9.777e-04],
         [1.627e-04],
         [1.490e-03],
         [1.808e-03],
         [6.943e-03],
         [4.113e-03],
         [4.350e-03]],

        [[2.521e-03],
         [8.969e-03],
         [1.680e-04],
         [1.557e-04],
         [9.450e-03],
         [7.501e-04],
         [4.089e-03],
         [3.097e-04],
         [1.446e-04],
         [1.424e-03]],

        [[1.113e-04],
         [3.919e-03],
         [4.618e-05],
         [1.383e-03],
         [8.561e-04],
         [4.085e-05],
         [1.320e-03],
         [2.367e-03],
         [6.715e-03],
         [5.656e-03]],

        [[3.156e-03],
         [1.427e-03],
         [1.175e-03],
         [5.317e-05],
         [6.089e-04],
         [4.029e-04],
         [1.835e-03],
         [2.052e-04],
         [2.977e-03],
         [8.295e-04]]], device='cuda:0')
b after update for 1 param tensor([[[0.238],
         [0.013],
         [0.146],
         [0.115],
         [0.142],
         [0.129],
         [0.187],
         [0.302],
         [0.176],
         [0.217]],

        [[0.052],
         [0.073],
         [0.040],
         [0.055],
         [0.090],
         [0.255],
         [0.087],
         [0.041],
         [0.090],
         [0.065]],

        [[0.237],
         [0.231],
         [0.050],
         [0.050],
         [0.265],
         [0.329],
         [0.330],
         [0.140],
         [0.103],
         [0.082]],

        [[0.069],
         [0.171],
         [0.540],
         [0.032],
         [0.073],
         [0.178],
         [0.043],
         [0.089],
         [0.103],
         [0.035]],

        [[0.035],
         [0.681],
         [0.496],
         [0.124],
         [0.074],
         [0.074],
         [0.116],
         [0.031],
         [0.307],
         [0.091]],

        [[0.262],
         [0.593],
         [0.143],
         [0.054],
         [0.203],
         [0.138],
         [0.073],
         [0.056],
         [0.032],
         [0.387]],

        [[0.234],
         [0.431],
         [0.143],
         [0.056],
         [0.154],
         [0.182],
         [0.044],
         [0.152],
         [0.208],
         [0.152]],

        [[0.114],
         [0.211],
         [0.378],
         [0.132],
         [0.017],
         [0.360],
         [0.359],
         [0.057],
         [0.060],
         [0.039]],

        [[0.070],
         [0.052],
         [0.100],
         [0.028],
         [0.127],
         [0.121],
         [0.135],
         [0.108],
         [0.113],
         [0.293]],

        [[0.229],
         [0.109],
         [0.082],
         [0.079],
         [0.124],
         [0.094],
         [0.518],
         [0.154],
         [0.068],
         [0.165]],

        [[0.358],
         [0.134],
         [0.319],
         [0.211],
         [0.264],
         [0.348],
         [0.155],
         [0.058],
         [0.566],
         [0.151]],

        [[0.152],
         [0.083],
         [0.295],
         [0.260],
         [0.141],
         [0.313],
         [0.148],
         [0.205],
         [0.277],
         [0.054]],

        [[0.198],
         [0.197],
         [0.035],
         [0.672],
         [0.169],
         [0.034],
         [0.123],
         [0.121],
         [0.217],
         [0.238]],

        [[0.212],
         [0.204],
         [0.275],
         [0.161],
         [0.425],
         [0.401],
         [0.105],
         [0.048],
         [0.105],
         [0.016]],

        [[0.173],
         [0.101],
         [0.143],
         [0.127],
         [0.206],
         [0.174],
         [0.015],
         [0.369],
         [0.191],
         [0.352]],

        [[0.017],
         [0.109],
         [0.176],
         [0.363],
         [0.134],
         [0.067],
         [0.157],
         [0.016],
         [0.323],
         [0.073]],

        [[0.026],
         [0.294],
         [0.345],
         [0.150],
         [0.061],
         [0.185],
         [0.204],
         [0.400],
         [0.308],
         [0.317]],

        [[0.241],
         [0.455],
         [0.062],
         [0.060],
         [0.467],
         [0.131],
         [0.307],
         [0.084],
         [0.058],
         [0.181]],

        [[0.051],
         [0.301],
         [0.033],
         [0.179],
         [0.140],
         [0.031],
         [0.174],
         [0.234],
         [0.393],
         [0.361]],

        [[0.270],
         [0.181],
         [0.165],
         [0.035],
         [0.118],
         [0.096],
         [0.206],
         [0.069],
         [0.262],
         [0.138]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 2.987e-06],
        [-2.370e-06],
        [ 1.989e-05],
        [ 1.345e-05],
        [ 9.414e-06],
        [-5.966e-06],
        [ 1.603e-05],
        [ 3.050e-06],
        [ 1.641e-05],
        [ 5.088e-06],
        [ 3.123e-05],
        [ 1.877e-05],
        [-9.721e-06],
        [ 9.628e-06],
        [-9.508e-06],
        [ 5.882e-06],
        [-1.830e-05],
        [-5.009e-06],
        [-2.146e-05],
        [ 1.088e-05]], device='cuda:0')
s after update for 1 param tensor([[0.000],
        [0.000],
        [0.001],
        [0.000],
        [0.000],
        [0.001],
        [0.001],
        [0.000],
        [0.004],
        [0.004],
        [0.014],
        [0.004],
        [0.001],
        [0.005],
        [0.000],
        [0.001],
        [0.006],
        [0.000],
        [0.004],
        [0.002]], device='cuda:0')
b after update for 1 param tensor([[0.077],
        [0.073],
        [0.116],
        [0.071],
        [0.094],
        [0.123],
        [0.115],
        [0.066],
        [0.306],
        [0.295],
        [0.558],
        [0.313],
        [0.127],
        [0.334],
        [0.072],
        [0.117],
        [0.364],
        [0.095],
        [0.302],
        [0.187]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 0.333607258083502
exp ma of ||w||^2 31.438891211152146
||w|| 0.5775874462655002
exp ma of ||w|| 0.7008203326481396
||w||^2 0.13666732822218605
exp ma of ||w||^2 0.15306759234124562
||w|| 0.36968544496934963
exp ma of ||w|| 0.3610893111472698
||w||^2 0.056531922892055865
exp ma of ||w||^2 0.09076048112658605
||w|| 0.23776442730580172
exp ma of ||w|| 0.2940659660713046
cuda
Objective function 20.77 = squared loss an data 19.88 + 0.5*rho*h**2 0.244570 + alpha*h 0.073459 + L2reg 0.47 + L1reg 0.10 ; SHD = 46 ; DAG True
Proportion of microbatches that were clipped  0.7250284229332468
iteration 3 in inner loop, alpha 33.21468961079323 rho 100000.0 h 0.002211652893869598
iteration 4 in outer loop, alpha = 254.37997899775303, rho = 100000.0, h = 0.002211652893869598
cuda
4420
cuda
Objective function 21.26 = squared loss an data 19.88 + 0.5*rho*h**2 0.244570 + alpha*h 0.562600 + L2reg 0.47 + L1reg 0.10 ; SHD = 46 ; DAG True
||w||^2 22724668879.58651
exp ma of ||w||^2 5210363152.125378
||w|| 150747.03605572652
exp ma of ||w|| 36589.09178676163
||w||^2 252071703.61416054
exp ma of ||w||^2 261486084224.74292
||w|| 15876.766157318074
exp ma of ||w|| 177806.58452785987
||w||^2 15071337.563348234
exp ma of ||w||^2 9737141729.330551
||w|| 3882.1820621073703
exp ma of ||w|| 13314.445212205565
||w||^2 9.776942208887462
exp ma of ||w||^2 163585.79376852792
||w|| 3.1268102291132833
exp ma of ||w|| 11.458931753206524
||w||^2 0.618827699537778
exp ma of ||w||^2 963.4597583266337
||w|| 0.7866560236455182
exp ma of ||w|| 1.0733284297155155
||w||^2 0.36501036400269277
exp ma of ||w||^2 169.75586789048643
||w|| 0.6041608759285003
exp ma of ||w|| 0.7819186758537175
||w||^2 0.19383259012497922
exp ma of ||w||^2 10.825829991190222
||w|| 0.44026422762357065
exp ma of ||w|| 0.6049214157909534
||w||^2 0.13807608209685857
exp ma of ||w||^2 0.09190388878932465
||w|| 0.37158590136986974
exp ma of ||w|| 0.2971707530955351
||w||^2 0.05868678418232005
exp ma of ||w||^2 0.08884467100080438
||w|| 0.24225355349781774
exp ma of ||w|| 0.29143951774619065
cuda
Objective function 21.06 = squared loss an data 20.04 + 0.5*rho*h**2 0.094993 + alpha*h 0.350626 + L2reg 0.48 + L1reg 0.09 ; SHD = 47 ; DAG True
Proportion of microbatches that were clipped  0.7201731879409878
iteration 1 in inner loop, alpha 254.37997899775303 rho 100000.0 h 0.0013783543549976685
iteration 5 in outer loop, alpha = 1632.7343339954214, rho = 1000000.0, h = 0.0013783543549976685
Threshold 0.3
[[0.002 0.061 0.173 0.021 0.193 0.088 0.072 0.055 0.025 0.181 0.184 0.125
  0.392 0.012 0.094 0.092 0.061 0.331 0.278 0.101]
 [0.04  0.003 0.045 0.037 0.147 0.395 0.038 0.012 0.015 0.261 0.053 0.125
  0.127 0.055 0.107 0.072 0.025 0.201 0.104 0.063]
 [0.013 0.037 0.002 0.04  0.088 0.023 0.022 0.015 0.005 0.232 0.036 0.025
  0.067 0.003 0.03  0.053 0.035 0.048 0.049 0.063]
 [0.111 0.05  0.067 0.003 0.229 0.074 0.148 0.054 0.006 0.063 0.127 0.191
  0.163 0.027 0.033 0.075 0.152 0.113 0.052 0.036]
 [0.012 0.012 0.032 0.009 0.003 0.025 0.016 0.007 0.004 0.031 0.043 0.003
  0.091 0.016 0.023 0.08  0.028 0.067 0.064 0.018]
 [0.031 0.01  0.102 0.036 0.09  0.003 0.021 0.006 0.018 0.156 0.075 0.056
  0.139 0.012 0.007 0.048 0.034 0.046 0.251 0.031]
 [0.037 0.036 0.139 0.014 0.217 0.172 0.002 0.017 0.016 0.097 0.019 0.023
  0.094 0.029 0.01  0.152 0.654 0.078 0.044 0.07 ]
 [0.037 0.156 0.111 0.04  0.187 0.214 0.156 0.003 0.018 0.085 0.093 0.673
  0.547 0.069 0.332 0.239 0.102 1.115 0.042 0.408]
 [0.075 0.138 0.422 0.381 0.552 0.149 0.104 0.089 0.003 0.401 0.419 0.277
  0.384 0.3   0.125 0.326 0.101 0.22  0.467 0.213]
 [0.011 0.007 0.014 0.04  0.08  0.025 0.028 0.017 0.005 0.004 0.011 0.02
  0.022 0.006 0.023 0.19  0.045 0.079 0.013 0.018]
 [0.012 0.052 0.104 0.019 0.031 0.038 0.152 0.022 0.005 0.327 0.002 0.031
  0.086 0.015 0.041 0.077 0.065 0.05  0.013 0.051]
 [0.028 0.025 0.054 0.016 0.486 0.045 0.229 0.003 0.009 0.109 0.131 0.002
  0.223 0.035 0.046 0.266 0.086 0.247 0.024 0.042]
 [0.008 0.017 0.029 0.017 0.024 0.018 0.048 0.004 0.007 0.167 0.026 0.015
  0.002 0.008 0.004 0.071 0.035 0.079 0.027 0.022]
 [0.152 0.047 1.154 0.071 0.074 0.214 0.069 0.026 0.01  0.248 0.095 0.075
  0.159 0.002 0.071 0.183 0.108 0.113 0.34  0.376]
 [0.021 0.025 0.064 0.068 0.096 0.309 0.164 0.006 0.022 0.125 0.053 0.057
  0.619 0.036 0.003 0.096 0.153 0.155 0.075 0.207]
 [0.021 0.023 0.043 0.032 0.036 0.041 0.015 0.007 0.006 0.013 0.04  0.009
  0.026 0.008 0.021 0.002 0.042 0.052 0.007 0.028]
 [0.036 0.035 0.058 0.019 0.095 0.06  0.004 0.016 0.012 0.052 0.026 0.025
  0.062 0.017 0.016 0.063 0.002 0.058 0.02  0.014]
 [0.007 0.01  0.067 0.016 0.039 0.052 0.03  0.002 0.005 0.032 0.051 0.009
  0.04  0.014 0.017 0.044 0.038 0.003 0.034 0.008]
 [0.008 0.014 0.072 0.047 0.059 0.01  0.029 0.028 0.005 0.243 0.149 0.073
  0.047 0.007 0.027 0.453 0.08  0.072 0.002 0.02 ]
 [0.019 0.031 0.044 0.054 0.14  0.065 0.042 0.003 0.013 0.16  0.105 0.076
  0.108 0.008 0.017 0.07  0.15  0.251 0.074 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.392 0.    0.    0.    0.    0.331 0.    0.   ]
 [0.    0.    0.    0.    0.    0.395 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.654 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.673
  0.547 0.    0.332 0.    0.    1.115 0.    0.408]
 [0.    0.    0.422 0.381 0.552 0.    0.    0.    0.    0.401 0.419 0.
  0.384 0.3   0.    0.326 0.    0.    0.467 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.327 0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.486 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.154 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.34  0.376]
 [0.    0.    0.    0.    0.    0.309 0.    0.    0.    0.    0.    0.
  0.619 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.453 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.34615384615384615, 'tpr': 0.2833333333333333, 'fpr': 0.06923076923076923, 'f1': 0.39534883720930236, 'shd': 47, 'npred': 26, 'ntrue': 60}
[0.061 0.173 0.021 0.193 0.088 0.072 0.055 0.025 0.181 0.184 0.125 0.392
 0.012 0.094 0.092 0.061 0.331 0.278 0.101 0.04  0.045 0.037 0.147 0.395
 0.038 0.012 0.015 0.261 0.053 0.125 0.127 0.055 0.107 0.072 0.025 0.201
 0.104 0.063 0.013 0.037 0.04  0.088 0.023 0.022 0.015 0.005 0.232 0.036
 0.025 0.067 0.003 0.03  0.053 0.035 0.048 0.049 0.063 0.111 0.05  0.067
 0.229 0.074 0.148 0.054 0.006 0.063 0.127 0.191 0.163 0.027 0.033 0.075
 0.152 0.113 0.052 0.036 0.012 0.012 0.032 0.009 0.025 0.016 0.007 0.004
 0.031 0.043 0.003 0.091 0.016 0.023 0.08  0.028 0.067 0.064 0.018 0.031
 0.01  0.102 0.036 0.09  0.021 0.006 0.018 0.156 0.075 0.056 0.139 0.012
 0.007 0.048 0.034 0.046 0.251 0.031 0.037 0.036 0.139 0.014 0.217 0.172
 0.017 0.016 0.097 0.019 0.023 0.094 0.029 0.01  0.152 0.654 0.078 0.044
 0.07  0.037 0.156 0.111 0.04  0.187 0.214 0.156 0.018 0.085 0.093 0.673
 0.547 0.069 0.332 0.239 0.102 1.115 0.042 0.408 0.075 0.138 0.422 0.381
 0.552 0.149 0.104 0.089 0.401 0.419 0.277 0.384 0.3   0.125 0.326 0.101
 0.22  0.467 0.213 0.011 0.007 0.014 0.04  0.08  0.025 0.028 0.017 0.005
 0.011 0.02  0.022 0.006 0.023 0.19  0.045 0.079 0.013 0.018 0.012 0.052
 0.104 0.019 0.031 0.038 0.152 0.022 0.005 0.327 0.031 0.086 0.015 0.041
 0.077 0.065 0.05  0.013 0.051 0.028 0.025 0.054 0.016 0.486 0.045 0.229
 0.003 0.009 0.109 0.131 0.223 0.035 0.046 0.266 0.086 0.247 0.024 0.042
 0.008 0.017 0.029 0.017 0.024 0.018 0.048 0.004 0.007 0.167 0.026 0.015
 0.008 0.004 0.071 0.035 0.079 0.027 0.022 0.152 0.047 1.154 0.071 0.074
 0.214 0.069 0.026 0.01  0.248 0.095 0.075 0.159 0.071 0.183 0.108 0.113
 0.34  0.376 0.021 0.025 0.064 0.068 0.096 0.309 0.164 0.006 0.022 0.125
 0.053 0.057 0.619 0.036 0.096 0.153 0.155 0.075 0.207 0.021 0.023 0.043
 0.032 0.036 0.041 0.015 0.007 0.006 0.013 0.04  0.009 0.026 0.008 0.021
 0.042 0.052 0.007 0.028 0.036 0.035 0.058 0.019 0.095 0.06  0.004 0.016
 0.012 0.052 0.026 0.025 0.062 0.017 0.016 0.063 0.058 0.02  0.014 0.007
 0.01  0.067 0.016 0.039 0.052 0.03  0.002 0.005 0.032 0.051 0.009 0.04
 0.014 0.017 0.044 0.038 0.034 0.008 0.008 0.014 0.072 0.047 0.059 0.01
 0.029 0.028 0.005 0.243 0.149 0.073 0.047 0.007 0.027 0.453 0.08  0.072
 0.02  0.019 0.031 0.044 0.054 0.14  0.065 0.042 0.003 0.013 0.16  0.105
 0.076 0.108 0.008 0.017 0.07  0.15  0.251 0.074]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.6507291666666666, 0.4543440459256919)
Iterations 630
Achieves (14.15454886112812, 1e-05)-DP
