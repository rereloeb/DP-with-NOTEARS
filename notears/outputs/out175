samples  5000  graph  20 60 ER mim  minibatch size  100  noise  0.8  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.8940950442230857
iteration 1 in outer loop, alpha = 1.8940950442230857, rho = 1.0, h = 1.8940950442230857
cuda
iteration 1 in inner loop,alpha 1.8940950442230857 rho 1.0 h 1.182773599915663
iteration 2 in inner loop,alpha 1.8940950442230857 rho 10.0 h 0.459195570993554
iteration 2 in outer loop, alpha = 6.486050754158626, rho = 10.0, h = 0.459195570993554
cuda
iteration 1 in inner loop,alpha 6.486050754158626 rho 10.0 h 0.25981398971351055
iteration 2 in inner loop,alpha 6.486050754158626 rho 100.0 h 0.0921353771940332
iteration 3 in outer loop, alpha = 15.699588473561946, rho = 100.0, h = 0.0921353771940332
cuda
iteration 1 in inner loop,alpha 15.699588473561946 rho 100.0 h 0.053550958585276476
iteration 2 in inner loop,alpha 15.699588473561946 rho 1000.0 h 0.02038579576674948
iteration 4 in outer loop, alpha = 36.08538424031143, rho = 1000.0, h = 0.02038579576674948
cuda
iteration 1 in inner loop,alpha 36.08538424031143 rho 1000.0 h 0.01099154260426971
iteration 2 in inner loop,alpha 36.08538424031143 rho 10000.0 h 0.0034185940561357597
iteration 5 in outer loop, alpha = 70.27132480166902, rho = 10000.0, h = 0.0034185940561357597
cuda
iteration 1 in inner loop,alpha 70.27132480166902 rho 10000.0 h 0.0015074349105717033
iteration 2 in inner loop,alpha 70.27132480166902 rho 100000.0 h 0.0005843083089587253
iteration 6 in outer loop, alpha = 128.70215569754157, rho = 100000.0, h = 0.0005843083089587253
cuda
iteration 1 in inner loop,alpha 128.70215569754157 rho 100000.0 h 0.0003572736221784112
iteration 7 in outer loop, alpha = 485.97577787595276, rho = 1000000.0, h = 0.0003572736221784112
Threshold 0.3
[[0.    0.024 0.009 0.043 0.155 0.183 0.071 0.001 0.04  0.05  0.459 0.63
  1.063 0.    0.096 0.001 0.116 2.567 0.515 0.014]
 [0.    0.001 0.    0.035 0.084 0.463 0.13  0.    0.006 0.06  0.112 0.
  0.    0.    0.072 0.14  0.009 0.    0.086 0.802]
 [0.001 0.288 0.002 0.123 0.542 0.096 0.073 0.    1.458 0.79  0.1   0.005
  0.003 0.    0.034 0.215 0.104 0.019 0.136 0.113]
 [0.    0.007 0.    0.002 0.001 0.005 0.    0.    0.    0.01  0.126 0.
  0.    0.    0.051 0.001 0.    0.    0.002 0.029]
 [0.    0.003 0.    0.165 0.002 0.009 0.004 0.    0.    0.055 0.142 0.
  0.    0.    0.037 0.12  0.004 0.    0.001 0.008]
 [0.    0.002 0.005 0.1   0.031 0.005 0.01  0.    0.026 0.023 0.051 0.
  0.003 0.    1.188 0.072 0.007 0.    0.293 0.046]
 [0.    0.005 0.005 0.213 0.295 0.243 0.003 0.    0.013 0.16  0.399 0.001
  0.004 0.    0.091 0.063 0.001 0.    0.009 0.143]
 [0.001 0.1   0.003 1.141 0.047 0.058 0.171 0.    0.36  0.078 0.061 2.139
  1.355 0.038 0.109 0.018 0.034 2.196 0.047 0.033]
 [0.    0.025 0.    1.439 1.389 0.018 0.099 0.    0.003 0.379 1.159 0.
  0.001 0.    0.419 0.139 0.027 0.    0.006 0.016]
 [0.    0.002 0.001 0.036 0.005 0.195 0.013 0.    0.005 0.004 1.091 0.001
  0.002 0.    0.099 0.007 0.008 0.    0.007 0.664]
 [0.    0.002 0.    0.001 0.001 0.013 0.004 0.    0.    0.001 0.002 0.
  0.    0.    0.013 0.004 0.003 0.    0.003 0.749]
 [0.    2.089 0.003 1.581 1.301 0.06  0.558 0.    1.035 0.073 0.966 0.001
  0.001 0.001 0.009 0.901 0.424 0.    0.068 0.14 ]
 [0.    0.033 0.006 0.177 0.079 0.115 0.336 0.    0.001 0.585 0.051 0.575
  0.003 0.    1.589 0.159 0.204 0.    0.005 0.04 ]
 [0.001 1.037 4.611 0.096 0.045 0.167 0.072 0.001 0.834 0.182 0.036 0.019
  0.049 0.    0.057 0.67  0.002 0.074 0.708 1.198]
 [0.    0.001 0.    0.013 0.02  0.001 0.001 0.    0.002 0.004 0.011 0.
  0.    0.    0.002 0.011 0.001 0.    0.001 0.004]
 [0.    0.001 0.002 0.07  0.005 0.014 0.008 0.    0.004 0.289 0.043 0.
  0.    0.    0.037 0.003 0.008 0.    0.001 0.1  ]
 [0.    0.014 0.006 0.989 0.096 0.002 0.906 0.    0.023 0.007 0.101 0.001
  0.003 0.    0.552 0.006 0.002 0.    0.009 0.053]
 [0.    0.048 0.005 0.289 0.049 0.049 0.067 0.    0.105 0.045 0.115 0.919
  2.021 0.002 0.002 0.07  0.034 0.002 0.025 1.248]
 [0.    0.001 0.01  0.093 0.773 0.016 0.028 0.    0.418 0.201 0.147 0.001
  0.006 0.    0.692 1.033 0.01  0.007 0.004 0.066]
 [0.    0.    0.    0.003 0.001 0.018 0.001 0.    0.001 0.001 0.001 0.
  0.    0.    0.052 0.001 0.001 0.    0.002 0.002]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.459 0.63
  1.063 0.    0.    0.    0.    2.567 0.515 0.   ]
 [0.    0.    0.    0.    0.    0.463 0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.802]
 [0.    0.    0.    0.    0.542 0.    0.    0.    1.458 0.79  0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.188 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.399 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.141 0.    0.    0.    0.    0.36  0.    0.    2.139
  1.355 0.    0.    0.    0.    2.196 0.    0.   ]
 [0.    0.    0.    1.439 1.389 0.    0.    0.    0.    0.379 1.159 0.
  0.    0.    0.419 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.091 0.
  0.    0.    0.    0.    0.    0.    0.    0.664]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.749]
 [0.    2.089 0.    1.581 1.301 0.    0.558 0.    1.035 0.    0.966 0.
  0.    0.    0.    0.901 0.424 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.336 0.    0.    0.585 0.    0.575
  0.    0.    1.589 0.    0.    0.    0.    0.   ]
 [0.    1.037 4.611 0.    0.    0.    0.    0.    0.834 0.    0.    0.
  0.    0.    0.    0.67  0.    0.    0.708 1.198]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.989 0.    0.    0.906 0.    0.    0.    0.    0.
  0.    0.    0.552 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.919
  2.021 0.    0.    0.    0.    0.    0.    1.248]
 [0.    0.    0.    0.    0.773 0.    0.    0.    0.418 0.    0.    0.
  0.    0.    0.692 1.033 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.11320754716981132, 'tpr': 0.7833333333333333, 'fpr': 0.046153846153846156, 'f1': 0.8318584070796461, 'shd': 15, 'npred': 53, 'ntrue': 60}
[2.409e-02 8.701e-03 4.317e-02 1.549e-01 1.829e-01 7.118e-02 7.316e-04
 4.013e-02 5.001e-02 4.588e-01 6.304e-01 1.063e+00 1.807e-04 9.620e-02
 7.302e-04 1.164e-01 2.567e+00 5.148e-01 1.386e-02 4.814e-06 2.249e-04
 3.503e-02 8.425e-02 4.634e-01 1.298e-01 4.871e-06 5.505e-03 5.984e-02
 1.124e-01 9.139e-05 1.314e-04 2.245e-05 7.223e-02 1.401e-01 8.554e-03
 5.771e-05 8.608e-02 8.019e-01 1.473e-03 2.881e-01 1.232e-01 5.423e-01
 9.587e-02 7.269e-02 2.082e-04 1.458e+00 7.895e-01 9.966e-02 5.046e-03
 3.010e-03 1.320e-05 3.371e-02 2.146e-01 1.043e-01 1.884e-02 1.355e-01
 1.126e-01 7.725e-06 7.269e-03 5.474e-05 1.380e-03 4.684e-03 3.568e-04
 8.201e-06 4.010e-04 9.645e-03 1.259e-01 1.152e-04 2.470e-04 2.676e-06
 5.066e-02 7.891e-04 4.032e-04 3.463e-05 2.038e-03 2.892e-02 6.451e-05
 3.331e-03 1.642e-04 1.647e-01 8.531e-03 4.346e-03 3.524e-06 4.929e-04
 5.519e-02 1.419e-01 1.522e-04 2.463e-04 6.221e-06 3.712e-02 1.199e-01
 3.994e-03 8.641e-05 6.648e-04 8.257e-03 3.855e-05 1.766e-03 5.280e-03
 9.990e-02 3.080e-02 1.012e-02 1.720e-05 2.636e-02 2.305e-02 5.094e-02
 3.553e-04 3.074e-03 7.294e-05 1.188e+00 7.239e-02 7.391e-03 4.027e-04
 2.934e-01 4.586e-02 2.458e-05 4.726e-03 4.937e-03 2.134e-01 2.949e-01
 2.428e-01 1.836e-05 1.349e-02 1.595e-01 3.988e-01 1.036e-03 3.912e-03
 1.188e-04 9.104e-02 6.263e-02 1.264e-03 9.231e-05 9.207e-03 1.428e-01
 1.453e-03 9.959e-02 2.826e-03 1.141e+00 4.748e-02 5.820e-02 1.714e-01
 3.601e-01 7.762e-02 6.126e-02 2.139e+00 1.355e+00 3.816e-02 1.087e-01
 1.772e-02 3.397e-02 2.196e+00 4.727e-02 3.282e-02 4.630e-05 2.539e-02
 2.304e-04 1.439e+00 1.389e+00 1.771e-02 9.898e-02 3.991e-05 3.788e-01
 1.159e+00 2.631e-04 6.064e-04 7.721e-06 4.187e-01 1.391e-01 2.696e-02
 1.557e-04 5.801e-03 1.570e-02 2.192e-05 2.316e-03 6.807e-04 3.571e-02
 4.875e-03 1.947e-01 1.327e-02 4.275e-05 4.586e-03 1.091e+00 9.060e-04
 1.809e-03 7.320e-06 9.857e-02 7.469e-03 7.731e-03 1.423e-04 6.886e-03
 6.640e-01 4.309e-05 2.035e-03 1.944e-04 1.457e-03 5.894e-04 1.291e-02
 3.583e-03 4.814e-06 4.787e-04 8.100e-04 1.588e-04 1.068e-04 5.148e-06
 1.270e-02 4.402e-03 3.162e-03 1.326e-04 2.936e-03 7.493e-01 4.855e-06
 2.089e+00 3.486e-03 1.581e+00 1.301e+00 6.012e-02 5.582e-01 8.531e-06
 1.035e+00 7.311e-02 9.663e-01 1.150e-03 1.175e-03 8.641e-03 9.009e-01
 4.236e-01 1.553e-04 6.817e-02 1.397e-01 3.081e-06 3.344e-02 6.384e-03
 1.766e-01 7.945e-02 1.145e-01 3.365e-01 5.255e-06 1.326e-03 5.853e-01
 5.070e-02 5.749e-01 1.817e-04 1.589e+00 1.595e-01 2.036e-01 7.910e-05
 4.737e-03 4.032e-02 5.237e-04 1.037e+00 4.611e+00 9.638e-02 4.539e-02
 1.671e-01 7.156e-02 7.462e-04 8.337e-01 1.821e-01 3.596e-02 1.856e-02
 4.862e-02 5.687e-02 6.700e-01 2.325e-03 7.448e-02 7.077e-01 1.198e+00
 3.116e-06 8.308e-04 2.797e-04 1.325e-02 1.985e-02 8.015e-04 1.492e-03
 2.410e-06 1.670e-03 4.380e-03 1.062e-02 2.290e-04 3.906e-04 2.311e-05
 1.147e-02 8.325e-04 3.450e-05 1.263e-03 4.045e-03 1.823e-05 7.322e-04
 1.957e-03 6.964e-02 4.587e-03 1.433e-02 7.990e-03 5.273e-05 4.013e-03
 2.887e-01 4.331e-02 4.433e-04 4.217e-04 1.108e-05 3.676e-02 8.407e-03
 1.672e-04 1.475e-03 9.984e-02 3.748e-05 1.401e-02 5.640e-03 9.890e-01
 9.622e-02 1.578e-03 9.060e-01 1.731e-05 2.264e-02 6.898e-03 1.014e-01
 1.142e-03 3.324e-03 1.713e-04 5.518e-01 6.372e-03 5.630e-05 9.107e-03
 5.269e-02 2.453e-06 4.768e-02 4.964e-03 2.888e-01 4.851e-02 4.859e-02
 6.657e-02 1.774e-05 1.055e-01 4.505e-02 1.152e-01 9.188e-01 2.021e+00
 1.788e-03 1.924e-03 6.964e-02 3.441e-02 2.481e-02 1.248e+00 1.007e-04
 1.001e-03 9.822e-03 9.280e-02 7.735e-01 1.594e-02 2.810e-02 2.829e-04
 4.177e-01 2.008e-01 1.473e-01 5.258e-04 6.021e-03 1.427e-04 6.922e-01
 1.033e+00 1.018e-02 6.631e-03 6.602e-02 2.216e-06 3.255e-04 1.050e-04
 2.964e-03 1.223e-03 1.797e-02 1.180e-03 3.675e-06 6.931e-04 5.873e-04
 7.447e-04 3.330e-05 4.781e-04 1.531e-05 5.191e-02 7.239e-04 1.461e-03
 3.468e-04 1.865e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9138020833333333, 0.8434072764176226)
cuda
4420
cuda
Objective function 242.67 = squared loss an data 26.66 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 207 ; DAG False
||w||^2 0.03919230568476034
exp ma of ||w||^2 51.75574215923324
||w|| 0.19797046669834453
exp ma of ||w|| 0.1791987779582594
||w||^2 0.030225961719002544
exp ma of ||w||^2 0.03957159766180155
||w|| 0.1738561523760449
exp ma of ||w|| 0.1932112592957633
||w||^2 0.07105978829255237
exp ma of ||w||^2 0.04290092161015082
||w|| 0.26657041901259854
exp ma of ||w|| 0.2023590604299494
||w||^2 0.05784527124743629
exp ma of ||w||^2 0.067842955728683
||w|| 0.24051043895730656
exp ma of ||w|| 0.25596023713993993
||w||^2 0.05262159024319447
exp ma of ||w||^2 0.07948898834146953
||w|| 0.2293939629615271
exp ma of ||w|| 0.27454678028356533
cuda
Objective function 22.17 = squared loss an data 21.20 + 0.5*rho*h**2 0.582305 + alpha*h 0.000000 + L2reg 0.16 + L1reg 0.23 ; SHD = 71 ; DAG False
Proportion of microbatches that were clipped  0.7167409293443666
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.0791710078722936
iteration 1 in outer loop, alpha = 1.0791710078722936, rho = 1.0, h = 1.0791710078722936
cuda
4420
cuda
Objective function 23.34 = squared loss an data 21.20 + 0.5*rho*h**2 0.582305 + alpha*h 1.164610 + L2reg 0.16 + L1reg 0.23 ; SHD = 71 ; DAG False
||w||^2 49.302973381714956
exp ma of ||w||^2 5537.580780067382
||w|| 7.021607606646427
exp ma of ||w|| 24.63215904277219
||w||^2 2.767103531322998
exp ma of ||w||^2 883.1496114399112
||w|| 1.6634613104376663
exp ma of ||w|| 7.439745016957345
||w||^2 0.02988263288340166
exp ma of ||w||^2 0.03718274936996663
||w|| 0.1728659390493155
exp ma of ||w|| 0.18980045519481661
||w||^2 0.06279436189586127
exp ma of ||w||^2 0.047909393688978334
||w|| 0.25058803222792037
exp ma of ||w|| 0.2145705372752166
||w||^2 0.029494375292010162
exp ma of ||w||^2 0.05480026504758597
||w|| 0.171739265434583
exp ma of ||w|| 0.2296857768374238
cuda
Objective function 21.91 = squared loss an data 20.09 + 0.5*rho*h**2 0.403770 + alpha*h 0.969778 + L2reg 0.21 + L1reg 0.23 ; SHD = 61 ; DAG False
Proportion of microbatches that were clipped  0.7080765488160883
iteration 1 in inner loop, alpha 1.0791710078722936 rho 1.0 h 0.8986321499961498
4420
cuda
Objective function 25.54 = squared loss an data 20.09 + 0.5*rho*h**2 4.037699 + alpha*h 0.969778 + L2reg 0.21 + L1reg 0.23 ; SHD = 61 ; DAG False
||w||^2 48834.442483519815
exp ma of ||w||^2 470280.3130407249
||w|| 220.98516349185033
exp ma of ||w|| 410.16578344818186
||w||^2 0.01964990046531103
exp ma of ||w||^2 0.03639118680644567
||w|| 0.14017810265983424
exp ma of ||w|| 0.17700152557231877
||w||^2 0.0879311623175194
exp ma of ||w||^2 0.047143360364512166
||w|| 0.29653189089458726
exp ma of ||w|| 0.21254548194027173
||w||^2 0.03362767822423588
exp ma of ||w||^2 0.06297500771292347
||w|| 0.1833785108027543
exp ma of ||w|| 0.24493765342468501
||w||^2 0.07951515922631826
exp ma of ||w||^2 0.08423603793577665
||w|| 0.2819843244336789
exp ma of ||w|| 0.28296992557165884
cuda
Objective function 21.43 = squared loss an data 20.08 + 0.5*rho*h**2 0.572635 + alpha*h 0.365211 + L2reg 0.22 + L1reg 0.19 ; SHD = 51 ; DAG False
Proportion of microbatches that were clipped  0.7196481859588503
iteration 2 in inner loop, alpha 1.0791710078722936 rho 10.0 h 0.3384183461231416
4420
cuda
Objective function 26.58 = squared loss an data 20.08 + 0.5*rho*h**2 5.726349 + alpha*h 0.365211 + L2reg 0.22 + L1reg 0.19 ; SHD = 51 ; DAG False
||w||^2 147799061.4729179
exp ma of ||w||^2 404844512.59063953
||w|| 12157.263732967132
exp ma of ||w|| 17671.27866322544
||w||^2 0.03451348209325606
exp ma of ||w||^2 0.042826382217302957
||w|| 0.18577804524016303
exp ma of ||w|| 0.2024150981636277
||w||^2 0.05631645942551492
exp ma of ||w||^2 0.04713693224465632
||w|| 0.23731089192347435
exp ma of ||w|| 0.21307667454403287
||w||^2 0.07833310029098736
exp ma of ||w||^2 0.08654891030330585
||w|| 0.2798805107380422
exp ma of ||w|| 0.2863259095354137
cuda
Objective function 21.25 = squared loss an data 20.30 + 0.5*rho*h**2 0.450060 + alpha*h 0.102386 + L2reg 0.25 + L1reg 0.15 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.7226995781909077
iteration 3 in inner loop, alpha 1.0791710078722936 rho 100.0 h 0.09487465991548305
iteration 2 in outer loop, alpha = 10.566636999420599, rho = 100.0, h = 0.09487465991548305
cuda
4420
cuda
Objective function 22.15 = squared loss an data 20.30 + 0.5*rho*h**2 0.450060 + alpha*h 1.002506 + L2reg 0.25 + L1reg 0.15 ; SHD = 51 ; DAG True
||w||^2 583197144.7231231
exp ma of ||w||^2 680826991.911447
||w|| 24149.475040321748
exp ma of ||w|| 23589.114999044654
||w||^2 394041863.2338049
exp ma of ||w||^2 655125951.7663805
||w|| 19850.487732894748
exp ma of ||w|| 23177.165636028833
||w||^2 4217403.989807374
exp ma of ||w||^2 46432759.09484968
||w|| 2053.631902217964
exp ma of ||w|| 5404.379667607941
||w||^2 30.345565840715082
exp ma of ||w||^2 3438.9051025490667
||w|| 5.5086809528883665
exp ma of ||w|| 14.964013926962549
||w||^2 1.7887382010633917
exp ma of ||w||^2 276.24460558352155
||w|| 1.3374371764921864
exp ma of ||w|| 3.0335716868922358
||w||^2 0.1330695740359169
exp ma of ||w||^2 0.5196165659788656
||w|| 0.3647870255860492
exp ma of ||w|| 0.2733554011003236
||w||^2 0.04900834581783307
exp ma of ||w||^2 0.12146386893252492
||w|| 0.22137828669007506
exp ma of ||w|| 0.23255742891416326
||w||^2 0.029823479009295623
exp ma of ||w||^2 0.06777460682017195
||w|| 0.1726947567510248
exp ma of ||w|| 0.2535687757728767
||w||^2 0.09001139626841882
exp ma of ||w||^2 0.08796790530724503
||w|| 0.30001899317946323
exp ma of ||w|| 0.2883190890612056
||w||^2 0.1129516104156016
exp ma of ||w||^2 0.09030777007173262
||w|| 0.3360827434064439
exp ma of ||w|| 0.2921970063540719
cuda
Objective function 21.64 = squared loss an data 20.44 + 0.5*rho*h**2 0.167676 + alpha*h 0.611910 + L2reg 0.27 + L1reg 0.14 ; SHD = 52 ; DAG True
Proportion of microbatches that were clipped  0.7117675860943365
iteration 1 in inner loop, alpha 10.566636999420599 rho 100.0 h 0.057909635066216936
4420
cuda
Objective function 23.15 = squared loss an data 20.44 + 0.5*rho*h**2 1.676763 + alpha*h 0.611910 + L2reg 0.27 + L1reg 0.14 ; SHD = 52 ; DAG True
||w||^2 18449530.574320313
exp ma of ||w||^2 93960752.83975111
||w|| 4295.291675115942
exp ma of ||w|| 7961.368667658391
||w||^2 10.376658222205812
exp ma of ||w||^2 3707.282569163407
||w|| 3.2212820774042457
exp ma of ||w|| 16.76298983911315
||w||^2 0.07579083569641736
exp ma of ||w||^2 0.5182400829800975
||w|| 0.2753013543308811
exp ma of ||w|| 0.334383037451068
cuda
Objective function 21.33 = squared loss an data 20.44 + 0.5*rho*h**2 0.241903 + alpha*h 0.232420 + L2reg 0.29 + L1reg 0.13 ; SHD = 52 ; DAG True
Proportion of microbatches that were clipped  0.7197513151602104
iteration 2 in inner loop, alpha 10.566636999420599 rho 1000.0 h 0.021995604665875135
iteration 3 in outer loop, alpha = 32.56224166529573, rho = 1000.0, h = 0.021995604665875135
cuda
4420
cuda
Objective function 21.82 = squared loss an data 20.44 + 0.5*rho*h**2 0.241903 + alpha*h 0.716226 + L2reg 0.29 + L1reg 0.13 ; SHD = 52 ; DAG True
||w||^2 329.6686620476433
exp ma of ||w||^2 38760.594484393616
||w|| 18.15678005725804
exp ma of ||w|| 70.27078215880559
||w||^2 0.0523049031728728
exp ma of ||w||^2 0.05273692616849365
||w|| 0.2287026523083473
exp ma of ||w|| 0.2246434535101893
||w||^2 0.14304292570401447
exp ma of ||w||^2 0.05628648323812167
||w|| 0.3782101607625243
exp ma of ||w|| 0.2310426000593767
||w||^2 0.04139498867659567
exp ma of ||w||^2 0.0663825355556729
||w|| 0.20345758446564646
exp ma of ||w|| 0.2509473643389242
||w||^2 0.053793228299964496
exp ma of ||w||^2 0.07209055616440646
||w|| 0.23193367219954178
exp ma of ||w|| 0.2605236266022231
cuda
Objective function 21.48 = squared loss an data 20.43 + 0.5*rho*h**2 0.114300 + alpha*h 0.492327 + L2reg 0.32 + L1reg 0.13 ; SHD = 55 ; DAG True
Proportion of microbatches that were clipped  0.7153237526239302
iteration 1 in inner loop, alpha 32.56224166529573 rho 1000.0 h 0.015119555565028975
4420
cuda
Objective function 22.51 = squared loss an data 20.43 + 0.5*rho*h**2 1.143005 + alpha*h 0.492327 + L2reg 0.32 + L1reg 0.13 ; SHD = 55 ; DAG True
||w||^2 28585927343.538727
exp ma of ||w||^2 4653448179.533002
||w|| 169073.73345241635
exp ma of ||w|| 33978.73241891463
||w||^2 696344835.6407377
exp ma of ||w||^2 3036133113.92919
||w|| 26388.3465878546
exp ma of ||w|| 41949.73770220171
||w||^2 0.03360986663091607
exp ma of ||w||^2 0.06529571045727912
||w|| 0.18332993926502042
exp ma of ||w|| 0.251843947296082
||w||^2 0.05267238167090429
exp ma of ||w||^2 0.061790230845256165
||w|| 0.22950464411620147
exp ma of ||w|| 0.2439847034891873
||w||^2 0.05921779257222102
exp ma of ||w||^2 0.06348287299484541
||w|| 0.24334706197573255
exp ma of ||w|| 0.24632561322875227
cuda
Objective function 21.20 = squared loss an data 20.36 + 0.5*rho*h**2 0.174742 + alpha*h 0.192499 + L2reg 0.35 + L1reg 0.12 ; SHD = 51 ; DAG True
Proportion of microbatches that were clipped  0.7159687947779017
iteration 2 in inner loop, alpha 32.56224166529573 rho 10000.0 h 0.005911722224652749
4420
cuda
Objective function 22.77 = squared loss an data 20.36 + 0.5*rho*h**2 1.747423 + alpha*h 0.192499 + L2reg 0.35 + L1reg 0.12 ; SHD = 51 ; DAG True
||w||^2 51172659141.12443
exp ma of ||w||^2 8461750281.248559
||w|| 226213.74657859417
exp ma of ||w|| 37407.42199755564
||w||^2 55545289.792372845
exp ma of ||w||^2 11305563549.949203
||w|| 7452.871244854083
exp ma of ||w|| 19691.825381851988
v before min max tensor([[ 3.090e-02, -1.586e-05, -4.820e-03,  ...,  3.036e-04,  1.601e-02,
         -1.179e-03],
        [-2.993e-02, -7.613e-04, -1.274e-03,  ...,  1.910e-05, -4.632e-04,
          6.573e-03],
        [ 4.329e-01, -5.687e-03, -5.043e-04,  ...,  6.988e-03,  2.311e-04,
         -2.600e-03],
        ...,
        [ 5.230e-04, -2.893e-04, -7.267e-03,  ...,  2.221e-02, -8.628e-04,
         -5.353e-02],
        [-9.839e-06, -1.414e-03, -5.785e-03,  ...,  1.476e-04,  3.548e-04,
          3.645e-03],
        [ 2.139e-03, -2.041e-03, -4.059e-03,  ..., -1.787e-02, -1.766e-02,
          6.785e-01]], device='cuda:0')
v tensor([[3.090e-02, 1.000e-12, 1.000e-12,  ..., 3.036e-04, 1.601e-02,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.910e-05, 1.000e-12,
         6.573e-03],
        [4.329e-01, 1.000e-12, 1.000e-12,  ..., 6.988e-03, 2.311e-04,
         1.000e-12],
        ...,
        [5.230e-04, 1.000e-12, 1.000e-12,  ..., 2.221e-02, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.476e-04, 3.548e-04,
         3.645e-03],
        [2.139e-03, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         6.785e-01]], device='cuda:0')
v before min max tensor([-3.394e-06,  5.039e-06, -3.038e-04, -2.405e-03, -1.683e-04, -1.206e-05,
        -2.654e-04,  2.164e-04,  1.905e-03,  2.829e-02, -3.278e-05, -8.672e-06,
         3.344e-04, -2.451e-03, -5.441e-04, -3.064e-04, -3.303e-05, -7.347e-04,
         2.718e-04,  3.019e-06, -1.869e-02, -5.120e-04,  5.100e-03,  2.643e-04,
        -3.170e-07, -3.010e-04, -4.124e-03, -5.871e-04, -2.536e-03, -3.830e-04,
        -2.574e-03,  1.155e-04, -5.966e-07, -2.445e-06,  1.503e-04, -8.443e-05,
        -2.627e-06, -1.493e-05,  1.304e-05, -2.183e-04,  8.042e-05, -3.153e-02,
        -5.451e-04, -1.883e-06, -1.089e-03,  1.746e-03, -1.543e-05,  5.606e-05,
        -5.292e-04, -5.646e-04, -6.231e-04, -2.797e-03, -1.193e-05,  3.198e-04,
        -1.287e-04, -1.970e-04,  1.500e-03, -1.319e-06, -5.865e-04, -6.571e-03,
         1.239e-03, -2.164e-03,  4.472e-03, -5.603e-03, -3.502e-05, -1.490e-03,
        -1.451e-03,  1.719e-03,  4.554e-04,  9.231e-03, -5.704e-06, -1.278e-04,
         1.658e-03,  7.285e-04,  1.385e-02, -7.224e-03, -4.613e-06, -1.063e-05,
        -9.577e-04, -1.762e-04, -2.073e-02,  2.299e-03, -2.054e-05, -2.902e-04,
        -3.069e-04, -1.762e-03, -3.350e-04, -2.846e-06,  3.034e-03,  1.436e-04,
        -1.145e-04,  4.437e-06, -4.552e-04, -4.983e-05, -4.566e-04,  3.768e-03,
         3.456e-03, -1.721e-04, -1.984e-04, -1.938e-03, -7.798e-04, -1.418e-04,
        -5.001e-04,  5.901e-03, -1.194e-05, -1.279e-04, -1.110e-03,  1.004e-03,
        -5.748e-03,  1.619e-06,  9.145e-04,  2.515e-03,  2.056e-05,  5.873e-05,
        -1.308e-03, -2.148e-05, -1.405e-05, -2.102e-05, -3.901e-04,  6.906e-03,
         2.923e-03, -2.215e-03, -3.687e-03, -4.172e-05, -1.310e-05, -8.912e-04,
        -3.017e-03, -1.136e-03, -3.524e-02,  1.595e-03, -7.186e-03, -3.297e-04,
        -3.576e-04,  1.575e-04,  3.438e-04,  7.634e-07, -2.490e-05, -2.904e-03,
        -7.302e-04, -3.210e-04,  1.809e-04, -4.624e-05, -2.303e-03,  2.602e-05,
        -5.384e-03, -3.276e-05, -3.136e-04, -4.861e-03, -1.341e-03, -7.463e-04,
        -2.998e-02, -1.361e-03, -7.222e-04, -2.666e-05, -2.262e-04, -1.228e-04,
         2.246e-02,  5.184e-04, -3.368e-04,  1.081e-02, -1.588e-05,  3.650e-04,
        -3.430e-04, -2.204e-03, -8.183e-04, -4.362e-05, -6.472e-03,  5.124e-03,
        -1.859e-03,  7.673e-06, -1.069e-04,  1.379e-03, -5.850e-03, -6.715e-04,
        -1.421e-03, -2.421e-06, -3.033e-03, -2.094e-03, -2.323e-06, -6.127e-04,
        -6.390e-04,  1.959e-04,  1.126e-03, -2.178e-06, -3.656e-05,  1.427e-04,
        -2.590e-04,  3.814e-02, -6.285e-04, -7.641e-05,  2.265e-02, -1.999e-03,
         1.159e-02,  5.257e-05,  4.848e-03,  1.941e-05, -8.218e-05,  3.735e-03,
        -2.518e-03, -6.785e-03], device='cuda:0')
v tensor([1.000e-12, 5.039e-06, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.164e-04, 1.905e-03, 2.829e-02, 1.000e-12, 1.000e-12,
        3.344e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        2.718e-04, 3.019e-06, 1.000e-12, 1.000e-12, 5.100e-03, 2.643e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.155e-04, 1.000e-12, 1.000e-12, 1.503e-04, 1.000e-12,
        1.000e-12, 1.000e-12, 1.304e-05, 1.000e-12, 8.042e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.746e-03, 1.000e-12, 5.606e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.198e-04,
        1.000e-12, 1.000e-12, 1.500e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.239e-03, 1.000e-12, 4.472e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.719e-03, 4.554e-04, 9.231e-03, 1.000e-12, 1.000e-12,
        1.658e-03, 7.285e-04, 1.385e-02, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 2.299e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.034e-03, 1.436e-04,
        1.000e-12, 4.437e-06, 1.000e-12, 1.000e-12, 1.000e-12, 3.768e-03,
        3.456e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 5.901e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.004e-03,
        1.000e-12, 1.619e-06, 9.145e-04, 2.515e-03, 2.056e-05, 5.873e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 6.906e-03,
        2.923e-03, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.595e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.575e-04, 3.438e-04, 7.634e-07, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.809e-04, 1.000e-12, 1.000e-12, 2.602e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        2.246e-02, 5.184e-04, 1.000e-12, 1.081e-02, 1.000e-12, 3.650e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 5.124e-03,
        1.000e-12, 7.673e-06, 1.000e-12, 1.379e-03, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.959e-04, 1.126e-03, 1.000e-12, 1.000e-12, 1.427e-04,
        1.000e-12, 3.814e-02, 1.000e-12, 1.000e-12, 2.265e-02, 1.000e-12,
        1.159e-02, 5.257e-05, 4.848e-03, 1.941e-05, 1.000e-12, 3.735e-03,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 1.054e-03],
         [ 2.243e-03],
         [ 4.944e-05],
         [-1.583e-03],
         [-1.967e-03],
         [ 2.270e-05],
         [ 1.812e-04],
         [ 8.059e-04],
         [-4.608e-03],
         [ 2.778e-04]],

        [[-1.267e-04],
         [-3.856e-04],
         [-1.859e-05],
         [-1.126e-04],
         [-9.533e-04],
         [-9.025e-03],
         [-7.530e-04],
         [-1.656e-04],
         [-3.944e-04],
         [-5.017e-04]],

        [[-9.095e-03],
         [ 4.300e-04],
         [-5.251e-05],
         [-8.665e-05],
         [-9.025e-03],
         [ 1.504e-03],
         [ 4.773e-04],
         [ 3.972e-05],
         [-7.295e-04],
         [-4.407e-05]],

        [[-1.755e-04],
         [-3.857e-05],
         [ 1.830e-02],
         [-1.218e-04],
         [-6.522e-04],
         [-4.655e-03],
         [-8.719e-04],
         [-9.983e-04],
         [-1.442e-03],
         [-9.079e-05]],

        [[-2.638e-04],
         [ 3.887e-02],
         [ 8.244e-03],
         [-6.098e-04],
         [-6.829e-04],
         [-1.802e-03],
         [-1.745e-03],
         [-5.723e-05],
         [ 1.514e-03],
         [-8.887e-04]],

        [[-5.806e-03],
         [ 3.085e-02],
         [ 3.010e-05],
         [-2.773e-04],
         [-5.279e-03],
         [-9.845e-04],
         [-7.673e-05],
         [-2.645e-04],
         [-5.356e-05],
         [ 2.488e-03]],

        [[-2.007e-03],
         [ 3.123e-03],
         [-5.721e-04],
         [-1.363e-04],
         [-3.138e-03],
         [-3.689e-03],
         [-1.446e-04],
         [-1.100e-03],
         [-4.711e-03],
         [-1.289e-03]],

        [[-1.835e-03],
         [-7.128e-03],
         [ 4.143e-03],
         [-1.010e-03],
         [ 9.708e-07],
         [ 3.395e-03],
         [-2.258e-02],
         [-6.317e-05],
         [-1.348e-04],
         [-8.961e-05]],

        [[-4.802e-04],
         [-1.977e-04],
         [-4.270e-04],
         [-1.092e-04],
         [-1.377e-03],
         [-4.714e-04],
         [-1.124e-04],
         [-5.155e-04],
         [-1.453e-03],
         [ 1.080e-03]],

        [[-6.918e-03],
         [-2.342e-03],
         [-3.469e-04],
         [-6.126e-04],
         [-1.834e-03],
         [-1.789e-03],
         [ 8.764e-03],
         [-9.585e-04],
         [-1.018e-03],
         [-2.949e-03]],

        [[ 1.388e-03],
         [ 2.130e-07],
         [ 1.265e-03],
         [-3.443e-04],
         [-3.305e-03],
         [-1.545e-02],
         [ 1.390e-05],
         [-1.297e-04],
         [ 1.798e-02],
         [-9.821e-04]],

        [[-3.298e-03],
         [-8.886e-04],
         [-1.172e-02],
         [ 4.643e-04],
         [-1.854e-03],
         [-1.624e-02],
         [-4.301e-03],
         [ 2.770e-04],
         [-8.901e-03],
         [-6.360e-04]],

        [[ 1.470e-05],
         [-4.772e-03],
         [-5.187e-06],
         [ 4.075e-02],
         [ 2.123e-04],
         [-1.694e-05],
         [-1.421e-03],
         [ 3.119e-04],
         [ 1.041e-04],
         [-1.032e-03]],

        [[ 1.925e-04],
         [-3.373e-03],
         [ 1.272e-03],
         [-1.736e-03],
         [-3.307e-02],
         [ 5.244e-03],
         [-3.034e-04],
         [-1.032e-04],
         [ 4.382e-06],
         [-1.137e-03]],

        [[-3.972e-03],
         [-3.564e-05],
         [-2.152e-03],
         [-1.214e-03],
         [ 2.993e-04],
         [ 5.856e-05],
         [-1.937e-05],
         [ 2.460e-03],
         [ 1.865e-04],
         [ 3.083e-03]],

        [[-1.608e-04],
         [-7.710e-04],
         [ 9.029e-05],
         [ 2.405e-03],
         [ 5.428e-05],
         [-2.428e-04],
         [-4.206e-03],
         [-9.536e-06],
         [-1.209e-02],
         [-6.997e-04]],

        [[-6.684e-05],
         [ 5.033e-04],
         [-6.239e-03],
         [-1.521e-03],
         [-1.896e-04],
         [ 7.089e-05],
         [-5.949e-03],
         [ 1.897e-03],
         [ 6.749e-04],
         [ 6.617e-03]],

        [[ 3.569e-04],
         [ 7.771e-03],
         [-3.530e-04],
         [-4.311e-04],
         [ 1.069e-02],
         [-3.121e-03],
         [ 1.547e-03],
         [-1.044e-03],
         [-7.283e-05],
         [-2.401e-03]],

        [[-9.096e-04],
         [-1.396e-02],
         [-1.041e-03],
         [-2.352e-03],
         [-1.425e-05],
         [-6.671e-05],
         [-3.460e-03],
         [ 5.834e-04],
         [ 6.467e-03],
         [ 1.209e-03]],

        [[-7.062e-03],
         [-2.970e-03],
         [-2.515e-03],
         [-1.343e-04],
         [-1.493e-03],
         [ 6.151e-05],
         [ 2.298e-04],
         [-2.963e-04],
         [-1.208e-02],
         [-3.483e-04]]], device='cuda:0')
v tensor([[[1.054e-03],
         [2.243e-03],
         [4.944e-05],
         [1.000e-12],
         [1.000e-12],
         [2.270e-05],
         [1.812e-04],
         [8.059e-04],
         [1.000e-12],
         [2.778e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [4.300e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.504e-03],
         [4.773e-04],
         [3.972e-05],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.830e-02],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [3.887e-02],
         [8.244e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.514e-03],
         [1.000e-12]],

        [[1.000e-12],
         [3.085e-02],
         [3.010e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.488e-03]],

        [[1.000e-12],
         [3.123e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [4.143e-03],
         [1.000e-12],
         [9.708e-07],
         [3.395e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.080e-03]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.764e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.388e-03],
         [2.130e-07],
         [1.265e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.390e-05],
         [1.000e-12],
         [1.798e-02],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.643e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.770e-04],
         [1.000e-12],
         [1.000e-12]],

        [[1.470e-05],
         [1.000e-12],
         [1.000e-12],
         [4.075e-02],
         [2.123e-04],
         [1.000e-12],
         [1.000e-12],
         [3.119e-04],
         [1.041e-04],
         [1.000e-12]],

        [[1.925e-04],
         [1.000e-12],
         [1.272e-03],
         [1.000e-12],
         [1.000e-12],
         [5.244e-03],
         [1.000e-12],
         [1.000e-12],
         [4.382e-06],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.993e-04],
         [5.856e-05],
         [1.000e-12],
         [2.460e-03],
         [1.865e-04],
         [3.083e-03]],

        [[1.000e-12],
         [1.000e-12],
         [9.029e-05],
         [2.405e-03],
         [5.428e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [5.033e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.089e-05],
         [1.000e-12],
         [1.897e-03],
         [6.749e-04],
         [6.617e-03]],

        [[3.569e-04],
         [7.771e-03],
         [1.000e-12],
         [1.000e-12],
         [1.069e-02],
         [1.000e-12],
         [1.547e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [5.834e-04],
         [6.467e-03],
         [1.209e-03]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.151e-05],
         [2.298e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-0.001],
        [-0.000],
        [-0.000],
        [-0.000],
        [-0.000],
        [-0.002],
        [-0.001],
        [-0.000],
        [ 0.000],
        [-0.004],
        [ 0.017],
        [-0.013],
        [-0.003],
        [-0.017],
        [-0.001],
        [-0.001],
        [ 0.001],
        [-0.000],
        [-0.013],
        [ 0.000]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.538e-04],
        [1.000e-12],
        [1.706e-02],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.290e-03],
        [1.000e-12],
        [1.000e-12],
        [1.442e-04]], device='cuda:0')
a after update for 1 param tensor([[ 1.031e-04, -2.552e-06, -2.671e-05,  ...,  3.970e-05, -1.270e-04,
          1.196e-05],
        [-2.196e-04,  1.464e-05,  8.367e-06,  ...,  1.586e-05,  6.393e-06,
         -1.219e-04],
        [-5.224e-04, -5.252e-05, -3.135e-05,  ..., -1.371e-04,  1.683e-05,
          2.482e-05],
        ...,
        [-3.993e-05, -2.650e-05, -1.510e-05,  ...,  2.325e-04, -1.724e-05,
         -1.765e-04],
        [ 3.869e-07,  4.608e-07,  2.023e-05,  ..., -2.182e-05,  1.122e-05,
          3.049e-04],
        [-8.062e-05,  6.344e-05, -3.357e-05,  ..., -8.724e-06, -5.427e-05,
         -7.110e-04]], device='cuda:0')
s after update for 1 param tensor([[6.997e-02, 2.129e-05, 3.404e-03,  ..., 6.100e-03, 4.016e-02,
         1.037e-03],
        [3.205e-02, 6.356e-04, 9.209e-04,  ..., 1.386e-03, 3.723e-04,
         2.588e-02],
        [2.102e-01, 4.882e-03, 1.294e-03,  ..., 2.713e-02, 4.810e-03,
         1.902e-03],
        ...,
        [7.330e-03, 2.573e-04, 5.211e-03,  ..., 4.844e-02, 1.054e-03,
         5.390e-02],
        [8.154e-06, 1.077e-03, 4.112e-03,  ..., 3.843e-03, 5.964e-03,
         4.749e-02],
        [1.601e-02, 8.631e-03, 2.872e-03,  ..., 1.352e-02, 1.251e-02,
         2.611e-01]], device='cuda:0')
b after update for 1 param tensor([[1.989, 0.035, 0.439,  ..., 0.587, 1.507, 0.242],
        [1.346, 0.190, 0.228,  ..., 0.280, 0.145, 1.209],
        [3.447, 0.525, 0.270,  ..., 1.238, 0.521, 0.328],
        ...,
        [0.644, 0.121, 0.543,  ..., 1.655, 0.244, 1.745],
        [0.021, 0.247, 0.482,  ..., 0.466, 0.581, 1.638],
        [0.951, 0.698, 0.403,  ..., 0.874, 0.841, 3.842]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 9.189e-07,  7.662e-06,  7.481e-06, -3.116e-06,  2.227e-05, -1.388e-07,
        -9.481e-07,  2.879e-05, -3.233e-05,  2.568e-04,  5.011e-06,  2.638e-06,
        -3.118e-05,  3.649e-06,  4.864e-06,  5.090e-07,  3.893e-06, -1.019e-05,
        -1.885e-05, -9.980e-06, -2.415e-05, -2.476e-05, -1.079e-04, -2.941e-05,
         8.493e-06, -2.817e-05,  3.722e-05, -3.286e-05,  1.742e-05, -9.453e-06,
        -8.256e-06, -3.362e-05,  2.159e-06, -3.627e-07, -1.055e-05, -1.841e-06,
        -4.732e-07, -7.640e-07, -1.006e-05, -3.628e-06, -2.767e-05, -1.163e-04,
         4.506e-06, -2.868e-06,  2.188e-05,  6.527e-05, -6.315e-06, -1.598e-05,
         1.764e-05,  1.379e-05, -4.710e-05,  1.341e-05,  2.833e-06, -2.650e-05,
        -6.076e-06, -3.825e-06,  5.822e-05,  7.847e-07, -2.929e-06,  7.430e-06,
         3.801e-05,  8.224e-06, -1.134e-04, -3.157e-06, -1.194e-05,  3.444e-05,
         7.027e-05, -6.990e-05,  7.015e-05,  1.181e-04,  9.054e-06, -7.809e-06,
         6.945e-05,  2.923e-05, -1.516e-04, -3.029e-05, -4.977e-07,  2.044e-06,
         8.770e-06, -9.348e-06,  9.907e-05, -9.842e-05, -2.823e-06, -1.440e-05,
        -4.342e-06,  2.230e-05,  9.861e-06, -1.852e-06,  5.550e-05, -1.815e-04,
        -2.030e-05,  2.968e-06,  3.274e-07, -8.237e-06,  2.240e-05, -7.267e-05,
         7.408e-05,  2.565e-05,  3.075e-05,  1.847e-06,  4.525e-05, -8.049e-06,
        -7.858e-06, -1.144e-04, -4.531e-07,  1.143e-05, -2.146e-05, -3.463e-05,
         4.079e-06, -1.671e-06, -4.198e-05, -4.892e-05, -4.737e-06, -9.733e-06,
        -2.012e-05,  6.374e-06,  1.201e-05,  1.192e-05, -3.658e-05, -9.964e-05,
         6.198e-05, -2.814e-05,  5.658e-05, -1.919e-06, -2.883e-07,  5.142e-05,
        -9.028e-06,  1.699e-05, -7.899e-05,  7.264e-05,  2.532e-05,  1.472e-05,
        -7.331e-05, -1.180e-05, -2.891e-05,  3.611e-06,  1.687e-06, -3.672e-06,
        -1.406e-06, -1.459e-06, -2.880e-05, -1.266e-06,  3.244e-05,  1.480e-05,
        -4.299e-05,  1.639e-05, -1.779e-05, -5.241e-05, -4.663e-06,  8.296e-06,
         8.075e-05,  2.432e-05,  6.625e-05,  9.197e-07,  8.623e-06, -1.858e-05,
        -1.337e-04, -3.157e-05,  1.020e-04, -1.054e-04, -1.359e-06,  1.775e-05,
        -3.179e-05,  1.096e-05, -4.840e-05,  6.405e-07, -7.829e-05,  1.205e-04,
        -5.601e-05, -4.506e-06,  1.744e-05,  7.866e-05,  1.644e-05,  2.258e-05,
        -2.470e-05, -5.137e-07,  2.053e-06,  2.858e-05, -1.127e-06, -4.207e-05,
         1.632e-05,  1.951e-05,  4.127e-05,  1.060e-06, -7.232e-06, -7.214e-05,
         6.935e-06, -2.827e-04, -4.461e-06,  2.933e-06,  1.754e-04, -7.860e-05,
         1.386e-04,  1.827e-05,  1.448e-04,  4.581e-06,  9.610e-06, -1.041e-04,
        -1.674e-04, -1.002e-04], device='cuda:0')
s after update for 1 param tensor([2.746e-06, 7.139e-04, 2.155e-04, 1.830e-03, 2.331e-04, 9.999e-06,
        2.897e-04, 4.674e-03, 1.400e-02, 5.443e-02, 4.499e-05, 7.843e-06,
        5.813e-03, 1.749e-03, 4.035e-04, 2.447e-04, 2.493e-05, 7.513e-04,
        5.213e-03, 5.530e-04, 1.946e-02, 5.334e-04, 2.288e-02, 5.159e-03,
        8.819e-05, 5.435e-04, 3.329e-03, 9.886e-04, 2.053e-03, 3.170e-04,
        2.045e-03, 3.438e-03, 1.882e-06, 1.882e-06, 3.878e-03, 1.804e-04,
        1.882e-06, 1.585e-05, 1.145e-03, 1.542e-04, 2.891e-03, 2.249e-02,
        3.856e-04, 1.188e-05, 7.689e-04, 1.370e-02, 1.781e-05, 2.368e-03,
        3.895e-04, 7.264e-04, 2.613e-03, 2.461e-03, 8.609e-06, 5.660e-03,
        9.960e-05, 1.395e-04, 1.469e-02, 1.882e-06, 4.522e-04, 4.675e-03,
        1.168e-02, 1.568e-03, 2.196e-02, 4.652e-03, 6.267e-05, 2.668e-03,
        1.866e-03, 1.354e-02, 7.064e-03, 3.046e-02, 2.098e-05, 1.560e-04,
        1.289e-02, 8.543e-03, 3.736e-02, 5.233e-03, 3.924e-06, 9.522e-06,
        8.137e-04, 2.137e-04, 1.467e-02, 1.566e-02, 1.825e-05, 2.287e-04,
        2.559e-04, 1.422e-03, 3.114e-04, 3.507e-06, 1.743e-02, 1.275e-02,
        3.811e-04, 6.661e-04, 3.409e-04, 1.396e-04, 5.346e-04, 1.953e-02,
        1.863e-02, 4.549e-04, 3.293e-04, 1.465e-03, 9.129e-04, 1.113e-04,
        3.594e-04, 2.450e-02, 1.052e-05, 1.380e-04, 1.007e-03, 1.003e-02,
        4.383e-03, 4.023e-04, 9.752e-03, 1.593e-02, 1.434e-03, 2.424e-03,
        1.059e-03, 1.920e-05, 6.915e-05, 7.238e-05, 6.785e-04, 2.652e-02,
        1.713e-02, 1.656e-03, 3.676e-03, 3.627e-05, 9.507e-06, 9.988e-04,
        2.393e-03, 8.198e-04, 2.884e-02, 1.340e-02, 9.217e-03, 3.163e-04,
        2.497e-03, 3.969e-03, 5.869e-03, 3.008e-04, 2.060e-05, 2.064e-03,
        6.009e-04, 2.592e-04, 4.258e-03, 4.803e-05, 2.397e-03, 1.616e-03,
        4.153e-03, 2.034e-04, 3.606e-04, 4.335e-03, 1.384e-03, 5.699e-04,
        2.117e-02, 1.005e-03, 1.563e-03, 1.886e-05, 2.042e-04, 2.705e-04,
        4.776e-02, 7.204e-03, 2.769e-03, 3.299e-02, 1.141e-05, 6.044e-03,
        1.922e-03, 1.605e-03, 1.048e-03, 3.080e-05, 6.798e-03, 2.278e-02,
        2.598e-03, 8.760e-04, 1.926e-04, 1.195e-02, 4.334e-03, 5.734e-04,
        1.017e-03, 1.882e-06, 2.187e-03, 1.902e-03, 1.882e-06, 2.219e-03,
        5.272e-04, 4.428e-03, 1.062e-02, 1.882e-06, 5.113e-05, 5.616e-03,
        1.853e-04, 6.255e-02, 5.602e-04, 5.533e-05, 5.085e-02, 2.312e-03,
        3.411e-02, 2.305e-03, 2.224e-02, 1.393e-03, 1.407e-04, 1.966e-02,
        1.365e-02, 6.400e-03], device='cuda:0')
b after update for 1 param tensor([0.012, 0.201, 0.110, 0.322, 0.115, 0.024, 0.128, 0.514, 0.890, 1.754,
        0.050, 0.021, 0.573, 0.314, 0.151, 0.118, 0.038, 0.206, 0.543, 0.177,
        1.049, 0.174, 1.137, 0.540, 0.071, 0.175, 0.434, 0.236, 0.341, 0.134,
        0.340, 0.441, 0.010, 0.010, 0.468, 0.101, 0.010, 0.030, 0.254, 0.093,
        0.404, 1.128, 0.148, 0.026, 0.208, 0.880, 0.032, 0.366, 0.148, 0.203,
        0.384, 0.373, 0.022, 0.566, 0.075, 0.089, 0.911, 0.010, 0.160, 0.514,
        0.813, 0.298, 1.114, 0.513, 0.060, 0.388, 0.325, 0.875, 0.632, 1.312,
        0.034, 0.094, 0.854, 0.695, 1.453, 0.544, 0.015, 0.023, 0.214, 0.110,
        0.911, 0.941, 0.032, 0.114, 0.120, 0.284, 0.133, 0.014, 0.992, 0.849,
        0.147, 0.194, 0.139, 0.089, 0.174, 1.051, 1.026, 0.160, 0.136, 0.288,
        0.227, 0.079, 0.143, 1.177, 0.024, 0.088, 0.239, 0.753, 0.498, 0.151,
        0.742, 0.949, 0.285, 0.370, 0.245, 0.033, 0.063, 0.064, 0.196, 1.224,
        0.984, 0.306, 0.456, 0.045, 0.023, 0.238, 0.368, 0.215, 1.277, 0.870,
        0.722, 0.134, 0.376, 0.474, 0.576, 0.130, 0.034, 0.342, 0.184, 0.121,
        0.491, 0.052, 0.368, 0.302, 0.485, 0.107, 0.143, 0.495, 0.280, 0.179,
        1.094, 0.238, 0.297, 0.033, 0.107, 0.124, 1.643, 0.638, 0.396, 1.365,
        0.025, 0.584, 0.330, 0.301, 0.243, 0.042, 0.620, 1.135, 0.383, 0.223,
        0.104, 0.822, 0.495, 0.180, 0.240, 0.010, 0.352, 0.328, 0.010, 0.354,
        0.173, 0.500, 0.775, 0.010, 0.054, 0.563, 0.102, 1.880, 0.178, 0.056,
        1.695, 0.362, 1.389, 0.361, 1.121, 0.281, 0.089, 1.054, 0.878, 0.601],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 8.888e-05],
         [-5.216e-05],
         [ 1.227e-05],
         [ 2.781e-05],
         [-8.515e-06],
         [ 8.176e-06],
         [ 3.313e-05],
         [ 3.245e-05],
         [-3.013e-05],
         [-1.717e-05]],

        [[ 1.459e-05],
         [ 1.450e-05],
         [ 1.388e-06],
         [-1.903e-05],
         [ 4.109e-05],
         [ 7.692e-05],
         [ 3.471e-07],
         [-3.723e-06],
         [ 1.141e-05],
         [ 2.801e-05]],

        [[ 4.486e-06],
         [-3.137e-05],
         [-1.064e-05],
         [-3.046e-05],
         [ 2.770e-05],
         [ 5.322e-05],
         [ 1.542e-05],
         [-1.727e-05],
         [ 2.472e-06],
         [-5.337e-06]],

        [[-3.866e-06],
         [ 8.648e-06],
         [ 1.407e-04],
         [-6.976e-06],
         [ 6.713e-06],
         [ 4.319e-05],
         [-1.469e-06],
         [ 3.226e-05],
         [ 5.653e-06],
         [ 7.088e-06]],

        [[-1.217e-05],
         [ 1.941e-04],
         [ 9.855e-05],
         [-2.594e-05],
         [-4.638e-06],
         [-2.200e-05],
         [-1.389e-05],
         [ 3.185e-06],
         [-6.179e-05],
         [ 1.777e-05]],

        [[ 4.342e-05],
         [-2.547e-04],
         [ 4.802e-06],
         [ 7.790e-07],
         [-3.383e-05],
         [ 2.859e-05],
         [-2.978e-05],
         [-1.102e-05],
         [ 1.034e-05],
         [ 7.024e-05]],

        [[ 1.011e-04],
         [ 7.533e-05],
         [-4.820e-05],
         [ 6.435e-06],
         [-1.993e-05],
         [ 2.191e-05],
         [ 1.308e-05],
         [ 3.844e-05],
         [-3.029e-05],
         [-2.596e-05]],

        [[-4.564e-06],
         [-2.710e-06],
         [ 8.259e-05],
         [ 4.790e-05],
         [-1.175e-05],
         [-6.512e-05],
         [-8.293e-05],
         [ 4.045e-06],
         [-3.985e-06],
         [-2.181e-06]],

        [[-2.038e-05],
         [ 1.922e-06],
         [-2.832e-05],
         [-1.309e-06],
         [-6.656e-06],
         [ 9.649e-06],
         [-3.593e-05],
         [ 6.761e-06],
         [ 8.754e-06],
         [ 3.062e-05]],

        [[-7.085e-06],
         [-1.821e-05],
         [-5.310e-06],
         [ 5.681e-06],
         [-2.580e-05],
         [ 1.901e-05],
         [-1.712e-04],
         [-2.770e-05],
         [-1.092e-05],
         [-1.320e-05]],

        [[-4.540e-05],
         [-5.871e-06],
         [-5.229e-05],
         [-4.413e-05],
         [-5.049e-05],
         [ 2.656e-05],
         [-6.271e-06],
         [-3.483e-05],
         [-1.888e-04],
         [ 8.503e-06]],

        [[ 9.402e-05],
         [ 7.318e-05],
         [-2.106e-05],
         [-3.376e-05],
         [ 3.017e-05],
         [ 7.652e-05],
         [-8.172e-06],
         [-3.313e-05],
         [-9.629e-05],
         [ 3.388e-06]],

        [[ 1.275e-05],
         [ 4.224e-05],
         [-4.064e-06],
         [ 1.679e-04],
         [ 1.785e-05],
         [-2.599e-05],
         [-2.307e-05],
         [-3.484e-05],
         [ 1.107e-05],
         [-4.481e-05]],

        [[-2.568e-05],
         [ 8.278e-05],
         [ 5.224e-05],
         [-6.736e-05],
         [-4.891e-05],
         [ 1.433e-04],
         [ 1.955e-05],
         [-1.258e-05],
         [-1.729e-06],
         [-1.487e-05]],

        [[ 2.315e-05],
         [ 3.988e-05],
         [ 1.217e-05],
         [-6.512e-06],
         [ 1.643e-05],
         [-4.677e-05],
         [-8.213e-07],
         [-6.446e-05],
         [-2.903e-05],
         [-1.209e-04]],

        [[-8.591e-06],
         [-1.814e-05],
         [ 1.297e-05],
         [ 4.236e-05],
         [-1.527e-05],
         [ 9.315e-06],
         [ 1.326e-05],
         [-4.081e-06],
         [-1.639e-04],
         [ 1.157e-06]],

        [[-2.257e-06],
         [-2.325e-05],
         [-1.709e-04],
         [-5.160e-05],
         [-8.714e-06],
         [-9.131e-06],
         [ 6.181e-05],
         [-6.580e-05],
         [-3.172e-05],
         [-9.063e-05]],

        [[-2.184e-05],
         [-5.798e-05],
         [ 1.704e-05],
         [ 7.644e-06],
         [ 1.112e-04],
         [-9.902e-06],
         [ 7.811e-05],
         [-9.381e-06],
         [ 2.643e-06],
         [ 5.344e-05]],

        [[ 2.209e-05],
         [-1.439e-05],
         [ 4.520e-06],
         [ 2.073e-05],
         [ 1.135e-05],
         [ 6.750e-07],
         [-1.124e-05],
         [-4.899e-05],
         [-1.033e-04],
         [ 7.300e-05]],

        [[ 1.241e-05],
         [ 4.366e-05],
         [ 1.179e-05],
         [-3.317e-06],
         [-1.355e-05],
         [ 5.380e-05],
         [ 1.930e-05],
         [-1.890e-06],
         [-1.400e-05],
         [ 4.978e-05]]], device='cuda:0')
s after update for 1 param tensor([[[1.062e-02],
         [1.499e-02],
         [2.225e-03],
         [1.440e-03],
         [1.406e-03],
         [1.507e-03],
         [4.283e-03],
         [8.982e-03],
         [3.364e-03],
         [5.273e-03]],

        [[1.796e-04],
         [4.597e-04],
         [1.578e-05],
         [2.328e-04],
         [1.194e-03],
         [7.002e-03],
         [9.349e-04],
         [1.184e-04],
         [3.011e-04],
         [1.257e-03]],

        [[7.485e-03],
         [6.578e-03],
         [2.071e-04],
         [2.773e-04],
         [6.508e-03],
         [1.228e-02],
         [6.910e-03],
         [1.994e-03],
         [5.214e-04],
         [3.705e-05]],

        [[3.439e-04],
         [5.516e-05],
         [4.324e-02],
         [8.636e-05],
         [4.605e-04],
         [3.416e-03],
         [6.192e-04],
         [8.131e-04],
         [1.020e-03],
         [1.121e-04]],

        [[5.136e-04],
         [6.293e-02],
         [2.880e-02],
         [5.295e-04],
         [4.857e-04],
         [1.850e-03],
         [1.310e-03],
         [4.780e-05],
         [1.254e-02],
         [7.344e-04]],

        [[7.660e-03],
         [5.674e-02],
         [1.735e-03],
         [2.077e-04],
         [4.253e-03],
         [1.744e-03],
         [2.280e-04],
         [2.682e-04],
         [1.005e-04],
         [1.580e-02]],

        [[5.476e-03],
         [1.769e-02],
         [1.489e-03],
         [2.154e-04],
         [2.262e-03],
         [2.748e-03],
         [1.480e-04],
         [9.497e-04],
         [5.418e-03],
         [1.776e-03]],

        [[1.300e-03],
         [5.198e-03],
         [2.064e-02],
         [1.751e-03],
         [3.166e-04],
         [1.846e-02],
         [1.600e-02],
         [2.115e-04],
         [9.521e-05],
         [6.749e-05]],

        [[3.829e-04],
         [1.423e-04],
         [7.517e-04],
         [7.748e-05],
         [1.032e-03],
         [3.332e-04],
         [1.148e-03],
         [3.672e-04],
         [1.058e-03],
         [1.040e-02]],

        [[5.170e-03],
         [1.660e-03],
         [3.185e-04],
         [4.392e-04],
         [1.323e-03],
         [1.263e-03],
         [3.074e-02],
         [2.048e-03],
         [7.560e-04],
         [2.098e-03]],

        [[1.178e-02],
         [1.506e-04],
         [1.130e-02],
         [1.700e-03],
         [9.161e-03],
         [1.478e-02],
         [1.179e-03],
         [2.887e-04],
         [4.262e-02],
         [2.530e-03]],

        [[2.715e-03],
         [1.375e-03],
         [9.815e-03],
         [6.949e-03],
         [2.596e-03],
         [1.147e-02],
         [3.284e-03],
         [5.285e-03],
         [9.166e-03],
         [5.487e-04]],

        [[1.222e-03],
         [3.473e-03],
         [2.412e-05],
         [6.451e-02],
         [4.756e-03],
         [4.799e-04],
         [1.355e-03],
         [5.771e-03],
         [3.255e-03],
         [5.409e-03]],

        [[4.395e-03],
         [4.363e-03],
         [1.129e-02],
         [3.249e-03],
         [2.394e-02],
         [2.367e-02],
         [1.053e-03],
         [1.775e-04],
         [6.620e-04],
         [8.884e-04]],

        [[2.813e-03],
         [1.428e-03],
         [1.622e-03],
         [1.347e-03],
         [5.474e-03],
         [2.516e-03],
         [1.550e-05],
         [1.570e-02],
         [4.323e-03],
         [1.853e-02]],

        [[1.301e-04],
         [1.122e-03],
         [3.005e-03],
         [1.555e-02],
         [2.331e-03],
         [4.989e-04],
         [2.982e-03],
         [1.408e-05],
         [1.660e-02],
         [5.224e-04]],

        [[4.742e-05],
         [7.094e-03],
         [1.352e-02],
         [2.036e-03],
         [1.551e-04],
         [2.663e-03],
         [5.542e-03],
         [1.400e-02],
         [8.216e-03],
         [2.587e-02]],

        [[5.975e-03],
         [2.841e-02],
         [5.571e-04],
         [3.167e-04],
         [3.293e-02],
         [2.253e-03],
         [1.247e-02],
         [8.234e-04],
         [5.142e-05],
         [2.830e-03]],

        [[6.474e-04],
         [1.127e-02],
         [7.352e-04],
         [3.013e-03],
         [5.460e-05],
         [4.899e-05],
         [2.456e-03],
         [7.653e-03],
         [2.592e-02],
         [1.187e-02]],

        [[8.070e-03],
         [2.290e-03],
         [1.796e-03],
         [1.206e-04],
         [1.212e-03],
         [2.885e-03],
         [4.796e-03],
         [4.209e-04],
         [8.535e-03],
         [1.534e-03]]], device='cuda:0')
b after update for 1 param tensor([[[0.775],
         [0.920],
         [0.355],
         [0.285],
         [0.282],
         [0.292],
         [0.492],
         [0.712],
         [0.436],
         [0.546]],

        [[0.101],
         [0.161],
         [0.030],
         [0.115],
         [0.260],
         [0.629],
         [0.230],
         [0.082],
         [0.130],
         [0.267]],

        [[0.650],
         [0.610],
         [0.108],
         [0.125],
         [0.606],
         [0.833],
         [0.625],
         [0.336],
         [0.172],
         [0.046]],

        [[0.139],
         [0.056],
         [1.563],
         [0.070],
         [0.161],
         [0.439],
         [0.187],
         [0.214],
         [0.240],
         [0.080]],

        [[0.170],
         [1.886],
         [1.276],
         [0.173],
         [0.166],
         [0.323],
         [0.272],
         [0.052],
         [0.842],
         [0.204]],

        [[0.658],
         [1.791],
         [0.313],
         [0.108],
         [0.490],
         [0.314],
         [0.114],
         [0.123],
         [0.075],
         [0.945]],

        [[0.556],
         [1.000],
         [0.290],
         [0.110],
         [0.358],
         [0.394],
         [0.091],
         [0.232],
         [0.553],
         [0.317]],

        [[0.271],
         [0.542],
         [1.080],
         [0.315],
         [0.134],
         [1.021],
         [0.951],
         [0.109],
         [0.073],
         [0.062]],

        [[0.147],
         [0.090],
         [0.206],
         [0.066],
         [0.241],
         [0.137],
         [0.255],
         [0.144],
         [0.245],
         [0.767]],

        [[0.541],
         [0.306],
         [0.134],
         [0.158],
         [0.273],
         [0.267],
         [1.318],
         [0.340],
         [0.207],
         [0.344]],

        [[0.816],
         [0.092],
         [0.799],
         [0.310],
         [0.720],
         [0.914],
         [0.258],
         [0.128],
         [1.552],
         [0.378]],

        [[0.392],
         [0.279],
         [0.745],
         [0.627],
         [0.383],
         [0.805],
         [0.431],
         [0.547],
         [0.720],
         [0.176]],

        [[0.263],
         [0.443],
         [0.037],
         [1.909],
         [0.518],
         [0.165],
         [0.277],
         [0.571],
         [0.429],
         [0.553]],

        [[0.498],
         [0.497],
         [0.799],
         [0.429],
         [1.163],
         [1.157],
         [0.244],
         [0.100],
         [0.193],
         [0.224]],

        [[0.399],
         [0.284],
         [0.303],
         [0.276],
         [0.556],
         [0.377],
         [0.030],
         [0.942],
         [0.494],
         [1.023]],

        [[0.086],
         [0.252],
         [0.412],
         [0.938],
         [0.363],
         [0.168],
         [0.411],
         [0.028],
         [0.969],
         [0.172]],

        [[0.052],
         [0.633],
         [0.874],
         [0.339],
         [0.094],
         [0.388],
         [0.560],
         [0.890],
         [0.681],
         [1.209]],

        [[0.581],
         [1.267],
         [0.177],
         [0.134],
         [1.364],
         [0.357],
         [0.839],
         [0.216],
         [0.054],
         [0.400]],

        [[0.191],
         [0.798],
         [0.204],
         [0.413],
         [0.056],
         [0.053],
         [0.373],
         [0.658],
         [1.210],
         [0.819]],

        [[0.675],
         [0.360],
         [0.319],
         [0.083],
         [0.262],
         [0.404],
         [0.521],
         [0.154],
         [0.695],
         [0.294]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 6.253e-06],
        [-9.868e-07],
        [ 2.747e-05],
        [ 3.401e-05],
        [ 4.728e-06],
        [-1.553e-05],
        [ 3.223e-05],
        [ 8.394e-06],
        [ 4.274e-05],
        [ 2.571e-05],
        [ 1.079e-04],
        [ 6.176e-05],
        [-3.868e-05],
        [ 3.435e-05],
        [-2.437e-05],
        [ 1.056e-05],
        [-3.564e-05],
        [-1.919e-05],
        [-7.063e-05],
        [ 2.752e-05]], device='cuda:0')
s after update for 1 param tensor([[0.001],
        [0.001],
        [0.000],
        [0.000],
        [0.000],
        [0.001],
        [0.001],
        [0.000],
        [0.005],
        [0.008],
        [0.041],
        [0.012],
        [0.003],
        [0.013],
        [0.000],
        [0.001],
        [0.011],
        [0.001],
        [0.010],
        [0.005]], device='cuda:0')
b after update for 1 param tensor([[0.170],
        [0.173],
        [0.156],
        [0.149],
        [0.088],
        [0.264],
        [0.236],
        [0.125],
        [0.541],
        [0.664],
        [1.529],
        [0.808],
        [0.394],
        [0.859],
        [0.151],
        [0.258],
        [0.801],
        [0.191],
        [0.737],
        [0.515]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 0.39955138885456737
exp ma of ||w||^2 26.940145756489034
||w|| 0.6321007742872709
exp ma of ||w|| 0.7301849750507158
||w||^2 0.11982654506114518
exp ma of ||w||^2 0.14301003180807745
||w|| 0.34615971033779364
exp ma of ||w|| 0.3521209854162481
||w||^2 0.04420702876832763
exp ma of ||w||^2 0.06817224777886853
||w|| 0.21025467597256342
exp ma of ||w|| 0.25657314460405156
cuda
Objective function 21.36 = squared loss an data 20.60 + 0.5*rho*h**2 0.243595 + alpha*h 0.071873 + L2reg 0.35 + L1reg 0.10 ; SHD = 48 ; DAG True
Proportion of microbatches that were clipped  0.7164203345785285
iteration 3 in inner loop, alpha 32.56224166529573 rho 100000.0 h 0.0022072375179646997
iteration 4 in outer loop, alpha = 253.2859934617657, rho = 100000.0, h = 0.0022072375179646997
cuda
4420
cuda
Objective function 21.85 = squared loss an data 20.60 + 0.5*rho*h**2 0.243595 + alpha*h 0.559062 + L2reg 0.35 + L1reg 0.10 ; SHD = 48 ; DAG True
||w||^2 18412180890.239105
exp ma of ||w||^2 4230879417.909629
||w|| 135691.4915911794
exp ma of ||w|| 33018.5064147714
||w||^2 187150484.9965475
exp ma of ||w||^2 224947967639.15668
||w|| 13680.295501068224
exp ma of ||w|| 167683.14221817488
||w||^2 8123107.329955463
exp ma of ||w||^2 8365992453.048585
||w|| 2850.1065471233637
exp ma of ||w|| 11663.41478349825
||w||^2 3.8159438902411034
exp ma of ||w||^2 139835.93421602188
||w|| 1.9534441098329647
exp ma of ||w|| 7.043462760620113
||w||^2 0.7552383838083337
exp ma of ||w||^2 823.6347867516888
||w|| 0.8690445234902143
exp ma of ||w|| 1.0239537356189607
||w||^2 0.45191991935780307
exp ma of ||w||^2 145.29558230312927
||w|| 0.6722498935349883
exp ma of ||w|| 0.8469718566537802
||w||^2 0.3049708877434362
exp ma of ||w||^2 9.378009600455666
||w|| 0.5522416932317191
exp ma of ||w|| 0.6625957104677811
||w||^2 0.09284374109574568
exp ma of ||w||^2 0.07478008890200535
||w|| 0.3047027093672547
exp ma of ||w|| 0.2699130011947392
||w||^2 0.0475090674282075
exp ma of ||w||^2 0.07261702697839485
||w|| 0.21796574829134852
exp ma of ||w|| 0.26565775273086584
cuda
Objective function 21.68 = squared loss an data 20.77 + 0.5*rho*h**2 0.099728 + alpha*h 0.357714 + L2reg 0.36 + L1reg 0.09 ; SHD = 52 ; DAG True
Proportion of microbatches that were clipped  0.7115137908915972
iteration 1 in inner loop, alpha 253.2859934617657 rho 100000.0 h 0.0014122913425538286
iteration 5 in outer loop, alpha = 1665.5773360155943, rho = 1000000.0, h = 0.0014122913425538286
Threshold 0.3
[[0.002 0.056 0.183 0.027 0.141 0.06  0.069 0.029 0.022 0.094 0.265 0.035
  0.234 0.007 0.031 0.068 0.052 0.196 0.144 0.051]
 [0.03  0.002 0.055 0.02  0.117 0.098 0.019 0.012 0.013 0.078 0.052 0.024
  0.101 0.054 0.077 0.053 0.011 0.128 0.054 0.027]
 [0.015 0.04  0.003 0.029 0.055 0.027 0.006 0.011 0.004 0.017 0.042 0.031
  0.087 0.003 0.029 0.072 0.033 0.041 0.036 0.018]
 [0.111 0.127 0.116 0.002 0.312 0.088 0.076 0.065 0.062 0.056 0.162 0.261
  0.202 0.042 0.044 0.076 0.194 0.126 0.077 0.052]
 [0.025 0.022 0.06  0.007 0.002 0.031 0.003 0.009 0.003 0.013 0.084 0.007
  0.114 0.022 0.028 0.07  0.022 0.03  0.059 0.017]
 [0.054 0.037 0.102 0.031 0.065 0.003 0.004 0.008 0.014 0.019 0.086 0.06
  0.143 0.007 0.006 0.054 0.034 0.056 0.205 0.023]
 [0.055 0.113 0.224 0.04  0.369 0.302 0.002 0.016 0.078 0.095 0.366 0.35
  0.335 0.065 0.024 0.23  0.738 0.091 0.095 0.094]
 [0.058 0.176 0.146 0.032 0.195 0.208 0.116 0.003 0.01  0.062 0.109 0.547
  0.524 0.069 0.14  0.201 0.168 0.973 0.05  0.293]
 [0.08  0.163 0.51  0.059 0.475 0.141 0.025 0.108 0.002 0.087 0.481 0.218
  0.37  0.085 0.11  0.226 0.205 0.317 0.448 0.191]
 [0.022 0.039 0.228 0.049 0.131 0.168 0.024 0.034 0.016 0.003 0.43  0.026
  0.129 0.015 0.05  0.266 0.06  0.143 0.085 0.043]
 [0.011 0.057 0.075 0.015 0.023 0.03  0.007 0.021 0.005 0.006 0.003 0.02
  0.047 0.016 0.036 0.042 0.027 0.047 0.012 0.031]
 [0.04  0.152 0.062 0.01  0.422 0.054 0.008 0.004 0.014 0.063 0.085 0.002
  0.293 0.042 0.043 0.178 0.075 0.267 0.035 0.017]
 [0.01  0.031 0.025 0.015 0.028 0.018 0.008 0.005 0.007 0.035 0.044 0.01
  0.002 0.008 0.003 0.053 0.021 0.081 0.06  0.014]
 [0.256 0.048 1.017 0.057 0.057 0.22  0.037 0.029 0.042 0.153 0.117 0.07
  0.204 0.002 0.064 0.177 0.085 0.163 0.293 0.158]
 [0.058 0.033 0.072 0.056 0.09  0.367 0.093 0.022 0.026 0.072 0.057 0.061
  0.665 0.048 0.003 0.077 0.134 0.18  0.093 0.183]
 [0.033 0.042 0.044 0.029 0.038 0.042 0.008 0.011 0.01  0.012 0.043 0.012
  0.044 0.011 0.027 0.003 0.038 0.079 0.008 0.05 ]
 [0.051 0.096 0.061 0.014 0.112 0.065 0.003 0.012 0.007 0.043 0.108 0.039
  0.089 0.025 0.016 0.067 0.003 0.067 0.021 0.021]
 [0.016 0.022 0.075 0.019 0.092 0.052 0.022 0.003 0.009 0.021 0.058 0.011
  0.036 0.015 0.017 0.034 0.035 0.003 0.037 0.01 ]
 [0.013 0.038 0.088 0.04  0.044 0.019 0.016 0.029 0.005 0.054 0.147 0.059
  0.035 0.007 0.019 0.334 0.094 0.039 0.002 0.022]
 [0.044 0.079 0.154 0.051 0.169 0.096 0.027 0.009 0.016 0.071 0.131 0.125
  0.174 0.019 0.017 0.06  0.136 0.288 0.071 0.003]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.312 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.369 0.302 0.    0.    0.    0.    0.366 0.35
  0.335 0.    0.    0.    0.738 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.547
  0.524 0.    0.    0.    0.    0.973 0.    0.   ]
 [0.    0.    0.51  0.    0.475 0.    0.    0.    0.    0.    0.481 0.
  0.37  0.    0.    0.    0.    0.317 0.448 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.43  0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.422 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.017 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.367 0.    0.    0.    0.    0.    0.
  0.665 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.334 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.5, 'tpr': 0.18333333333333332, 'fpr': 0.08461538461538462, 'f1': 0.26829268292682923, 'shd': 52, 'npred': 22, 'ntrue': 60}
[0.056 0.183 0.027 0.141 0.06  0.069 0.029 0.022 0.094 0.265 0.035 0.234
 0.007 0.031 0.068 0.052 0.196 0.144 0.051 0.03  0.055 0.02  0.117 0.098
 0.019 0.012 0.013 0.078 0.052 0.024 0.101 0.054 0.077 0.053 0.011 0.128
 0.054 0.027 0.015 0.04  0.029 0.055 0.027 0.006 0.011 0.004 0.017 0.042
 0.031 0.087 0.003 0.029 0.072 0.033 0.041 0.036 0.018 0.111 0.127 0.116
 0.312 0.088 0.076 0.065 0.062 0.056 0.162 0.261 0.202 0.042 0.044 0.076
 0.194 0.126 0.077 0.052 0.025 0.022 0.06  0.007 0.031 0.003 0.009 0.003
 0.013 0.084 0.007 0.114 0.022 0.028 0.07  0.022 0.03  0.059 0.017 0.054
 0.037 0.102 0.031 0.065 0.004 0.008 0.014 0.019 0.086 0.06  0.143 0.007
 0.006 0.054 0.034 0.056 0.205 0.023 0.055 0.113 0.224 0.04  0.369 0.302
 0.016 0.078 0.095 0.366 0.35  0.335 0.065 0.024 0.23  0.738 0.091 0.095
 0.094 0.058 0.176 0.146 0.032 0.195 0.208 0.116 0.01  0.062 0.109 0.547
 0.524 0.069 0.14  0.201 0.168 0.973 0.05  0.293 0.08  0.163 0.51  0.059
 0.475 0.141 0.025 0.108 0.087 0.481 0.218 0.37  0.085 0.11  0.226 0.205
 0.317 0.448 0.191 0.022 0.039 0.228 0.049 0.131 0.168 0.024 0.034 0.016
 0.43  0.026 0.129 0.015 0.05  0.266 0.06  0.143 0.085 0.043 0.011 0.057
 0.075 0.015 0.023 0.03  0.007 0.021 0.005 0.006 0.02  0.047 0.016 0.036
 0.042 0.027 0.047 0.012 0.031 0.04  0.152 0.062 0.01  0.422 0.054 0.008
 0.004 0.014 0.063 0.085 0.293 0.042 0.043 0.178 0.075 0.267 0.035 0.017
 0.01  0.031 0.025 0.015 0.028 0.018 0.008 0.005 0.007 0.035 0.044 0.01
 0.008 0.003 0.053 0.021 0.081 0.06  0.014 0.256 0.048 1.017 0.057 0.057
 0.22  0.037 0.029 0.042 0.153 0.117 0.07  0.204 0.064 0.177 0.085 0.163
 0.293 0.158 0.058 0.033 0.072 0.056 0.09  0.367 0.093 0.022 0.026 0.072
 0.057 0.061 0.665 0.048 0.077 0.134 0.18  0.093 0.183 0.033 0.042 0.044
 0.029 0.038 0.042 0.008 0.011 0.01  0.012 0.043 0.012 0.044 0.011 0.027
 0.038 0.079 0.008 0.05  0.051 0.096 0.061 0.014 0.112 0.065 0.003 0.012
 0.007 0.043 0.108 0.039 0.089 0.025 0.016 0.067 0.067 0.021 0.021 0.016
 0.022 0.075 0.019 0.092 0.052 0.022 0.003 0.009 0.021 0.058 0.011 0.036
 0.015 0.017 0.034 0.035 0.037 0.01  0.013 0.038 0.088 0.04  0.044 0.019
 0.016 0.029 0.005 0.054 0.147 0.059 0.035 0.007 0.019 0.334 0.094 0.039
 0.022 0.044 0.079 0.154 0.051 0.169 0.096 0.027 0.009 0.016 0.071 0.131
 0.125 0.174 0.019 0.017 0.06  0.136 0.288 0.071]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.5532291666666667, 0.3403093226854438)
Iterations 630
Achieves (6.682218196030888, 1e-05)-DP
