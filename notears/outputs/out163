samples  5000  graph  20 40 ER mlp  minibatch size  100  noise  0.8  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.5634617290166446
iteration 1 in outer loop, alpha = 1.5634617290166446, rho = 1.0, h = 1.5634617290166446
cuda
iteration 1 in inner loop,alpha 1.5634617290166446 rho 1.0 h 1.0341694562898596
iteration 2 in inner loop,alpha 1.5634617290166446 rho 10.0 h 0.44426713155511877
iteration 3 in inner loop,alpha 1.5634617290166446 rho 100.0 h 0.13684757114982915
iteration 2 in outer loop, alpha = 15.24821884399956, rho = 100.0, h = 0.13684757114982915
cuda
iteration 1 in inner loop,alpha 15.24821884399956 rho 100.0 h 0.07377334863559071
iteration 2 in inner loop,alpha 15.24821884399956 rho 1000.0 h 0.024461043258938275
iteration 3 in outer loop, alpha = 39.709262102937835, rho = 1000.0, h = 0.024461043258938275
cuda
iteration 1 in inner loop,alpha 39.709262102937835 rho 1000.0 h 0.00901476091947373
iteration 2 in inner loop,alpha 39.709262102937835 rho 10000.0 h 0.0027525605240832363
iteration 4 in outer loop, alpha = 67.23486734377019, rho = 10000.0, h = 0.0027525605240832363
cuda
iteration 1 in inner loop,alpha 67.23486734377019 rho 10000.0 h 0.001199817952031168
iteration 2 in inner loop,alpha 67.23486734377019 rho 100000.0 h 0.0004412393068378151
iteration 5 in outer loop, alpha = 111.3587980275517, rho = 100000.0, h = 0.0004412393068378151
cuda
iteration 1 in inner loop,alpha 111.3587980275517 rho 100000.0 h 0.0002572745372262375
iteration 6 in outer loop, alpha = 368.6333352537892, rho = 1000000.0, h = 0.0002572745372262375
Threshold 0.3
[[0.    0.011 0.044 2.314 0.035 0.03  0.042 0.003 1.992 0.015 1.858 0.004
  0.098 0.007 0.191 0.888 0.005 0.001 0.041 0.225]
 [0.035 0.006 0.22  0.057 0.389 0.105 0.029 0.002 3.047 0.048 0.015 0.01
  0.606 0.002 1.817 1.633 0.298 0.    0.212 0.306]
 [0.001 0.    0.003 0.072 0.126 1.248 0.001 0.    2.295 0.077 0.059 0.
  2.298 0.    1.417 0.832 0.001 0.    3.744 2.262]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.159 1.495 0.    0.    0.    0.001]
 [0.    0.    0.    1.699 0.001 0.198 0.001 0.    0.001 0.023 0.018 0.
  0.372 0.001 0.083 0.381 0.    0.    0.036 0.186]
 [0.    0.001 0.    0.131 0.002 0.001 0.    0.    0.001 0.027 0.068 0.
  0.238 0.    1.897 0.411 0.    0.    0.001 0.106]
 [0.    0.017 0.593 0.055 0.224 0.097 0.006 0.    0.036 0.003 0.015 0.
  0.037 0.    0.065 1.741 0.006 0.    0.062 0.027]
 [0.011 0.014 0.706 0.046 0.19  0.09  4.476 0.001 0.149 0.01  0.01  0.052
  0.01  0.006 0.125 0.326 0.057 0.005 0.065 0.055]
 [0.    0.    0.    0.063 1.012 0.112 0.    0.    0.003 0.044 0.044 0.
  0.294 0.    0.298 0.37  0.    0.    0.018 0.271]
 [0.001 0.    0.    0.096 0.    0.    0.    0.    0.    0.002 0.836 0.
  0.    0.    0.174 0.272 0.    0.    0.    0.196]
 [0.    0.    0.    0.098 0.001 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    1.274 0.249 0.    0.    0.    2.306]
 [0.006 0.012 2.796 0.069 0.025 0.233 3.617 0.003 0.22  0.018 0.017 0.001
  0.223 0.005 0.611 0.318 0.008 0.023 0.208 0.155]
 [0.001 0.    0.    0.127 0.    0.001 0.    0.    0.001 3.223 0.124 0.
  0.002 0.    0.365 0.459 0.    0.    0.    0.423]
 [0.01  0.009 0.019 0.052 0.032 0.129 0.006 0.007 0.096 0.033 0.077 0.027
  0.279 0.    0.176 0.165 0.006 0.019 2.63  0.235]
 [0.    0.001 0.    0.004 0.    0.    0.    0.    0.001 0.    0.    0.
  0.    0.    0.005 0.22  0.    0.    0.    0.004]
 [0.    0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.009 0.006 0.    0.    0.    0.   ]
 [0.009 0.006 0.352 0.065 0.361 0.196 0.03  0.003 0.273 0.044 0.057 0.012
  0.408 0.001 1.83  1.531 0.003 0.    0.335 0.452]
 [0.011 4.286 0.01  0.049 0.2   0.002 0.01  0.005 0.593 0.008 0.009 0.003
  1.393 0.009 0.132 0.181 2.963 0.001 1.425 1.012]
 [0.001 0.001 0.    0.069 0.015 1.246 0.    0.    0.083 0.049 0.017 0.
  1.039 0.    0.372 1.44  0.    0.    0.004 0.45 ]
 [0.    0.    0.    2.12  0.    0.002 0.    0.    0.    0.    0.    0.
  0.    0.    0.208 0.275 0.    0.    0.    0.003]]
[[0.    0.    0.    2.314 0.    0.    0.    0.    1.992 0.    1.858 0.
  0.    0.    0.    0.888 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.389 0.    0.    0.    3.047 0.    0.    0.
  0.606 0.    1.817 1.633 0.    0.    0.    0.306]
 [0.    0.    0.    0.    0.    1.248 0.    0.    2.295 0.    0.    0.
  2.298 0.    1.417 0.832 0.    0.    3.744 2.262]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.495 0.    0.    0.    0.   ]
 [0.    0.    0.    1.699 0.    0.    0.    0.    0.    0.    0.    0.
  0.372 0.    0.    0.381 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.897 0.411 0.    0.    0.    0.   ]
 [0.    0.    0.593 0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.741 0.    0.    0.    0.   ]
 [0.    0.    0.706 0.    0.    0.    4.476 0.    0.    0.    0.    0.
  0.    0.    0.    0.326 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.012 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.37  0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.836 0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.274 0.    0.    0.    0.    2.306]
 [0.    0.    2.796 0.    0.    0.    3.617 0.    0.    0.    0.    0.
  0.    0.    0.611 0.318 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.223 0.    0.
  0.    0.    0.365 0.459 0.    0.    0.    0.423]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    2.63  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.352 0.    0.361 0.    0.    0.    0.    0.    0.    0.
  0.408 0.    1.83  1.531 0.    0.    0.335 0.452]
 [0.    4.286 0.    0.    0.    0.    0.    0.    0.593 0.    0.    0.
  1.393 0.    0.    0.    2.963 0.    1.425 1.012]
 [0.    0.    0.    0.    0.    1.246 0.    0.    0.    0.    0.    0.
  1.039 0.    0.372 1.44  0.    0.    0.    0.45 ]
 [0.    0.    0.    2.12  0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]]
{'fdr': 0.3442622950819672, 'tpr': 1.0, 'fpr': 0.14, 'f1': 0.7920792079207921, 'shd': 21, 'npred': 61, 'ntrue': 40}
[1.054e-02 4.430e-02 2.314e+00 3.469e-02 3.021e-02 4.164e-02 3.097e-03
 1.992e+00 1.488e-02 1.858e+00 3.997e-03 9.825e-02 7.095e-03 1.913e-01
 8.885e-01 4.626e-03 8.706e-04 4.082e-02 2.253e-01 3.501e-02 2.200e-01
 5.674e-02 3.885e-01 1.054e-01 2.925e-02 2.381e-03 3.047e+00 4.759e-02
 1.454e-02 1.031e-02 6.062e-01 2.252e-03 1.817e+00 1.633e+00 2.978e-01
 3.809e-04 2.121e-01 3.060e-01 6.907e-04 3.918e-04 7.207e-02 1.262e-01
 1.248e+00 5.235e-04 6.631e-05 2.295e+00 7.748e-02 5.889e-02 2.442e-04
 2.298e+00 4.250e-04 1.417e+00 8.319e-01 5.444e-04 6.888e-05 3.744e+00
 2.262e+00 9.089e-06 1.934e-04 1.937e-04 5.681e-04 1.235e-03 1.164e-04
 3.343e-05 4.789e-04 1.654e-05 6.325e-05 8.421e-05 6.702e-05 1.476e-04
 1.593e-01 1.495e+00 2.116e-04 8.597e-05 2.802e-04 1.023e-03 5.710e-05
 1.550e-04 1.733e-04 1.699e+00 1.984e-01 6.832e-04 4.811e-05 7.450e-04
 2.324e-02 1.750e-02 5.439e-05 3.720e-01 6.725e-04 8.340e-02 3.814e-01
 3.549e-04 3.076e-05 3.565e-02 1.863e-01 2.590e-04 6.823e-04 1.597e-05
 1.312e-01 1.825e-03 9.024e-05 1.052e-05 5.182e-04 2.667e-02 6.823e-02
 3.005e-05 2.380e-01 3.128e-05 1.897e+00 4.109e-01 7.918e-05 1.760e-05
 7.356e-04 1.062e-01 1.928e-04 1.749e-02 5.929e-01 5.515e-02 2.239e-01
 9.672e-02 1.768e-04 3.577e-02 2.624e-03 1.508e-02 5.907e-05 3.658e-02
 4.274e-04 6.466e-02 1.741e+00 5.839e-03 3.056e-04 6.191e-02 2.749e-02
 1.122e-02 1.429e-02 7.064e-01 4.646e-02 1.902e-01 8.990e-02 4.476e+00
 1.490e-01 9.646e-03 9.601e-03 5.245e-02 9.983e-03 6.294e-03 1.252e-01
 3.262e-01 5.664e-02 5.018e-03 6.520e-02 5.472e-02 4.135e-05 2.613e-04
 3.868e-04 6.287e-02 1.012e+00 1.120e-01 3.925e-04 2.488e-05 4.364e-02
 4.447e-02 8.217e-05 2.937e-01 2.790e-04 2.978e-01 3.702e-01 3.067e-04
 3.311e-05 1.765e-02 2.715e-01 7.336e-04 7.391e-05 4.841e-05 9.636e-02
 8.186e-05 3.610e-04 1.083e-04 8.277e-06 1.930e-04 8.356e-01 2.085e-05
 6.535e-05 2.792e-05 1.745e-01 2.722e-01 7.330e-05 1.235e-05 3.973e-05
 1.957e-01 5.110e-05 2.331e-04 2.718e-05 9.751e-02 5.353e-04 4.824e-04
 2.518e-04 4.681e-06 2.279e-04 1.627e-04 5.762e-05 9.510e-05 4.674e-05
 1.274e+00 2.491e-01 2.150e-04 3.792e-05 4.023e-05 2.306e+00 5.675e-03
 1.241e-02 2.796e+00 6.906e-02 2.469e-02 2.330e-01 3.617e+00 3.431e-03
 2.197e-01 1.803e-02 1.738e-02 2.234e-01 5.074e-03 6.110e-01 3.182e-01
 7.751e-03 2.306e-02 2.081e-01 1.548e-01 8.841e-04 3.758e-05 1.605e-04
 1.272e-01 2.721e-04 1.428e-03 6.169e-05 3.029e-05 6.099e-04 3.223e+00
 1.242e-01 4.480e-05 6.426e-05 3.646e-01 4.594e-01 4.318e-04 3.221e-05
 2.719e-04 4.228e-01 9.754e-03 8.912e-03 1.928e-02 5.225e-02 3.215e-02
 1.287e-01 6.033e-03 6.564e-03 9.585e-02 3.342e-02 7.690e-02 2.731e-02
 2.787e-01 1.756e-01 1.655e-01 5.627e-03 1.850e-02 2.630e+00 2.347e-01
 2.407e-05 5.047e-04 9.595e-05 4.267e-03 4.472e-04 1.331e-04 1.796e-04
 1.099e-05 5.281e-04 1.407e-04 2.119e-04 2.993e-05 7.557e-05 2.395e-05
 2.198e-01 4.112e-04 7.951e-05 4.860e-05 4.459e-03 4.869e-05 3.398e-04
 4.915e-05 1.447e-04 4.160e-05 6.438e-04 3.809e-04 3.085e-05 5.267e-05
 3.219e-05 9.114e-05 1.099e-05 3.375e-05 2.213e-05 9.493e-03 8.043e-05
 3.154e-05 4.377e-05 1.072e-04 8.703e-03 6.124e-03 3.524e-01 6.450e-02
 3.610e-01 1.957e-01 3.014e-02 3.353e-03 2.731e-01 4.405e-02 5.658e-02
 1.221e-02 4.085e-01 6.649e-04 1.830e+00 1.531e+00 4.329e-04 3.353e-01
 4.523e-01 1.069e-02 4.286e+00 9.577e-03 4.905e-02 2.000e-01 2.473e-03
 9.598e-03 5.012e-03 5.927e-01 8.326e-03 9.284e-03 2.891e-03 1.393e+00
 8.613e-03 1.316e-01 1.813e-01 2.963e+00 1.425e+00 1.012e+00 6.089e-04
 9.616e-04 4.413e-04 6.857e-02 1.469e-02 1.246e+00 1.215e-04 3.020e-05
 8.255e-02 4.906e-02 1.696e-02 9.069e-05 1.039e+00 1.533e-04 3.719e-01
 1.440e+00 1.490e-04 1.733e-04 4.502e-01 1.019e-04 6.795e-05 1.807e-04
 2.120e+00 3.395e-04 1.502e-03 7.539e-05 2.569e-05 2.796e-04 3.362e-05
 5.500e-05 6.424e-05 3.372e-05 3.942e-05 2.084e-01 2.747e-01 9.083e-05
 1.196e-04 1.328e-04]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9992647058823529, 0.9935121936235677)
cuda
4420
cuda
Objective function 737.19 = squared loss an data 521.17 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 208 ; DAG False
||w||^2 0.20574305674567883
exp ma of ||w||^2 58.97345806788029
||w|| 0.4535890835830144
exp ma of ||w|| 0.5117581328845762
||w||^2 0.19754755141646121
exp ma of ||w||^2 0.21678094236586204
||w|| 0.444463217169274
exp ma of ||w|| 0.455077971571719
||w||^2 0.1066485282987398
exp ma of ||w||^2 0.2087062361307357
||w|| 0.32657086259912993
exp ma of ||w|| 0.4475731602247554
||w||^2 0.17525546401038897
exp ma of ||w||^2 0.17546246241933497
||w|| 0.4186352398095375
exp ma of ||w|| 0.4091135061036088
||w||^2 0.06309044446734131
exp ma of ||w||^2 0.17654659420240998
||w|| 0.2511781130340407
exp ma of ||w|| 0.41169017454187096
cuda
Objective function 171.94 = squared loss an data 169.64 + 0.5*rho*h**2 1.670469 + alpha*h 0.000000 + L2reg 0.36 + L1reg 0.26 ; SHD = 96 ; DAG False
Proportion of microbatches that were clipped  0.7269255251432208
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.8278235440341533
iteration 1 in outer loop, alpha = 1.8278235440341533, rho = 1.0, h = 1.8278235440341533
cuda
4420
cuda
Objective function 175.28 = squared loss an data 169.64 + 0.5*rho*h**2 1.670469 + alpha*h 3.340939 + L2reg 0.36 + L1reg 0.26 ; SHD = 96 ; DAG False
||w||^2 4.625828179699253
exp ma of ||w||^2 3045.975810299879
||w|| 2.1507738560107272
exp ma of ||w|| 11.205599260745515
||w||^2 2.3535871996871047
exp ma of ||w||^2 478.19731171682724
||w|| 1.5341405410480178
exp ma of ||w|| 3.3332118961237267
||w||^2 0.32770175353363196
exp ma of ||w||^2 0.361134256343508
||w|| 0.5724524028542739
exp ma of ||w|| 0.564867564379978
||w||^2 0.6821383120722906
exp ma of ||w||^2 0.4064754466233219
||w|| 0.8259166495914043
exp ma of ||w|| 0.604566967154295
||w||^2 0.5802852798533483
exp ma of ||w||^2 0.4978764920975691
||w|| 0.7617645829607388
exp ma of ||w|| 0.6713298594288231
cuda
Objective function 78.88 = squared loss an data 72.92 + 0.5*rho*h**2 1.667829 + alpha*h 3.338297 + L2reg 0.70 + L1reg 0.25 ; SHD = 76 ; DAG False
Proportion of microbatches that were clipped  0.7452156989944859
iteration 1 in inner loop, alpha 1.8278235440341533 rho 1.0 h 1.8263783409735979
4420
cuda
Objective function 93.89 = squared loss an data 72.92 + 0.5*rho*h**2 16.678289 + alpha*h 3.338297 + L2reg 0.70 + L1reg 0.25 ; SHD = 76 ; DAG False
||w||^2 582017.1621820414
exp ma of ||w||^2 14659991.770327382
||w|| 762.9004929753562
exp ma of ||w|| 2200.8488541962606
||w||^2 0.4932673545130479
exp ma of ||w||^2 1.3100193074288475
||w|| 0.7023299470427328
exp ma of ||w|| 1.0508568363978497
||w||^2 0.5499460005868637
exp ma of ||w||^2 1.229145576834294
||w|| 0.741583441419011
exp ma of ||w|| 1.0695577763696797
||w||^2 0.6669801442231722
exp ma of ||w||^2 1.2876892795014974
||w|| 0.8166885233815718
exp ma of ||w|| 1.091383471065653
||w||^2 1.3045335888036587
exp ma of ||w||^2 1.27822705810492
||w|| 1.142161805001226
exp ma of ||w|| 1.0778297541789268
cuda
Objective function 52.10 = squared loss an data 44.56 + 0.5*rho*h**2 4.657517 + alpha*h 1.764113 + L2reg 0.89 + L1reg 0.22 ; SHD = 53 ; DAG False
Proportion of microbatches that were clipped  0.768493796136328
iteration 2 in inner loop, alpha 1.8278235440341533 rho 10.0 h 0.9651442414210223
4420
cuda
Objective function 94.01 = squared loss an data 44.56 + 0.5*rho*h**2 46.575170 + alpha*h 1.764113 + L2reg 0.89 + L1reg 0.22 ; SHD = 53 ; DAG False
||w||^2 7225499808.010138
exp ma of ||w||^2 22440146446.597782
||w|| 85002.93999627388
exp ma of ||w|| 130056.67403617494
||w||^2 1.8534763499233307
exp ma of ||w||^2 2.5557792703742948
||w|| 1.3614243827415942
exp ma of ||w|| 1.533716835137864
||w||^2 1.5213497812281382
exp ma of ||w||^2 2.617638339495016
||w|| 1.23343008769372
exp ma of ||w|| 1.5564740145826415
||w||^2 1.5184782956372158
exp ma of ||w||^2 2.3129676902637883
||w|| 1.2322655134496039
exp ma of ||w|| 1.4574722702648948
cuda
Objective function 48.58 = squared loss an data 41.24 + 0.5*rho*h**2 5.490832 + alpha*h 0.605715 + L2reg 1.05 + L1reg 0.18 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.780190595219497
iteration 3 in inner loop, alpha 1.8278235440341533 rho 100.0 h 0.3313859524230267
iteration 2 in outer loop, alpha = 34.966418786336824, rho = 100.0, h = 0.3313859524230267
cuda
4420
cuda
Objective function 59.56 = squared loss an data 41.24 + 0.5*rho*h**2 5.490832 + alpha*h 11.587380 + L2reg 1.05 + L1reg 0.18 ; SHD = 43 ; DAG True
||w||^2 22410425703.188995
exp ma of ||w||^2 43967587992.58522
||w|| 149701.12124893718
exp ma of ||w|| 182563.7332873688
||w||^2 46292283473.275116
exp ma of ||w||^2 42242599061.611115
||w|| 215156.41629585467
exp ma of ||w|| 179443.36618141396
||w||^2 166664225.99704498
exp ma of ||w||^2 2845573480.924069
||w|| 12909.849960284007
exp ma of ||w|| 40912.16125026646
||w||^2 623.220902469249
exp ma of ||w||^2 173391.36371305096
||w|| 24.964392691777004
exp ma of ||w|| 90.15810321548746
||w||^2 412.05535292656845
exp ma of ||w||^2 13834.842322615674
||w|| 20.299146605869137
exp ma of ||w|| 18.905555174810733
||w||^2 3.2574886488066888
exp ma of ||w||^2 26.97570562497274
||w|| 1.8048514201470127
exp ma of ||w|| 2.1154199496097976
||w||^2 4.78031157151406
exp ma of ||w||^2 7.038912622028767
||w|| 2.1863923644931758
exp ma of ||w|| 1.8669182414500811
||w||^2 3.790626893137487
exp ma of ||w||^2 3.107173062578753
||w|| 1.9469532334233113
exp ma of ||w|| 1.68282680304828
||w||^2 1.2351807516408668
exp ma of ||w||^2 2.929935510297252
||w|| 1.111386859577198
exp ma of ||w|| 1.653317989593561
||w||^2 1.6340579274505738
exp ma of ||w||^2 2.8476575643436206
||w|| 1.2783027526570432
exp ma of ||w|| 1.6315034999860998
cuda
Objective function 51.37 = squared loss an data 39.64 + 0.5*rho*h**2 2.512991 + alpha*h 7.839018 + L2reg 1.19 + L1reg 0.18 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.7783580871552146
iteration 1 in inner loop, alpha 34.966418786336824 rho 100.0 h 0.22418704179324322
4420
cuda
Objective function 73.99 = squared loss an data 39.64 + 0.5*rho*h**2 25.129915 + alpha*h 7.839018 + L2reg 1.19 + L1reg 0.18 ; SHD = 36 ; DAG True
||w||^2 1981322970.3110802
exp ma of ||w||^2 7289064345.041042
||w|| 44512.054213561976
exp ma of ||w|| 68578.29409132974
||w||^2 581.8845568726584
exp ma of ||w||^2 250581.44615875496
||w|| 24.12228340917705
exp ma of ||w|| 119.18750036887138
||w||^2 6.753281839869969
exp ma of ||w||^2 34.8667888597046
||w|| 2.598707724979854
exp ma of ||w|| 2.6170943697781373
cuda
Objective function 48.35 = squared loss an data 39.96 + 0.5*rho*h**2 3.869190 + alpha*h 3.075928 + L2reg 1.29 + L1reg 0.16 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.7871831659493066
iteration 2 in inner loop, alpha 34.966418786336824 rho 1000.0 h 0.08796806037787164
4420
cuda
Objective function 83.17 = squared loss an data 39.96 + 0.5*rho*h**2 38.691898 + alpha*h 3.075928 + L2reg 1.29 + L1reg 0.16 ; SHD = 40 ; DAG True
||w||^2 5465.647225135007
exp ma of ||w||^2 1762972.1039824183
||w|| 73.93001572524523
exp ma of ||w|| 395.2269370757039
||w||^2 3.425302006101646
exp ma of ||w||^2 3.7944120772034595
||w|| 1.85075714400935
exp ma of ||w|| 1.8916089990543352
||w||^2 7.0678976744940245
exp ma of ||w||^2 3.8201365187623315
||w|| 2.658551800227715
exp ma of ||w|| 1.896226360817973
||w||^2 2.9457840285966057
exp ma of ||w||^2 3.5024840500173386
||w|| 1.7163286481896776
exp ma of ||w|| 1.8203123202435443
||w||^2 6.067879767106228
exp ma of ||w||^2 3.6207259292750624
||w|| 2.4633066733775206
exp ma of ||w|| 1.8256674604544783
cuda
Objective function 47.17 = squared loss an data 39.87 + 0.5*rho*h**2 4.695667 + alpha*h 1.071556 + L2reg 1.38 + L1reg 0.15 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.7866946552559341
iteration 3 in inner loop, alpha 34.966418786336824 rho 10000.0 h 0.030645283581630878
iteration 3 in outer loop, alpha = 341.4192546026456, rho = 10000.0, h = 0.030645283581630878
cuda
4420
cuda
Objective function 56.56 = squared loss an data 39.87 + 0.5*rho*h**2 4.695667 + alpha*h 10.462890 + L2reg 1.38 + L1reg 0.15 ; SHD = 39 ; DAG True
||w||^2 1316787506977.0576
exp ma of ||w||^2 349389624049.8425
||w|| 1147513.6195170223
exp ma of ||w|| 289206.3530244378
||w||^2 42310766502.631676
exp ma of ||w||^2 217853501238.95566
||w|| 205695.81061030793
exp ma of ||w|| 387595.35794209945
||w||^2 1.6638350253527303
exp ma of ||w||^2 4.275760012419617
||w|| 1.289897292559656
exp ma of ||w|| 2.0190321559114333
||w||^2 2.5299239277877765
exp ma of ||w||^2 4.0596065811198
||w|| 1.5905734587838993
exp ma of ||w|| 1.9711486638560947
||w||^2 6.559021181599913
exp ma of ||w||^2 3.844351945028728
||w|| 2.561058605655074
exp ma of ||w|| 1.9180548800320245
cuda
Objective function 47.48 = squared loss an data 38.41 + 0.5*rho*h**2 1.510584 + alpha*h 5.934382 + L2reg 1.48 + L1reg 0.15 ; SHD = 40 ; DAG True
Proportion of microbatches that were clipped  0.7855437032319694
iteration 1 in inner loop, alpha 341.4192546026456 rho 10000.0 h 0.017381509098559178
4420
cuda
Objective function 61.08 = squared loss an data 38.41 + 0.5*rho*h**2 15.105843 + alpha*h 5.934382 + L2reg 1.48 + L1reg 0.15 ; SHD = 40 ; DAG True
||w||^2 2839863444513.907
exp ma of ||w||^2 460422965987.46375
||w|| 1685189.4387616804
exp ma of ||w|| 274533.70978155074
||w||^2 11975746118.562641
exp ma of ||w||^2 98227367784.11325
||w|| 109433.75219082384
exp ma of ||w|| 137860.6539993175
v before min max tensor([[-7.804e-03, -1.052e-05, -8.406e-04,  ..., -3.817e-05,  2.702e-03,
         -4.945e-05],
        [ 1.575e-02, -1.990e-04, -1.421e-04,  ...,  1.044e-05, -2.883e-05,
          6.889e-04],
        [ 2.278e-02, -9.893e-04, -1.344e-05,  ...,  2.180e-05,  4.932e-05,
         -2.103e-05],
        ...,
        [ 3.584e-05, -1.278e-04, -2.896e-03,  ...,  1.927e-03,  1.259e-03,
         -8.418e-03],
        [-7.791e-05, -3.040e-03, -2.058e-04,  ...,  5.763e-04,  2.705e-03,
          8.152e-03],
        [ 4.512e-04, -6.911e-04, -3.730e-04,  ..., -2.314e-03, -4.503e-03,
          1.699e-02]], device='cuda:0')
v tensor([[1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 2.702e-03,
         1.000e-12],
        [1.575e-02, 1.000e-12, 1.000e-12,  ..., 1.044e-05, 1.000e-12,
         6.889e-04],
        [2.278e-02, 1.000e-12, 1.000e-12,  ..., 2.180e-05, 4.932e-05,
         1.000e-12],
        ...,
        [3.584e-05, 1.000e-12, 1.000e-12,  ..., 1.927e-03, 1.259e-03,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 5.763e-04, 2.705e-03,
         8.152e-03],
        [4.512e-04, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         1.699e-02]], device='cuda:0')
v before min max tensor([-1.259e-06, -4.296e-06, -8.183e-05, -4.023e-04, -3.644e-05, -4.731e-06,
        -6.596e-05,  4.723e-05,  3.811e-04,  5.758e-03, -1.115e-05, -4.442e-06,
         1.328e-04, -4.908e-04, -4.564e-05, -7.073e-05, -1.912e-04, -2.789e-04,
         1.227e-04,  1.567e-06, -3.576e-03, -2.081e-04,  1.126e-03,  8.178e-05,
         2.850e-06, -2.373e-05, -3.519e-04, -9.802e-05, -5.878e-04, -8.860e-05,
        -5.934e-04,  4.884e-05, -1.345e-05, -6.562e-05,  5.932e-05, -9.432e-06,
        -3.659e-05, -9.015e-06, -5.758e-06, -1.989e-06,  1.785e-05, -6.000e-03,
        -1.252e-04, -1.960e-06, -3.316e-04,  3.519e-04, -5.764e-06,  3.430e-06,
        -1.045e-04, -1.193e-04, -1.587e-04, -5.613e-04, -3.359e-06,  2.907e-06,
        -7.628e-05, -4.057e-05,  3.405e-04, -3.035e-06, -1.845e-04, -1.409e-03,
         2.157e-04, -4.834e-04,  9.597e-04, -8.409e-04, -1.000e-05, -3.455e-04,
        -3.557e-04,  6.925e-04,  4.411e-05,  1.938e-03, -2.044e-06, -2.984e-05,
         3.914e-04,  1.779e-04,  2.813e-03, -1.497e-03, -1.958e-06, -6.097e-05,
        -2.290e-04, -3.593e-05, -3.793e-03,  4.098e-04, -7.568e-06, -7.099e-05,
        -7.353e-05, -3.947e-04, -4.364e-04, -1.156e-06,  4.634e-04, -2.244e-05,
        -2.127e-06,  8.137e-05, -1.011e-04, -5.626e-06, -8.378e-05,  3.176e-03,
         7.273e-04, -1.911e-05, -1.217e-04, -7.614e-05, -1.858e-04, -2.090e-05,
        -1.180e-04,  1.246e-03, -6.092e-06, -4.324e-05, -2.406e-04,  2.699e-04,
        -1.069e-03,  5.334e-07,  1.639e-04,  5.956e-04, -2.507e-05,  1.024e-05,
        -2.540e-04, -3.608e-06, -7.406e-06, -5.999e-06, -1.019e-04,  1.486e-03,
         7.996e-04, -4.200e-04, -9.045e-04, -1.820e-05, -2.514e-06, -1.264e-04,
        -7.368e-04, -2.190e-04, -6.705e-03,  2.195e-04, -1.433e-03, -7.575e-05,
        -7.272e-05,  5.590e-05,  1.704e-04, -6.388e-06, -9.037e-06, -4.362e-04,
        -1.459e-04, -4.445e-05,  3.047e-05, -2.264e-05, -4.764e-04,  1.043e-05,
        -1.103e-03, -4.610e-06, -9.506e-05, -8.388e-04, -2.795e-04, -2.239e-04,
        -5.977e-03, -3.196e-04, -1.249e-04, -9.742e-06, -6.761e-05, -4.558e-05,
         4.658e-03,  1.458e-04, -6.756e-05,  4.629e-04, -9.493e-06,  5.164e-05,
        -7.442e-05, -4.980e-04, -1.499e-04, -9.500e-06, -6.016e-04,  9.472e-04,
        -4.744e-04, -7.702e-07, -5.687e-06,  2.719e-04, -1.253e-03, -1.709e-04,
        -3.271e-04, -4.530e-07, -6.208e-04, -4.674e-04, -1.425e-05, -1.418e-04,
        -1.355e-04,  7.157e-05,  3.089e-04,  1.543e-06, -1.963e-06, -4.261e-05,
        -2.230e-04,  6.618e-03, -1.397e-04, -2.365e-05,  4.533e-03, -4.144e-04,
         2.025e-03,  2.717e-05,  1.215e-03,  3.137e-04, -7.749e-06,  7.253e-04,
        -5.184e-04, -1.362e-03], device='cuda:0')
v tensor([1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 4.723e-05, 3.811e-04, 5.758e-03, 1.000e-12, 1.000e-12,
        1.328e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.227e-04, 1.567e-06, 1.000e-12, 1.000e-12, 1.126e-03, 8.178e-05,
        2.850e-06, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 4.884e-05, 1.000e-12, 1.000e-12, 5.932e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.785e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 3.519e-04, 1.000e-12, 3.430e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.907e-06,
        1.000e-12, 1.000e-12, 3.405e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        2.157e-04, 1.000e-12, 9.597e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 6.925e-04, 4.411e-05, 1.938e-03, 1.000e-12, 1.000e-12,
        3.914e-04, 1.779e-04, 2.813e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 4.098e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 4.634e-04, 1.000e-12,
        1.000e-12, 8.137e-05, 1.000e-12, 1.000e-12, 1.000e-12, 3.176e-03,
        7.273e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.246e-03, 1.000e-12, 1.000e-12, 1.000e-12, 2.699e-04,
        1.000e-12, 5.334e-07, 1.639e-04, 5.956e-04, 1.000e-12, 1.024e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.486e-03,
        7.996e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 2.195e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 5.590e-05, 1.704e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 3.047e-05, 1.000e-12, 1.000e-12, 1.043e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        4.658e-03, 1.458e-04, 1.000e-12, 4.629e-04, 1.000e-12, 5.164e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 9.472e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 2.719e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 7.157e-05, 3.089e-04, 1.543e-06, 1.000e-12, 1.000e-12,
        1.000e-12, 6.618e-03, 1.000e-12, 1.000e-12, 4.533e-03, 1.000e-12,
        2.025e-03, 2.717e-05, 1.215e-03, 3.137e-04, 1.000e-12, 7.253e-04,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 1.509e-04],
         [ 2.399e-03],
         [ 1.715e-06],
         [-3.041e-04],
         [-6.352e-05],
         [ 1.798e-06],
         [ 1.735e-05],
         [ 4.599e-05],
         [-7.103e-04],
         [ 4.739e-06]],

        [[-1.022e-05],
         [-3.951e-05],
         [-5.226e-05],
         [-1.180e-05],
         [-1.868e-05],
         [-4.801e-04],
         [-8.933e-06],
         [-5.474e-06],
         [-4.978e-05],
         [-6.905e-05]],

        [[-1.491e-03],
         [ 3.320e-05],
         [-2.493e-05],
         [-7.255e-05],
         [-1.199e-03],
         [ 8.789e-05],
         [ 8.001e-04],
         [ 6.492e-04],
         [-1.282e-05],
         [-3.009e-04]],

        [[-4.360e-05],
         [ 5.473e-06],
         [ 2.743e-03],
         [-1.901e-04],
         [-3.915e-05],
         [-6.388e-05],
         [-2.632e-05],
         [-8.449e-07],
         [-4.355e-06],
         [-2.199e-06]],

        [[-2.732e-04],
         [ 3.559e-03],
         [ 8.429e-05],
         [-1.409e-05],
         [-3.914e-05],
         [-2.947e-04],
         [-1.867e-04],
         [-7.055e-06],
         [ 2.027e-04],
         [-1.241e-04]],

        [[-1.017e-03],
         [ 3.675e-03],
         [ 1.825e-06],
         [-2.357e-05],
         [-6.747e-04],
         [-9.451e-05],
         [ 1.941e-06],
         [-2.227e-05],
         [-3.545e-05],
         [ 3.033e-04]],

        [[-2.878e-04],
         [ 1.516e-03],
         [-6.465e-07],
         [-8.675e-06],
         [-6.867e-04],
         [-2.322e-04],
         [-7.722e-06],
         [-2.187e-04],
         [-5.636e-04],
         [-1.083e-04]],

        [[-3.083e-04],
         [-1.442e-03],
         [ 8.711e-04],
         [-1.921e-04],
         [ 3.683e-05],
         [ 1.343e-03],
         [-4.374e-03],
         [-3.055e-06],
         [-1.532e-05],
         [-3.964e-05]],

        [[-1.326e-05],
         [-3.508e-05],
         [-1.105e-04],
         [-3.286e-05],
         [-7.096e-06],
         [-1.725e-06],
         [ 7.519e-06],
         [-1.132e-04],
         [-3.346e-05],
         [ 2.017e-04]],

        [[-4.312e-04],
         [-3.540e-04],
         [-7.406e-06],
         [-5.304e-06],
         [-1.967e-04],
         [-8.442e-04],
         [ 1.954e-03],
         [-3.041e-04],
         [-3.807e-04],
         [-3.162e-04]],

        [[ 1.500e-04],
         [-7.668e-07],
         [ 4.389e-05],
         [-1.596e-04],
         [-5.859e-04],
         [-2.472e-03],
         [-1.787e-05],
         [-1.050e-05],
         [ 6.676e-04],
         [-2.333e-04]],

        [[-9.988e-04],
         [-2.475e-04],
         [-1.054e-03],
         [ 2.146e-05],
         [-3.501e-04],
         [-3.072e-03],
         [-6.135e-04],
         [ 1.123e-05],
         [-1.816e-03],
         [-3.214e-05]],

        [[ 6.740e-06],
         [-8.840e-04],
         [ 5.139e-07],
         [ 1.693e-03],
         [ 6.006e-05],
         [-1.010e-06],
         [-5.136e-06],
         [ 4.433e-05],
         [ 1.372e-04],
         [-1.389e-04]],

        [[-2.149e-06],
         [-3.702e-04],
         [ 1.384e-04],
         [-3.830e-04],
         [-6.340e-03],
         [ 1.040e-03],
         [ 1.181e-05],
         [-4.384e-05],
         [-2.067e-06],
         [-2.826e-04]],

        [[-3.169e-04],
         [ 4.911e-06],
         [-1.850e-04],
         [-8.645e-05],
         [ 1.389e-04],
         [-4.306e-05],
         [ 3.231e-04],
         [ 3.588e-04],
         [ 5.409e-05],
         [ 6.356e-04]],

        [[-6.416e-05],
         [-7.606e-05],
         [ 1.516e-04],
         [ 3.879e-04],
         [ 1.035e-05],
         [-1.103e-05],
         [-1.024e-03],
         [ 2.837e-05],
         [-9.446e-04],
         [-8.715e-05]],

        [[-8.350e-07],
         [ 4.501e-05],
         [-1.246e-03],
         [-2.887e-04],
         [-6.212e-05],
         [-3.864e-06],
         [-1.131e-03],
         [ 1.249e-04],
         [ 4.139e-05],
         [ 2.906e-03]],

        [[ 8.641e-05],
         [ 6.271e-04],
         [-3.208e-05],
         [-4.806e-05],
         [ 1.570e-03],
         [-8.186e-04],
         [ 2.177e-04],
         [-2.428e-04],
         [-6.198e-07],
         [-1.573e-04]],

        [[-3.797e-04],
         [-2.038e-03],
         [-1.028e-03],
         [-2.574e-04],
         [-6.541e-06],
         [ 9.363e-07],
         [-2.341e-04],
         [ 9.034e-05],
         [ 1.314e-03],
         [ 2.203e-05]],

        [[-1.114e-03],
         [-1.498e-04],
         [-3.297e-05],
         [-2.207e-05],
         [-4.752e-05],
         [-6.911e-06],
         [ 8.774e-05],
         [-6.691e-04],
         [-2.024e-03],
         [-6.217e-06]]], device='cuda:0')
v tensor([[[1.509e-04],
         [2.399e-03],
         [1.715e-06],
         [1.000e-12],
         [1.000e-12],
         [1.798e-06],
         [1.735e-05],
         [4.599e-05],
         [1.000e-12],
         [4.739e-06]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [3.320e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.789e-05],
         [8.001e-04],
         [6.492e-04],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [5.473e-06],
         [2.743e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [3.559e-03],
         [8.429e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.027e-04],
         [1.000e-12]],

        [[1.000e-12],
         [3.675e-03],
         [1.825e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.941e-06],
         [1.000e-12],
         [1.000e-12],
         [3.033e-04]],

        [[1.000e-12],
         [1.516e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [8.711e-04],
         [1.000e-12],
         [3.683e-05],
         [1.343e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [7.519e-06],
         [1.000e-12],
         [1.000e-12],
         [2.017e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.954e-03],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.500e-04],
         [1.000e-12],
         [4.389e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.676e-04],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.146e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.123e-05],
         [1.000e-12],
         [1.000e-12]],

        [[6.740e-06],
         [1.000e-12],
         [5.139e-07],
         [1.693e-03],
         [6.006e-05],
         [1.000e-12],
         [1.000e-12],
         [4.433e-05],
         [1.372e-04],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.384e-04],
         [1.000e-12],
         [1.000e-12],
         [1.040e-03],
         [1.181e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [4.911e-06],
         [1.000e-12],
         [1.000e-12],
         [1.389e-04],
         [1.000e-12],
         [3.231e-04],
         [3.588e-04],
         [5.409e-05],
         [6.356e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.516e-04],
         [3.879e-04],
         [1.035e-05],
         [1.000e-12],
         [1.000e-12],
         [2.837e-05],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [4.501e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.249e-04],
         [4.139e-05],
         [2.906e-03]],

        [[8.641e-05],
         [6.271e-04],
         [1.000e-12],
         [1.000e-12],
         [1.570e-03],
         [1.000e-12],
         [2.177e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [9.363e-07],
         [1.000e-12],
         [9.034e-05],
         [1.314e-03],
         [2.203e-05]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [8.774e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]]], device='cuda:0')
v before min max tensor([[-6.282e-05],
        [-1.250e-04],
        [-2.659e-06],
        [-1.258e-05],
        [-8.744e-06],
        [-1.534e-05],
        [-1.433e-04],
        [-9.156e-07],
        [-9.130e-05],
        [-3.092e-04],
        [ 7.358e-04],
        [-2.435e-03],
        [-2.044e-05],
        [-2.314e-03],
        [-8.589e-05],
        [-1.472e-04],
        [ 1.497e-05],
        [-2.541e-05],
        [-3.480e-03],
        [-1.810e-04]], device='cuda:0')
v tensor([[1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [7.358e-04],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.497e-05],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-6.497e-05, -1.532e-07, -1.980e-05,  ...,  1.765e-06, -4.993e-05,
          2.758e-06],
        [ 1.089e-04,  6.909e-06,  3.886e-06,  ...,  3.964e-06, -1.729e-06,
         -3.619e-05],
        [-1.103e-04, -2.007e-05, -2.006e-06,  ..., -7.939e-06,  8.267e-06,
          2.061e-06],
        ...,
        [-2.149e-05, -1.321e-05, -4.696e-05,  ...,  6.924e-05, -3.966e-05,
         -7.364e-05],
        [ 1.196e-05, -2.365e-05,  2.070e-06,  ..., -2.780e-05,  5.070e-05,
          7.943e-05],
        [-3.671e-05,  3.122e-05, -1.969e-05,  ..., -2.084e-06, -4.112e-06,
         -1.202e-04]], device='cuda:0')
s after update for 1 param tensor([[1.398e-02, 2.232e-05, 1.543e-03,  ..., 1.036e-04, 1.647e-02,
         1.192e-04],
        [3.982e-02, 4.457e-04, 2.692e-04,  ..., 1.022e-03, 5.128e-05,
         8.332e-03],
        [4.881e-02, 2.044e-03, 4.613e-05,  ..., 1.477e-03, 2.224e-03,
         3.724e-05],
        ...,
        [2.295e-03, 2.564e-04, 5.962e-03,  ..., 1.418e-02, 1.139e-02,
         1.685e-02],
        [1.910e-04, 5.383e-03, 3.654e-04,  ..., 7.594e-03, 1.645e-02,
         2.908e-02],
        [7.592e-03, 4.614e-03, 7.442e-04,  ..., 4.434e-03, 8.649e-03,
         4.175e-02]], device='cuda:0')
b after update for 1 param tensor([[0.567, 0.023, 0.188,  ..., 0.049, 0.616, 0.052],
        [0.958, 0.101, 0.079,  ..., 0.153, 0.034, 0.438],
        [1.060, 0.217, 0.033,  ..., 0.184, 0.226, 0.029],
        ...,
        [0.230, 0.077, 0.371,  ..., 0.571, 0.512, 0.623],
        [0.066, 0.352, 0.092,  ..., 0.418, 0.615, 0.818],
        [0.418, 0.326, 0.131,  ..., 0.320, 0.446, 0.980]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 6.890e-07,  7.496e-06,  4.230e-06, -6.085e-07,  1.037e-05,  5.517e-08,
        -9.043e-07,  1.393e-05, -1.303e-05,  1.175e-04,  2.394e-06,  1.401e-06,
        -2.035e-05,  2.796e-06,  8.688e-07,  3.511e-07,  3.696e-06, -6.372e-06,
        -1.325e-05, -6.336e-06, -1.491e-05, -1.463e-05, -5.132e-05, -1.588e-05,
         2.222e-06, -8.263e-06,  1.051e-05, -1.458e-05,  8.321e-06, -4.651e-06,
        -2.248e-06, -1.790e-05,  5.188e-06,  1.256e-07, -7.034e-06, -1.189e-06,
        -8.239e-06, -7.448e-07, -5.474e-06, -5.307e-07, -1.304e-05, -5.490e-05,
         2.148e-06, -1.053e-06,  1.172e-05,  2.870e-05, -2.966e-06, -4.168e-06,
         1.008e-05,  5.410e-06, -2.169e-05,  6.281e-06,  1.564e-06, -3.227e-06,
        -3.963e-06, -7.753e-07,  2.286e-05,  2.521e-06, -1.350e-06,  4.878e-06,
         1.313e-05,  4.135e-06, -5.377e-05, -1.108e-06, -5.947e-06,  1.388e-05,
         3.731e-05, -3.810e-05,  2.273e-05,  5.470e-05,  5.214e-06, -3.541e-06,
         3.425e-05,  1.422e-05, -6.897e-05, -1.525e-05, -2.352e-07,  1.287e-06,
         3.585e-06, -3.874e-06,  4.496e-05, -4.328e-05, -6.883e-07, -7.727e-06,
        -2.568e-06,  1.127e-05,  2.441e-06, -1.132e-06,  2.092e-05, -8.303e-05,
        -5.629e-06,  1.035e-05, -1.266e-07, -3.157e-06,  9.864e-06, -5.771e-05,
         3.301e-05,  8.368e-06,  1.900e-05,  6.504e-07,  2.116e-05, -4.041e-06,
        -2.617e-06, -5.186e-05, -1.868e-06,  5.642e-06, -9.601e-06, -1.820e-05,
         3.557e-06, -9.559e-07, -1.771e-05, -2.255e-05, -1.160e-05, -3.907e-06,
        -1.015e-05,  2.190e-06,  4.513e-06,  5.898e-06, -1.723e-05, -4.540e-05,
         3.183e-05, -1.332e-05,  2.977e-05, -1.329e-06, -4.760e-07,  2.033e-05,
        -3.347e-06,  7.963e-06, -4.218e-05,  2.770e-05,  1.540e-05,  7.014e-06,
        -3.616e-05, -6.583e-06, -2.268e-05,  1.031e-06,  2.038e-06, -8.701e-07,
        -8.482e-07, -2.134e-06, -1.252e-05, -5.066e-07,  1.542e-05,  9.339e-06,
        -2.093e-05,  7.917e-06, -1.007e-05, -2.365e-05, -2.191e-06,  5.003e-06,
         4.005e-05,  1.222e-05,  3.007e-05, -1.146e-06,  4.846e-06, -1.370e-05,
        -5.706e-05, -1.700e-05,  4.678e-05, -2.003e-05, -2.978e-06,  6.426e-06,
        -1.418e-05,  7.595e-06, -2.348e-05, -9.721e-07, -2.232e-05,  5.498e-05,
        -2.654e-05, -1.118e-06,  2.947e-06,  3.621e-05,  7.605e-06,  1.158e-05,
        -1.291e-05, -1.458e-06,  1.002e-06,  1.430e-05, -9.504e-07, -1.825e-05,
         1.067e-05,  1.052e-05,  2.350e-05,  1.133e-06, -5.568e-06, -3.248e-05,
         7.032e-06, -1.171e-04, -2.458e-06,  1.898e-06,  7.601e-05, -3.567e-05,
         5.837e-05,  1.328e-05,  7.236e-05,  2.343e-05,  5.241e-06, -4.821e-05,
        -7.296e-05, -4.669e-05], device='cuda:0')
s after update for 1 param tensor([2.722e-06, 2.558e-04, 1.455e-04, 7.649e-04, 1.218e-04, 9.626e-06,
        1.874e-04, 2.190e-03, 6.281e-03, 2.471e-02, 3.351e-05, 7.917e-06,
        3.691e-03, 8.842e-04, 8.221e-05, 1.512e-04, 3.438e-04, 7.431e-04,
        3.504e-03, 3.989e-04, 9.307e-03, 5.249e-04, 1.080e-02, 2.877e-03,
        5.340e-04, 1.220e-04, 7.178e-04, 4.451e-04, 1.200e-03, 1.820e-04,
        1.171e-03, 2.239e-03, 4.113e-05, 1.171e-04, 2.437e-03, 1.921e-04,
        1.534e-04, 1.757e-05, 7.394e-05, 3.661e-06, 1.373e-03, 1.073e-02,
        2.221e-04, 9.576e-06, 5.876e-04, 6.219e-03, 1.433e-05, 5.857e-04,
        2.007e-04, 3.645e-04, 1.442e-03, 1.269e-03, 6.837e-06, 5.393e-04,
        1.447e-04, 7.287e-05, 7.106e-03, 2.159e-05, 3.601e-04, 2.514e-03,
        5.069e-03, 8.847e-04, 1.029e-02, 1.808e-03, 4.125e-05, 1.430e-03,
        1.143e-03, 8.729e-03, 2.177e-03, 1.397e-02, 1.705e-05, 9.112e-05,
        6.268e-03, 4.224e-03, 1.686e-02, 2.720e-03, 3.943e-06, 1.283e-04,
        4.885e-04, 1.074e-04, 6.739e-03, 6.674e-03, 1.386e-05, 1.419e-04,
        1.587e-04, 8.054e-04, 9.056e-04, 3.156e-06, 6.812e-03, 6.036e-03,
        3.420e-05, 2.854e-03, 1.919e-04, 5.982e-05, 2.387e-04, 1.837e-02,
        8.550e-03, 1.313e-04, 4.623e-04, 1.434e-04, 4.881e-04, 4.644e-05,
        2.091e-04, 1.127e-02, 1.080e-05, 1.061e-04, 5.326e-04, 5.202e-03,
        2.026e-03, 2.310e-04, 4.176e-03, 7.770e-03, 3.431e-04, 1.012e-03,
        5.446e-04, 7.326e-06, 3.640e-05, 4.559e-05, 4.051e-04, 1.233e-02,
        8.969e-03, 7.889e-04, 2.243e-03, 4.015e-05, 4.928e-06, 3.446e-04,
        1.472e-03, 3.954e-04, 1.378e-02, 4.949e-03, 4.576e-03, 1.833e-04,
        1.378e-03, 2.365e-03, 4.147e-03, 6.792e-05, 2.751e-05, 7.799e-04,
        3.045e-04, 9.413e-05, 1.749e-03, 7.360e-05, 1.254e-03, 1.027e-03,
        2.126e-03, 1.193e-04, 2.979e-04, 1.917e-03, 7.026e-04, 4.307e-04,
        1.059e-02, 5.968e-04, 6.013e-04, 1.754e-05, 1.491e-04, 3.206e-04,
        2.179e-02, 3.821e-03, 1.360e-03, 6.809e-03, 2.608e-05, 2.274e-03,
        1.180e-03, 9.158e-04, 5.503e-04, 2.031e-05, 1.617e-03, 9.822e-03,
        1.651e-03, 6.810e-06, 1.700e-05, 5.335e-03, 2.317e-03, 3.682e-04,
        5.905e-04, 3.014e-06, 1.126e-03, 1.087e-03, 2.526e-05, 1.140e-03,
        3.562e-04, 2.677e-03, 5.566e-03, 3.929e-04, 5.543e-05, 2.239e-03,
        4.211e-04, 2.605e-02, 3.254e-04, 4.418e-05, 2.300e-02, 1.162e-03,
        1.426e-02, 1.699e-03, 1.119e-02, 5.638e-03, 5.760e-05, 8.719e-03,
        6.484e-03, 3.162e-03], device='cuda:0')
b after update for 1 param tensor([0.008, 0.077, 0.058, 0.133, 0.053, 0.015, 0.066, 0.225, 0.380, 0.754,
        0.028, 0.014, 0.292, 0.143, 0.044, 0.059, 0.089, 0.131, 0.284, 0.096,
        0.463, 0.110, 0.499, 0.257, 0.111, 0.053, 0.129, 0.101, 0.166, 0.065,
        0.164, 0.227, 0.031, 0.052, 0.237, 0.067, 0.059, 0.020, 0.041, 0.009,
        0.178, 0.497, 0.072, 0.015, 0.116, 0.378, 0.018, 0.116, 0.068, 0.092,
        0.182, 0.171, 0.013, 0.111, 0.058, 0.041, 0.404, 0.022, 0.091, 0.241,
        0.342, 0.143, 0.487, 0.204, 0.031, 0.181, 0.162, 0.448, 0.224, 0.567,
        0.020, 0.046, 0.380, 0.312, 0.623, 0.250, 0.010, 0.054, 0.106, 0.050,
        0.394, 0.392, 0.018, 0.057, 0.060, 0.136, 0.144, 0.009, 0.396, 0.373,
        0.028, 0.256, 0.066, 0.037, 0.074, 0.650, 0.444, 0.055, 0.103, 0.057,
        0.106, 0.033, 0.069, 0.509, 0.016, 0.049, 0.111, 0.346, 0.216, 0.073,
        0.310, 0.423, 0.089, 0.153, 0.112, 0.013, 0.029, 0.032, 0.097, 0.533,
        0.454, 0.135, 0.227, 0.030, 0.011, 0.089, 0.184, 0.095, 0.563, 0.338,
        0.325, 0.065, 0.178, 0.233, 0.309, 0.040, 0.025, 0.134, 0.084, 0.047,
        0.201, 0.041, 0.170, 0.154, 0.221, 0.052, 0.083, 0.210, 0.127, 0.100,
        0.494, 0.117, 0.118, 0.020, 0.059, 0.086, 0.708, 0.297, 0.177, 0.396,
        0.025, 0.229, 0.165, 0.145, 0.113, 0.022, 0.193, 0.476, 0.195, 0.013,
        0.020, 0.350, 0.231, 0.092, 0.117, 0.008, 0.161, 0.158, 0.024, 0.162,
        0.091, 0.248, 0.358, 0.095, 0.036, 0.227, 0.098, 0.774, 0.087, 0.032,
        0.728, 0.164, 0.573, 0.198, 0.508, 0.360, 0.036, 0.448, 0.386, 0.270],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 3.661e-05],
         [-5.299e-05],
         [ 5.719e-06],
         [ 1.214e-05],
         [-3.488e-06],
         [ 3.630e-06],
         [ 1.652e-05],
         [ 7.651e-06],
         [-1.188e-05],
         [-2.013e-06]],

        [[ 5.010e-06],
         [ 6.250e-06],
         [ 5.227e-06],
         [-6.237e-06],
         [ 5.053e-06],
         [ 1.462e-05],
         [ 5.125e-07],
         [-2.511e-06],
         [ 4.694e-06],
         [ 1.961e-05]],

        [[ 2.444e-06],
         [-1.236e-05],
         [-7.067e-06],
         [-2.523e-05],
         [ 1.048e-05],
         [ 1.173e-05],
         [ 2.567e-05],
         [-6.691e-05],
         [ 1.909e-06],
         [-1.027e-05]],

        [[-2.369e-06],
         [ 2.623e-06],
         [ 5.603e-05],
         [ 1.963e-06],
         [ 4.785e-07],
         [ 9.711e-06],
         [ 3.047e-06],
         [-1.544e-07],
         [ 5.419e-07],
         [ 7.462e-07]],

        [[-1.314e-05],
         [ 6.042e-05],
         [ 8.984e-06],
         [-1.938e-06],
         [-1.759e-07],
         [-8.500e-06],
         [-1.638e-06],
         [ 4.120e-06],
         [-2.031e-05],
         [ 9.725e-06]],

        [[ 1.536e-05],
         [-8.468e-05],
         [ 3.572e-06],
         [ 4.221e-08],
         [-9.959e-06],
         [ 5.385e-06],
         [-9.059e-06],
         [-3.117e-06],
         [ 9.629e-06],
         [ 2.655e-05]],

        [[ 3.536e-05],
         [ 4.937e-05],
         [-1.736e-05],
         [ 1.525e-06],
         [-1.261e-05],
         [ 2.958e-06],
         [ 3.129e-06],
         [ 1.472e-05],
         [-1.645e-05],
         [-7.985e-07]],

        [[-2.272e-06],
         [-2.076e-06],
         [ 3.814e-05],
         [ 2.023e-05],
         [-1.184e-05],
         [-3.823e-05],
         [-4.149e-05],
         [ 4.952e-07],
         [-8.081e-07],
         [-7.659e-07]],

        [[-1.052e-05],
         [-3.951e-06],
         [-3.261e-05],
         [-8.314e-06],
         [-1.964e-06],
         [-1.047e-06],
         [-4.513e-06],
         [-2.009e-07],
         [-3.480e-06],
         [ 1.235e-05]],

        [[ 8.993e-07],
         [-1.017e-05],
         [ 1.693e-06],
         [ 4.658e-07],
         [-6.987e-06],
         [ 1.286e-05],
         [-8.481e-05],
         [-1.580e-05],
         [-1.841e-05],
         [-6.768e-06]],

        [[-1.527e-05],
         [ 8.977e-07],
         [-1.678e-05],
         [-1.634e-05],
         [-1.793e-05],
         [ 1.269e-05],
         [ 4.543e-06],
         [-3.955e-06],
         [-3.717e-05],
         [ 6.883e-06]],

        [[ 5.259e-05],
         [ 4.763e-05],
         [-7.397e-06],
         [-1.245e-05],
         [ 1.469e-05],
         [ 3.678e-05],
         [-2.831e-06],
         [-3.841e-06],
         [-4.078e-05],
         [ 1.465e-06]],

        [[ 3.874e-06],
         [ 1.731e-05],
         [-2.036e-06],
         [ 3.890e-05],
         [ 1.095e-05],
         [-1.298e-06],
         [-6.332e-07],
         [-1.278e-05],
         [ 8.859e-06],
         [-1.458e-05]],

        [[-1.126e-05],
         [ 3.116e-05],
         [ 1.596e-05],
         [-2.664e-05],
         [-2.196e-05],
         [ 6.142e-05],
         [ 7.182e-06],
         [-5.549e-06],
         [-4.083e-07],
         [-6.193e-06]],

        [[ 8.798e-06],
         [ 2.154e-06],
         [ 6.451e-06],
         [-8.878e-07],
         [ 9.710e-06],
         [-2.860e-05],
         [ 1.162e-05],
         [-2.668e-05],
         [-1.748e-05],
         [-6.469e-05]],

        [[-1.269e-05],
         [-7.412e-06],
         [ 1.711e-05],
         [ 1.521e-05],
         [-4.265e-06],
         [-1.340e-06],
         [ 6.580e-06],
         [-6.138e-06],
         [-4.708e-05],
         [-1.704e-06]],

        [[ 2.522e-06],
         [-5.814e-06],
         [-6.630e-05],
         [-1.894e-05],
         [ 1.799e-06],
         [ 6.868e-07],
         [ 3.194e-05],
         [-2.228e-05],
         [-7.220e-06],
         [-5.355e-05]],

        [[-9.651e-06],
         [-1.982e-05],
         [ 5.004e-06],
         [ 1.043e-06],
         [ 4.298e-05],
         [-8.031e-06],
         [ 3.300e-05],
         [-6.569e-06],
         [-9.723e-07],
         [ 1.004e-05]],

        [[ 1.355e-05],
         [-8.712e-06],
         [ 6.816e-07],
         [ 4.243e-06],
         [ 1.720e-06],
         [-1.960e-06],
         [-3.767e-06],
         [-1.563e-05],
         [-4.411e-05],
         [ 1.955e-05]],

        [[ 1.348e-06],
         [ 1.355e-05],
         [ 8.590e-07],
         [-8.017e-06],
         [-3.628e-06],
         [ 1.054e-05],
         [ 1.384e-05],
         [-1.081e-06],
         [-8.312e-06],
         [ 7.868e-06]]], device='cuda:0')
s after update for 1 param tensor([[[4.086e-03],
         [1.556e-02],
         [4.185e-04],
         [6.473e-04],
         [1.155e-04],
         [4.241e-04],
         [1.363e-03],
         [2.145e-03],
         [1.310e-03],
         [6.884e-04]],

        [[1.131e-04],
         [1.557e-04],
         [1.819e-04],
         [3.960e-05],
         [4.463e-05],
         [9.568e-04],
         [3.717e-05],
         [1.030e-05],
         [1.056e-04],
         [1.455e-03]],

        [[3.136e-03],
         [1.832e-03],
         [1.318e-04],
         [4.366e-04],
         [2.145e-03],
         [2.966e-03],
         [8.956e-03],
         [8.387e-03],
         [2.271e-05],
         [5.361e-04]],

        [[1.327e-04],
         [7.400e-04],
         [1.671e-02],
         [3.630e-04],
         [6.986e-05],
         [1.221e-04],
         [5.468e-05],
         [1.882e-06],
         [8.270e-06],
         [1.930e-05]],

        [[1.039e-03],
         [1.897e-02],
         [2.904e-03],
         [2.967e-05],
         [6.937e-05],
         [7.138e-04],
         [3.653e-04],
         [2.205e-05],
         [4.596e-03],
         [2.877e-04]],

        [[3.065e-03],
         [1.945e-02],
         [4.277e-04],
         [5.595e-05],
         [1.393e-03],
         [3.073e-04],
         [4.443e-04],
         [7.757e-05],
         [1.292e-04],
         [5.518e-03]],

        [[1.785e-03],
         [1.236e-02],
         [2.859e-04],
         [4.121e-05],
         [1.234e-03],
         [4.226e-04],
         [2.009e-05],
         [4.506e-04],
         [1.770e-03],
         [4.081e-04]],

        [[5.467e-04],
         [2.621e-03],
         [9.522e-03],
         [7.497e-04],
         [1.926e-03],
         [1.164e-02],
         [7.764e-03],
         [1.650e-05],
         [2.762e-05],
         [8.296e-05]],

        [[8.196e-05],
         [9.537e-05],
         [1.299e-03],
         [1.116e-04],
         [2.066e-05],
         [5.406e-06],
         [8.673e-04],
         [2.380e-04],
         [6.043e-05],
         [4.507e-03]],

        [[8.251e-04],
         [6.274e-04],
         [1.312e-05],
         [9.543e-06],
         [3.658e-04],
         [1.495e-03],
         [1.486e-02],
         [1.651e-03],
         [7.183e-04],
         [5.643e-04]],

        [[3.876e-03],
         [1.882e-06],
         [2.134e-03],
         [8.046e-04],
         [2.808e-03],
         [6.328e-03],
         [5.916e-05],
         [2.035e-05],
         [8.186e-03],
         [7.917e-04]],

        [[2.253e-03],
         [1.337e-03],
         [2.156e-03],
         [1.710e-03],
         [1.335e-03],
         [5.442e-03],
         [1.168e-03],
         [1.060e-03],
         [4.481e-03],
         [7.343e-05]],

        [[8.210e-04],
         [1.635e-03],
         [2.270e-04],
         [1.304e-02],
         [2.497e-03],
         [3.495e-06],
         [9.998e-06],
         [2.168e-03],
         [3.722e-03],
         [1.583e-03]],

        [[1.492e-04],
         [1.421e-03],
         [3.722e-03],
         [1.425e-03],
         [1.164e-02],
         [1.054e-02],
         [1.138e-03],
         [1.124e-04],
         [3.691e-06],
         [5.298e-04]],

        [[5.691e-04],
         [7.008e-04],
         [3.309e-04],
         [1.998e-04],
         [3.728e-03],
         [6.867e-04],
         [5.756e-03],
         [5.998e-03],
         [2.332e-03],
         [9.081e-03]],

        [[1.782e-04],
         [2.225e-04],
         [3.903e-03],
         [6.264e-03],
         [1.017e-03],
         [2.298e-05],
         [1.843e-03],
         [1.685e-03],
         [3.669e-03],
         [1.810e-04]],

        [[4.960e-05],
         [2.122e-03],
         [5.568e-03],
         [8.329e-04],
         [1.154e-04],
         [6.851e-06],
         [2.477e-03],
         [3.653e-03],
         [2.035e-03],
         [1.750e-02]],

        [[2.940e-03],
         [7.989e-03],
         [1.039e-04],
         [8.512e-05],
         [1.262e-02],
         [1.464e-03],
         [4.686e-03],
         [5.252e-04],
         [2.427e-06],
         [3.993e-04]],

        [[6.965e-04],
         [3.915e-03],
         [1.908e-03],
         [6.322e-04],
         [1.163e-05],
         [3.060e-04],
         [4.296e-04],
         [3.007e-03],
         [1.162e-02],
         [1.889e-03]],

        [[3.386e-03],
         [2.833e-04],
         [5.886e-05],
         [7.871e-05],
         [9.223e-05],
         [1.432e-04],
         [2.972e-03],
         [1.753e-03],
         [3.588e-03],
         [6.808e-05]]], device='cuda:0')
b after update for 1 param tensor([[[0.307],
         [0.599],
         [0.098],
         [0.122],
         [0.052],
         [0.099],
         [0.177],
         [0.222],
         [0.174],
         [0.126]],

        [[0.051],
         [0.060],
         [0.065],
         [0.030],
         [0.032],
         [0.148],
         [0.029],
         [0.015],
         [0.049],
         [0.183]],

        [[0.269],
         [0.205],
         [0.055],
         [0.100],
         [0.222],
         [0.261],
         [0.454],
         [0.439],
         [0.023],
         [0.111]],

        [[0.055],
         [0.131],
         [0.620],
         [0.091],
         [0.040],
         [0.053],
         [0.035],
         [0.007],
         [0.014],
         [0.021]],

        [[0.155],
         [0.661],
         [0.259],
         [0.026],
         [0.040],
         [0.128],
         [0.092],
         [0.023],
         [0.325],
         [0.081]],

        [[0.266],
         [0.669],
         [0.099],
         [0.036],
         [0.179],
         [0.084],
         [0.101],
         [0.042],
         [0.055],
         [0.356]],

        [[0.203],
         [0.533],
         [0.081],
         [0.031],
         [0.169],
         [0.099],
         [0.022],
         [0.102],
         [0.202],
         [0.097]],

        [[0.112],
         [0.246],
         [0.468],
         [0.131],
         [0.211],
         [0.518],
         [0.423],
         [0.019],
         [0.025],
         [0.044]],

        [[0.043],
         [0.047],
         [0.173],
         [0.051],
         [0.022],
         [0.011],
         [0.141],
         [0.074],
         [0.037],
         [0.322]],

        [[0.138],
         [0.120],
         [0.017],
         [0.015],
         [0.092],
         [0.186],
         [0.585],
         [0.195],
         [0.129],
         [0.114]],

        [[0.299],
         [0.007],
         [0.222],
         [0.136],
         [0.254],
         [0.382],
         [0.037],
         [0.022],
         [0.434],
         [0.135]],

        [[0.228],
         [0.175],
         [0.223],
         [0.198],
         [0.175],
         [0.354],
         [0.164],
         [0.156],
         [0.321],
         [0.041]],

        [[0.137],
         [0.194],
         [0.072],
         [0.548],
         [0.240],
         [0.009],
         [0.015],
         [0.223],
         [0.293],
         [0.191]],

        [[0.059],
         [0.181],
         [0.293],
         [0.181],
         [0.518],
         [0.493],
         [0.162],
         [0.051],
         [0.009],
         [0.110]],

        [[0.114],
         [0.127],
         [0.087],
         [0.068],
         [0.293],
         [0.126],
         [0.364],
         [0.372],
         [0.232],
         [0.457]],

        [[0.064],
         [0.072],
         [0.300],
         [0.380],
         [0.153],
         [0.023],
         [0.206],
         [0.197],
         [0.291],
         [0.065]],

        [[0.034],
         [0.221],
         [0.358],
         [0.138],
         [0.052],
         [0.013],
         [0.239],
         [0.290],
         [0.216],
         [0.635]],

        [[0.260],
         [0.429],
         [0.049],
         [0.044],
         [0.539],
         [0.184],
         [0.328],
         [0.110],
         [0.007],
         [0.096]],

        [[0.127],
         [0.300],
         [0.210],
         [0.121],
         [0.016],
         [0.084],
         [0.099],
         [0.263],
         [0.517],
         [0.209]],

        [[0.279],
         [0.081],
         [0.037],
         [0.043],
         [0.046],
         [0.057],
         [0.262],
         [0.201],
         [0.287],
         [0.040]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 1.689e-06],
        [ 1.679e-05],
        [ 1.036e-06],
        [ 1.417e-05],
        [ 5.708e-06],
        [-1.447e-06],
        [ 1.460e-06],
        [ 5.691e-08],
        [ 7.625e-06],
        [ 7.668e-06],
        [ 2.327e-05],
        [ 3.293e-05],
        [-3.722e-06],
        [ 1.475e-05],
        [-1.047e-05],
        [ 7.108e-07],
        [-3.433e-06],
        [-9.077e-06],
        [-4.700e-05],
        [ 2.093e-05]], device='cuda:0')
s after update for 1 param tensor([[2.124e-04],
        [2.774e-03],
        [6.144e-06],
        [1.715e-04],
        [2.849e-05],
        [3.304e-05],
        [2.981e-04],
        [1.882e-06],
        [6.395e-04],
        [1.769e-03],
        [8.580e-03],
        [5.588e-03],
        [3.683e-05],
        [4.790e-03],
        [1.528e-04],
        [2.662e-04],
        [1.234e-03],
        [2.199e-04],
        [6.826e-03],
        [3.850e-03]], device='cuda:0')
b after update for 1 param tensor([[0.070],
        [0.253],
        [0.012],
        [0.063],
        [0.026],
        [0.028],
        [0.083],
        [0.007],
        [0.121],
        [0.202],
        [0.444],
        [0.359],
        [0.029],
        [0.332],
        [0.059],
        [0.078],
        [0.169],
        [0.071],
        [0.396],
        [0.298]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 5.659062947464013
exp ma of ||w||^2 288.65262521036425
||w|| 2.378878506242808
exp ma of ||w|| 3.4064761334300715
||w||^2 4.546050845640538
exp ma of ||w||^2 5.619378724016126
||w|| 2.1321470037594823
exp ma of ||w|| 2.3009480030822664
||w||^2 3.943694343233606
exp ma of ||w||^2 4.017849118206636
||w|| 1.985873697704264
exp ma of ||w|| 1.9656860042861337
cuda
Objective function 44.86 = squared loss an data 36.79 + 0.5*rho*h**2 3.506171 + alpha*h 2.859035 + L2reg 1.57 + L1reg 0.14 ; SHD = 39 ; DAG True
Proportion of microbatches that were clipped  0.7909696280656163
iteration 2 in inner loop, alpha 341.4192546026456 rho 100000.0 h 0.008373972333053814
iteration 4 in outer loop, alpha = 8715.39158765646, rho = 1000000.0, h = 0.008373972333053814
Threshold 0.3
[[0.002 0.054 0.029 0.141 0.023 0.029 0.206 0.053 0.089 0.122 0.168 0.073
  0.022 0.171 0.216 0.094 0.058 0.018 0.234 0.029]
 [0.089 0.006 0.032 0.063 0.073 0.068 0.143 0.014 0.741 0.088 0.078 0.045
  0.038 0.056 0.213 0.089 0.019 0.004 0.112 0.019]
 [0.205 0.121 0.007 0.401 0.158 0.251 0.155 0.058 0.584 0.258 0.165 0.092
  0.115 0.206 0.405 0.252 0.102 0.057 1.892 0.465]
 [0.06  0.066 0.01  0.005 0.005 0.046 0.099 0.015 0.137 0.098 0.019 0.048
  0.039 0.098 0.056 0.08  0.045 0.028 0.081 0.006]
 [0.269 0.096 0.024 0.938 0.005 0.079 0.22  0.051 0.634 0.126 0.07  0.136
  0.078 0.181 0.189 0.085 0.035 0.039 0.202 0.028]
 [0.14  0.038 0.03  0.124 0.085 0.005 0.18  0.027 0.249 0.084 0.113 0.19
  0.033 0.097 0.213 0.09  0.067 0.037 0.797 0.068]
 [0.031 0.047 0.022 0.069 0.026 0.045 0.007 0.002 0.063 0.019 0.042 0.022
  0.012 0.029 0.107 0.03  0.011 0.011 0.059 0.023]
 [0.107 0.234 0.101 0.287 0.101 0.213 2.661 0.005 0.417 0.217 0.121 0.294
  0.132 0.2   0.373 0.928 0.159 0.146 0.258 0.154]
 [0.054 0.006 0.01  0.055 0.007 0.021 0.064 0.013 0.005 0.04  0.021 0.029
  0.024 0.046 0.047 0.053 0.022 0.002 0.071 0.013]
 [0.056 0.044 0.032 0.068 0.037 0.074 0.389 0.033 0.091 0.007 0.037 0.041
  0.004 0.053 0.067 0.134 0.039 0.023 0.073 0.035]
 [0.036 0.065 0.025 0.229 0.062 0.035 0.168 0.036 0.262 0.147 0.005 0.136
  0.082 0.161 0.142 0.117 0.052 0.033 0.13  0.039]
 [0.095 0.138 0.08  0.156 0.052 0.044 0.234 0.02  0.172 0.127 0.055 0.004
  0.053 0.085 0.11  0.11  0.027 0.021 0.472 0.056]
 [0.121 0.119 0.072 0.18  0.066 0.179 0.35  0.047 0.199 0.941 0.056 0.098
  0.005 0.116 0.126 0.198 0.092 0.043 0.333 0.193]
 [0.03  0.063 0.044 0.07  0.025 0.051 0.219 0.03  0.142 0.107 0.04  0.087
  0.043 0.005 0.09  0.025 0.04  0.037 0.248 0.036]
 [0.03  0.04  0.01  0.085 0.033 0.03  0.057 0.011 0.122 0.066 0.054 0.038
  0.043 0.074 0.007 0.061 0.007 0.005 0.13  0.039]
 [0.063 0.091 0.035 0.079 0.057 0.059 0.313 0.006 0.094 0.056 0.052 0.085
  0.03  0.159 0.132 0.012 0.075 0.023 0.163 0.028]
 [0.081 0.748 0.057 0.083 0.162 0.066 0.361 0.031 0.253 0.204 0.156 0.17
  0.088 0.106 0.929 0.113 0.006 0.006 0.234 0.046]
 [0.227 1.078 0.092 0.174 0.166 0.152 0.319 0.038 0.404 0.251 0.25  0.166
  0.115 0.129 0.351 0.203 1.108 0.003 0.436 0.058]
 [0.023 0.078 0.002 0.069 0.02  0.008 0.049 0.018 0.057 0.044 0.058 0.014
  0.015 0.029 0.042 0.027 0.019 0.008 0.005 0.014]
 [0.244 0.284 0.021 1.266 0.295 0.098 0.186 0.058 0.258 0.212 0.179 0.146
  0.044 0.111 0.084 0.197 0.142 0.085 0.389 0.004]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.741 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.401 0.    0.    0.    0.    0.584 0.    0.    0.
  0.    0.    0.405 0.    0.    0.    1.892 0.465]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.938 0.    0.    0.    0.    0.634 0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.797 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    2.661 0.    0.417 0.    0.    0.
  0.    0.    0.373 0.928 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.389 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.472 0.   ]
 [0.    0.    0.    0.    0.    0.    0.35  0.    0.    0.941 0.    0.
  0.    0.    0.    0.    0.    0.    0.333 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.313 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.748 0.    0.    0.    0.    0.361 0.    0.    0.    0.    0.
  0.    0.    0.929 0.    0.    0.    0.    0.   ]
 [0.    1.078 0.    0.    0.    0.    0.319 0.    0.404 0.    0.    0.
  0.    0.    0.351 0.    1.108 0.    0.436 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.266 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.389 0.   ]]
{'fdr': 0.5333333333333333, 'tpr': 0.35, 'fpr': 0.10666666666666667, 'f1': 0.4, 'shd': 39, 'npred': 30, 'ntrue': 40}
[5.441e-02 2.859e-02 1.406e-01 2.308e-02 2.878e-02 2.055e-01 5.286e-02
 8.904e-02 1.223e-01 1.678e-01 7.271e-02 2.246e-02 1.715e-01 2.161e-01
 9.375e-02 5.808e-02 1.828e-02 2.342e-01 2.911e-02 8.852e-02 3.172e-02
 6.323e-02 7.280e-02 6.764e-02 1.434e-01 1.424e-02 7.408e-01 8.829e-02
 7.763e-02 4.498e-02 3.846e-02 5.637e-02 2.128e-01 8.924e-02 1.871e-02
 3.700e-03 1.116e-01 1.899e-02 2.048e-01 1.208e-01 4.007e-01 1.581e-01
 2.513e-01 1.548e-01 5.848e-02 5.840e-01 2.582e-01 1.651e-01 9.212e-02
 1.145e-01 2.058e-01 4.049e-01 2.521e-01 1.017e-01 5.690e-02 1.892e+00
 4.655e-01 5.971e-02 6.645e-02 1.020e-02 5.351e-03 4.597e-02 9.931e-02
 1.549e-02 1.369e-01 9.833e-02 1.939e-02 4.822e-02 3.944e-02 9.841e-02
 5.638e-02 7.961e-02 4.514e-02 2.843e-02 8.065e-02 5.656e-03 2.694e-01
 9.592e-02 2.391e-02 9.384e-01 7.899e-02 2.198e-01 5.052e-02 6.336e-01
 1.264e-01 6.993e-02 1.358e-01 7.786e-02 1.810e-01 1.893e-01 8.461e-02
 3.517e-02 3.942e-02 2.017e-01 2.774e-02 1.401e-01 3.819e-02 2.986e-02
 1.235e-01 8.520e-02 1.804e-01 2.710e-02 2.490e-01 8.356e-02 1.127e-01
 1.902e-01 3.328e-02 9.673e-02 2.135e-01 9.011e-02 6.718e-02 3.701e-02
 7.973e-01 6.803e-02 3.100e-02 4.664e-02 2.184e-02 6.934e-02 2.605e-02
 4.477e-02 1.961e-03 6.326e-02 1.932e-02 4.202e-02 2.244e-02 1.173e-02
 2.872e-02 1.072e-01 3.046e-02 1.090e-02 1.054e-02 5.901e-02 2.307e-02
 1.065e-01 2.335e-01 1.009e-01 2.866e-01 1.006e-01 2.126e-01 2.661e+00
 4.174e-01 2.167e-01 1.207e-01 2.936e-01 1.317e-01 1.995e-01 3.734e-01
 9.281e-01 1.593e-01 1.464e-01 2.578e-01 1.541e-01 5.383e-02 5.693e-03
 1.004e-02 5.466e-02 7.484e-03 2.130e-02 6.374e-02 1.278e-02 4.046e-02
 2.148e-02 2.933e-02 2.410e-02 4.567e-02 4.740e-02 5.346e-02 2.200e-02
 2.267e-03 7.113e-02 1.313e-02 5.556e-02 4.432e-02 3.244e-02 6.778e-02
 3.737e-02 7.430e-02 3.890e-01 3.303e-02 9.059e-02 3.746e-02 4.067e-02
 3.698e-03 5.290e-02 6.664e-02 1.343e-01 3.938e-02 2.275e-02 7.268e-02
 3.540e-02 3.583e-02 6.548e-02 2.462e-02 2.288e-01 6.216e-02 3.479e-02
 1.682e-01 3.640e-02 2.618e-01 1.471e-01 1.362e-01 8.237e-02 1.614e-01
 1.423e-01 1.173e-01 5.168e-02 3.323e-02 1.297e-01 3.936e-02 9.524e-02
 1.384e-01 8.038e-02 1.563e-01 5.225e-02 4.449e-02 2.343e-01 1.962e-02
 1.718e-01 1.272e-01 5.494e-02 5.283e-02 8.475e-02 1.098e-01 1.101e-01
 2.697e-02 2.107e-02 4.722e-01 5.629e-02 1.206e-01 1.192e-01 7.169e-02
 1.797e-01 6.587e-02 1.787e-01 3.504e-01 4.704e-02 1.991e-01 9.409e-01
 5.606e-02 9.821e-02 1.157e-01 1.256e-01 1.981e-01 9.208e-02 4.258e-02
 3.331e-01 1.930e-01 3.005e-02 6.273e-02 4.426e-02 7.017e-02 2.534e-02
 5.134e-02 2.191e-01 3.012e-02 1.415e-01 1.070e-01 4.027e-02 8.682e-02
 4.300e-02 8.969e-02 2.522e-02 3.970e-02 3.666e-02 2.482e-01 3.638e-02
 2.957e-02 3.988e-02 1.010e-02 8.497e-02 3.263e-02 3.023e-02 5.682e-02
 1.094e-02 1.221e-01 6.615e-02 5.405e-02 3.820e-02 4.327e-02 7.412e-02
 6.144e-02 7.046e-03 4.831e-03 1.298e-01 3.899e-02 6.254e-02 9.059e-02
 3.521e-02 7.913e-02 5.671e-02 5.940e-02 3.129e-01 6.341e-03 9.433e-02
 5.571e-02 5.222e-02 8.480e-02 3.046e-02 1.593e-01 1.317e-01 7.547e-02
 2.326e-02 1.631e-01 2.798e-02 8.057e-02 7.480e-01 5.689e-02 8.277e-02
 1.625e-01 6.619e-02 3.613e-01 3.091e-02 2.535e-01 2.042e-01 1.560e-01
 1.699e-01 8.796e-02 1.061e-01 9.288e-01 1.134e-01 5.554e-03 2.345e-01
 4.585e-02 2.266e-01 1.078e+00 9.202e-02 1.745e-01 1.655e-01 1.522e-01
 3.191e-01 3.820e-02 4.038e-01 2.507e-01 2.502e-01 1.656e-01 1.151e-01
 1.285e-01 3.512e-01 2.028e-01 1.108e+00 4.359e-01 5.828e-02 2.350e-02
 7.819e-02 2.445e-03 6.945e-02 2.022e-02 7.901e-03 4.902e-02 1.800e-02
 5.696e-02 4.401e-02 5.759e-02 1.375e-02 1.535e-02 2.893e-02 4.221e-02
 2.675e-02 1.942e-02 7.924e-03 1.430e-02 2.440e-01 2.842e-01 2.121e-02
 1.266e+00 2.955e-01 9.781e-02 1.858e-01 5.835e-02 2.581e-01 2.122e-01
 1.795e-01 1.462e-01 4.368e-02 1.113e-01 8.417e-02 1.970e-01 1.420e-01
 8.457e-02 3.891e-01]
[[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.735735294117647, 0.4444931310216994)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
