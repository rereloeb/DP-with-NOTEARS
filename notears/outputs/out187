samples  5000  graph  20 80 ER mlp  minibatch size  100  noise  0.8  minibatches per NN training  63 adaclip_and_quantile
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.7622777646747636
iteration 1 in outer loop, alpha = 1.7622777646747636, rho = 1.0, h = 1.7622777646747636
cuda
iteration 1 in inner loop,alpha 1.7622777646747636 rho 1.0 h 1.1769941079911774
iteration 2 in inner loop,alpha 1.7622777646747636 rho 10.0 h 0.5472632403059237
iteration 3 in inner loop,alpha 1.7622777646747636 rho 100.0 h 0.18636008893995282
iteration 2 in outer loop, alpha = 20.398286658670045, rho = 100.0, h = 0.18636008893995282
cuda
iteration 1 in inner loop,alpha 20.398286658670045 rho 100.0 h 0.108946762839075
iteration 2 in inner loop,alpha 20.398286658670045 rho 1000.0 h 0.04389151760590693
iteration 3 in outer loop, alpha = 64.28980426457697, rho = 1000.0, h = 0.04389151760590693
cuda
iteration 1 in inner loop,alpha 64.28980426457697 rho 1000.0 h 0.02369372183978058
iteration 2 in inner loop,alpha 64.28980426457697 rho 10000.0 h 0.008096379608844018
iteration 4 in outer loop, alpha = 145.25360035301713, rho = 10000.0, h = 0.008096379608844018
cuda
iteration 1 in inner loop,alpha 145.25360035301713 rho 10000.0 h 0.0042945271682128805
iteration 2 in inner loop,alpha 145.25360035301713 rho 100000.0 h 0.0013932532203000392
iteration 5 in outer loop, alpha = 284.57892238302105, rho = 100000.0, h = 0.0013932532203000392
cuda
iteration 1 in inner loop,alpha 284.57892238302105 rho 100000.0 h 0.0006826203830172517
iteration 6 in outer loop, alpha = 967.1993054002728, rho = 1000000.0, h = 0.0006826203830172517
Threshold 0.3
[[0.001 3.14  0.084 0.236 0.905 1.114 0.314 0.    1.927 1.08  0.128 1.106
  0.27  0.002 0.362 0.437 0.293 1.197 1.28  0.465]
 [0.    0.001 0.026 0.473 0.49  1.63  1.791 0.    0.583 0.469 0.098 0.676
  0.317 0.001 1.159 0.3   0.367 0.858 0.486 1.873]
 [0.002 0.008 0.001 0.056 0.427 0.541 1.511 0.    0.179 0.444 0.093 0.195
  1.289 0.    1.467 0.312 0.572 0.    1.173 0.325]
 [0.    0.    0.    0.02  0.001 0.003 0.001 0.    0.    0.002 0.    0.
  0.    0.    0.197 0.232 0.225 0.    0.001 0.707]
 [0.    0.    0.    0.533 0.004 0.319 0.218 0.    0.    0.403 0.002 0.001
  0.003 0.    0.201 0.335 1.146 0.    0.341 1.05 ]
 [0.    0.    0.    0.482 0.002 0.003 0.002 0.    0.    0.    0.    0.
  0.    0.    0.333 1.629 1.751 0.    0.297 0.29 ]
 [0.    0.    0.    0.198 0.001 0.221 0.002 0.    0.    0.01  0.    0.
  0.    0.    0.325 0.343 0.311 0.    0.167 0.203]
 [2.422 0.129 0.    0.096 0.703 0.222 0.706 0.001 0.298 0.446 0.133 0.369
  1.913 0.679 0.217 0.173 0.173 1.12  0.489 0.444]
 [0.    0.    0.002 0.311 1.572 0.936 1.286 0.    0.001 0.808 0.421 0.297
  1.747 0.    1.192 1.236 0.798 0.    1.308 0.685]
 [0.    0.    0.    0.095 0.017 2.179 0.112 0.    0.    0.008 0.001 0.002
  0.    0.    1.157 1.37  1.705 0.    0.812 0.233]
 [0.    0.    0.    0.883 0.103 0.586 2.31  0.    0.    0.252 0.001 0.
  0.    0.    1.49  0.467 1.002 0.    0.268 1.244]
 [0.    0.    0.    0.141 0.438 0.262 1.652 0.    0.    0.243 0.126 0.001
  0.121 0.    0.159 0.802 0.272 0.    0.119 0.23 ]
 [0.    0.    0.    0.11  0.137 1.36  1.081 0.    0.    0.162 2.119 0.002
  0.001 0.    0.307 0.318 0.279 0.    0.477 1.052]
 [0.148 0.038 0.033 0.682 0.332 1.605 0.312 0.001 0.281 0.347 1.696 0.532
  2.04  0.001 0.436 0.448 0.965 0.354 0.721 1.924]
 [0.    0.    0.    0.004 0.001 0.001 0.    0.    0.    0.    0.    0.
  0.    0.    0.002 0.247 0.003 0.    0.001 0.003]
 [0.    0.    0.    0.003 0.001 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.001 0.006 0.002 0.    0.001 0.   ]
 [0.    0.    0.    0.004 0.001 0.001 0.001 0.    0.    0.    0.    0.
  0.    0.    1.058 1.146 0.005 0.    0.001 0.003]
 [0.    0.    0.033 0.034 0.44  0.285 1.371 0.    3.18  0.527 0.094 0.719
  0.079 0.    0.344 0.193 0.185 0.001 0.408 0.248]
 [0.    0.    0.    0.705 0.001 0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.321 1.021 0.401 0.    0.001 0.232]
 [0.    0.    0.    0.003 0.    0.001 0.001 0.    0.    0.001 0.    0.
  0.    0.    0.12  1.742 0.255 0.    0.    0.002]]
[[0.    3.14  0.    0.    0.905 1.114 0.314 0.    1.927 1.08  0.    1.106
  0.    0.    0.362 0.437 0.    1.197 1.28  0.465]
 [0.    0.    0.    0.473 0.49  1.63  1.791 0.    0.583 0.469 0.    0.676
  0.317 0.    1.159 0.3   0.367 0.858 0.486 1.873]
 [0.    0.    0.    0.    0.427 0.541 1.511 0.    0.    0.444 0.    0.
  1.289 0.    1.467 0.312 0.572 0.    1.173 0.325]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.707]
 [0.    0.    0.    0.533 0.    0.319 0.    0.    0.    0.403 0.    0.
  0.    0.    0.    0.335 1.146 0.    0.341 1.05 ]
 [0.    0.    0.    0.482 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.333 1.629 1.751 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.325 0.343 0.311 0.    0.    0.   ]
 [2.422 0.    0.    0.    0.703 0.    0.706 0.    0.    0.446 0.    0.369
  1.913 0.679 0.    0.    0.    1.12  0.489 0.444]
 [0.    0.    0.    0.311 1.572 0.936 1.286 0.    0.    0.808 0.421 0.
  1.747 0.    1.192 1.236 0.798 0.    1.308 0.685]
 [0.    0.    0.    0.    0.    2.179 0.    0.    0.    0.    0.    0.
  0.    0.    1.157 1.37  1.705 0.    0.812 0.   ]
 [0.    0.    0.    0.883 0.    0.586 2.31  0.    0.    0.    0.    0.
  0.    0.    1.49  0.467 1.002 0.    0.    1.244]
 [0.    0.    0.    0.    0.438 0.    1.652 0.    0.    0.    0.    0.
  0.    0.    0.    0.802 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.36  1.081 0.    0.    0.    2.119 0.
  0.    0.    0.307 0.318 0.    0.    0.477 1.052]
 [0.    0.    0.    0.682 0.332 1.605 0.312 0.    0.    0.347 1.696 0.532
  2.04  0.    0.436 0.448 0.965 0.354 0.721 1.924]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    1.058 1.146 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.44  0.    1.371 0.    3.18  0.527 0.    0.719
  0.    0.    0.344 0.    0.    0.    0.408 0.   ]
 [0.    0.    0.    0.705 0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.321 1.021 0.401 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    1.742 0.    0.    0.    0.   ]]
{'fdr': 0.43089430894308944, 'tpr': 0.875, 'fpr': 0.4818181818181818, 'f1': 0.689655172413793, 'shd': 58, 'npred': 123, 'ntrue': 80}
[3.140e+00 8.424e-02 2.360e-01 9.054e-01 1.114e+00 3.142e-01 2.808e-04
 1.927e+00 1.080e+00 1.279e-01 1.106e+00 2.704e-01 1.586e-03 3.620e-01
 4.368e-01 2.928e-01 1.197e+00 1.280e+00 4.650e-01 3.953e-04 2.636e-02
 4.727e-01 4.901e-01 1.630e+00 1.791e+00 1.968e-04 5.827e-01 4.687e-01
 9.840e-02 6.756e-01 3.170e-01 1.369e-03 1.159e+00 3.001e-01 3.669e-01
 8.575e-01 4.856e-01 1.873e+00 2.208e-03 7.647e-03 5.611e-02 4.273e-01
 5.411e-01 1.511e+00 2.080e-04 1.787e-01 4.444e-01 9.263e-02 1.949e-01
 1.289e+00 2.563e-04 1.467e+00 3.122e-01 5.718e-01 1.404e-04 1.173e+00
 3.254e-01 9.655e-05 7.828e-05 1.936e-05 6.722e-04 2.594e-03 1.143e-03
 1.120e-04 1.217e-04 2.353e-03 4.698e-04 2.836e-04 1.048e-04 1.022e-05
 1.973e-01 2.320e-01 2.249e-01 2.391e-05 1.452e-03 7.073e-01 8.179e-06
 1.258e-05 5.984e-05 5.332e-01 3.195e-01 2.180e-01 2.143e-05 1.642e-04
 4.030e-01 1.896e-03 1.257e-03 3.094e-03 7.641e-05 2.010e-01 3.352e-01
 1.146e+00 2.945e-05 3.415e-01 1.050e+00 9.304e-05 2.039e-04 8.198e-05
 4.816e-01 2.135e-03 1.773e-03 5.892e-05 8.128e-05 4.619e-04 6.778e-05
 3.698e-04 2.778e-04 3.800e-05 3.326e-01 1.629e+00 1.751e+00 1.390e-05
 2.972e-01 2.902e-01 6.236e-05 1.137e-04 1.009e-05 1.981e-01 7.028e-04
 2.212e-01 4.269e-05 9.863e-06 1.012e-02 1.349e-04 2.227e-04 7.698e-05
 4.148e-06 3.253e-01 3.433e-01 3.110e-01 1.849e-06 1.668e-01 2.029e-01
 2.422e+00 1.286e-01 4.687e-04 9.586e-02 7.035e-01 2.223e-01 7.056e-01
 2.985e-01 4.460e-01 1.325e-01 3.689e-01 1.913e+00 6.785e-01 2.174e-01
 1.735e-01 1.734e-01 1.120e+00 4.890e-01 4.439e-01 1.238e-05 1.806e-04
 2.039e-03 3.110e-01 1.572e+00 9.358e-01 1.286e+00 1.220e-05 8.077e-01
 4.211e-01 2.971e-01 1.747e+00 2.702e-05 1.192e+00 1.236e+00 7.976e-01
 4.416e-05 1.308e+00 6.854e-01 2.998e-05 3.053e-04 1.211e-04 9.485e-02
 1.672e-02 2.179e+00 1.125e-01 6.824e-05 1.905e-04 1.151e-03 1.517e-03
 3.302e-04 1.539e-04 1.157e+00 1.370e+00 1.705e+00 3.610e-05 8.115e-01
 2.325e-01 9.969e-06 6.313e-05 6.270e-05 8.835e-01 1.035e-01 5.859e-01
 2.310e+00 2.628e-05 4.933e-05 2.524e-01 4.939e-04 2.518e-04 5.236e-05
 1.490e+00 4.668e-01 1.002e+00 1.504e-05 2.677e-01 1.244e+00 2.473e-05
 2.537e-05 3.097e-04 1.412e-01 4.379e-01 2.616e-01 1.652e+00 3.835e-05
 2.394e-04 2.433e-01 1.261e-01 1.215e-01 4.372e-05 1.590e-01 8.017e-01
 2.722e-01 1.764e-04 1.188e-01 2.304e-01 7.775e-06 4.819e-05 1.969e-04
 1.103e-01 1.367e-01 1.360e+00 1.081e+00 3.628e-05 2.570e-04 1.621e-01
 2.119e+00 1.515e-03 9.117e-05 3.068e-01 3.178e-01 2.787e-01 1.717e-05
 4.775e-01 1.052e+00 1.484e-01 3.754e-02 3.296e-02 6.824e-01 3.319e-01
 1.605e+00 3.122e-01 7.272e-04 2.807e-01 3.468e-01 1.696e+00 5.319e-01
 2.040e+00 4.361e-01 4.482e-01 9.652e-01 3.544e-01 7.207e-01 1.924e+00
 5.221e-05 1.979e-04 3.991e-05 4.499e-03 1.258e-03 9.509e-04 4.117e-04
 3.696e-05 8.725e-06 8.548e-05 8.908e-05 2.665e-04 6.766e-05 7.478e-06
 2.466e-01 3.274e-03 2.221e-06 6.461e-04 2.623e-03 3.208e-05 2.413e-05
 3.720e-05 2.629e-03 6.939e-04 1.250e-04 3.370e-04 3.174e-05 2.618e-05
 3.495e-04 1.063e-04 7.450e-05 3.160e-05 1.534e-05 1.455e-03 1.563e-03
 6.279e-06 7.182e-04 2.744e-04 6.066e-05 1.003e-04 3.355e-05 3.687e-03
 1.048e-03 6.623e-04 8.508e-04 5.308e-05 2.564e-05 5.774e-05 9.239e-05
 1.011e-04 3.712e-05 3.209e-05 1.058e+00 1.146e+00 6.167e-06 5.635e-04
 3.157e-03 1.013e-04 1.548e-05 3.272e-02 3.403e-02 4.404e-01 2.854e-01
 1.371e+00 9.790e-05 3.180e+00 5.271e-01 9.378e-02 7.187e-01 7.918e-02
 1.456e-04 3.445e-01 1.932e-01 1.854e-01 4.075e-01 2.477e-01 1.173e-04
 1.702e-05 1.619e-04 7.054e-01 1.092e-03 9.232e-04 9.703e-04 5.503e-05
 1.749e-04 5.589e-04 2.820e-04 3.333e-04 1.614e-04 2.401e-05 3.206e-01
 1.021e+00 4.013e-01 9.255e-06 2.317e-01 4.216e-05 8.103e-05 7.973e-05
 3.184e-03 2.905e-04 1.104e-03 1.058e-03 2.313e-05 8.952e-05 1.030e-03
 4.198e-04 1.630e-04 2.576e-04 8.459e-05 1.198e-01 1.742e+00 2.545e-01
 1.819e-05 3.924e-04]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.935625, 0.8869295009713748)
cuda
4420
cuda
Objective function 730.81 = squared loss an data 514.80 + 0.5*rho*h**2 215.201283 + alpha*h 0.000000 + L2reg 0.37 + L1reg 0.45 ; SHD = 206 ; DAG False
||w||^2 0.27432452617957953
exp ma of ||w||^2 60.881763062529664
||w|| 0.5237599890976586
exp ma of ||w|| 0.5646979671687108
||w||^2 0.18384411596194775
exp ma of ||w||^2 0.21221434605345763
||w|| 0.42877047002090496
exp ma of ||w|| 0.4513152487071472
||w||^2 0.13988133285996396
exp ma of ||w||^2 0.2060509491109132
||w|| 0.3740071294239776
exp ma of ||w|| 0.4462477779229204
||w||^2 0.0998016147227446
exp ma of ||w||^2 0.20545686742223213
||w|| 0.3159139356260572
exp ma of ||w|| 0.44152139823352027
||w||^2 0.37159838356084374
exp ma of ||w||^2 0.21564046524422165
||w|| 0.6095887003224746
exp ma of ||w|| 0.4473151025907536
cuda
Objective function 174.50 = squared loss an data 172.01 + 0.5*rho*h**2 1.830931 + alpha*h 0.000000 + L2reg 0.40 + L1reg 0.26 ; SHD = 120 ; DAG False
Proportion of microbatches that were clipped  0.7280394653087205
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.9135991056056483
iteration 1 in outer loop, alpha = 1.9135991056056483, rho = 1.0, h = 1.9135991056056483
cuda
4420
cuda
Objective function 178.16 = squared loss an data 172.01 + 0.5*rho*h**2 1.830931 + alpha*h 3.661862 + L2reg 0.40 + L1reg 0.26 ; SHD = 120 ; DAG False
||w||^2 54.87734144000458
exp ma of ||w||^2 20506.700913093835
||w|| 7.407924232874184
exp ma of ||w|| 30.934695523859688
||w||^2 44.021917870090704
exp ma of ||w||^2 3239.694052908281
||w|| 6.634901496638116
exp ma of ||w|| 9.796549002469034
||w||^2 0.46971331409468364
exp ma of ||w||^2 0.3601809173884714
||w|| 0.6853563409604405
exp ma of ||w|| 0.5704896373469541
||w||^2 0.18876606252774839
exp ma of ||w||^2 0.36362518929886123
||w|| 0.4344721654234577
exp ma of ||w|| 0.5707769447578775
||w||^2 0.12914949566560896
exp ma of ||w||^2 0.33856786685533347
||w|| 0.359373754836951
exp ma of ||w|| 0.5473369392640337
cuda
Objective function 70.97 = squared loss an data 66.00 + 0.5*rho*h**2 1.112453 + alpha*h 2.854347 + L2reg 0.75 + L1reg 0.25 ; SHD = 100 ; DAG False
Proportion of microbatches that were clipped  0.734998378203049
iteration 1 in inner loop, alpha 1.9135991056056483 rho 1.0 h 1.4916116396876689
4420
cuda
Objective function 80.98 = squared loss an data 66.00 + 0.5*rho*h**2 11.124526 + alpha*h 2.854347 + L2reg 0.75 + L1reg 0.25 ; SHD = 100 ; DAG False
||w||^2 903505.4146976003
exp ma of ||w||^2 11379755.202651089
||w|| 950.5290183353691
exp ma of ||w|| 1889.2063299507672
||w||^2 0.3908336680467802
exp ma of ||w||^2 0.8820266533997883
||w|| 0.6251669121496916
exp ma of ||w|| 0.829997140410128
||w||^2 0.886260895003743
exp ma of ||w||^2 0.7780849522380175
||w|| 0.9414143057144092
exp ma of ||w|| 0.8219878687830036
||w||^2 0.7106536013786794
exp ma of ||w||^2 0.8364562446060578
||w|| 0.8430027291644312
exp ma of ||w|| 0.8556861395920394
||w||^2 0.7734464316834481
exp ma of ||w||^2 0.8719975679030637
||w|| 0.879458032929058
exp ma of ||w|| 0.8605579114528282
cuda
Objective function 48.19 = squared loss an data 42.71 + 0.5*rho*h**2 2.830937 + alpha*h 1.439896 + L2reg 0.99 + L1reg 0.22 ; SHD = 84 ; DAG False
Proportion of microbatches that were clipped  0.7564001884718078
iteration 2 in inner loop, alpha 1.9135991056056483 rho 10.0 h 0.7524542972579802
4420
cuda
Objective function 73.67 = squared loss an data 42.71 + 0.5*rho*h**2 28.309373 + alpha*h 1.439896 + L2reg 0.99 + L1reg 0.22 ; SHD = 84 ; DAG False
||w||^2 29423900380.853256
exp ma of ||w||^2 14199294327.716002
||w|| 171533.96276205263
exp ma of ||w|| 100857.09943832454
||w||^2 1.2359332718985554
exp ma of ||w||^2 1.3093916363129854
||w|| 1.111725358125178
exp ma of ||w|| 1.0879144512714276
||w||^2 1.9402861779214473
exp ma of ||w||^2 1.2054256746341911
||w|| 1.392941555816843
exp ma of ||w|| 1.050988326663387
||w||^2 0.21838943960304513
exp ma of ||w||^2 1.3228190072703678
||w|| 0.46732155910362744
exp ma of ||w|| 1.0760803641199297
cuda
Objective function 45.25 = squared loss an data 39.95 + 0.5*rho*h**2 3.453940 + alpha*h 0.502948 + L2reg 1.16 + L1reg 0.18 ; SHD = 78 ; DAG True
Proportion of microbatches that were clipped  0.7675363224496172
iteration 3 in inner loop, alpha 1.9135991056056483 rho 100.0 h 0.2628284487429937
iteration 2 in outer loop, alpha = 28.19644397990502, rho = 100.0, h = 0.2628284487429937
cuda
4420
cuda
Objective function 52.15 = squared loss an data 39.95 + 0.5*rho*h**2 3.453940 + alpha*h 7.410828 + L2reg 1.16 + L1reg 0.18 ; SHD = 78 ; DAG True
||w||^2 8103954066.273571
exp ma of ||w||^2 34653972506.10686
||w|| 90021.96435467053
exp ma of ||w|| 162508.9888324636
||w||^2 22403438284.20064
exp ma of ||w||^2 33857817184.149036
||w|| 149677.78153153072
exp ma of ||w|| 161349.07502652868
||w||^2 219270917.36722478
exp ma of ||w||^2 2380475970.0506496
||w|| 14807.799207418528
exp ma of ||w|| 37168.23594210602
||w||^2 2191.4387700634697
exp ma of ||w||^2 158754.84501660502
||w|| 46.812805620508044
exp ma of ||w|| 91.09993444463076
||w||^2 57.15574896321222
exp ma of ||w||^2 12750.82003528441
||w|| 7.5601421258606125
exp ma of ||w|| 20.792387921069988
||w||^2 1.426314236584376
exp ma of ||w||^2 23.51020790556166
||w|| 1.1942839848982219
exp ma of ||w|| 1.6648338334428174
||w||^2 1.758089282987095
exp ma of ||w||^2 5.206101986024995
||w|| 1.3259295920172742
exp ma of ||w|| 1.4023981238518
||w||^2 1.746540903791129
exp ma of ||w||^2 1.8037560591045003
||w|| 1.3215675933493258
exp ma of ||w|| 1.281779927004387
||w||^2 0.9749548284542967
exp ma of ||w||^2 1.7887329378580172
||w|| 0.9873980091403348
exp ma of ||w|| 1.2636149695725958
||w||^2 0.8236007747284424
exp ma of ||w||^2 1.7588053082186097
||w|| 0.9075245311992631
exp ma of ||w|| 1.2571386184292097
cuda
Objective function 47.79 = squared loss an data 39.34 + 0.5*rho*h**2 1.739184 + alpha*h 5.258745 + L2reg 1.28 + L1reg 0.17 ; SHD = 73 ; DAG True
Proportion of microbatches that were clipped  0.7656275501876938
iteration 1 in inner loop, alpha 28.19644397990502 rho 100.0 h 0.18650383904071077
4420
cuda
Objective function 63.44 = squared loss an data 39.34 + 0.5*rho*h**2 17.391841 + alpha*h 5.258745 + L2reg 1.28 + L1reg 0.17 ; SHD = 73 ; DAG True
||w||^2 651995777.72999
exp ma of ||w||^2 6160692024.62583
||w|| 25534.207991045856
exp ma of ||w|| 62787.295168672536
||w||^2 1104.7805185106033
exp ma of ||w||^2 237957.02104215053
||w|| 33.23823879977102
exp ma of ||w|| 130.93448452595632
||w||^2 11.69946759117003
exp ma of ||w||^2 31.45009150335135
||w|| 3.4204484488397178
exp ma of ||w|| 2.1307009900272234
cuda
Objective function 47.46 = squared loss an data 40.42 + 0.5*rho*h**2 3.237626 + alpha*h 2.268938 + L2reg 1.39 + L1reg 0.14 ; SHD = 71 ; DAG True
Proportion of microbatches that were clipped  0.7782560178542962
iteration 2 in inner loop, alpha 28.19644397990502 rho 1000.0 h 0.08046894967919371
4420
cuda
Objective function 76.60 = squared loss an data 40.42 + 0.5*rho*h**2 32.376259 + alpha*h 2.268938 + L2reg 1.39 + L1reg 0.14 ; SHD = 71 ; DAG True
||w||^2 7524.8995795663795
exp ma of ||w||^2 1781568.5953345113
||w|| 86.74617904879949
exp ma of ||w|| 472.81349760161845
||w||^2 4.801411857678399
exp ma of ||w||^2 2.9609162981931245
||w|| 2.1912124172882916
exp ma of ||w|| 1.6549284112497196
||w||^2 3.558037983688272
exp ma of ||w||^2 2.9035123166266583
||w|| 1.8862762214713602
exp ma of ||w|| 1.6409679141004296
||w||^2 2.3038165892369307
exp ma of ||w||^2 2.603958423388009
||w|| 1.5178328594535468
exp ma of ||w|| 1.5496729436411119
||w||^2 4.887673906376366
exp ma of ||w||^2 2.3227258550343497
||w|| 2.210808428239852
exp ma of ||w|| 1.4566777626637197
cuda
Objective function 49.89 = squared loss an data 42.91 + 0.5*rho*h**2 4.507337 + alpha*h 0.846583 + L2reg 1.49 + L1reg 0.12 ; SHD = 73 ; DAG True
Proportion of microbatches that were clipped  0.7802357500403682
iteration 3 in inner loop, alpha 28.19644397990502 rho 10000.0 h 0.030024447686251676
iteration 3 in outer loop, alpha = 328.4409208424218, rho = 10000.0, h = 0.030024447686251676
cuda
4420
cuda
Objective function 58.90 = squared loss an data 42.91 + 0.5*rho*h**2 4.507337 + alpha*h 9.861257 + L2reg 1.49 + L1reg 0.12 ; SHD = 73 ; DAG True
||w||^2 379807735610.76526
exp ma of ||w||^2 227340724931.05402
||w|| 616285.4335539379
exp ma of ||w|| 234009.5190121684
||w||^2 85908350123.71776
exp ma of ||w||^2 193823877628.19736
||w|| 293101.2625761236
exp ma of ||w|| 385038.6432913973
||w||^2 2.74282627306609
exp ma of ||w||^2 3.9309384711070714
||w|| 1.6561480226918397
exp ma of ||w|| 1.9087541352910131
||w||^2 2.4877252587100243
exp ma of ||w||^2 3.196590394799259
||w|| 1.577252439754025
exp ma of ||w|| 1.7291044303227925
||w||^2 1.4899723460643548
exp ma of ||w||^2 2.869653258421827
||w|| 1.2206442340274068
exp ma of ||w|| 1.6393922741741527
cuda
Objective function 51.92 = squared loss an data 42.77 + 0.5*rho*h**2 1.579112 + alpha*h 5.836853 + L2reg 1.61 + L1reg 0.12 ; SHD = 72 ; DAG True
Proportion of microbatches that were clipped  0.782996338162713
iteration 1 in inner loop, alpha 328.4409208424218 rho 10000.0 h 0.0177713946782454
4420
cuda
Objective function 66.13 = squared loss an data 42.77 + 0.5*rho*h**2 15.791123 + alpha*h 5.836853 + L2reg 1.61 + L1reg 0.12 ; SHD = 72 ; DAG True
||w||^2 1542348658607.9392
exp ma of ||w||^2 321348625667.4029
||w|| 1241913.3055926003
exp ma of ||w|| 228000.77461413117
||w||^2 8725015458.041563
exp ma of ||w||^2 51785408251.11532
||w|| 93407.79120630978
exp ma of ||w|| 158556.38695260428
v before min max tensor([[-2.591e-03,  1.584e-03, -4.163e-04,  ..., -1.192e-04,  8.971e-04,
         -2.279e-05],
        [-2.106e-04,  3.526e-04, -7.839e-05,  ...,  3.229e-05, -3.928e-06,
          1.851e-04],
        [ 4.945e-03,  6.848e-04, -5.708e-05,  ...,  2.381e-05,  4.535e-05,
         -1.570e-05],
        ...,
        [ 2.953e-05, -2.119e-05, -8.894e-04,  ...,  4.437e-04, -1.990e-04,
         -3.180e-03],
        [-3.228e-06, -8.947e-04, -4.205e-04,  ...,  1.161e-04, -1.754e-05,
          1.427e-03],
        [ 1.706e-04, -3.775e-04, -2.858e-04,  ..., -8.240e-04, -1.211e-03,
          8.483e-03]], device='cuda:0')
v tensor([[1.000e-12, 1.584e-03, 1.000e-12,  ..., 1.000e-12, 8.971e-04,
         1.000e-12],
        [1.000e-12, 3.526e-04, 1.000e-12,  ..., 3.229e-05, 1.000e-12,
         1.851e-04],
        [4.945e-03, 6.848e-04, 1.000e-12,  ..., 2.381e-05, 4.535e-05,
         1.000e-12],
        ...,
        [2.953e-05, 1.000e-12, 1.000e-12,  ..., 4.437e-04, 1.000e-12,
         1.000e-12],
        [1.000e-12, 1.000e-12, 1.000e-12,  ..., 1.161e-04, 1.000e-12,
         1.427e-03],
        [1.706e-04, 1.000e-12, 1.000e-12,  ..., 1.000e-12, 1.000e-12,
         8.483e-03]], device='cuda:0')
v before min max tensor([-7.556e-07,  1.738e-07, -3.580e-05, -1.436e-04, -1.894e-05, -2.453e-06,
        -2.702e-05,  2.125e-05,  1.459e-04,  1.959e-03, -3.215e-06, -2.292e-06,
         2.572e-05, -1.535e-04, -1.247e-04, -3.077e-05, -7.418e-05, -1.076e-04,
         1.248e-04, -4.282e-06, -1.210e-03, -7.625e-05,  4.573e-04,  3.638e-05,
         2.835e-07, -5.363e-06, -1.412e-04, -4.595e-05, -2.274e-04, -3.733e-05,
        -2.392e-04,  1.759e-05,  1.977e-07, -4.412e-07,  2.047e-05, -1.087e-05,
        -7.722e-06, -4.364e-06,  2.020e-06, -2.118e-05,  9.622e-06, -2.000e-03,
        -5.106e-05, -7.961e-07, -1.326e-04,  1.288e-04, -3.153e-06,  1.887e-06,
        -4.259e-05, -5.102e-05, -4.356e-05, -1.906e-04, -2.204e-06,  2.705e-05,
        -9.109e-07, -1.532e-05,  9.096e-05, -3.199e-06, -3.597e-05, -4.970e-04,
         7.161e-05, -1.701e-04,  3.438e-04, -3.450e-04, -3.951e-06, -1.215e-04,
        -1.325e-04,  2.516e-04, -5.466e-08,  6.766e-04, -8.601e-07, -1.201e-05,
         1.494e-04,  7.063e-05,  1.036e-03, -5.344e-04, -1.079e-06, -2.823e-05,
        -9.331e-05, -1.743e-05, -1.297e-03,  1.454e-04, -3.456e-06, -3.301e-05,
        -3.622e-05, -1.596e-04, -1.644e-04, -3.390e-07,  2.204e-04,  1.480e-05,
        -6.345e-06,  1.354e-05, -1.732e-05, -1.619e-06, -1.416e-05,  4.187e-04,
         7.520e-05, -2.238e-05, -3.327e-05, -3.678e-05, -7.984e-05, -9.273e-06,
        -4.792e-05,  2.798e-04, -3.324e-06, -1.673e-05, -8.312e-05,  1.093e-04,
        -3.957e-04,  5.634e-07,  6.163e-05,  2.403e-04, -1.408e-05,  3.858e-06,
        -4.635e-05, -5.491e-07, -4.275e-06, -3.306e-06, -4.091e-05,  5.374e-04,
         2.629e-04, -1.683e-04, -1.563e-04, -9.152e-06, -2.917e-07, -7.042e-05,
        -2.837e-04, -9.230e-05, -2.289e-03,  8.936e-05, -4.951e-04, -2.949e-05,
        -2.799e-05,  2.164e-05,  6.217e-05, -2.440e-06, -3.718e-06, -1.803e-04,
        -5.150e-05, -2.853e-05,  1.887e-05, -4.123e-06, -1.703e-04, -2.786e-07,
        -4.012e-04, -1.340e-06, -3.135e-05, -2.924e-04, -1.197e-04, -8.805e-05,
        -2.143e-03, -1.162e-04, -4.821e-05, -5.703e-06, -2.530e-05, -2.749e-06,
         1.624e-03,  5.579e-05, -2.345e-05,  7.856e-04, -2.175e-06,  2.163e-05,
        -4.017e-05, -1.956e-04, -7.273e-05, -4.680e-06, -5.823e-04,  3.757e-04,
        -1.641e-04, -1.245e-06, -7.026e-06,  1.059e-04, -4.193e-04, -6.571e-05,
        -1.208e-04, -6.513e-07, -2.213e-04, -1.742e-04, -6.344e-06, -5.535e-05,
        -5.900e-05,  3.338e-05,  7.268e-05,  5.027e-06, -4.309e-06, -1.368e-05,
        -7.907e-05,  2.389e-03, -6.222e-05, -1.176e-05,  1.510e-03, -1.108e-04,
         7.542e-04,  3.263e-07,  4.395e-04,  1.558e-04, -1.953e-06,  2.542e-04,
        -1.454e-04, -4.559e-04], device='cuda:0')
v tensor([1.000e-12, 1.738e-07, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.125e-05, 1.459e-04, 1.959e-03, 1.000e-12, 1.000e-12,
        2.572e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.248e-04, 1.000e-12, 1.000e-12, 1.000e-12, 4.573e-04, 3.638e-05,
        2.835e-07, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.759e-05, 1.977e-07, 1.000e-12, 2.047e-05, 1.000e-12,
        1.000e-12, 1.000e-12, 2.020e-06, 1.000e-12, 9.622e-06, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.288e-04, 1.000e-12, 1.887e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.705e-05,
        1.000e-12, 1.000e-12, 9.096e-05, 1.000e-12, 1.000e-12, 1.000e-12,
        7.161e-05, 1.000e-12, 3.438e-04, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.516e-04, 1.000e-12, 6.766e-04, 1.000e-12, 1.000e-12,
        1.494e-04, 7.063e-05, 1.036e-03, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.454e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 2.204e-04, 1.480e-05,
        1.000e-12, 1.354e-05, 1.000e-12, 1.000e-12, 1.000e-12, 4.187e-04,
        7.520e-05, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 2.798e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.093e-04,
        1.000e-12, 5.634e-07, 6.163e-05, 2.403e-04, 1.000e-12, 3.858e-06,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 5.374e-04,
        2.629e-04, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 8.936e-05, 1.000e-12, 1.000e-12,
        1.000e-12, 2.164e-05, 6.217e-05, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.887e-05, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.624e-03, 5.579e-05, 1.000e-12, 7.856e-04, 1.000e-12, 2.163e-05,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 3.757e-04,
        1.000e-12, 1.000e-12, 1.000e-12, 1.059e-04, 1.000e-12, 1.000e-12,
        1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12, 1.000e-12,
        1.000e-12, 3.338e-05, 7.268e-05, 5.027e-06, 1.000e-12, 1.000e-12,
        1.000e-12, 2.389e-03, 1.000e-12, 1.000e-12, 1.510e-03, 1.000e-12,
        7.542e-04, 3.263e-07, 4.395e-04, 1.558e-04, 1.000e-12, 2.542e-04,
        1.000e-12, 1.000e-12], device='cuda:0')
v before min max tensor([[[ 9.460e-05],
         [ 7.153e-04],
         [ 5.501e-06],
         [-9.033e-05],
         [-5.231e-05],
         [ 3.354e-06],
         [ 2.363e-05],
         [ 1.679e-05],
         [-2.411e-04],
         [ 1.153e-05]],

        [[ 1.325e-06],
         [-1.566e-05],
         [-1.065e-05],
         [-6.045e-06],
         [-2.894e-05],
         [-4.462e-04],
         [-4.166e-06],
         [-5.850e-05],
         [-4.679e-05],
         [-1.647e-05]],

        [[-4.540e-04],
         [ 1.004e-06],
         [-1.315e-05],
         [-2.547e-05],
         [-5.139e-04],
         [ 4.730e-05],
         [ 1.065e-05],
         [ 1.560e-04],
         [-4.440e-05],
         [-7.387e-05]],

        [[-1.436e-05],
         [ 1.527e-05],
         [ 9.427e-04],
         [-6.929e-07],
         [-5.600e-05],
         [-1.279e-04],
         [-2.392e-05],
         [-4.488e-05],
         [-6.353e-05],
         [-8.196e-06]],

        [[-1.557e-04],
         [ 1.016e-03],
         [ 1.266e-05],
         [-4.900e-06],
         [-4.988e-05],
         [-2.109e-04],
         [-9.468e-05],
         [-9.182e-06],
         [ 6.752e-05],
         [-4.410e-05]],

        [[-3.071e-04],
         [ 1.146e-03],
         [ 1.736e-05],
         [-1.234e-05],
         [-3.369e-04],
         [-2.262e-05],
         [-8.265e-06],
         [-8.666e-06],
         [-1.987e-05],
         [ 6.515e-05]],

        [[-3.612e-05],
         [ 3.462e-04],
         [ 9.024e-06],
         [-8.277e-05],
         [-2.861e-04],
         [-9.765e-05],
         [-1.690e-06],
         [-6.263e-05],
         [-1.987e-04],
         [-2.523e-05]],

        [[-1.431e-04],
         [-5.256e-04],
         [ 3.371e-04],
         [-6.630e-05],
         [ 1.641e-05],
         [ 5.455e-04],
         [-1.511e-03],
         [-1.256e-06],
         [-6.299e-06],
         [-2.271e-05]],

        [[-9.391e-06],
         [-8.354e-06],
         [-6.797e-05],
         [-3.679e-05],
         [-6.822e-06],
         [-1.430e-06],
         [-2.584e-06],
         [-6.254e-05],
         [-4.769e-06],
         [ 1.077e-04]],

        [[-5.499e-05],
         [-7.107e-05],
         [-4.282e-05],
         [-1.235e-05],
         [-1.327e-04],
         [-2.741e-04],
         [ 1.676e-04],
         [-8.487e-05],
         [-1.993e-04],
         [-6.800e-06]],

        [[ 2.236e-05],
         [-1.043e-06],
         [ 1.432e-05],
         [-7.129e-05],
         [-2.301e-04],
         [-7.938e-04],
         [-1.989e-05],
         [-6.898e-06],
         [ 4.309e-04],
         [-1.062e-04]],

        [[-3.472e-04],
         [-8.576e-05],
         [-3.693e-04],
         [-1.141e-05],
         [-1.042e-04],
         [-1.309e-03],
         [-3.302e-04],
         [-9.342e-07],
         [-6.207e-04],
         [-4.031e-07]],

        [[ 7.577e-06],
         [-2.595e-04],
         [-3.377e-06],
         [ 1.414e-03],
         [ 4.808e-05],
         [-8.821e-07],
         [-3.986e-05],
         [ 4.855e-06],
         [ 2.659e-05],
         [-1.045e-04]],

        [[ 2.341e-06],
         [-8.309e-05],
         [ 1.673e-04],
         [-1.690e-04],
         [-2.219e-03],
         [ 3.049e-04],
         [ 1.318e-06],
         [-2.002e-05],
         [-1.100e-06],
         [-1.294e-04]],

        [[-2.507e-04],
         [ 4.984e-07],
         [-1.173e-04],
         [-6.257e-05],
         [ 7.625e-05],
         [-8.989e-06],
         [ 1.282e-04],
         [ 1.211e-04],
         [ 1.502e-05],
         [ 1.399e-04]],

        [[-2.160e-05],
         [-1.896e-05],
         [ 1.584e-05],
         [ 1.579e-04],
         [ 1.791e-05],
         [-9.139e-06],
         [-3.749e-04],
         [ 1.206e-05],
         [-2.829e-04],
         [-3.096e-05]],

        [[ 6.271e-06],
         [ 1.353e-05],
         [-2.492e-04],
         [-1.080e-04],
         [-1.281e-05],
         [-2.545e-07],
         [-5.028e-04],
         [ 3.968e-05],
         [ 1.847e-05],
         [ 5.415e-04]],

        [[ 2.571e-05],
         [ 2.154e-04],
         [-1.008e-05],
         [-1.866e-05],
         [ 4.830e-04],
         [-2.439e-04],
         [ 1.083e-04],
         [-1.143e-04],
         [-7.512e-07],
         [-7.117e-05]],

        [[-6.833e-05],
         [-5.235e-04],
         [-2.218e-04],
         [-9.381e-05],
         [-4.433e-07],
         [-4.539e-07],
         [-8.509e-05],
         [ 3.417e-05],
         [ 4.564e-04],
         [ 4.763e-05]],

        [[-4.145e-04],
         [-7.125e-05],
         [-3.839e-05],
         [-1.693e-05],
         [-2.104e-05],
         [ 2.755e-06],
         [ 4.011e-06],
         [-2.052e-04],
         [-4.881e-04],
         [ 1.155e-06]]], device='cuda:0')
v tensor([[[9.460e-05],
         [7.153e-04],
         [5.501e-06],
         [1.000e-12],
         [1.000e-12],
         [3.354e-06],
         [2.363e-05],
         [1.679e-05],
         [1.000e-12],
         [1.153e-05]],

        [[1.325e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.004e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.730e-05],
         [1.065e-05],
         [1.560e-04],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.527e-05],
         [9.427e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.016e-03],
         [1.266e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.752e-05],
         [1.000e-12]],

        [[1.000e-12],
         [1.146e-03],
         [1.736e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [6.515e-05]],

        [[1.000e-12],
         [3.462e-04],
         [9.024e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [3.371e-04],
         [1.000e-12],
         [1.641e-05],
         [5.455e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.077e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.676e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[2.236e-05],
         [1.000e-12],
         [1.432e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [4.309e-04],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[7.577e-06],
         [1.000e-12],
         [1.000e-12],
         [1.414e-03],
         [4.808e-05],
         [1.000e-12],
         [1.000e-12],
         [4.855e-06],
         [2.659e-05],
         [1.000e-12]],

        [[2.341e-06],
         [1.000e-12],
         [1.673e-04],
         [1.000e-12],
         [1.000e-12],
         [3.049e-04],
         [1.318e-06],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [4.984e-07],
         [1.000e-12],
         [1.000e-12],
         [7.625e-05],
         [1.000e-12],
         [1.282e-04],
         [1.211e-04],
         [1.502e-05],
         [1.399e-04]],

        [[1.000e-12],
         [1.000e-12],
         [1.584e-05],
         [1.579e-04],
         [1.791e-05],
         [1.000e-12],
         [1.000e-12],
         [1.206e-05],
         [1.000e-12],
         [1.000e-12]],

        [[6.271e-06],
         [1.353e-05],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.968e-05],
         [1.847e-05],
         [5.415e-04]],

        [[2.571e-05],
         [2.154e-04],
         [1.000e-12],
         [1.000e-12],
         [4.830e-04],
         [1.000e-12],
         [1.083e-04],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [3.417e-05],
         [4.564e-04],
         [4.763e-05]],

        [[1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [1.000e-12],
         [2.755e-06],
         [4.011e-06],
         [1.000e-12],
         [1.000e-12],
         [1.155e-06]]], device='cuda:0')
v before min max tensor([[-3.093e-05],
        [ 2.753e-05],
        [-3.624e-06],
        [-2.056e-05],
        [-6.092e-06],
        [-1.349e-04],
        [-8.325e-05],
        [-4.502e-07],
        [-1.015e-05],
        [-9.881e-05],
        [ 4.002e-04],
        [-8.872e-04],
        [-8.273e-06],
        [-8.498e-04],
        [-1.947e-05],
        [-2.714e-05],
        [ 4.058e-07],
        [-2.489e-05],
        [-4.332e-04],
        [-4.993e-05]], device='cuda:0')
v tensor([[1.000e-12],
        [2.753e-05],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [4.002e-04],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12],
        [4.058e-07],
        [1.000e-12],
        [1.000e-12],
        [1.000e-12]], device='cuda:0')
a after update for 1 param tensor([[-3.559e-05, -3.260e-05, -1.052e-05,  ...,  1.500e-06, -2.898e-05,
          9.162e-07],
        [-2.008e-06, -2.357e-05,  2.463e-06,  ...,  6.262e-06, -7.242e-07,
         -2.072e-05],
        [-6.060e-05,  1.869e-05, -1.148e-05,  ..., -5.956e-06,  7.559e-06,
          1.156e-06],
        ...,
        [-1.238e-05, -2.143e-05, -2.410e-05,  ...,  3.597e-05,  3.319e-06,
         -4.948e-05],
        [ 1.173e-06, -1.393e-05,  7.676e-06,  ..., -1.211e-05,  6.895e-06,
          4.050e-05],
        [-2.310e-05,  1.561e-05, -2.033e-05,  ..., -5.105e-06, -1.267e-05,
         -7.781e-05]], device='cuda:0')
s after update for 1 param tensor([[8.362e-03, 1.284e-02, 1.322e-03,  ..., 5.679e-04, 9.506e-03,
         8.791e-05],
        [2.852e-03, 5.968e-03, 2.533e-04,  ..., 1.797e-03, 1.269e-05,
         4.327e-03],
        [2.241e-02, 9.500e-03, 6.217e-04,  ..., 1.545e-03, 2.132e-03,
         5.007e-05],
        ...,
        [1.905e-03, 1.105e-03, 3.155e-03,  ..., 6.839e-03, 6.590e-04,
         1.163e-02],
        [1.095e-05, 2.911e-03, 1.341e-03,  ..., 3.408e-03, 8.424e-04,
         1.242e-02],
        [4.808e-03, 2.994e-03, 1.043e-03,  ..., 2.719e-03, 3.847e-03,
         2.928e-02]], device='cuda:0')
b after update for 1 param tensor([[0.312, 0.386, 0.124,  ..., 0.081, 0.332, 0.032],
        [0.182, 0.263, 0.054,  ..., 0.144, 0.012, 0.224],
        [0.510, 0.332, 0.085,  ..., 0.134, 0.157, 0.024],
        ...,
        [0.149, 0.113, 0.191,  ..., 0.282, 0.087, 0.367],
        [0.011, 0.184, 0.125,  ..., 0.199, 0.099, 0.380],
        [0.236, 0.186, 0.110,  ..., 0.178, 0.211, 0.583]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([ 4.220e-07,  4.421e-06,  2.700e-06, -3.959e-07,  7.400e-06, -5.914e-09,
        -5.200e-07,  8.938e-06, -7.801e-06,  6.890e-05,  2.159e-06,  1.249e-06,
        -9.143e-06,  1.604e-06,  1.175e-06,  6.259e-07,  2.533e-06, -3.350e-06,
        -1.357e-05, -3.211e-06, -9.515e-06, -9.076e-06, -3.315e-05, -1.040e-05,
         6.170e-07, -4.744e-06,  6.845e-06, -1.051e-05,  5.015e-06, -3.046e-06,
        -1.334e-06, -1.656e-05,  1.413e-06,  8.803e-08, -4.313e-06, -1.245e-06,
        -2.586e-06, -4.642e-07, -4.359e-06, -1.514e-06, -8.878e-06, -3.313e-05,
         1.560e-06, -6.689e-07,  7.818e-06,  1.786e-05, -2.437e-06, -2.963e-06,
         6.145e-06,  3.476e-06, -1.358e-05,  3.102e-06,  1.221e-06, -7.423e-06,
        -6.681e-07, -1.430e-06,  1.211e-05,  1.412e-06, -2.159e-06,  2.804e-06,
         6.421e-06,  2.443e-06, -3.316e-05,  5.331e-07, -3.579e-06,  8.283e-06,
         2.497e-05, -2.239e-05,  7.135e-07,  3.231e-05,  3.511e-06, -2.169e-06,
         2.116e-05,  9.027e-06, -4.212e-05, -9.671e-06, -1.155e-07,  9.589e-07,
         1.935e-06, -2.641e-06,  2.632e-05, -2.520e-05, -9.173e-07, -5.248e-06,
        -1.421e-06,  7.026e-06,  1.841e-06, -8.689e-07,  1.475e-05, -5.053e-05,
        -7.326e-06,  3.457e-06, -1.338e-06, -2.308e-06,  6.711e-06, -2.238e-05,
         1.084e-05,  7.558e-06,  1.049e-05, -3.925e-07,  1.379e-05, -2.683e-06,
        -1.705e-06, -2.394e-05, -1.283e-06,  3.316e-06, -6.104e-06, -1.170e-05,
         1.703e-06, -7.422e-07, -1.070e-05, -1.391e-05, -7.290e-06, -2.489e-06,
        -4.106e-06,  6.763e-07,  2.568e-06,  5.046e-06, -1.073e-05, -2.704e-05,
         1.833e-05, -8.472e-06,  1.086e-05, -1.037e-06, -6.572e-07,  1.378e-05,
        -1.774e-06,  5.036e-06, -2.748e-05,  1.753e-05,  9.897e-06,  4.326e-06,
        -2.257e-05, -4.050e-06, -1.363e-05,  6.955e-07,  1.156e-06, -8.236e-07,
        -4.184e-07, -1.667e-06, -8.970e-06, -1.003e-07,  8.690e-06,  2.314e-06,
        -1.316e-05,  5.126e-06, -6.851e-06, -1.480e-05, -1.336e-06,  2.074e-06,
         2.551e-05,  8.051e-06,  1.782e-05, -1.018e-07,  3.205e-06, -3.472e-06,
        -3.258e-05, -1.026e-05,  2.913e-05, -2.855e-05, -1.557e-06,  4.220e-06,
        -7.335e-06,  5.784e-06, -1.405e-05, -8.844e-07, -2.516e-05,  3.464e-05,
        -1.503e-05, -7.961e-07,  4.014e-06,  2.263e-05,  5.285e-06,  7.608e-06,
        -7.741e-06, -9.584e-07,  5.772e-07,  8.622e-06, -6.710e-07, -1.104e-05,
         5.224e-06,  7.661e-06,  1.136e-05,  1.925e-06, -3.180e-06, -1.965e-05,
         4.438e-06, -7.162e-05, -1.122e-06,  8.011e-07,  4.357e-05, -1.997e-05,
         3.625e-05,  7.227e-06,  4.375e-05,  1.599e-05,  4.915e-07, -2.839e-05,
        -4.220e-05, -2.940e-05], device='cuda:0')
s after update for 1 param tensor([2.663e-06, 2.074e-04, 1.144e-04, 4.932e-04, 1.111e-04, 9.320e-06,
        1.393e-04, 1.470e-03, 3.903e-03, 1.445e-02, 2.627e-05, 7.711e-06,
        1.625e-03, 4.965e-04, 4.125e-04, 1.153e-04, 2.397e-04, 4.939e-04,
        3.535e-03, 4.506e-05, 5.641e-03, 3.464e-04, 6.905e-03, 1.922e-03,
        1.684e-04, 6.274e-05, 5.129e-04, 3.968e-04, 8.287e-04, 1.392e-04,
        8.489e-04, 1.393e-03, 1.406e-04, 1.882e-06, 1.431e-03, 1.730e-04,
        4.260e-05, 1.509e-05, 4.538e-04, 6.740e-05, 1.020e-03, 6.423e-03,
        1.623e-04, 8.449e-06, 4.215e-04, 3.809e-03, 1.488e-05, 4.345e-04,
        1.446e-04, 2.698e-04, 9.853e-04, 7.575e-04, 7.193e-06, 1.646e-03,
        2.952e-06, 4.870e-05, 3.743e-03, 1.993e-05, 1.263e-04, 1.591e-03,
        2.972e-03, 5.649e-04, 6.190e-03, 1.301e-03, 2.881e-05, 9.591e-04,
        8.310e-04, 5.305e-03, 1.882e-06, 8.259e-03, 1.374e-05, 6.581e-05,
        3.874e-03, 2.662e-03, 1.024e-02, 1.742e-03, 3.952e-06, 1.065e-04,
        3.569e-04, 9.395e-05, 4.134e-03, 3.980e-03, 1.272e-05, 1.199e-04,
        1.388e-04, 5.739e-04, 6.229e-04, 2.780e-06, 4.699e-03, 3.979e-03,
        1.670e-04, 1.164e-03, 5.897e-05, 4.709e-05, 6.780e-05, 6.548e-03,
        2.745e-03, 2.354e-04, 2.234e-04, 1.216e-04, 3.652e-04, 3.775e-05,
        1.522e-04, 5.324e-03, 1.063e-05, 7.254e-05, 3.363e-04, 3.312e-03,
        1.353e-03, 2.374e-04, 2.580e-03, 4.945e-03, 2.687e-04, 6.214e-04,
        1.807e-04, 1.882e-06, 2.764e-05, 5.598e-05, 2.883e-04, 7.428e-03,
        5.145e-03, 5.678e-04, 6.658e-04, 3.714e-05, 2.268e-06, 3.130e-04,
        1.029e-03, 2.960e-04, 8.416e-03, 3.244e-03, 2.832e-03, 1.280e-04,
        8.992e-04, 1.471e-03, 2.507e-03, 5.437e-05, 1.963e-05, 5.782e-04,
        1.931e-04, 1.072e-04, 1.376e-03, 2.498e-05, 7.792e-04, 1.490e-05,
        1.388e-03, 9.372e-05, 1.982e-04, 1.229e-03, 5.239e-04, 2.952e-04,
        6.807e-03, 3.958e-04, 4.669e-04, 1.812e-05, 9.859e-05, 3.129e-05,
        1.289e-02, 2.365e-03, 9.049e-04, 8.904e-03, 1.011e-05, 1.472e-03,
        7.857e-04, 6.701e-04, 3.858e-04, 1.762e-05, 2.670e-03, 6.198e-03,
        9.762e-04, 5.230e-06, 4.759e-05, 3.339e-03, 1.399e-03, 2.608e-04,
        3.898e-04, 4.012e-06, 7.193e-04, 7.228e-04, 2.021e-05, 7.681e-04,
        2.426e-04, 1.829e-03, 2.699e-03, 7.093e-04, 4.150e-05, 1.470e-03,
        2.644e-04, 1.569e-02, 2.492e-04, 3.756e-05, 1.338e-02, 6.078e-04,
        8.709e-03, 3.136e-04, 6.754e-03, 3.993e-03, 8.793e-06, 5.157e-03,
        3.795e-03, 1.957e-03], device='cuda:0')
b after update for 1 param tensor([0.006, 0.049, 0.036, 0.076, 0.036, 0.010, 0.040, 0.131, 0.213, 0.410,
        0.017, 0.009, 0.137, 0.076, 0.069, 0.037, 0.053, 0.076, 0.203, 0.023,
        0.256, 0.063, 0.283, 0.149, 0.044, 0.027, 0.077, 0.068, 0.098, 0.040,
        0.099, 0.127, 0.040, 0.005, 0.129, 0.045, 0.022, 0.013, 0.073, 0.028,
        0.109, 0.273, 0.043, 0.010, 0.070, 0.210, 0.013, 0.071, 0.041, 0.056,
        0.107, 0.094, 0.009, 0.138, 0.006, 0.024, 0.208, 0.015, 0.038, 0.136,
        0.186, 0.081, 0.268, 0.123, 0.018, 0.106, 0.098, 0.248, 0.005, 0.310,
        0.013, 0.028, 0.212, 0.176, 0.345, 0.142, 0.007, 0.035, 0.064, 0.033,
        0.219, 0.215, 0.012, 0.037, 0.040, 0.082, 0.085, 0.006, 0.234, 0.215,
        0.044, 0.116, 0.026, 0.023, 0.028, 0.276, 0.179, 0.052, 0.051, 0.038,
        0.065, 0.021, 0.042, 0.249, 0.011, 0.029, 0.062, 0.196, 0.125, 0.052,
        0.173, 0.240, 0.056, 0.085, 0.046, 0.005, 0.018, 0.025, 0.058, 0.294,
        0.244, 0.081, 0.088, 0.021, 0.005, 0.060, 0.109, 0.059, 0.313, 0.194,
        0.181, 0.039, 0.102, 0.131, 0.171, 0.025, 0.015, 0.082, 0.047, 0.035,
        0.126, 0.017, 0.095, 0.013, 0.127, 0.033, 0.048, 0.119, 0.078, 0.059,
        0.281, 0.068, 0.074, 0.015, 0.034, 0.019, 0.387, 0.166, 0.103, 0.322,
        0.011, 0.131, 0.096, 0.088, 0.067, 0.014, 0.176, 0.268, 0.106, 0.008,
        0.024, 0.197, 0.127, 0.055, 0.067, 0.007, 0.091, 0.092, 0.015, 0.094,
        0.053, 0.146, 0.177, 0.091, 0.022, 0.131, 0.055, 0.427, 0.054, 0.021,
        0.394, 0.084, 0.318, 0.060, 0.280, 0.215, 0.010, 0.245, 0.210, 0.151],
       device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[[ 2.636e-05],
         [-2.923e-05],
         [ 4.453e-06],
         [ 7.984e-06],
         [-3.605e-06],
         [ 3.605e-06],
         [ 1.129e-05],
         [ 4.373e-06],
         [-7.481e-06],
         [-3.082e-06]],

        [[ 3.588e-06],
         [ 3.820e-06],
         [ 1.800e-06],
         [-4.174e-06],
         [ 8.184e-06],
         [ 2.161e-05],
         [ 4.230e-07],
         [ 9.862e-09],
         [ 5.879e-06],
         [ 1.183e-05]],

        [[ 1.802e-06],
         [-3.928e-06],
         [-4.528e-06],
         [-1.202e-05],
         [ 8.847e-06],
         [ 8.825e-06],
         [ 1.950e-06],
         [-3.409e-05],
         [ 1.772e-06],
         [-3.770e-06]],

        [[-1.124e-06],
         [ 8.294e-06],
         [ 3.242e-05],
         [-6.017e-07],
         [ 1.832e-06],
         [ 6.512e-06],
         [ 7.477e-07],
         [ 5.865e-06],
         [ 6.968e-07],
         [ 1.140e-06]],

        [[-9.808e-06],
         [ 3.184e-05],
         [ 3.037e-06],
         [-2.361e-06],
         [ 1.123e-07],
         [-7.495e-06],
         [-2.363e-06],
         [ 3.477e-06],
         [-1.002e-05],
         [ 5.408e-06]],

        [[ 1.172e-05],
         [-4.936e-05],
         [ 4.474e-06],
         [ 2.346e-06],
         [-1.022e-05],
         [ 6.894e-06],
         [-2.858e-06],
         [-4.304e-06],
         [ 3.332e-06],
         [ 1.216e-05]],

        [[ 9.905e-06],
         [ 2.367e-05],
         [-1.149e-05],
         [ 2.515e-06],
         [-1.189e-05],
         [ 3.688e-07],
         [ 2.320e-08],
         [ 5.631e-06],
         [-1.367e-05],
         [-2.934e-06]],

        [[-1.849e-06],
         [-1.321e-06],
         [ 2.340e-05],
         [ 1.199e-05],
         [-7.997e-06],
         [-2.370e-05],
         [-2.581e-05],
         [ 3.225e-07],
         [-3.774e-07],
         [ 1.272e-07]],

        [[-3.199e-06],
         [ 6.250e-08],
         [-1.962e-05],
         [-3.364e-06],
         [ 1.006e-06],
         [ 1.690e-06],
         [-7.228e-07],
         [ 2.090e-06],
         [ 2.898e-07],
         [ 9.930e-06]],

        [[-8.500e-07],
         [-5.323e-06],
         [ 2.307e-06],
         [ 3.050e-06],
         [-5.321e-06],
         [ 4.417e-06],
         [-2.352e-05],
         [-1.086e-05],
         [-1.143e-05],
         [ 7.256e-07]],

        [[-5.673e-06],
         [ 9.347e-07],
         [-9.565e-06],
         [-8.685e-06],
         [-9.834e-06],
         [ 7.413e-06],
         [ 4.873e-06],
         [-3.179e-06],
         [-3.200e-05],
         [ 3.565e-06]],

        [[ 3.132e-05],
         [ 3.245e-05],
         [-3.717e-06],
         [-6.421e-06],
         [ 9.556e-06],
         [ 2.772e-05],
         [-1.496e-06],
         [-1.452e-06],
         [-2.253e-05],
         [ 3.157e-07]],

        [[ 3.420e-06],
         [ 9.786e-06],
         [ 4.814e-07],
         [ 3.303e-05],
         [ 7.446e-06],
         [ 5.608e-07],
         [-1.526e-06],
         [-9.360e-06],
         [ 3.856e-06],
         [-7.441e-06]],

        [[-7.479e-06],
         [ 1.411e-05],
         [ 1.729e-05],
         [-1.891e-05],
         [-1.417e-05],
         [ 3.387e-05],
         [ 4.361e-06],
         [-3.704e-06],
         [-5.358e-07],
         [-4.139e-06]],

        [[ 9.738e-06],
         [ 2.774e-07],
         [ 5.185e-06],
         [ 1.241e-06],
         [ 7.216e-06],
         [-1.566e-05],
         [ 7.542e-06],
         [-1.544e-05],
         [-9.235e-06],
         [-3.465e-05]],

        [[-8.696e-06],
         [-5.028e-06],
         [ 5.760e-06],
         [ 9.670e-06],
         [-4.701e-06],
         [-6.600e-07],
         [ 4.897e-06],
         [-3.089e-06],
         [-2.952e-05],
         [-8.743e-07]],

        [[ 1.671e-06],
         [-3.955e-06],
         [-2.816e-05],
         [-1.156e-05],
         [-7.354e-08],
         [ 1.939e-06],
         [ 2.281e-05],
         [-1.842e-05],
         [-5.960e-06],
         [-2.492e-05]],

        [[-5.678e-06],
         [-1.104e-05],
         [ 3.925e-06],
         [ 1.449e-06],
         [ 2.349e-05],
         [-3.100e-06],
         [ 2.236e-05],
         [-3.264e-06],
         [ 1.058e-07],
         [ 8.276e-06]],

        [[ 4.500e-06],
         [-2.801e-06],
         [ 8.303e-07],
         [ 4.229e-06],
         [ 2.728e-07],
         [-7.700e-07],
         [-1.732e-06],
         [-1.000e-05],
         [-2.593e-05],
         [ 1.526e-05]],

        [[ 2.985e-06],
         [ 1.123e-05],
         [ 2.940e-06],
         [-3.413e-06],
         [-2.282e-06],
         [ 6.827e-06],
         [ 1.768e-06],
         [-2.692e-06],
         [-3.246e-06],
         [ 6.349e-06]]], device='cuda:0')
s after update for 1 param tensor([[[3.248e-03],
         [8.494e-03],
         [7.429e-04],
         [3.770e-04],
         [1.740e-04],
         [5.792e-04],
         [1.555e-03],
         [1.296e-03],
         [8.060e-04],
         [1.074e-03]],

        [[3.743e-04],
         [1.198e-04],
         [7.376e-05],
         [2.952e-05],
         [1.672e-04],
         [1.711e-03],
         [2.206e-05],
         [1.964e-04],
         [2.061e-04],
         [1.000e-03]],

        [[1.772e-03],
         [3.223e-04],
         [1.169e-04],
         [2.062e-04],
         [1.639e-03],
         [2.176e-03],
         [1.032e-03],
         [4.096e-03],
         [1.411e-04],
         [2.347e-04]],

        [[1.282e-04],
         [1.246e-03],
         [9.820e-03],
         [3.087e-06],
         [1.783e-04],
         [4.196e-04],
         [7.698e-05],
         [1.593e-04],
         [2.026e-04],
         [4.231e-05]],

        [[1.147e-03],
         [1.013e-02],
         [1.125e-03],
         [2.265e-05],
         [1.590e-04],
         [9.399e-04],
         [3.230e-04],
         [4.544e-05],
         [2.641e-03],
         [1.824e-04]],

        [[2.081e-03],
         [1.092e-02],
         [1.318e-03],
         [3.982e-05],
         [1.275e-03],
         [3.861e-04],
         [2.953e-05],
         [7.501e-05],
         [8.757e-05],
         [2.558e-03]],

        [[2.927e-04],
         [5.906e-03],
         [9.671e-04],
         [3.025e-04],
         [9.091e-04],
         [3.115e-04],
         [5.398e-06],
         [2.115e-04],
         [1.350e-03],
         [3.326e-04]],

        [[4.547e-04],
         [1.715e-03],
         [5.945e-03],
         [4.646e-04],
         [1.288e-03],
         [7.430e-03],
         [4.816e-03],
         [1.059e-05],
         [2.026e-05],
         [8.073e-05]],

        [[3.878e-05],
         [2.861e-05],
         [9.691e-04],
         [1.392e-04],
         [2.196e-05],
         [5.392e-06],
         [1.514e-05],
         [2.116e-04],
         [1.703e-05],
         [3.289e-03]],

        [[1.893e-04],
         [2.262e-04],
         [1.823e-04],
         [4.122e-05],
         [4.357e-04],
         [8.708e-04],
         [4.179e-03],
         [8.267e-04],
         [6.683e-04],
         [2.197e-05]],

        [[1.496e-03],
         [4.988e-06],
         [1.218e-03],
         [5.322e-04],
         [1.829e-03],
         [3.697e-03],
         [1.204e-04],
         [2.280e-05],
         [6.591e-03],
         [6.037e-04]],

        [[1.426e-03],
         [1.075e-03],
         [1.327e-03],
         [6.015e-04],
         [8.022e-04],
         [4.159e-03],
         [1.111e-03],
         [3.057e-05],
         [2.700e-03],
         [1.984e-06]],

        [[8.705e-04],
         [8.793e-04],
         [1.187e-05],
         [1.197e-02],
         [2.209e-03],
         [3.219e-06],
         [1.445e-04],
         [8.951e-04],
         [1.635e-03],
         [1.216e-03]],

        [[4.934e-04],
         [5.259e-04],
         [4.097e-03],
         [1.200e-03],
         [7.288e-03],
         [5.713e-03],
         [4.376e-04],
         [9.275e-05],
         [3.740e-06],
         [4.362e-04]],

        [[8.222e-04],
         [2.233e-04],
         [3.763e-04],
         [2.268e-04],
         [2.763e-03],
         [3.728e-04],
         [3.611e-03],
         [3.487e-03],
         [1.229e-03],
         [4.477e-03]],

        [[1.518e-04],
         [6.982e-05],
         [1.259e-03],
         [4.017e-03],
         [1.338e-03],
         [3.529e-05],
         [1.198e-03],
         [1.098e-03],
         [2.321e-03],
         [1.057e-04]],

        [[7.935e-04],
         [1.163e-03],
         [2.040e-03],
         [5.259e-04],
         [4.196e-05],
         [6.771e-06],
         [1.916e-03],
         [2.663e-03],
         [1.359e-03],
         [7.469e-03]],

        [[1.604e-03],
         [4.696e-03],
         [9.318e-05],
         [6.175e-05],
         [6.989e-03],
         [7.947e-04],
         [3.307e-03],
         [4.169e-04],
         [2.492e-06],
         [3.750e-04]],

        [[2.375e-04],
         [1.899e-03],
         [7.193e-04],
         [4.966e-04],
         [1.882e-06],
         [1.882e-06],
         [2.731e-04],
         [1.850e-03],
         [6.886e-03],
         [2.368e-03]],

        [[2.170e-03],
         [2.556e-04],
         [1.272e-04],
         [6.783e-05],
         [7.216e-05],
         [5.329e-04],
         [6.333e-04],
         [1.054e-03],
         [1.551e-03],
         [3.452e-04]]], device='cuda:0')
b after update for 1 param tensor([[[0.194],
         [0.314],
         [0.093],
         [0.066],
         [0.045],
         [0.082],
         [0.134],
         [0.123],
         [0.097],
         [0.112]],

        [[0.066],
         [0.037],
         [0.029],
         [0.019],
         [0.044],
         [0.141],
         [0.016],
         [0.048],
         [0.049],
         [0.108]],

        [[0.143],
         [0.061],
         [0.037],
         [0.049],
         [0.138],
         [0.159],
         [0.109],
         [0.218],
         [0.040],
         [0.052]],

        [[0.039],
         [0.120],
         [0.338],
         [0.006],
         [0.046],
         [0.070],
         [0.030],
         [0.043],
         [0.048],
         [0.022]],

        [[0.115],
         [0.343],
         [0.114],
         [0.016],
         [0.043],
         [0.104],
         [0.061],
         [0.023],
         [0.175],
         [0.046]],

        [[0.155],
         [0.356],
         [0.124],
         [0.022],
         [0.122],
         [0.067],
         [0.019],
         [0.030],
         [0.032],
         [0.172]],

        [[0.058],
         [0.262],
         [0.106],
         [0.059],
         [0.103],
         [0.060],
         [0.008],
         [0.050],
         [0.125],
         [0.062]],

        [[0.073],
         [0.141],
         [0.263],
         [0.073],
         [0.122],
         [0.294],
         [0.236],
         [0.011],
         [0.015],
         [0.031]],

        [[0.021],
         [0.018],
         [0.106],
         [0.040],
         [0.016],
         [0.008],
         [0.013],
         [0.050],
         [0.014],
         [0.195]],

        [[0.047],
         [0.051],
         [0.046],
         [0.022],
         [0.071],
         [0.101],
         [0.220],
         [0.098],
         [0.088],
         [0.016]],

        [[0.132],
         [0.008],
         [0.119],
         [0.079],
         [0.146],
         [0.207],
         [0.037],
         [0.016],
         [0.277],
         [0.084]],

        [[0.129],
         [0.112],
         [0.124],
         [0.084],
         [0.097],
         [0.220],
         [0.114],
         [0.019],
         [0.177],
         [0.005]],

        [[0.101],
         [0.101],
         [0.012],
         [0.373],
         [0.160],
         [0.006],
         [0.041],
         [0.102],
         [0.138],
         [0.119]],

        [[0.076],
         [0.078],
         [0.218],
         [0.118],
         [0.291],
         [0.258],
         [0.071],
         [0.033],
         [0.007],
         [0.071]],

        [[0.098],
         [0.051],
         [0.066],
         [0.051],
         [0.179],
         [0.066],
         [0.205],
         [0.201],
         [0.119],
         [0.228]],

        [[0.042],
         [0.028],
         [0.121],
         [0.216],
         [0.125],
         [0.020],
         [0.118],
         [0.113],
         [0.164],
         [0.035]],

        [[0.096],
         [0.116],
         [0.154],
         [0.078],
         [0.022],
         [0.009],
         [0.149],
         [0.176],
         [0.126],
         [0.294]],

        [[0.136],
         [0.233],
         [0.033],
         [0.027],
         [0.285],
         [0.096],
         [0.196],
         [0.070],
         [0.005],
         [0.066]],

        [[0.053],
         [0.148],
         [0.091],
         [0.076],
         [0.005],
         [0.005],
         [0.056],
         [0.147],
         [0.283],
         [0.166]],

        [[0.159],
         [0.054],
         [0.038],
         [0.028],
         [0.029],
         [0.079],
         [0.086],
         [0.111],
         [0.134],
         [0.063]]], device='cuda:0')
clipping threshold 2.8909198392912656
a after update for 1 param tensor([[ 1.949e-06],
        [ 9.211e-06],
        [ 6.138e-06],
        [ 6.713e-06],
        [ 2.439e-06],
        [-4.321e-06],
        [ 2.190e-06],
        [ 3.163e-07],
        [ 6.230e-06],
        [ 1.629e-06],
        [ 1.695e-05],
        [ 2.432e-05],
        [-1.062e-06],
        [ 8.680e-06],
        [-3.132e-06],
        [-1.085e-06],
        [-2.114e-06],
        [-4.510e-06],
        [-1.378e-05],
        [ 1.040e-05]], device='cuda:0')
s after update for 1 param tensor([[1.542e-04],
        [1.959e-03],
        [5.319e-05],
        [7.824e-05],
        [2.577e-05],
        [4.290e-04],
        [3.015e-04],
        [1.882e-06],
        [4.724e-04],
        [1.068e-03],
        [6.329e-03],
        [3.823e-03],
        [2.926e-05],
        [3.124e-03],
        [6.551e-05],
        [8.637e-05],
        [2.032e-04],
        [1.626e-04],
        [1.523e-03],
        [2.235e-03]], device='cuda:0')
b after update for 1 param tensor([[0.042],
        [0.151],
        [0.025],
        [0.030],
        [0.017],
        [0.071],
        [0.059],
        [0.005],
        [0.074],
        [0.111],
        [0.271],
        [0.211],
        [0.018],
        [0.190],
        [0.028],
        [0.032],
        [0.049],
        [0.043],
        [0.133],
        [0.161]], device='cuda:0')
clipping threshold 2.8909198392912656
||w||^2 4.846210865843199
exp ma of ||w||^2 269.6224458251755
||w|| 2.2014111078676786
exp ma of ||w|| 3.6682532849966054
||w||^2 3.4349643320609426
exp ma of ||w||^2 4.501649228379513
||w|| 1.853365676832541
exp ma of ||w|| 2.0336690446952406
||w||^2 2.333492310303117
exp ma of ||w||^2 3.5263040458965857
||w|| 1.527577268194024
exp ma of ||w|| 1.8203232452310931
cuda
Objective function 52.13 = squared loss an data 42.88 + 0.5*rho*h**2 4.362471 + alpha*h 3.067881 + L2reg 1.71 + L1reg 0.11 ; SHD = 66 ; DAG True
Proportion of microbatches that were clipped  0.7886957934058795
iteration 2 in inner loop, alpha 328.4409208424218 rho 100000.0 h 0.009340739397089237
iteration 4 in outer loop, alpha = 9669.18031793166, rho = 1000000.0, h = 0.009340739397089237
Threshold 0.3
[[0.004 1.233 0.153 0.138 0.313 0.34  0.238 0.068 0.016 0.155 0.115 0.091
  0.159 0.182 0.212 0.271 0.161 0.165 0.05  0.044]
 [0.005 0.007 0.074 0.073 0.158 0.473 0.567 0.013 0.03  0.04  0.028 0.043
  0.057 0.056 0.074 0.073 0.124 0.104 0.048 0.032]
 [0.046 0.059 0.005 0.094 0.235 0.165 0.266 0.073 0.035 0.151 0.114 0.074
  0.172 0.035 0.159 0.181 0.106 0.059 0.022 0.089]
 [0.072 0.081 0.058 0.006 0.327 0.138 0.108 0.136 0.047 0.094 0.031 0.093
  0.064 0.128 0.065 0.079 0.105 0.094 0.093 0.02 ]
 [0.013 0.027 0.014 0.02  0.005 0.048 0.062 0.055 0.018 0.023 0.038 0.087
  0.065 0.124 0.129 0.071 0.051 0.022 0.026 0.027]
 [0.011 0.015 0.039 0.032 0.101 0.005 0.152 0.02  0.019 0.026 0.051 0.068
  0.049 0.042 0.317 0.092 0.917 0.071 0.103 0.113]
 [0.007 0.015 0.018 0.082 0.088 0.054 0.004 0.023 0.013 0.038 0.022 0.079
  0.092 0.024 0.186 0.068 0.035 0.079 0.047 0.01 ]
 [0.111 0.37  0.088 0.058 0.099 0.179 0.117 0.003 0.058 0.052 0.061 0.244
  0.11  0.126 0.18  0.16  0.254 0.145 0.1   0.052]
 [0.193 0.132 0.14  0.133 0.381 0.34  0.38  0.102 0.006 0.203 0.123 0.13
  0.075 0.154 0.164 0.289 0.491 0.295 0.131 0.061]
 [0.047 0.153 0.033 0.064 0.202 0.214 0.187 0.081 0.03  0.007 0.034 0.082
  0.135 0.13  0.092 0.185 0.173 0.051 0.043 0.052]
 [0.054 0.2   0.061 0.142 0.201 0.149 0.339 0.078 0.078 0.2   0.004 0.241
  0.46  0.152 0.186 0.178 0.116 0.047 0.084 0.027]
 [0.055 0.074 0.052 0.064 0.077 0.089 0.081 0.013 0.052 0.114 0.029 0.004
  0.047 0.091 0.033 0.095 0.062 0.044 0.047 0.041]
 [0.064 0.099 0.022 0.091 0.096 0.136 0.069 0.061 0.101 0.057 0.011 0.077
  0.005 0.052 0.064 0.148 0.133 0.092 0.066 0.025]
 [0.017 0.07  0.105 0.052 0.049 0.128 0.236 0.032 0.043 0.035 0.036 0.078
  0.081 0.004 0.109 0.093 0.121 0.048 0.044 0.05 ]
 [0.022 0.052 0.039 0.061 0.05  0.017 0.049 0.034 0.031 0.039 0.044 0.072
  0.07  0.045 0.005 0.233 0.016 0.036 0.056 0.056]
 [0.034 0.113 0.045 0.072 0.091 0.054 0.091 0.033 0.025 0.041 0.029 0.066
  0.043 0.053 0.027 0.006 0.021 0.031 0.007 0.009]
 [0.019 0.027 0.044 0.034 0.098 0.006 0.143 0.029 0.009 0.029 0.048 0.073
  0.05  0.032 0.47  0.352 0.005 0.032 0.148 0.038]
 [0.034 0.055 0.097 0.078 0.343 0.09  0.109 0.037 0.024 0.135 0.126 0.101
  0.063 0.078 0.14  0.134 0.092 0.003 0.058 0.06 ]
 [0.117 0.17  0.202 0.052 0.159 0.047 0.119 0.079 0.037 0.135 0.094 0.132
  0.045 0.206 0.106 0.916 0.06  0.084 0.006 0.119]
 [0.195 0.298 0.089 0.379 0.307 0.073 0.394 0.137 0.084 0.144 0.267 0.19
  0.245 0.098 0.084 0.645 0.156 0.144 0.082 0.004]]
[[0.    1.233 0.    0.    0.313 0.34  0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.473 0.567 0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.327 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.317 0.    0.917 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.381 0.34  0.38  0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.491 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.339 0.    0.    0.    0.    0.
  0.46  0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.47  0.352 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.343 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.    0.916 0.    0.    0.    0.   ]
 [0.    0.    0.    0.379 0.307 0.    0.394 0.    0.    0.    0.    0.
  0.    0.    0.    0.645 0.    0.    0.    0.   ]]
{'fdr': 0.2608695652173913, 'tpr': 0.2125, 'fpr': 0.05454545454545454, 'f1': 0.33009708737864074, 'shd': 66, 'npred': 23, 'ntrue': 80}
[1.233 0.153 0.138 0.313 0.34  0.238 0.068 0.016 0.155 0.115 0.091 0.159
 0.182 0.212 0.271 0.161 0.165 0.05  0.044 0.005 0.074 0.073 0.158 0.473
 0.567 0.013 0.03  0.04  0.028 0.043 0.057 0.056 0.074 0.073 0.124 0.104
 0.048 0.032 0.046 0.059 0.094 0.235 0.165 0.266 0.073 0.035 0.151 0.114
 0.074 0.172 0.035 0.159 0.181 0.106 0.059 0.022 0.089 0.072 0.081 0.058
 0.327 0.138 0.108 0.136 0.047 0.094 0.031 0.093 0.064 0.128 0.065 0.079
 0.105 0.094 0.093 0.02  0.013 0.027 0.014 0.02  0.048 0.062 0.055 0.018
 0.023 0.038 0.087 0.065 0.124 0.129 0.071 0.051 0.022 0.026 0.027 0.011
 0.015 0.039 0.032 0.101 0.152 0.02  0.019 0.026 0.051 0.068 0.049 0.042
 0.317 0.092 0.917 0.071 0.103 0.113 0.007 0.015 0.018 0.082 0.088 0.054
 0.023 0.013 0.038 0.022 0.079 0.092 0.024 0.186 0.068 0.035 0.079 0.047
 0.01  0.111 0.37  0.088 0.058 0.099 0.179 0.117 0.058 0.052 0.061 0.244
 0.11  0.126 0.18  0.16  0.254 0.145 0.1   0.052 0.193 0.132 0.14  0.133
 0.381 0.34  0.38  0.102 0.203 0.123 0.13  0.075 0.154 0.164 0.289 0.491
 0.295 0.131 0.061 0.047 0.153 0.033 0.064 0.202 0.214 0.187 0.081 0.03
 0.034 0.082 0.135 0.13  0.092 0.185 0.173 0.051 0.043 0.052 0.054 0.2
 0.061 0.142 0.201 0.149 0.339 0.078 0.078 0.2   0.241 0.46  0.152 0.186
 0.178 0.116 0.047 0.084 0.027 0.055 0.074 0.052 0.064 0.077 0.089 0.081
 0.013 0.052 0.114 0.029 0.047 0.091 0.033 0.095 0.062 0.044 0.047 0.041
 0.064 0.099 0.022 0.091 0.096 0.136 0.069 0.061 0.101 0.057 0.011 0.077
 0.052 0.064 0.148 0.133 0.092 0.066 0.025 0.017 0.07  0.105 0.052 0.049
 0.128 0.236 0.032 0.043 0.035 0.036 0.078 0.081 0.109 0.093 0.121 0.048
 0.044 0.05  0.022 0.052 0.039 0.061 0.05  0.017 0.049 0.034 0.031 0.039
 0.044 0.072 0.07  0.045 0.233 0.016 0.036 0.056 0.056 0.034 0.113 0.045
 0.072 0.091 0.054 0.091 0.033 0.025 0.041 0.029 0.066 0.043 0.053 0.027
 0.021 0.031 0.007 0.009 0.019 0.027 0.044 0.034 0.098 0.006 0.143 0.029
 0.009 0.029 0.048 0.073 0.05  0.032 0.47  0.352 0.032 0.148 0.038 0.034
 0.055 0.097 0.078 0.343 0.09  0.109 0.037 0.024 0.135 0.126 0.101 0.063
 0.078 0.14  0.134 0.092 0.058 0.06  0.117 0.17  0.202 0.052 0.159 0.047
 0.119 0.079 0.037 0.135 0.094 0.132 0.045 0.206 0.106 0.916 0.06  0.084
 0.119 0.195 0.298 0.089 0.379 0.307 0.073 0.394 0.137 0.084 0.144 0.267
 0.19  0.245 0.098 0.084 0.645 0.156 0.144 0.082]
[[0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
aucroc, aucpr (0.6679583333333332, 0.45022861667442327)
Iterations 567
Achieves (6.423795639751447, 1e-05)-DP
