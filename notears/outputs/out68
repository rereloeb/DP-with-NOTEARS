samples  5000  graph  15 30 ER mlp  minibatch size  75  noise  0.5  minibatches per NN training  111 quantile adaptive clipping
cuda
cuda
iteration 1 in inner loop,alpha 0.0 rho 1.0 h 1.3401316691164595
iteration 1 in outer loop, alpha = 1.3401316691164595, rho = 1.0, h = 1.3401316691164595
cuda
iteration 1 in inner loop,alpha 1.3401316691164595 rho 1.0 h 0.8741315174292428
iteration 2 in inner loop,alpha 1.3401316691164595 rho 10.0 h 0.38347274229216843
iteration 3 in inner loop,alpha 1.3401316691164595 rho 100.0 h 0.13357487921889089
iteration 2 in outer loop, alpha = 14.697619591005548, rho = 100.0, h = 0.13357487921889089
cuda
iteration 1 in inner loop,alpha 14.697619591005548 rho 100.0 h 0.06991993740608571
iteration 2 in inner loop,alpha 14.697619591005548 rho 1000.0 h 0.02592698775515956
iteration 3 in outer loop, alpha = 40.62460734616511, rho = 1000.0, h = 0.02592698775515956
cuda
iteration 1 in inner loop,alpha 40.62460734616511 rho 1000.0 h 0.016465661543334065
iteration 2 in inner loop,alpha 40.62460734616511 rho 10000.0 h 0.006583550827196305
iteration 3 in inner loop,alpha 40.62460734616511 rho 100000.0 h 0.0011537970975332712
iteration 4 in outer loop, alpha = 156.00431709949223, rho = 100000.0, h = 0.0011537970975332712
cuda
iteration 1 in inner loop,alpha 156.00431709949223 rho 100000.0 h 0.00033575511004180214
iteration 5 in outer loop, alpha = 491.7594271412944, rho = 1000000.0, h = 0.00033575511004180214
Threshold 0.3
[[0.002 0.    0.    0.005 0.194 0.    0.    0.    0.    2.734 0.179 0.001
  0.001 0.    0.006]
 [2.52  0.001 0.002 0.262 0.215 0.01  0.01  0.001 0.002 2.189 0.199 1.342
  0.001 0.001 0.098]
 [0.027 0.068 0.001 0.147 0.166 0.035 0.137 0.001 0.067 0.125 0.135 0.162
  0.001 0.001 0.006]
 [0.153 0.    0.001 0.001 0.204 0.    0.    0.    0.    0.214 0.335 0.001
  0.    0.    1.022]
 [0.    0.    0.    0.    0.004 0.    0.    0.    0.    0.    0.002 0.
  0.    0.    0.001]
 [2.646 0.03  0.001 0.968 0.376 0.001 1.377 0.    0.055 1.746 0.956 1.472
  0.012 0.002 1.923]
 [0.414 0.044 0.    0.336 0.408 0.    0.001 0.    0.059 0.139 1.162 0.096
  0.001 0.    0.163]
 [2.69  0.002 0.    2.092 0.137 0.612 0.046 0.    0.003 0.313 0.139 0.061
  0.014 0.022 0.118]
 [0.103 0.383 0.002 0.638 0.415 0.002 0.001 0.001 0.003 2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.001 0.    0.    0.002 0.402 0.001
  0.    0.    0.002]
 [0.001 0.    0.    0.001 3.057 0.    0.001 0.    0.001 0.001 0.015 0.
  0.    0.    0.005]
 [0.37  0.    0.    0.18  0.153 0.    0.004 0.    0.002 0.427 0.58  0.001
  0.    0.001 0.032]
 [0.041 0.078 0.    0.112 3.047 0.017 0.008 0.    1.212 0.107 1.585 0.031
  0.001 0.016 0.021]
 [0.126 1.398 0.    1.127 1.231 0.003 0.006 0.    3.368 0.386 0.434 0.158
  0.011 0.002 0.562]
 [0.089 0.    0.    0.    0.111 0.    0.    0.    0.    0.106 0.081 0.002
  0.004 0.    0.001]]
[[0.    0.    0.    0.    0.    0.    0.    0.    0.    2.734 0.    0.
  0.    0.    0.   ]
 [2.52  0.    0.    0.    0.    0.    0.    0.    0.    2.189 0.    1.342
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.335 0.
  0.    0.    1.022]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [2.646 0.    0.    0.968 0.376 0.    1.377 0.    0.    1.746 0.956 1.472
  0.    0.    1.923]
 [0.414 0.    0.    0.336 0.408 0.    0.    0.    0.    0.    1.162 0.
  0.    0.    0.   ]
 [2.69  0.    0.    2.092 0.    0.612 0.    0.    0.    0.313 0.    0.
  0.    0.    0.   ]
 [0.    0.383 0.    0.638 0.415 0.    0.    0.    0.    2.308 1.722 1.03
  0.    0.    0.367]
 [0.    0.    0.    0.    2.722 0.    0.    0.    0.    0.    0.402 0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.057 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.37  0.    0.    0.    0.    0.    0.    0.    0.    0.427 0.58  0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    3.047 0.    0.    0.    1.212 0.    1.585 0.
  0.    0.    0.   ]
 [0.    1.398 0.    1.127 1.231 0.    0.    0.    3.368 0.386 0.434 0.
  0.    0.    0.562]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.35555555555555557, 'tpr': 0.9666666666666667, 'fpr': 0.21333333333333335, 'f1': 0.7733333333333333, 'shd': 17, 'npred': 45, 'ntrue': 30}
[7.166e-05 3.031e-04 4.737e-03 1.942e-01 6.655e-05 2.207e-04 4.993e-05
 2.937e-04 2.734e+00 1.793e-01 5.105e-04 1.050e-03 1.242e-04 5.740e-03
 2.520e+00 1.954e-03 2.620e-01 2.147e-01 9.724e-03 9.801e-03 9.328e-04
 1.620e-03 2.189e+00 1.994e-01 1.342e+00 5.488e-04 9.886e-04 9.781e-02
 2.679e-02 6.849e-02 1.472e-01 1.660e-01 3.508e-02 1.368e-01 5.716e-04
 6.687e-02 1.254e-01 1.352e-01 1.619e-01 8.477e-04 1.272e-03 6.092e-03
 1.532e-01 1.509e-04 9.882e-04 2.036e-01 4.044e-05 3.897e-04 2.489e-04
 6.858e-05 2.135e-01 3.353e-01 1.155e-03 4.106e-04 1.293e-04 1.022e+00
 6.064e-05 4.791e-06 5.172e-05 2.471e-04 1.136e-05 1.928e-04 4.191e-06
 2.163e-04 3.383e-04 1.833e-03 1.007e-04 2.631e-05 7.614e-05 1.086e-03
 2.646e+00 3.009e-02 1.478e-03 9.684e-01 3.765e-01 1.377e+00 1.229e-04
 5.513e-02 1.746e+00 9.560e-01 1.472e+00 1.180e-02 1.838e-03 1.923e+00
 4.141e-01 4.437e-02 3.959e-04 3.360e-01 4.079e-01 4.254e-04 5.935e-05
 5.872e-02 1.392e-01 1.162e+00 9.588e-02 6.731e-04 4.063e-04 1.632e-01
 2.690e+00 2.126e-03 4.852e-04 2.092e+00 1.369e-01 6.121e-01 4.646e-02
 3.330e-03 3.125e-01 1.385e-01 6.062e-02 1.448e-02 2.181e-02 1.178e-01
 1.034e-01 3.829e-01 2.017e-03 6.383e-01 4.147e-01 2.074e-03 1.394e-03
 1.328e-03 2.308e+00 1.722e+00 1.030e+00 5.492e-05 3.972e-04 3.674e-01
 1.834e-04 4.607e-06 8.949e-05 4.394e-04 2.722e+00 6.113e-06 8.992e-04
 1.147e-05 4.538e-04 4.020e-01 1.020e-03 1.265e-04 1.194e-04 2.179e-03
 6.634e-04 5.288e-05 3.430e-04 1.176e-03 3.057e+00 1.252e-04 7.885e-04
 1.415e-04 9.900e-04 8.589e-04 1.728e-04 2.153e-04 3.103e-04 5.194e-03
 3.696e-01 4.210e-04 8.175e-05 1.805e-01 1.529e-01 2.371e-04 3.593e-03
 6.136e-05 1.729e-03 4.267e-01 5.799e-01 2.072e-04 6.430e-04 3.169e-02
 4.077e-02 7.764e-02 2.020e-04 1.125e-01 3.047e+00 1.651e-02 8.033e-03
 3.454e-04 1.212e+00 1.069e-01 1.585e+00 3.144e-02 1.628e-02 2.119e-02
 1.264e-01 1.398e+00 2.136e-04 1.127e+00 1.231e+00 3.196e-03 6.293e-03
 3.254e-04 3.368e+00 3.864e-01 4.339e-01 1.576e-01 1.130e-02 5.624e-01
 8.923e-02 3.157e-04 3.306e-04 7.760e-05 1.109e-01 7.500e-05 1.536e-04
 2.248e-04 2.530e-04 1.060e-01 8.126e-02 2.152e-03 4.216e-03 1.418e-04]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.9805555555555556, 0.9709424613206593)
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 454.76 = squared loss an data 233.99 + 0.5*rho*h**2 220.188202 + alpha*h 0.000000 + L2reg 0.28 + L1reg 0.30 ; SHD = 120 ; DAG False
total norm for a microbatch 39.29498155563945 clip 11.646765979092327
total norm for a microbatch 40.49947986685788 clip 12.832099025143812
total norm for a microbatch 35.221256738780546 clip 21.86182732016133
total norm for a microbatch 24.020925744912866 clip 31.267054388002112
total norm for a microbatch 39.96408225381135 clip 27.82555619841206
total norm for a microbatch 28.05741733897944 clip 27.610894345129907
total norm for a microbatch 39.94061234688613 clip 25.415924898499647
total norm for a microbatch 27.000892905160097 clip 26.19088651681394
total norm for a microbatch 19.647702091834233 clip 28.27096874988638
cuda
Objective function 18.62 = squared loss an data 16.61 + 0.5*rho*h**2 1.338913 + alpha*h 0.000000 + L2reg 0.52 + L1reg 0.15 ; SHD = 44 ; DAG False
Proportion of microbatches that were clipped  0.8443793482240938
iteration 1 in inner loop, alpha 0.0 rho 1.0 h 1.6364061375175254
iteration 1 in outer loop, alpha = 1.6364061375175254, rho = 1.0, h = 1.6364061375175254
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 21.30 = squared loss an data 16.61 + 0.5*rho*h**2 1.338913 + alpha*h 2.677825 + L2reg 0.52 + L1reg 0.15 ; SHD = 44 ; DAG False
total norm for a microbatch 18.8029789216875 clip 1.3194894913699642
total norm for a microbatch 106.64955258716745 clip 1.6060315521341
total norm for a microbatch 39.518793684897844 clip 1.748764085087888
total norm for a microbatch 38.579952448456325 clip 6.867741606557044
total norm for a microbatch 107.47962959460155 clip 35.71972250254575
total norm for a microbatch 31.494168628520534 clip 35.71972250254575
total norm for a microbatch 23.128696149851653 clip 38.06280965822239
total norm for a microbatch 88.32896760571664 clip 36.725259482195014
total norm for a microbatch 106.8324687705858 clip 39.191234958403996
total norm for a microbatch 34.30641959032403 clip 37.94731987406364
cuda
Objective function 13.31 = squared loss an data 10.22 + 0.5*rho*h**2 0.459226 + alpha*h 1.568265 + L2reg 0.92 + L1reg 0.15 ; SHD = 37 ; DAG False
Proportion of microbatches that were clipped  0.8574850299401198
iteration 1 in inner loop, alpha 1.6364061375175254 rho 1.0 h 0.9583592282934994
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 17.44 = squared loss an data 10.22 + 0.5*rho*h**2 4.592262 + alpha*h 1.568265 + L2reg 0.92 + L1reg 0.15 ; SHD = 37 ; DAG False
total norm for a microbatch 65.9455145915286 clip 1.0938275778638167
total norm for a microbatch 37.03550426648351 clip 8.755562416941816
total norm for a microbatch 41.76596628074259 clip 27.496215014687795
total norm for a microbatch 54.043493279119474 clip 32.19565564862621
total norm for a microbatch 49.88955716579087 clip 49.79899479075252
total norm for a microbatch 61.28534249886752 clip 46.28277580921363
total norm for a microbatch 64.26046037480911 clip 48.23497958607405
total norm for a microbatch 43.43657444447201 clip 47.30055242181168
total norm for a microbatch 49.52364369621174 clip 50.16767717967147
cuda
Objective function 14.06 = squared loss an data 11.00 + 0.5*rho*h**2 1.037225 + alpha*h 0.745320 + L2reg 1.14 + L1reg 0.14 ; SHD = 36 ; DAG True
Proportion of microbatches that were clipped  0.8566678980421131
iteration 2 in inner loop, alpha 1.6364061375175254 rho 10.0 h 0.455461286589788
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 23.40 = squared loss an data 11.00 + 0.5*rho*h**2 10.372249 + alpha*h 0.745320 + L2reg 1.14 + L1reg 0.14 ; SHD = 36 ; DAG True
total norm for a microbatch 124.70856590873365 clip 1.2940664649871105
total norm for a microbatch 80.78883774673389 clip 2.82385577227029
total norm for a microbatch 56.9834912876481 clip 4.783423642450896
total norm for a microbatch 50.89387978609878 clip 55.33350980546974
total norm for a microbatch 150.1663846683146 clip 56.70839823439054
total norm for a microbatch 68.99343574220157 clip 55.00951692081136
total norm for a microbatch 50.1508123174274 clip 58.032208881689925
total norm for a microbatch 92.35500790607182 clip 58.77824978252091
total norm for a microbatch 81.36319927704805 clip 58.77824978252091
total norm for a microbatch 94.5033465401165 clip 51.91220865699663
total norm for a microbatch 53.75119169324418 clip 54.2182199015268
cuda
Objective function 15.01 = squared loss an data 12.14 + 0.5*rho*h**2 1.211838 + alpha*h 0.254758 + L2reg 1.26 + L1reg 0.13 ; SHD = 37 ; DAG True
Proportion of microbatches that were clipped  0.8669802219006271
iteration 3 in inner loop, alpha 1.6364061375175254 rho 100.0 h 0.1556816130016081
iteration 2 in outer loop, alpha = 17.204567437678335, rho = 100.0, h = 0.1556816130016081
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 17.43 = squared loss an data 12.14 + 0.5*rho*h**2 1.211838 + alpha*h 2.678435 + L2reg 1.26 + L1reg 0.13 ; SHD = 37 ; DAG True
total norm for a microbatch 86.16633667542305 clip 27.93811421563345
total norm for a microbatch 99.07284239198115 clip 33.29151195180398
total norm for a microbatch 84.02331746429249 clip 63.14326458249031
total norm for a microbatch 93.62001157479887 clip 59.91146875040838
total norm for a microbatch 64.08967395848775 clip 59.92281690715031
total norm for a microbatch 138.26649187894222 clip 58.02114676327914
total norm for a microbatch 72.55502794026523 clip 56.11066666892356
total norm for a microbatch 65.42373242677643 clip 57.96738385217157
total norm for a microbatch 69.99762864013275 clip 59.726392952357244
cuda
Objective function 16.28 = squared loss an data 12.69 + 0.5*rho*h**2 0.448702 + alpha*h 1.629814 + L2reg 1.37 + L1reg 0.14 ; SHD = 38 ; DAG True
Proportion of microbatches that were clipped  0.8665850673194615
iteration 1 in inner loop, alpha 17.204567437678335 rho 100.0 h 0.09473145134234429
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 20.32 = squared loss an data 12.69 + 0.5*rho*h**2 4.487024 + alpha*h 1.629814 + L2reg 1.37 + L1reg 0.14 ; SHD = 38 ; DAG True
total norm for a microbatch 129.1870227694546 clip 1.3033700060553304
total norm for a microbatch 44.36177767987798 clip 1.553622412985671
total norm for a microbatch 84.26055810023782 clip 1.7161882334462981
total norm for a microbatch 132.4863902458578 clip 2.0410998053092286
total norm for a microbatch 95.68545630046299 clip 8.767485646731284
total norm for a microbatch 50.41974174533037 clip 10.351037010167929
total norm for a microbatch 56.76334361912181 clip 31.77902601294091
total norm for a microbatch 54.88995601651835 clip 60.818365767848526
total norm for a microbatch 50.62687173856488 clip 63.754412896828796
total norm for a microbatch 92.10292101402158 clip 62.93255541785936
total norm for a microbatch 65.93738442236686 clip 62.9807881139904
total norm for a microbatch 77.55372716340368 clip 67.76370207714656
total norm for a microbatch 76.93635041314239 clip 60.072039462166714
cuda
Objective function 16.68 = squared loss an data 13.62 + 0.5*rho*h**2 0.782819 + alpha*h 0.680753 + L2reg 1.45 + L1reg 0.14 ; SHD = 50 ; DAG True
Proportion of microbatches that were clipped  0.8673839458413927
iteration 2 in inner loop, alpha 17.204567437678335 rho 1000.0 h 0.039568149892325266
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 23.72 = squared loss an data 13.62 + 0.5*rho*h**2 7.828192 + alpha*h 0.680753 + L2reg 1.45 + L1reg 0.14 ; SHD = 50 ; DAG True
total norm for a microbatch 189.43090325589145 clip 64.67751896019409
total norm for a microbatch 107.99745267186748 clip 64.67751896019409
total norm for a microbatch 112.7293732887553 clip 72.41906658780569
total norm for a microbatch 71.8076120901804 clip 73.3309575261956
total norm for a microbatch 55.123851751590955 clip 71.28125442902108
total norm for a microbatch 113.46871665744999 clip 66.90992438251513
total norm for a microbatch 124.9007756298006 clip 67.82605614241795
total norm for a microbatch 135.06868757099713 clip 71.67559382934719
total norm for a microbatch 81.55013917078533 clip 71.7460523817652
total norm for a microbatch 130.541011571009 clip 70.554990221284
cuda
Objective function 15.97 = squared loss an data 13.35 + 0.5*rho*h**2 0.709850 + alpha*h 0.204994 + L2reg 1.55 + L1reg 0.15 ; SHD = 43 ; DAG True
Proportion of microbatches that were clipped  0.8684944801649884
iteration 3 in inner loop, alpha 17.204567437678335 rho 10000.0 h 0.011915118584093776
iteration 3 in outer loop, alpha = 136.35575327861608, rho = 10000.0, h = 0.011915118584093776
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 17.39 = squared loss an data 13.35 + 0.5*rho*h**2 0.709850 + alpha*h 1.624695 + L2reg 1.55 + L1reg 0.15 ; SHD = 43 ; DAG True
total norm for a microbatch 237.48899385130412 clip 1.5777083448755445
total norm for a microbatch 100.44532089738134 clip 78.94487673577366
total norm for a microbatch 98.52593863096033 clip 81.50286449204033
total norm for a microbatch 110.71370163922651 clip 77.48848673153826
total norm for a microbatch 131.79687558638278 clip 80.10595079066476
total norm for a microbatch 72.57260537624089 clip 75.56051938448391
total norm for a microbatch 63.46896044371506 clip 76.34749855552752
total norm for a microbatch 88.89416181728939 clip 73.69482317697705
total norm for a microbatch 134.0677525550046 clip 74.82228884991459
total norm for a microbatch 128.06887161687055 clip 73.83767811124117
total norm for a microbatch 114.35875717502341 clip 69.64459450476497
cuda
Objective function 16.10 = squared loss an data 12.94 + 0.5*rho*h**2 0.314304 + alpha*h 1.081094 + L2reg 1.61 + L1reg 0.16 ; SHD = 45 ; DAG True
Proportion of microbatches that were clipped  0.8708848366173051
iteration 1 in inner loop, alpha 136.35575327861608 rho 10000.0 h 0.007928479737993044
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 18.93 = squared loss an data 12.94 + 0.5*rho*h**2 3.143040 + alpha*h 1.081094 + L2reg 1.61 + L1reg 0.16 ; SHD = 45 ; DAG True
total norm for a microbatch 159.49153389545657 clip 1.0
total norm for a microbatch 401.98657650540525 clip 2.764708645463334
total norm for a microbatch 181.6503977071517 clip 143.943294628453
total norm for a microbatch 132.74110939380762 clip 71.69837236099909
total norm for a microbatch 85.73858777550099 clip 70.25962893052599
total norm for a microbatch 74.69022873993717 clip 73.00380453320385
cuda
Objective function 15.55 = squared loss an data 13.03 + 0.5*rho*h**2 0.363839 + alpha*h 0.367827 + L2reg 1.63 + L1reg 0.16 ; SHD = 42 ; DAG True
Proportion of microbatches that were clipped  0.8710300688322666
iteration 2 in inner loop, alpha 136.35575327861608 rho 100000.0 h 0.0026975523128367485
iteration 4 in outer loop, alpha = 406.1109845622909, rho = 100000.0, h = 0.0026975523128367485
cuda
noise_multiplier  0.5  noise_multiplier_b  3.75  noise_multiplier_delta  0.5011148285857957
cuda
Objective function 16.28 = squared loss an data 13.03 + 0.5*rho*h**2 0.363839 + alpha*h 1.095506 + L2reg 1.63 + L1reg 0.16 ; SHD = 42 ; DAG True
total norm for a microbatch 8977.16465048819 clip 1.1774026852066053
total norm for a microbatch 442.9242036969075 clip 3.1927171618395507
total norm for a microbatch 214.43188089221795 clip 25.81189907771033
total norm for a microbatch 225.4835092325734 clip 98.08384993700066
cuda
Objective function 16.44 = squared loss an data 13.55 + 0.5*rho*h**2 0.210447 + alpha*h 0.833164 + L2reg 1.69 + L1reg 0.16 ; SHD = 41 ; DAG True
Proportion of microbatches that were clipped  0.8766336875534384
iteration 1 in inner loop, alpha 406.1109845622909 rho 100000.0 h 0.0020515683239121785
iteration 5 in outer loop, alpha = 2457.6793084744695, rho = 1000000.0, h = 0.0020515683239121785
Threshold 0.3
[[0.003 0.013 0.33  0.168 0.598 0.006 0.015 0.002 0.015 1.257 0.098 0.23
  0.011 0.003 0.106]
 [0.266 0.004 0.283 0.574 0.649 0.069 0.039 0.011 0.013 0.947 0.427 0.892
  0.017 0.003 0.469]
 [0.01  0.017 0.002 0.141 0.379 0.015 0.006 0.005 0.015 0.052 0.071 0.078
  0.02  0.003 0.031]
 [0.028 0.006 0.027 0.003 0.151 0.017 0.002 0.003 0.024 0.06  0.022 0.089
  0.002 0.002 0.047]
 [0.002 0.002 0.011 0.038 0.005 0.002 0.002 0.001 0.002 0.002 0.002 0.009
  0.001 0.001 0.012]
 [0.441 0.056 0.333 0.197 0.588 0.003 0.012 0.007 0.074 0.983 0.145 0.843
  0.037 0.009 0.09 ]
 [0.328 0.098 0.606 1.059 0.727 0.347 0.004 0.024 0.097 0.269 1.015 0.367
  0.026 0.011 0.515]
 [1.423 0.401 0.508 1.13  0.64  0.534 0.225 0.003 0.407 1.216 0.292 0.84
  0.06  0.012 0.4  ]
 [0.22  0.144 0.193 0.238 0.343 0.089 0.067 0.012 0.003 1.014 0.83  0.711
  0.012 0.001 0.13 ]
 [0.004 0.004 0.064 0.061 1.824 0.003 0.007 0.002 0.005 0.005 0.027 0.067
  0.003 0.001 0.049]
 [0.079 0.015 0.072 0.186 2.681 0.029 0.004 0.011 0.007 0.222 0.006 0.107
  0.003 0.002 0.149]
 [0.026 0.003 0.081 0.068 0.24  0.005 0.009 0.002 0.005 0.07  0.058 0.004
  0.006 0.001 0.064]
 [0.271 0.265 0.195 1.333 2.256 0.141 0.119 0.085 0.363 0.751 1.226 0.522
  0.003 0.015 0.456]
 [0.751 1.262 0.867 0.94  0.986 0.461 0.395 0.293 2.971 1.289 0.884 1.386
  0.265 0.004 0.758]
 [0.028 0.009 0.156 0.114 0.281 0.06  0.008 0.012 0.027 0.092 0.032 0.076
  0.006 0.003 0.003]]
[[0.    0.    0.33  0.    0.598 0.    0.    0.    0.    1.257 0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.574 0.649 0.    0.    0.    0.    0.947 0.427 0.892
  0.    0.    0.469]
 [0.    0.    0.    0.    0.379 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.441 0.    0.333 0.    0.588 0.    0.    0.    0.    0.983 0.    0.843
  0.    0.    0.   ]
 [0.328 0.    0.606 1.059 0.727 0.347 0.    0.    0.    0.    1.015 0.367
  0.    0.    0.515]
 [1.423 0.401 0.508 1.13  0.64  0.534 0.    0.    0.407 1.216 0.    0.84
  0.    0.    0.4  ]
 [0.    0.    0.    0.    0.343 0.    0.    0.    0.    1.014 0.83  0.711
  0.    0.    0.   ]
 [0.    0.    0.    0.    1.824 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    2.681 0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]
 [0.    0.    0.    1.333 2.256 0.    0.    0.    0.363 0.751 1.226 0.522
  0.    0.    0.456]
 [0.751 1.262 0.867 0.94  0.986 0.461 0.395 0.    2.971 1.289 0.884 1.386
  0.    0.    0.758]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.    0.   ]]
{'fdr': 0.603448275862069, 'tpr': 0.7666666666666667, 'fpr': 0.4666666666666667, 'f1': 0.5227272727272728, 'shd': 41, 'npred': 58, 'ntrue': 30}
[1.346e-02 3.300e-01 1.684e-01 5.980e-01 6.429e-03 1.534e-02 1.731e-03
 1.536e-02 1.257e+00 9.764e-02 2.297e-01 1.131e-02 3.371e-03 1.058e-01
 2.656e-01 2.833e-01 5.738e-01 6.486e-01 6.945e-02 3.870e-02 1.118e-02
 1.259e-02 9.473e-01 4.274e-01 8.923e-01 1.678e-02 2.528e-03 4.687e-01
 1.003e-02 1.681e-02 1.406e-01 3.788e-01 1.504e-02 6.076e-03 5.293e-03
 1.545e-02 5.158e-02 7.107e-02 7.833e-02 2.027e-02 2.843e-03 3.091e-02
 2.771e-02 5.781e-03 2.736e-02 1.514e-01 1.675e-02 2.463e-03 2.737e-03
 2.365e-02 5.983e-02 2.176e-02 8.882e-02 2.009e-03 2.399e-03 4.671e-02
 2.187e-03 1.899e-03 1.071e-02 3.829e-02 2.001e-03 1.832e-03 1.485e-03
 1.646e-03 2.252e-03 1.574e-03 9.128e-03 8.924e-04 5.529e-04 1.242e-02
 4.407e-01 5.645e-02 3.327e-01 1.967e-01 5.877e-01 1.214e-02 7.039e-03
 7.365e-02 9.832e-01 1.452e-01 8.427e-01 3.659e-02 8.942e-03 8.970e-02
 3.275e-01 9.818e-02 6.056e-01 1.059e+00 7.269e-01 3.467e-01 2.391e-02
 9.726e-02 2.693e-01 1.015e+00 3.673e-01 2.570e-02 1.146e-02 5.149e-01
 1.423e+00 4.014e-01 5.077e-01 1.130e+00 6.404e-01 5.342e-01 2.251e-01
 4.066e-01 1.216e+00 2.917e-01 8.398e-01 6.021e-02 1.240e-02 4.000e-01
 2.198e-01 1.440e-01 1.932e-01 2.376e-01 3.428e-01 8.866e-02 6.715e-02
 1.157e-02 1.014e+00 8.299e-01 7.112e-01 1.181e-02 1.090e-03 1.302e-01
 4.136e-03 4.001e-03 6.376e-02 6.054e-02 1.824e+00 2.951e-03 7.230e-03
 1.515e-03 4.658e-03 2.698e-02 6.691e-02 3.447e-03 9.909e-04 4.940e-02
 7.903e-02 1.540e-02 7.212e-02 1.863e-01 2.681e+00 2.921e-02 3.781e-03
 1.096e-02 6.524e-03 2.224e-01 1.069e-01 2.979e-03 2.045e-03 1.486e-01
 2.551e-02 3.011e-03 8.090e-02 6.786e-02 2.400e-01 4.820e-03 8.736e-03
 2.055e-03 5.235e-03 6.978e-02 5.791e-02 5.594e-03 1.484e-03 6.351e-02
 2.705e-01 2.646e-01 1.952e-01 1.333e+00 2.256e+00 1.415e-01 1.188e-01
 8.480e-02 3.629e-01 7.507e-01 1.226e+00 5.223e-01 1.474e-02 4.560e-01
 7.506e-01 1.262e+00 8.666e-01 9.396e-01 9.864e-01 4.612e-01 3.952e-01
 2.926e-01 2.971e+00 1.289e+00 8.841e-01 1.386e+00 2.655e-01 7.582e-01
 2.768e-02 9.202e-03 1.558e-01 1.138e-01 2.809e-01 6.039e-02 8.099e-03
 1.193e-02 2.734e-02 9.169e-02 3.175e-02 7.572e-02 6.289e-03 2.940e-03]
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
aucroc, aucpr (0.8737037037037036, 0.6334152184607642)
Iterations 1110
Achieves (23.82233853610087, 1e-05)-DP
